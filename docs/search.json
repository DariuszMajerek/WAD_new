[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wielowymiarowa analiza danych",
    "section": "",
    "text": "Wstęp\nWielowymiarowa analiza danych stanowi jeden z filarów współczesnej statystyki i eksploracji danych, oferując metody pozwalające zrozumieć strukturę i zależności w zbiorach danych, w których każda obserwacja jest opisana wieloma zmiennymi jednocześnie. W dobie powszechnego dostępu do danych oraz rosnącego zapotrzebowania na ich zaawansowaną analizę, umiejętność stosowania metod wielowymiarowych staje się nieodzowna zarówno w badaniach naukowych, jak i w analizie danych stosowanej w przemyśle, finansach, biologii, medycynie czy naukach społecznych.\nNiniejsza książka została opracowana z myślą o dwóch kierunkach kształcenia akademickiego: matematyce oraz inżynierii i analizie danych. Jej celem jest zapewnienie solidnych podstaw teoretycznych oraz praktycznych umiejętności niezbędnych do stosowania metod wielowymiarowych w rzeczywistych problemach badawczych i aplikacyjnych. Zakres tematyczny książki został dobrany tak, aby uwzględniać zarówno klasyczne metody statystyczne, jak i techniki wykorzystywane we wspsółczesnej analizie danych.\nW pierwszej części książki omówione zostaną testy wielowymiarowe, które stanowią rozszerzenie klasycznych metod statystycznych na przypadki, w których każda obserwacja opisana jest wieloma zmiennymi. Szczególna uwaga zostanie poświęcona testowi Hotellinga T², będącemu odpowiednikiem testu t dla wielu zmiennych, oraz analizie wariancji dla wielu zmiennych (MANOVA), pozwalającej na badanie różnic między grupami z uwzględnieniem współzależności zmiennych. Celem tej części będzie zrozumienie podstaw inferencji w przestrzeni wielowymiarowej i interpretacji wyników testów z uwzględnieniem macierzy kowariancji.\nNastępnie przedstawiona zostanie analiza kanoniczna, która służy do badania zależności pomiędzy dwoma zestawami zmiennych. Czytelnik pozna konstrukcję zmiennych kanonicznych, sposoby ich interpretacji oraz znaczenie wag i korelacji kanonicznych. Analiza ta ma kluczowe znaczenie wszędzie tam, gdzie celem jest znalezienie skorelowanych struktur w dwóch grupach cech, np. w badaniach biologicznych, społecznych lub psychometrycznych.\nKolejna część książki będzie poświęcona analizie czynnikowej (FA), która umożliwia modelowanie współzmienności zestawu zmiennych za pomocą mniejszej liczby zmiennych ukrytych, zwanych czynnikami. Przedstawione zostaną metody estymacji, kryteria wyboru liczby czynników oraz techniki rotacji, które służą lepszej interpretacji wyników. Analiza czynnikowa jest często stosowana w badaniach ankietowych i psychometrycznych, ale znajduje również zastosowanie w analizie danych ekonomicznych i marketingowych.\nW dalszej kolejności wprowadzony zostanie model ścieżkowy oraz jego uogólnienie w postaci modeli równań strukturalnych (SEM). Modele te pozwalają na modelowanie zarówno obserwowalnych, jak i ukrytych zmiennych oraz relacji przyczynowych pomiędzy nimi. Czytelnik pozna strukturę modelu ścieżkowego, pojęcie identyfikowalności, miary dopasowania oraz techniki estymacji parametrów. Modele SEM są obecnie szeroko stosowane w naukach społecznych, biologii, psychologii i ekonomii.\nNastępnie omówione zostaną metody redukcji wymiarowości, których celem jest uproszczenie reprezentacji danych bez utraty istotnej informacji. Kluczową techniką będzie analiza składowych głównych (PCA), która pozwala na znalezienie nowych osi zmienności w danych. Kolejno zaprezentowana zostanie analiza niezależnych składowych (ICA), która poszukuje składników statystycznie niezależnych, co jest szczególnie użyteczne w analizie sygnałów. Obie metody znajdą zastosowanie zarówno w przygotowaniu danych, jak i w ich eksploracji.\nKolejna część książki poświęcona będzie metodom skalowania wielowymiarowego (Multidimensional Scaling, MDS), które umożliwiają odwzorowanie relacji odległościowych pomiędzy obiektami w przestrzeni o mniejszym wymiarze. Wariant metric zakłada zachowanie rzeczywistych wartości odległości, natomiast non-metric koncentruje się na porządku dystansów. Metody te pozwalają uzyskać intuicyjne wizualizacje struktur danych, szczególnie przydatne w psychologii, socjologii czy analizie rynku.\nW uzupełnieniu do klasycznych technik przedstawione zostaną nieliniowe metody redukcji wymiarowości, takie jak t-distributed Stochastic Neighbor Embedding (t-SNE) oraz Uniform Manifold Approximation and Projection (UMAP). Obie techniki pozwalają na odwzorowanie skomplikowanych struktur danych w przestrzeniach dwu- lub trójwymiarowych, zachowując lokalne sąsiedztwa. Choć są to metody przede wszystkim eksploracyjne i wizualizacyjne, ich wartość w analizie dużych zbiorów danych jest trudna do przecenienia.\nNastępnie przedstawiona zostanie analiza skupień, której celem jest odkrywanie naturalnych grup w zbiorze danych. Omówione zostaną zarówno metody hierarchiczne, jak i niehierarchiczne, w tym popularna metoda k-średnich. Poruszona zostanie problematyka doboru liczby skupień oraz oceny stabilności i jakości otrzymanych rozwiązań. Analiza skupień znajduje zastosowanie w segmentacji rynku, biologii molekularnej, diagnostyce medycznej i wielu innych dziedzinach.\nKolejna część książki poświęcona będzie analizie korespondencji, stosowanej do eksploracji związków pomiędzy zmiennymi jakościowymi przedstawionymi w postaci tablicy kontyngencji. Przedstawiona zostanie zarówno analiza korespondencji prosta (dla dwóch zmiennych), jak i złożona (dla więcej niż dwóch). Omówione zostaną interpretacja map percepcyjnych, odwzorowanie profili oraz związki z metodami takimi jak PCA czy MDS.\nOstatni rozdział poświęcony będzie analizie log-liniowej, która umożliwia modelowanie częstości w tablicach wielodzielczych na podstawie interakcji pomiędzy zmiennymi kategorycznymi. Zostaną zaprezentowane modele pełne i uproszczone, zasady testowania złożoności modeli oraz interpretacji parametrów. Analiza log-liniowa jest szczególnie przydatna przy badaniu wielowymiarowych zależności między zmiennymi kategorycznymi w badaniach społecznych, medycznych oraz w analizie zachowań konsumenckich.\nWszystkie metody zostaną zilustrowane przykładami praktycznymi, realizowanymi w języku R. Pozwoli to Czytelnikowi nie tylko zrozumieć teoretyczne podstawy omawianych technik, ale także nabyć umiejętność ich stosowania w praktyce analitycznej.",
    "crumbs": [
      "Wstęp"
    ]
  },
  {
    "objectID": "multi_tests.html",
    "href": "multi_tests.html",
    "title": "Testy wielowymiarowe",
    "section": "",
    "text": "Test Hotellinga dla znanej macierzy kowariancji\nW tradycyjnej analizie statystycznej często koncentrujemy się na porównywaniu grup ze względu na jedną zmienną – np. porównujemy średni wzrost kobiet i mężczyzn, wykorzystując test t-Studenta. Jednak w rzeczywistości badawczej rzadko interesuje nas tylko jedna cecha. Przykładowo, porównując grupy pacjentów, możemy jednocześnie rozważać poziom ciśnienia, cholesterolu i BMI. Albo, analizując dane socjologiczne, chcemy porównać grupy pod względem dochodów, wykształcenia i poziomu zadowolenia z życia.\nUżycie wielu testów jednowymiarowych wydaje się kuszące – testujemy każdą zmienną osobno. Jednak prowadzi to do trzech istotnych problemów:\n\\[\n\\mathbb{P}(\\text{co najmniej jeden błąd I rodzaju}) = 1 - (1 - \\alpha)^p = 1 - 0.95^{10} \\approx 0.40\n\\]\nPowyższe problemy uzasadniają potrzebę stosowania testów wielowymiarowych – uwzględniających strukturę współzmienności między cechami oraz pozwalających na testowanie hipotez dotyczących całych wektorów średnich.\nRozważmy próbkę \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\ldots, \\boldsymbol{y}_n \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), gdzie \\(\\boldsymbol{\\Sigma}\\) jest znana. Oznacza to, że mamy do czynienia z ciągiem niezależnych losowych wektorów \\(\\boldsymbol{y}_i\\), z których każdy ma ten sam wielowymiarowy rozkład normalny o wymiarze \\(p\\), średniej \\(\\boldsymbol{\\mu}\\) i macierzy kowariancji \\(\\boldsymbol{\\Sigma}\\). Każdy wektor \\(\\boldsymbol{y}_i\\) można interpretować jako punkt w przestrzeni \\(\\mathbb{R}^p\\), opisujący \\(p\\) cech (zmiennych) dla jednej obserwacji. Formalnie jest to wektor kolumnowy postaci \\(\\boldsymbol{y}_i = [y_{i1}, y_{i2}, \\ldots, y_{ip}]^\\top\\), gdzie indeks \\(i\\) numeruje jednostki (np. osoby, obiekty pomiaru), a indeksy \\(j = 1, \\ldots, p\\) odpowiadają poszczególnym zmiennym. Wektor ten traktowany jest jako zmienna losowa, ponieważ jego wartości są wynikiem losowego procesu generującego dane. Rozkład \\(\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) jest wielowymiarową wersją rozkładu normalnego. Opisuje on sytuację, w której każda kombinacja liniowa zmiennych losowych w \\(\\boldsymbol{y}_i\\) również ma rozkład normalny, a funkcja gęstości prawdopodobieństwa ma postać zależną od wartości wektora średnich oraz struktury kowariancji. Jest to fundamentalne założenie w klasycznej analizie statystycznej, pozwalające na stosowanie wielu narzędzi statystycznych.\nParametr \\(\\boldsymbol{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_p]^\\top\\) to wektor wartości oczekiwanych każdej z analizowanych zmiennych. Oznacza on przeciętny poziom zmiennej \\(y_{ij}\\) w populacji dla każdej cechy \\(j\\). Jest to parametr istotny z punktu widzenia testowania hipotez, ponieważ wiele testów statystycznych dotyczy właśnie równości lub różnic wektorów średnich między grupami.\nZ kolei macierz \\(\\boldsymbol{\\Sigma}\\) to dodatnio określona, symetryczna macierz kowariancji o wymiarach \\(p \\times p\\). Jej elementy \\(\\sigma_{jj}\\) opisują wariancje poszczególnych zmiennych, natomiast elementy poza główną przekątną \\(\\sigma_{jk}\\) (dla \\(j \\ne k\\)) opisują kowariancje, czyli współzmienność pomiędzy zmiennymi \\(y_{ij}\\) i \\(y_{ik}\\). W analizie wielowymiarowej uwzględnienie tych zależności między cechami jest kluczowe, ponieważ pozwala lepiej zrozumieć strukturę danych i dokonywać bardziej trafnych wniosków statystycznych.\nO próbie \\(\\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n\\) zakładamy, że wszystkie obserwacje są niezależne oraz pochodzą z tego samego rozkładu. Oznacza to, że mamy do czynienia z próbą losową, niezależną o identycznych rozkładach (i.i.d.), co jest podstawowym założeniem wielu testów i metod estymacji. Całą próbkę można przedstawić jako macierz danych o wymiarach \\(n \\times p\\), w której wiersze odpowiadają jednostkom, a kolumny cechom.\nW przypadku, gdy macierz kowariancji \\(\\boldsymbol{\\Sigma}\\) jest znana, możemy zastosować uproszczone wersje testów statystycznych, takie jak klasyczny test Hotellinga \\(T^2\\) dla jednej próby. Jest to jednak sytuacja czysto teoretyczna, ponieważ w praktyce \\(\\boldsymbol{\\Sigma}\\) musi być zazwyczaj estymowana na podstawie danych. Pomimo tego, przypadek znanej macierzy jest użyteczny do budowania intuicji, zrozumienia ról poszczególnych parametrów i wyprowadzania własności statystyk testowych.\nChcemy przetestować hipotezę:\n\\[\nH_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu} \\neq \\boldsymbol{\\mu}_0\n\\]\nStatystyka testowa opiera się na uogólnionej odległości Mahalanobisa:\n\\[\nT^2 = n (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Sigma}^{-1} (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)\n\\]\nPod warunkiem spełnienia \\(H_0\\), statystyka \\(T^2 \\sim \\chi^2_p\\). Zatem możemy porównać wartość \\(T^2\\) z odpowiednim kwantylem rozkładu chi-kwadrat.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#założenia",
    "href": "multi_tests.html#założenia",
    "title": "Testy wielowymiarowe",
    "section": "Założenia",
    "text": "Założenia\n\nPróby są niezależne;\nObserwacje w każdej grupie pochodzą z rozkładu wielowymiarowego normalnego;\n\nMacierze kowariancji są równe \\(\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}\\). Jest to kluczowe założenie umożliwiające zbudowanie wspólnego estymatora kowariancji i zastosowanie rozkładu \\(T^2\\) Hotellinga. Choć może być ono naruszone w praktyce, to dla dużych prób test zachowuje swoje właściwości asymptotyczne (Rencher 1998).\n\nWektory średnich z próby wyrażamy jako:\n\\[\n\\bar{\\boldsymbol{y}}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} \\boldsymbol{y}_{1,i}, \\quad\n\\bar{\\boldsymbol{y}}_2 = \\frac{1}{n_2} \\sum_{i=1}^{n_2} \\boldsymbol{y}_{2,i}\n\\]\nNieobciążonym estymatorem macierzy kowariancji (\\(\\boldsymbol{\\Sigma}\\)) jest tzw. połączony estymator kowariancji:\n\\[\n\\mathbf{S} =\n\\frac{(n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2}{n_1 + n_2 - 2}\n\\] gdzie \\[\n\\mathbf{S}_1 = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top\n\\]\n\\[\n\\mathbf{S}_2 = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top\n\\]\nZatem:\n\\[\n\\mathbf{S} = \\frac{\\mathbf{W}_1 + \\mathbf{W}_2}{n_1 + n_2 - 2}\n\\] gdzie: \\[\n\\mathbf{W}_1 = \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top = (n_1 - 1)\\mathbf{S}_1\n\\]\n\\[\n\\mathbf{W}_2 = \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top = (n_2 - 1)\\mathbf{S}_2\n\\] Statystyka testowa Hotellinga wówczas ma postać:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)\n\\]\nA gdy \\(H_0\\) jest prawdziwa, to po przekształceniu: \\[\nF = \\frac{(n_1 + n_2 - p - 1)}{p(n_1 + n_2 - 2)} T^2 \\sim F_{p, n_1 + n_2 - p - 1}\n\\]\nAlternatywnie, można zapisać, że: \\[\nT^2 \\sim \\frac{p(n_1 + n_2 - 2)}{n_1 + n_2 - p - 1} F_{p, n_1 + n_2 - p - 1}\n\\]\nHipotezę zerową \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) odrzucamy na poziomie istotności \\(\\alpha\\), jeśli: \\[\nT^2 &gt; T^2_{\\alpha, p, n_1 + n_2 - 2}\n\\] lub równoważnie: \\[\nF &gt; F_{\\alpha, p, n_1 + n_2 - p - 1}\n\\]\nAby test był możliwy do przeprowadzenia, konieczne jest, aby \\(n_1 + n_2 - 2 &gt; p\\), czyli liczba stopni swobody w estymacji wspólnej kowariancji była większa niż wymiar przestrzeni cech.\n\n\n\n\n\n\nAdnotacja\n\n\n\nW praktyce istotne jest, aby przed zastosowaniem testu \\(T^2\\) Hotellinga dla dwóch prób zweryfikować założenie o równości macierzy kowariancji — np. za pomocą testu Boxa. Test M Boxa (ang. Box’s M test) służy do statystycznej weryfikacji hipotezy równości macierzy kowariancji w wielu grupach.\nZałóżmy, że mamy \\(G\\) niezależnych prób z wielowymiarowego rozkładu normalnego:\n\\[\n\\boldsymbol{y}_{g} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_g, \\boldsymbol{\\Sigma}_g), \\quad g = 1, \\ldots, G\n\\] Testujemy hipotezę: \\[\nH_0: \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\ldots = \\boldsymbol{\\Sigma}_G = \\boldsymbol{\\Sigma}\n\\] przeciwko alternatywie: \\[\nH_1: \\exists\\, g, h: \\boldsymbol{\\Sigma}_g \\ne \\boldsymbol{\\Sigma}_h\n\\] Niech\n\n\n\\(\\mathbf{S}_g\\) – macierz kowariancji w grupie \\(g\\),\n\n\\(n_g\\) – liczba obserwacji w grupie \\(g\\),\n\n\\(\\mathbf{S}_p\\) – połączony estymator macierzy kowariancji:\n\n\\[\n\\mathbf{S}_p = \\frac{1}{N - G} \\sum_{g=1}^G (n_g - 1)\\mathbf{S}_g\n\\] gdzie \\(N = \\sum_{g=1}^G n_g\\) – łączna liczba obserwacji. Wówczas, statystyka testowa Boxa ma postać: \\[\nM = (N - G) \\cdot \\ln|\\mathbf{S}_p| - \\sum_{g=1}^G (n_g - 1) \\cdot \\ln|\\mathbf{S}_g|\n\\] Poprawka na skończoną próbkę prowadzi do statystyki: \\[\nC = \\left(1 - c\\right) \\cdot M\n\\] gdzie: \\[\nc = \\frac{1}{3(p + 1)(G - 1)} \\left[ \\sum_{g=1}^G \\frac{1}{n_g - 1} - \\frac{1}{N - G} \\right]\n\\] Statystyka \\(C\\) jest asymptotycznie zbierzna do rozkładu \\(\\chi^2\\left(\\frac{p}{2}(p + 1)(G - 1)\\right)\\) liczbą stopni swobody.\nHipotezę \\(H_0\\) o równości macierzy kowariancji odrzuca się, jeśli: \\[\nC &gt; \\chi^2_{1 - \\alpha, df}\n\\] lub gdy \\(p\\) testu jest mniejsza od poziomu istotności \\(\\alpha\\).\nUwagi praktyczne\n\nTest M Boxa jest wrażliwy na odchylenia od normalności – jeśli dane nie są zbliżone do normalnych, test może dawać mylące wyniki.\nW dużych próbach nawet drobne różnice między macierzami kowariancji mogą prowadzić do odrzucenia \\(H_0\\), choć nie mają istotnego wpływu praktycznego.\nW małych próbach test może być niestabilny – zaleca się ostrożność przy interpretacji.\n\n\n\n\nPrzykład 4.1 (Porównanie dwóch grup za pomocą testu \\(T^2\\) Hotellinga)  \n\nKodlibrary(MASS)\n# Parametry symulacji\nset.seed(44)\np &lt;- 2          # liczba zmiennych\nn1 &lt;- 30        # liczba obserwacji w grupie 1\nn2 &lt;- 35        # liczba obserwacji w grupie 2\n\n# Parametry rozkładu\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1, 1)\nSigma &lt;- matrix(c(1, 0.5,\n                  0.5, 1), nrow = 2)\n\n# Generowanie danych\nY1 &lt;- mvrnorm(n1, mu = mu1, Sigma = Sigma)\nY2 &lt;- mvrnorm(n2, mu = mu2, Sigma = Sigma)\n\n# Średnie z próby\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymatory kowariancji\nS1 &lt;- cov(Y1)\nS2 &lt;- cov(Y2)\n\n# Wspólna kowariancja (połączona)\nSp &lt;- ((n1 - 1)*S1 + (n2 - 1)*S2) / (n1 + n2 - 2)\n\n# Statystyka testowa Hotellinga T^2\ndiff_mean &lt;- y1_bar - y2_bar\nT2 &lt;- (n1 * n2) / (n1 + n2) * t(diff_mean) %*% solve(Sp) %*% diff_mean\nT2 &lt;- as.numeric(T2)\n\n# Przekształcenie do F\ndf1 &lt;- p\ndf2 &lt;- n1 + n2 - p - 1\nF_stat &lt;- (df2 / (df1 * (n1 + n2 - 2))) * T2\n\n# Wartość krytyczna\nalpha &lt;- 0.05\nF_crit &lt;- qf(1 - alpha, df1, df2)\n\n# p-wartość\np_val &lt;- 1 - pf(F_stat, df1, df2)\n\n# Wynik testu\nsprintf(\"Statystyka T² = %.3f\", T2)\n\n[1] \"Statystyka T² = 17.340\"\n\nKodsprintf(\"Statystyka F = %.3f\", F_stat)\n\n[1] \"Statystyka F = 8.532\"\n\nKodsprintf(\"Wartość krytyczna F = %.3f\", F_crit)\n\n[1] \"Wartość krytyczna F = 3.145\"\n\nKodsprintf(\"p-value = %e\", p_val)\n\n[1] \"p-value = 5.330310e-04\"\n\nKod# Wizualizacja \ndf1 &lt;- as.data.frame(Y1) %&gt;%\n  mutate(grupa = \"Grupa 1\")\n\ndf2 &lt;- as.data.frame(Y2) %&gt;%\n  mutate(grupa = \"Grupa 2\")\n\ndf_all &lt;- bind_rows(df1, df2)\ncolnames(df_all)[1:2] &lt;- c(\"X1\", \"X2\")\n\n# Ramka danych ze średnimi\nmeans &lt;- data.frame(\n  X1 = c(y1_bar[1], y2_bar[1]),\n  X2 = c(y1_bar[2], y2_bar[2]),\n  grupa = c(\"Grupa 1\", \"Grupa 2\")\n)\n\n# Wykres\nggplot(df_all, aes(x = X1, y = X2, color = grupa, shape = grupa)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_point(data = means, aes(x = X1, y = X2),\n             shape = c(1, 2), size = 5, stroke = 1.2, show.legend = FALSE) +\n  scale_shape_manual(values = c(16, 17)) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  coord_equal() +\n  theme_minimal() +\n  labs(title = \"Porównanie dwóch grup\",\n       x = \"X1\", y = \"X2\", color = \"Grupa\", shape = \"Grupa\")\n\n\n\n\n\n\n\n\nKod# Alternatywnie, użycie gotowej funkcji z pakietu ICSNP\nlibrary(ICSNP)\nHotellingsT2(rbind(Y1, Y2)~factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Hotelling's two sample T2-test\n\ndata:  rbind(Y1, Y2) by factor(c(rep(1, n1), rep(2, n2)))\nT.2 = 8.5321, df1 = 2, df2 = 62, p-value = 0.000533\nalternative hypothesis: true location difference is not equal to c(0,0)\n\n\nW analizowanym przykładzie zdefiniowaliśmy macierze kowariancji identycznie ale w rzeczywistości należałoby testować hipotezę o równości macierzy kowariancji. Tylko dla celów ćwiczeniowych pokażę jak to zrobić.\n\nKod# Test Boxa na równość macierzy kowariancji\nlibrary(biotools)\nboxM(rbind(Y1, Y2), factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  rbind(Y1, Y2)\nChi-Sq (approx.) = 2.1261, df = 3, p-value = 0.5466\n\nKod# lub z wykorzystaniem pakietu rstatix\nlibrary(rstatix)\nbox_m(df_all[,-3], df_all[,3])\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      2.13   0.547         3 Box's M-test for Homogeneity of Covariance Matric…\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nW sytuacji, gdy założenie o równości macierzy kowariancji jest naruszone, można stosować alternatywne metody, takie jak:\n\nTesty permutacyjne (Hotelling::hotelling.test(Y~Group, perm = TRUE, B = 5000)).\nUogólniony test Hotellinga - test Jamesa (ang. James’s second-order test) lub czasami nazywany również testem Welch-type Hotelling test (Hotelling::hotelling.test(Y~Group, var.equal = FALSE)).\nW przypadku danych charakteryzujących się dużą liczbą zmiennych w stosunku do liczby obserwacji, można rozważyć użycie estymatora Jamesa-Steina do stabilizaji macierzy kowariancji (Hotelling::hotelling.test(Y~Group, shrinkage = TRUE)).\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nOdrzucenie hipotezy zerowej \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) w teście Hotellinga \\(T^2\\) oznacza, że mamy statystycznie istotny dowód na to, iż rozkłady średnich wektorów dwóch populacji wielowymiarowych różnią się, biorąc pod uwagę współzmienność między zmiennymi. W kontekście zastosowań praktycznych, oznacza to, że przynajmniej jedna zmienna (lub kombinacja zmiennych) odróżnia grupy, nawet jeśli nie da się tego wykazać za pomocą testów jednowymiarowych.\nW sytuacji, gdy mamy dane wielowymiarowe \\(\\boldsymbol{y}_{gi} \\in \\mathbb{R}^p\\) z dwóch niezależnych grup (\\(g = 1,2\\)), testujemy:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu}_1 \\neq \\boldsymbol{\\mu}_2.\n\\]\nOdrzucenie \\(H_0\\) sugeruje, że istnieje różnica pomiędzy średnimi wektorami, ale nie musi oznaczać, że którakolwiek ze średnich poszczególnych zmiennych \\(\\mu_{1j}, \\mu_{2j}\\) różni się istotnie — zwłaszcza jeśli uwzględnimy korelacje między zmiennymi.\nTesty jednowymiarowe ignorują te współzależności, dlatego mogą nie wykazać istotnych różnic, mimo że ogólny profil wielowymiarowy się różni. Innymi słowy, może nie istnieć żadna istotna różnica w poszczególnych zmiennych, ale pewna kombinacja liniowa tych zmiennych pozwala na rozróżnienie grup.\nRozważmy kombinację liniową:\n\\[\nz = \\boldsymbol{a}^\\top \\boldsymbol{y},\n\\]\ngdzie \\(\\boldsymbol{a} \\in \\mathbb{R}^p\\) to niezerowy wektor wag. Wówczas \\(z\\) jest jednowymiarową zmienną losową będącą projekcją obserwacji \\(\\boldsymbol{y}\\) na kierunek \\(\\boldsymbol{a}\\).\nJeśli hipoteza \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) jest fałszywa, to istnieje taki wektor \\(\\boldsymbol{a}\\), dla którego:\n\\[\nH_0: \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_1 = \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_2\n\\]\nzostanie odrzucona w teście jednowymiarowym. Dla takiej kombinacji liniowej możemy zdefiniować statystykę \\(t\\)-Studenta:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\bar{z}_1 - \\bar{z}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) s_z^2}},\n\\]\ngdzie:\n\n\n\\(\\bar{z}_g = \\boldsymbol{a}^\\top \\bar{\\boldsymbol{y}}_g\\) – średnia z projekcji grupy \\(g\\),\n\n\\(s_z^2\\) – nieobciążony estymator wariancji \\(z\\), czyli:\n\n\\[\ns_z^2 = \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a},\n\\]\na \\(\\mathbf{S}\\) to wspólna macierz kowariancji.\nStąd pełna postać statystyki:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\boldsymbol{a}^\\top (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a}}}.\n\\]\nPonieważ statystyka \\(t(\\boldsymbol{a})\\) może być zarówno dodatnia, jak i ujemna, stosuje się często jej kwadrat jako miarę istotności:\n\\[\nT^2 = t^2(\\boldsymbol{a}).\n\\]\nStatystyka Hotellinga \\(T^2\\) przyjmuje postać:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nJest to forma uogólnionej odległości Mahalanobisa między średnimi wektorami. Można pokazać, że istnieje taki wektor \\(\\boldsymbol{a}\\), który maksymalizuje różnicę \\(t(\\boldsymbol{a})\\) — to tzw. funkcja dyskryminacyjna:\n\\[\n\\boldsymbol{a} = \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nDla tej wartości wektora \\(\\boldsymbol{a}\\), statystyka \\(T^2\\) przyjmuje największą wartość i jest najbardziej czuła na różnice między grupami.\nOdrzucenie \\(H_0\\) oznacza więc, że w przestrzeni \\(\\mathbb{R}^p\\) istnieje kierunek \\(\\boldsymbol{a}\\), dla którego grupy mają różne średnie projekcje. To otwiera drogę do:\n\nkonstrukcji funkcji dyskryminacyjnych (jak w analizie dyskryminacyjnej),\nidentyfikacji zmiennych lub ich kombinacji odpowiedzialnych za różnicę,\ndalszych analiz jednowymiarowych dla projekcji \\(z = \\boldsymbol{a}^\\top \\boldsymbol{y}\\).\n\n\nKodlibrary(gridExtra)  # dla strzałki jako warstwy\n\nset.seed(42)\n\n# Parametry\nn1 &lt;- n2 &lt;- 100\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1.5, 0.5)\nSigma &lt;- matrix(c(1, 0.8, 0.8, 1), ncol = 2)\n\n# Dane\nY1 &lt;- mvrnorm(n1, mu1, Sigma)\nY2 &lt;- mvrnorm(n2, mu2, Sigma)\n\n# Średnie\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymacja wspólnej kowariancji\nS_pooled &lt;- ((n1 - 1) * cov(Y1) + (n2 - 1) * cov(Y2)) / (n1 + n2 - 2)\n\n# Kierunek dyskryminacyjny a\ndiff &lt;- y1_bar - y2_bar\na &lt;- solve(S_pooled, diff)\na_norm &lt;- a / sqrt(sum(a^2))  # normalizacja\n\n# Punkt startowy strzałki (środek między średnimi)\norigin &lt;- (y1_bar + y2_bar) / 2\nscale &lt;- 3  # długość strzałki\narrow_end &lt;- origin + scale * a_norm\n\n# Ramka danych do wykresu\ndf &lt;- rbind(\n  data.frame(X1 = Y1[,1], X2 = Y1[,2], Grupa = \"Grupa 1\"),\n  data.frame(X1 = Y2[,1], X2 = Y2[,2], Grupa = \"Grupa 2\")\n)\n\n# Wektory średnich i strzałka\nmeans_df &lt;- data.frame(rbind(y1_bar, y2_bar))\ncolnames(means_df) &lt;- c(\"X1\", \"X2\")\nmeans_df$Grupa &lt;- c(\"Grupa 1\", \"Grupa 2\")  # te same etykiety co w danych punktów\n\narrow_df &lt;- data.frame(\n  x = origin[1],\n  y = origin[2],\n  xend = arrow_end[1],\n  yend = arrow_end[2]\n)\n\n# Wykres\nggplot(df, aes(x = X1, y = X2, color = Grupa)) +\n  geom_point(alpha = 0.5) +\n  geom_point(data = means_df, aes(x = X1, y = X2),\n             size = 4, shape = 16, show.legend = FALSE) +  # średnie tym samym kolorem; bez legendy\n  geom_segment(data = arrow_df, \n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")), color = \"black\", size = 1, show.legend = FALSE) +\n  labs(\n    title = latex2exp::TeX(r\"(Ilustracja kierunku dyskryminacyjnego $a = S^{-1} (\\bar{y}_1 - \\bar{y}_2)$)\"),\n    x = latex2exp::TeX(r\"($X_1$)\"),\n    y = latex2exp::TeX(r\"($X_2$)\")\n  ) +\n  coord_equal() +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#założenia-i-testowane-hipotezy",
    "href": "multi_tests.html#założenia-i-testowane-hipotezy",
    "title": "Testy wielowymiarowe",
    "section": "Założenia i testowane hipotezy",
    "text": "Założenia i testowane hipotezy\nZakłada się, że obserwacje są niezależne i pochodzą z wielowymiarowego rozkładu normalnego w każdej grupie, tj.:\n\\[\n\\boldsymbol{y}_{ij} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}), \\quad i = 1, \\dots, g,\\ j = 1, \\dots, n_i,\n\\]\ngdzie:\n\n\n\\(\\boldsymbol{y}_{ij} \\in \\mathbb{R}^p\\) — wektor obserwacji w grupie \\(i\\),\n\n\\(\\boldsymbol{\\mu}_i\\) — wektor średnich dla grupy \\(i\\),\n\n\\(\\boldsymbol{\\Sigma}\\) — wspólna macierz kowariancji we wszystkich grupach (założenie homogeniczności).\n\nTestowana jest hipoteza zerowa:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\dots = \\boldsymbol{\\mu}_g\n\\]\nwobec alternatywy:\n\\[\nH_1: \\exists\\ i,j\\ \\text{takie, że}\\ \\boldsymbol{\\mu}_i \\ne \\boldsymbol{\\mu}_j.\n\\]\nModel MANOVA opiera się na kilku fundamentalnych założeniach dotyczących danych, których spełnienie warunkuje poprawność i wiarygodność uzyskanych wyników. Ich naruszenie może prowadzić do fałszywych wniosków, zbyt dużej liczby odrzuceń hipotezy zerowej lub do błędnych ocen efektów czynników. Poniżej przedstawiono szczegółowo każde z tych założeń.\n\nPierwszym kluczowym założeniem jest odpowiednia wielkość próby. Przyjmuje się, że liczba obserwacji w każdej grupie (komórce) powinna przekraczać liczbę zmiennych zależnych, które są jednocześnie analizowane. To praktyczne zalecenie pozwala uniknąć problemów z oszacowaniem macierzy kowariancji i zapewnia dostateczną moc statystyczną.\nKolejnym istotnym założeniem jest niezależność obserwacji. Oznacza to, że każda jednostka obserwacyjna (np. osoba) powinna przynależeć wyłącznie do jednej grupy. Obserwacje wewnątrz i pomiędzy grupami nie mogą być ze sobą powiązane. W szczególności, model MANOVA nie jest odpowiedni dla danych z pomiarami powtarzanymi u tych samych obiektów. Dobór próby powinien być dokonany w sposób losowy, bez systematycznych zależności.\nTrzecim wymogiem jest brak obserwacji odstających, zarówno w sensie jednowymiarowym (dla każdej zmiennej z osobna), jak i wielowymiarowym (dla kombinacji wszystkich zmiennych zależnych). Obserwacje odstające mogą silnie zniekształcać wartości średnich i macierzy kowariancji, przez co wyniki MANOVA stają się niestabilne.\nFundamentalnym założeniem MANOVA jest wielowymiarowa normalność rozkładu danych w każdej z grup. Oznacza to, że wektor zmiennych zależnych w każdej grupie powinien mieć rozkład wielowymiarowy normalny. W R można zastosować funkcję mshapiro_test() z pakietu rstatix, aby przeprowadzić test Shapiro–Wilka dla sprawdzenia normalności wielowymiarowej.\nKolejne założenie dotyczy braku współliniowości. Oczekuje się, że zmienne zależne będą ze sobą skorelowane w umiarkowany sposób, ale nie nadmiernie. Wartości współczynników korelacji przekraczające \\(r = 0,90\\) są uznawane za niepożądane i mogą powodować problemy numeryczne oraz błędną interpretację wyników. Jak podają Tabachnick i Fidell (2012), zmienne powinny wnosić unikalne informacje do modelu.\nWażnym wymogiem jest również liniowość zależności między zmiennymi zależnymi w każdej grupie. Oznacza to, że zależności pomiędzy każdą parą zmiennych muszą być dobrze opisane przez funkcję liniową — jest to konieczne, aby poprawnie oszacować strukturę kowariancji.\nDla poprawnego działania MANOVA zakłada się także jednorodność wariancji dla każdej zmiennej zależnej między grupami. Można to testować za pomocą testu Levene’a. Nieistotny wynik testu Levene’a sugeruje, że wariancje są porównywalne w grupach.\nOstatnie, ale bardzo istotne, jest założenie o jednorodności macierzy kowariancji (homogeniczności macierzy wariancji–kowariancji) pomiędzy grupami. Oznacza to, że struktura współzależności między zmiennymi powinna być podobna w każdej grupie. Weryfikację tego założenia umożliwia test Boxa (Box’s M test), który stanowi wielowymiarowy odpowiednik testu Levene’a. Ze względu na dużą czułość testu Boxa na odstępstwa od założeń, przyjmuje się konserwatywny poziom istotności \\(\\alpha = 0,001\\) dla weryfikacji jego wyniku.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "href": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "title": "Testy wielowymiarowe",
    "section": "Konstrukcja modelu i statystyki testowe",
    "text": "Konstrukcja modelu i statystyki testowe\nPodobnie jak w jednowymiarowym przypadku, w MANOVA analizuje się rozkład wariancji całkowitej na wariancję międzygrupową i wewnątrzgrupową, ale w postaci macierzy kowariancji:\n\nMacierz wariancji międzygrupowej (ang. between-group SSCP):\n\n\\[\n\\mathbf{B} = \\sum_{i=1}^{g} n_i (\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})(\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})^\\top\n\\]\n\nMacierz wariancji wewnątrzgrupowej (ang. within-group SSCP):\n\n\\[\n\\mathbf{W} = \\sum_{i=1}^{g} \\sum_{j=1}^{n_i} (\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)(\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)^\\top\n\\]\nMacierz wariancji całkowitej to: \\(\\mathbf{T} = \\mathbf{B} + \\mathbf{W}\\).\nW celu przeprowadzenia testu MANOVA, wykorzystuje się statystyki oparte na stosunku macierzy:\n\\[\n\\mathbf{W}^{-1} \\mathbf{B}\n\\]\nNajczęściej spotykane statystyki testowe to:\n\nWilks’ Lambda:\n\n\\[\n\\Lambda = \\frac{\\det(\\mathbf{W})}{\\det(\\mathbf{B} + \\mathbf{W})}\n\\]\n\nStatystyka Pillai-Bartletta (Trace):\n\n\\[\nV = \\mathrm{tr}\\left[(\\mathbf{B} + \\mathbf{W})^{-1} \\mathbf{B}\\right]\n\\]\n\nStatystyka Hotellinga–Lawleya (Trace):\n\n\\[\nT = \\mathrm{tr}(\\mathbf{W}^{-1} \\mathbf{B})\n\\]\n\nNajwiększy pierwiastek Roy’a:\n\n\\[\n\\theta_{\\text{max}} = \\text{największa wartość własna}\\ (\\mathbf{W}^{-1} \\mathbf{B})\n\\]\nWybór konkretnej statystyki zależy od liczebności prób, wymiaru przestrzeni i liczby grup. W praktyce Wilks’ Lambda jest najczęściej stosowana.\n\nPrzykład 5.1 Na poziomie istotności \\(\\alpha=0.05\\) zweryfikuj hipotezę, że czynnik grupujący (Group) istotnie różnicuje zmienne Actions i Thoughts jednocześnie.\n\nKodlibrary(gtsummary)\nlibrary(rstatix)\nlibrary(easystats)\nlibrary(gt)\ndane &lt;- rio::import(\"data/OCD.dat\")\n\n\nStatystyki opisowe grup\n\nKoddane %&gt;% \n  tbl_summary(by = Group,\n              statistic = list(where(is.numeric) ~ \"{mean} ({sd})\"),\n              type = list(Actions ~ \"continuous\"),\n              digits = list(everything() ~ 2))\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nBT\nN = 101\n\n\nCBT\nN = 101\n\n\nNo Treatment Control\nN = 101\n\n\n\n\nActions\n3.70 (1.77)\n4.90 (1.20)\n5.00 (1.05)\n\n\nThoughts\n15.20 (2.10)\n13.40 (1.90)\n15.00 (2.36)\n\n\n\n\n1 Mean (SD)\n\n\n\n\nKodp &lt;- dane %&gt;% \n  select(-Group) %&gt;% \n  correlation()\n\np %&gt;% print_html()\n\n\n\n\n\n\nCorrelation Matrix (pearson-method)\n\n\nParameter1\nParameter2\nr\n95% CI\nt(28)\np\n\n\n\nActions\nThoughts\n0.06\n(-0.31, 0.41)\n0.31\n0.758\n\n\np-value adjustment method: Holm (1979); Observations: 30\n\n\n\n\n\nW kontekście zmiennej Actions widzimy najwyższy poziom w grupie No treatment, natomiast najniższy w grupie BT. Dla zmiennej Thoughts najwyższy poziom osiągnięto w grupie BT a najniższy w grupie CBT. Grupy różnią się również zmiennością obu cech. Związek pomiędzy zmiennymi Actions i Thoughts jest niemal niezauważalny. Korelacja pomiędzy tymi cechami jest nieistotnie różna od zera.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  ggplot(aes(x = Group, y = value, fill = name)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(fill = \"Variable\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\nPowyższe wykresy potwierdzają znaczne różnice pomiędzy grupami w kontekście analizowanych cech.\nZałożenia\n\nKoddane %&gt;% \n  group_split(Group) %&gt;% \n  map_df(~mshapiro_test(.x[,2:3])) %&gt;% \n  mutate(Group = dane %&gt;% \n           group_keys(Group) %&gt;% \n           pull(Group),\n         .before = statistic) %&gt;%\n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nstatistic\np.value\n\n\n\nBT\n0.891\n0.175\n\n\nCBT\n0.959\n0.777\n\n\nNo Treatment Control\n0.826\n0.030\n\n\n\n\n\n\nJedynie w grupie No treatment nie jest zachowana wielowymiarowa normalność rozkładu analizowanych cech. Można też przeprowadzić testy normalności poszczególnych zmiennych, ale należy pamiętać, że brak podstaw do odrzucenia hipotezy o normalności brzegowych zmiennych nie jest warunkiem dostatecznym, a jedynie koniecznym.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Actions) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nActions\n0.872\n0.106\n\n\nCBT\nActions\n0.952\n0.691\n\n\nNo Treatment Control\nActions\n0.859\n0.074\n\n\n\n\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nThoughts\n0.877\n0.120\n\n\nCBT\nThoughts\n0.914\n0.310\n\n\nNo Treatment Control\nThoughts\n0.826\n0.030\n\n\n\n\n\n\nPodobnie jak w przypadku wielowymiarowym brak normalności zarysował się w grupie No treatment i to tylko dla zmiennej Thoughts. Teraz przechodzimy do testowania jednorodności kowariancji.\n\nKodbox_m(dane[,2:3], dane$Group)  %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n8.893\n0.180\n6.000\nBox's M-test for Homogeneity of Covariance Matrices\n\n\n\n\n\nNa podstawie powyższego testu można stwierdzić, iż nie ma podstaw do odrzucenia hipotezy o jednorodności macierzy kowariancji.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Actions) \n\n[1] Group      Actions    Thoughts   is.outlier is.extreme\n&lt;0 rows&gt; (or 0-length row.names)\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nActions\nThoughts\nis.outlier\nis.extreme\n\n\nNo Treatment Control\n4\n20\nTRUE\nFALSE\n\n\n\n\nKodwhich(dane$Actions == 4 & dane$Thoughts == 20)\n\n[1] 26\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  mahalanobis_distance() %&gt;% \n  filter(is.outlier==TRUE)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: Actions &lt;int&gt;, Thoughts &lt;int&gt;, mahal.dist &lt;dbl&gt;,\n#   is.outlier &lt;lgl&gt;\n\n\nIstnieje jedna obserwacja odstająca w grupie No treatment (obserwacja nr 26). Test wielowymiarowy nie wykrył żadnego elementu odstającego.\nPomimo niespełnienia założenia o wielowymiarowej normalności cech w grupach, zastosujemy test MANOVA.\nManova\n\nKodmod &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane)\nManova(mod) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.32\n4\n54\n2.56\n0.049\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Wilk\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.70\n4\n52\n2.55\n0.050\n\n\nWilks test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Roy\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.33\n2\n27\n4.52\n0.020\n\n\nRoy test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Hotelling\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.41\n4\n50\n2.55\n0.051\n\n\nHotelling-Lawley test statistic Anova Table (Type 2 tests)\n\n\n\n\nKod# model bez obserawcji odstającej\nmod2 &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane[-26,])\nManova(mod2) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.36\n4\n52\n2.87\n0.032\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\n\nAnalizują wszystkie rodzaje testów Manova, widzimy, że jedynie test Hotellinga-Laweya nie daje podstaw do odrzucenia hipotezy o równości wektorów średnich. Natomiast ponieważ co najmniej jeden z nich wskazał istotność różnic, to przyjmujemy, że są podstawy aby odrzucić hipotezę o równości wektorów średnich pomiędzy grupami. Test wykluczający obserwację odstającą również każe odrzucić hipotezę \\(H_0\\).\nPrzeprowadzimy zatem analizę brzegową.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  anova_test(value~Group) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\nActions\nGroup\n2.000\n27.000\n2.771\n0.080\n\n0.170\n\n\nThoughts\nGroup\n2.000\n27.000\n2.154\n0.136\n\n0.138\n\n\n\n\n\n\nAnaliza brzegowa pokazuje ciekawy wynik, mianowicie, dla żadnej z analizowanych cech testy brzegowe nie wykazały istotnych różnic. To pokazuje jak ważne jest stosowanie testów wielowymiarowych w kontekście porównań grup.\nPost-hoc\nPonieważ testy brzegowe ANOVA nie wykazały różnic, to testów post-hoc nie powinno się wykonywać, ale dla celów ćwiczeniowych pokażę jak je wykonać.\n\nKodpwc &lt;- dane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  games_howell_test(value~Group)\npwc %&gt;% \n  select(-.y.) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\ngroup1\ngroup2\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\nActions\nBT\nCBT\n1.200\n−0.544\n2.944\n0.209\nns\n\n\nActions\nBT\nNo Treatment Control\n1.300\n−0.394\n2.994\n0.148\nns\n\n\nActions\nCBT\nNo Treatment Control\n0.100\n−1.189\n1.389\n0.979\nns\n\n\nThoughts\nBT\nCBT\n−1.800\n−4.085\n0.485\n0.138\nns\n\n\nThoughts\nBT\nNo Treatment Control\n−0.200\n−2.749\n2.349\n0.978\nns\n\n\nThoughts\nCBT\nNo Treatment Control\n1.600\n−0.852\n4.052\n0.244\nns\n\n\n\n\n\n\nTesty post-hoc potwierdzają wyniki testów brzegowych ANOVA, ponieważ brakuje różnic pomiędzy poziomami zmiennych grupujących.\n\n\n\n\n\nRencher, Alvin C. 1998. Multivariate statistical inference and applications. T. 635. Wiley New York.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Rencher, Alvin C. 1998. Multivariate Statistical Inference and\nApplications. Vol. 635. Wiley New York.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "cca.html",
    "href": "cca.html",
    "title": "Analiza kanoniczna",
    "section": "",
    "text": "Przypomnienie z algebry 😉\nAnaliza kanoniczna (ang. Canonical Correlation Analysis, CCA) jest klasyczną techniką statystyczną służącą do badania związków pomiędzy dwoma zestawami zmiennych wielowymiarowych. Jej podstawowym celem jest znalezienie takich kombinacji liniowych zmiennych z obu zestawów, które maksymalizują wzajemną korelację – są to tzw. kanoniczne zmienne lub kanoniczne składniki. Technika ta została wprowadzona przez Harolda Hotellinga w roku 1936, a więc w okresie intensywnego rozwoju metod statystycznych opartych na algebrze macierzy.\nW tym samym czasie powstawały także inne fundamenty analizy wielowymiarowej, takie jak analiza składowych głównych (PCA) czy dyskryminacja liniowa (LDA)1. Analiza kanoniczna stanowi zatem jeden z filarów klasycznej statystyki wielowymiarowej i do dziś pozostaje istotnym narzędziem eksploracji i modelowania złożonych zależności.\nW odróżnieniu od regresji wielorakiej, która przewiduje zestaw zmiennych zależnych na podstawie zestawu predyktorów, analiza kanoniczna traktuje obie grupy zmiennych symetrycznie – nie zakłada istnienia wyraźnego kierunku przyczynowego. Dlatego stosuje się ją w sytuacjach, gdy celem jest ogólna analiza współzależności pomiędzy dwoma zbiorami zmiennych, a nie przewidywanie jednego zestawu na podstawie drugiego.\nTypowe zastosowania analizy kanonicznej obejmują:\nNa potrzeby definicji modeli kanonicznego potrzebne będą nam pewne twierdzenia z zakresu algebry.\nOto matematyczna definicja modelu CCA oraz dowód istnienia rozwiązania, sformułowana ściśle w duchu Twojego tekstu:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#sformułowanie-problemu-własnego",
    "href": "cca.html#sformułowanie-problemu-własnego",
    "title": "Analiza kanoniczna",
    "section": "Sformułowanie problemu własnego",
    "text": "Sformułowanie problemu własnego\nPrzekształćmy zmienne \\[\nc=\\Sigma_{XX}^{1/2}a,\\quad d=\\Sigma_{YY}^{1/2}b.\n\\]\nWówczas \\[\n\\rho(a,b)=\\frac{c^\\top\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}d}{\\sqrt{c^\\top c}\\sqrt{d^\\top d}}.\n\\]\nZ lematu Cauchy’ego–Buniakowskiego–Schwarza mamy \\[\n\\left|c^\\top \\mathbf{M} d\\right| \\le\n\\bigl(c^\\top \\mathbf{M}\\mathbf{M}^\\top c\\bigr)^{1/2}\\bigl(d^\\top d\\bigr)^{1/2},\n\\quad \\text{gdzie }\\mathbf{M}=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}.\n\\tag{5.1}\\]\nZatem \\[\n\\rho(a,b)^2 \\le\n\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}.\n\\]\nPonieważ \\(\\mathbf{M}\\mathbf{M}^\\top=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\) jest macierzą symetryczną dodatnio określoną, z lematu Rayleigha–Ritza otrzymujemy \\[\n\\max_{c\\neq 0}\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}=\\lambda_1,\n\\] gdzie \\(\\lambda_1\\) to największa wartość własna tej macierzy, osiągana dla \\(c=e_1\\) – jej wektora własnego.\nJeśli \\[\nd \\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1\n\\] to Równanie 5.1 staje się równością.\nWracając do oryginalnych współczynników \\[\na_1=\\Sigma_{XX}^{-1/2}e_1,\\quad\nb_1\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1.\n\\]\nPierwsza korelacja kanoniczna wynosi wówczas \\[\n\\rho_1=\\sqrt{\\lambda_1}.\n\\]\nAnalogicznie dla kolejnych par, przy założeniu \\(c\\perp e_1,\\dots,e_{k-1}\\), mamy \\[\n\\rho_k=\\sqrt{\\lambda_k},\n\\] gdzie \\(\\lambda_k\\) to kolejne wartości własne macierzy \\(\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\), a odpowiadające im wektory własne \\(e_k\\) definiują kolejne wektory kanoniczne \\[\nU_k=e_k^\\top\\Sigma_{XX}^{-1/2}X,\\quad\nV_k=f_k^\\top\\Sigma_{YY}^{-1/2}Y,\\quad\nf_k\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_k.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#własności-rozwiązań",
    "href": "cca.html#własności-rozwiązań",
    "title": "Analiza kanoniczna",
    "section": "Własności rozwiązań",
    "text": "Własności rozwiązań\nDla każdej pary \\((U_k,V_k)\\) zachodzi \\[\n\\mathrm{Var}(U_k)=\\mathrm{Var}(V_k)=1,\\quad\n\\mathrm{Cov}(U_k,U_l)=\\mathrm{Cov}(V_k,V_l)=\\mathrm{Cov}(U_k,V_l)=0\\quad (k\\neq l).\n\\]\n\n\n\n\n\n\nDowód powyższych równości\n\n\n\nDla danej pary wektorów kanonicznych mamy\n\\[\nU_k = a_k^\\top X = e_k^\\top \\Sigma_{XX}^{-1/2} X,\n    \\qquad\n    V_k = b_k^\\top Y = f_k^\\top \\Sigma_{YY}^{-1/2} Y,\n\\] gdzie:\n\n\n\\(e_k\\) jest ortonormalnym6 wektorem własnym macierzy \\(\\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} \\Sigma_{XX}^{-1/2}\\)\n\n\n\\(f_k \\propto \\Sigma_{YY}^{-1/2} \\Sigma_{YX} \\Sigma_{XX}^{-1/2} e_k\\),\n\n\\(\\rho_k = \\sqrt{\\lambda_k}\\), gdzie \\(\\lambda_k\\) to odpowiadająca wartość własna.\n\nMacierze \\(\\Sigma_{XX}\\), \\(\\Sigma_{YY}\\) są dodatnio określone, więc można wprowadzić transformacje \\[\n\\tilde{X} = \\Sigma_{XX}^{-1/2}X, \\quad \\tilde{Y} = \\Sigma_{YY}^{-1/2}Y.\n\\] Zatem \\[\nU_k = e_k^\\top \\tilde{X},\\quad V_k = f_k^\\top \\tilde{Y}.\n\\]\nNajpierw udowodnimy, że \\(\\operatorname{Var}(U_k) = \\operatorname{Var}(V_k) = 1\\).\nZmienna \\(U_k = e_k^\\top \\tilde{X}\\), więc \\[\n\\operatorname{Var}(U_k) = \\operatorname{Var}(e_k^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_k.\n\\] Zauważmy, że \\[\n\\operatorname{Var}(\\tilde{X}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XX} \\Sigma_{XX}^{-1/2} = I,\n\\] więc \\[\n\\operatorname{Var}(U_k) = e_k^\\top I e_k = e_k^\\top e_k = 1.\n\\] Analogicznie \\[\n\\operatorname{Var}(V_k) = f_k^\\top \\operatorname{Var}(\\tilde{Y}) f_k = f_k^\\top f_k = 1,\n\\] ponieważ \\(\\tilde{Y}\\) ma jednostkową macierz kowariancji, a \\(f_k\\) są znormalizowane.\nTeraz dowiedziemy, że \\(\\operatorname{Cov}(U_k, U_l) = 0\\) dla \\(k \\neq l\\)\n\\[\n\\operatorname{Cov}(U_k, U_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, e_l^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_l = e_k^\\top e_l.\n\\] Ponieważ \\(e_k\\), \\(e_l\\) są ortonormalnymi wektorami własnymi symetrycznej macierzy, to \\[\ne_k^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, U_l) = 0.\n\\] Podobnie \\[\n\\operatorname{Cov}(V_k, V_l) = f_k^\\top f_l = 0 \\quad \\text{dla } k \\neq l.\n\\]\nNa koniec dowiedźmy, że \\(\\operatorname{Cov}(U_k, V_l) = 0\\) dla \\(k \\neq l\\) \\[\n\\operatorname{Cov}(U_k, V_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, f_l^\\top \\tilde{Y}) = e_k^\\top \\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) f_l.\n\\] Z definicji \\[\n\\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1/2} =: M.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = e_k^\\top M f_l.\n\\] Z poprzednich wyprowadzeń \\[\nf_l \\propto M^\\top e_l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) \\propto e_k^\\top M M^\\top e_l.\n\\] Ale macierz \\(MM^\\top\\) jest symetryczna, a \\(e_k\\) są jej ortonormalnymi wektorami własnymi, więc \\[\ne_k^\\top MM^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = 0.\n\\]\n\n\n6 ortonormalność wynika z niezmienniczości korelacji względem długości wektorówPonadto korelacje kanoniczne są niezmiennicze względem odwracalnych przekształceń liniowych \\(X\\) i \\(Y\\): \\[\nX^*=\\mathcal{U}^TX+u,\\quad Y^*=\\mathcal{V}^TY+v \\implies\n\\rho_i(X^*,Y^*)=\\rho_i(X,Y).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wniosek",
    "href": "cca.html#wniosek",
    "title": "Analiza kanoniczna",
    "section": "Wniosek",
    "text": "Wniosek\nPonieważ macierze kowariancji \\(\\Sigma_{XX}\\) i \\(\\Sigma_{YY}\\) są dodatnio określone, ich odwrotności istnieją. Macierze: \\[\n\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2},\\quad\n\\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}\n\\] są symetryczne i dodatnio określone, więc mają rzeczywiste, dodatnie wartości własne i ortonormalne wektory własne. Z lematu Rayleigha–Ritza otrzymujemy maksymalizację ilorazu Rayleigha oraz gwarancję istnienia rozwiązania. Tym samym wykazano, że pary \\((a_k,b_k)\\) istnieją, a odpowiadające im \\(\\rho_k=\\sqrt{\\lambda_k}\\) są kanonicznymi korelacjami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#hipoteza-zerowa-i-alternatywna",
    "href": "cca.html#hipoteza-zerowa-i-alternatywna",
    "title": "Analiza kanoniczna",
    "section": "Hipoteza zerowa i alternatywna",
    "text": "Hipoteza zerowa i alternatywna\nDla zbiorów zmiennych \\(X \\in \\mathbb{R}^p\\), \\(Y \\in \\mathbb{R}^q\\), testujemy\n\n\n\\(H_0: \\rho_1 = \\rho_2 = \\cdots = \\rho_s = 0\\) – brak istotnych korelacji kanonicznych (pierwiastków),\n\n\\(H_1\\): istnieje co najmniej jedna istotna korelacja kanoniczna, tj. \\(\\exists i \\leq s \\ \\text{takie, że } \\rho_i \\ne 0\\).\n\ngdzie \\(s = \\min(p, q)\\), a \\(\\rho_i\\) to \\(i\\)-ta korelacja kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#statystyka-testowa-test-wilka",
    "href": "cca.html#statystyka-testowa-test-wilka",
    "title": "Analiza kanoniczna",
    "section": "Statystyka testowa – test Wilka",
    "text": "Statystyka testowa – test Wilka\nW celu przetestowania tej hipotezy, wykorzystuje się statystykę Wilka, która bazuje na iloczynie składników postaci (\\(1 - \\lambda_i\\)), gdzie \\(\\lambda_i\\) to wartości własne odpowiadające kwadratom korelacji kanonicznych \\[\n\\lambda_i = \\rho_i^2.\n\\] Statystyka Wilkas jest zdefiniowana jako \\[\n\\Lambda = \\prod_{i=1}^s (1 - \\lambda_i).\n\\]\nInterpretacja - im mniejsze wartości \\(\\Lambda\\), tym większa zależność między zbiorami \\(X\\) i \\(Y\\). Duże wartości \\(\\lambda_i\\) (czyli silne korelacje kanoniczne) powodują, że \\(\\Lambda\\) dąży do zera.\nW praktyce, dla próby \\(n\\)-elementowej, stosujemy wersję testu bazującą na macierzach kowariancji estymowanych z próby \\(S_{XX}, S_{XY}, S_{YX}, S_{YY})\\) – odpowiedniki \\(\\Sigma_{XX}, \\Sigma_{XY}, \\Sigma_{YX}, \\Sigma_{YY}\\).\nWówczas \\[\nT^2/n = \\left|I - S_{YY}^{-1} S_{YX} S_{XX}^{-1} S_{XY} \\right| = \\prod_{i=1}^s (1 - \\hat{\\lambda}_i),\n\\] gdzie \\(\\hat{\\lambda}_i\\) to próbkowe wartości własne (szacunki \\(\\lambda_i\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#rozkład-asymptotyczny-i-transformacja-do-rozkładu-chi2",
    "href": "cca.html#rozkład-asymptotyczny-i-transformacja-do-rozkładu-chi2",
    "title": "Analiza kanoniczna",
    "section": "Rozkład asymptotyczny i transformacja do rozkładu \\(\\chi^2\\)\n",
    "text": "Rozkład asymptotyczny i transformacja do rozkładu \\(\\chi^2\\)\n\nWielu autorów (np. Marriott i Gittins (1986)) sugeruje przekształcenie statystyki Wilksa do postaci asymptotycznie zgodnej z rozkładem \\(\\chi^2\\), np. za pomocą transformacji\n\\[\n-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda) \\sim \\chi^2_{pq}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#procedura-testowania",
    "href": "cca.html#procedura-testowania",
    "title": "Analiza kanoniczna",
    "section": "Procedura testowania",
    "text": "Procedura testowania\n\nOszacuj wszystkie korelacje kanoniczne \\(\\hat{\\rho}_1, \\ldots, \\hat{\\rho}_p\\).\nOd \\(k = 0\\) do \\(p-1\\) oblicz \\(\\Lambda_k = \\prod_{i=k+1}^{p}(1 - \\hat{\\rho}_i^2)\\).\nOblicz transformację \\(\\chi^2_k=-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda_k)\\)\n\nPorównaj z odpowiednim kwantylem rozkładu \\(\\chi^2\\) z \\((q - k)(r - k)\\) stopniami swobody.\nJeśli wartość statystyki przekracza ten kwantyl (\\(p&lt;\\alpha\\)), odrzuć \\(H_0^{(k)}\\) i przejdź do \\(k+1\\). Jeśli nie, zatrzymaj się – kolejne korelacje uznajemy za nieistotne.\n\nOcena dopasowania modelu w analizie kanonicznej (CCA – Canonical Correlation Analysis) obejmuje kilka istotnych wskaźników diagnostycznych, które pozwalają zrozumieć siłę i strukturę relacji między dwoma zbiorami zmiennych. Poniżej omówione zostały trzy kluczowe miary: ładunki czynnikowe, wariancja wyjaśniona oraz redundancja.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#ładunki-czynnikowe-ang.-canonical-loadings",
    "href": "cca.html#ładunki-czynnikowe-ang.-canonical-loadings",
    "title": "Analiza kanoniczna",
    "section": "Ładunki czynnikowe (ang. canonical loadings)",
    "text": "Ładunki czynnikowe (ang. canonical loadings)\n\n\nDefinicja - korelacje pomiędzy zmiennymi kanonicznymi (czyli kombinacjami liniowymi wektorów \\(a_k'X\\) i \\(b_k'Y\\)) a oryginalnymi zmiennymi ze zbiorów \\(X\\) i \\(Y\\).\n\nInterpretacja:\n\nPokazują, które konkretne zmienne pierwotne w największym stopniu „ładują się” (czyli kontrybuują) na daną zmienną kanoniczną.\nWysoka wartość (np. &gt; 0.7) wskazuje na silną zależność między zmienną oryginalną a daną zmienną kanoniczną.\nZnaki dodatnie/ujemne pozwalają wnioskować o kierunku związku.\n\n\n\nWzór:\n\nDla zbioru \\(X\\): \\[\n\\text{loadings}_X = \\mathrm{Corr}(X, U_k) = \\Sigma_{XX} a_k\n\\]\n\nDla zbioru \\(Y\\): \\[\n\\text{loadings}_Y = \\mathrm{Corr}(Y, V_k) = \\Sigma_{YY} b_k\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wariancja-wyjaśniona-ang.-variance-explained",
    "href": "cca.html#wariancja-wyjaśniona-ang.-variance-explained",
    "title": "Analiza kanoniczna",
    "section": "Wariancja wyjaśniona (ang. variance explained)",
    "text": "Wariancja wyjaśniona (ang. variance explained)\n\n\nDefinicja - średnia kwadratów ładunków czynnikowych dla każdej zmiennej kanonicznej i każdego zbioru danych.\n\nInterpretacja:\n\nInformuje, jaką część wariancji oryginalnych zmiennych w danym zbiorze (\\(X\\) lub \\(Y\\)) wyjaśnia dana zmienna kanoniczna.\nMożna traktować ten wskaźnik jako odpowiednik współczynnika determinacji \\(R^2\\) dla pojedynczej zmiennej kanonicznej.\nWysoka wartość oznacza, że dana zmienna kanoniczna dobrze reprezentuje zbiór, z którego została utworzona.\n\n\n\nWzór: \\[\n\\text{Explained variance} = \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k)\n\\] gdzie \\(p\\) to liczba zmiennych w zbiorze \\(X\\), a \\(U_k\\) to \\(k\\)-ta zmienna kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#redundancja-ang.-redundancy-index",
    "href": "cca.html#redundancja-ang.-redundancy-index",
    "title": "Analiza kanoniczna",
    "section": "Redundancja (ang. redundancy index)",
    "text": "Redundancja (ang. redundancy index)\n\n\nDefinicja - iloczyn kwadratu korelacji kanonicznej \\(\\rho_k^2\\) oraz wariancji wyjaśnionej przez daną zmienną kanoniczną we własnym zbiorze.\n\nInterpretacja:\n\nInformuje, jaka część przeciętnej wariancji jednej grupy zmiennych jest wyjaśniana przez zmienną kanoniczną utworzoną na podstawie drugiego zbioru.\nMiara ta pokazuje, czy dany zbiór zmiennych wnosi unikalną informację o drugim zbiorze.\nWysoka redundancja oznacza, że istnieje istotny związek między strukturami dwóch zbiorów zmiennych.\n\n\n\nWzór: \\[\n\\text{Redundancy}_X = \\rho_k^2 \\cdot \\left( \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k) \\right)\n\\] Analogicznie definiujemy redundancję względem \\(Y\\).\n\n\n\n\n\n\n\n\nMiara\nCo opisuje\nInterpretacja praktyczna\n\n\n\nŁadunki czynnikowe\nSiłę powiązania zmiennej oryginalnej z kanoniczną\nWysoka wartość ⇒ silna reprezentacja zmiennej\n\n\nWariancja wyjaśniona\nŚrednia siła reprezentacji zbioru przez zm. kanoniczną\nMiara dopasowania struktury do zbioru\n\n\nRedundancja\nIlość informacji o jednym zbiorze zawarta w drugim\nMiara istotności relacji między zbiorami",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#normalność-wielowymiarowa",
    "href": "cca.html#normalność-wielowymiarowa",
    "title": "Analiza kanoniczna",
    "section": "Normalność wielowymiarowa",
    "text": "Normalność wielowymiarowa\nZakłada się, że obydwa zbiory zmiennych losowych – \\(X\\) i \\(Y\\) – są wspólnie rozkładem normalnym wielowymiarowym jak podano w Równanie 4.1. Normalność umożliwia stosowanie testów statystycznych (np. testu Wilksa) do oceny liczby istotnych korelacji kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-wartości-odstających-outliers",
    "href": "cca.html#brak-wartości-odstających-outliers",
    "title": "Analiza kanoniczna",
    "section": "Brak wartości odstających (outliers)",
    "text": "Brak wartości odstających (outliers)\nZarówno obserwacje odstające jednowymiarowe, jak i wielowymiarowe mogą istotnie zaburzać wynik analizy kanonicznej. Odstające wartości mogą wpływać na macierze kowariancji, zmieniając kierunki i siły relacji między zbiorami zmiennych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wystarczająca-liczba-obserwacji",
    "href": "cca.html#wystarczająca-liczba-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "Wystarczająca liczba obserwacji",
    "text": "Wystarczająca liczba obserwacji\nLiczba obserwacji powinna znacząco przekraczać liczbę zmiennych w każdym zbiorze. Liczba obserwacji \\(n\\) w każdej grupie powinna być większa niż suma liczby zmiennych w \\(X\\) i \\(Y\\) \\[\nn &gt; p + q\n\\] Zapewnia odwracalność macierzy kowariancji oraz stabilność estymatorów.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#liniowość-zależności",
    "href": "cca.html#liniowość-zależności",
    "title": "Analiza kanoniczna",
    "section": "Liniowość zależności",
    "text": "Liniowość zależności\nZakłada się, że związki między wszystkimi parami zmiennych są liniowe. Ponieważ CCA opiera się na maksymalizacji liniowych kombinacji, nieliniowe zależności mogą pozostać niewykryte.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-nadmiernej-współliniowości-multikolinearności",
    "href": "cca.html#brak-nadmiernej-współliniowości-multikolinearności",
    "title": "Analiza kanoniczna",
    "section": "Brak nadmiernej współliniowości (multikolinearności)",
    "text": "Brak nadmiernej współliniowości (multikolinearności)\nZmienne wewnątrz każdego zbioru (w \\(X\\) lub w \\(Y\\)) nie powinny być nadmiernie skorelowane. Wysoka współliniowość może prowadzić do niestabilnych i trudnych do interpretacji wektorów kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#niezależność-obserwacji",
    "href": "cca.html#niezależność-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "Niezależność obserwacji",
    "text": "Niezależność obserwacji\nKażda obserwacja powinna pochodzić od innej jednostki (brak powtórzeń pomiarów). Niezależność warunkuje poprawność estymatorów kowariancji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "fa.html",
    "href": "fa.html",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "Eksploracyjna analiza czynnikowa\nAnaliza czynnikowa należy do klasy metod wielowymiarowych, których celem jest odkrywanie ukrytych struktur stojących za obserwowanymi zmiennymi. W odróżnieniu od metod takich jak analiza głównych składowych1, które opierają się na czysto algebraicznych przekształceniach danych, analiza czynnikowa ma wyraźne odniesienie do modeli statystycznych i psychometrycznych, w których zakłada się istnienie czynników latentnych – czyli zmiennych ukrytych, niewidocznych bezpośrednio, ale wpływających na wartości zmiennych obserwowalnych. Przykładem może być konstrukt „inteligencja”, który przejawia się w wynikach testów logicznych, pamięciowych czy językowych. Głównym celem analizy czynnikowej jest redukcja wymiarowości poprzez reprezentację wielu zmiennych w postaci mniejszej liczby czynników oraz lepsze zrozumienie powiązań między zmiennymi poprzez ujawnienie wspólnych źródeł ich zmienności.\nMożna wyróżnić dwa podstawowe podejścia do analizy czynnikowej. Eksploracyjna analiza czynnikowa (EFA, Exploratory Factor Analysis) jest stosowana, gdy badacz nie ma wcześniej zdefiniowanych hipotez co do liczby czynników czy struktury powiązań między nimi. Jej celem jest odkrycie potencjalnych układów zależności i zidentyfikowanie liczby czynników najlepiej opisujących dane. Konfirmacyjna analiza czynnikowa (CFA, Confirmatory Factor Analysis) jest natomiast podejściem dedukcyjnym – badacz z góry formułuje model teoretyczny (np. że pewne zmienne mierzą „pamięć roboczą”, a inne „myślenie abstrakcyjne”) i testuje jego zgodność z danymi empirycznymi. CFA jest szczególnie istotna w kontekście walidacji narzędzi badawczych, np. kwestionariuszy psychologicznych, i stanowi fundament bardziej zaawansowanych modeli strukturalnych (SEM).\nHistoria analizy czynnikowej sięga początków XX wieku i jest ściśle związana z psychometrią. Jej pionierem był Charles Spearman, który w 1904 roku zaproponował model jednoczynnikowy, interpretując zmienne poznawcze jako przejawy ogólnego czynnika inteligencji. W kolejnych dekadach metoda była rozwijana przez psychologów, takich jak Thurstone, który wprowadził koncepcję wieloczynnikową oraz przez statystyków, którzy rozwijali formalne podstawy estymacji czynników i rotacji macierzy ładunków. W latach 60. i 70. analiza czynnikowa stała się jedną z najczęściej stosowanych metod w badaniach psychologicznych i społecznych, a wraz z rozwojem informatyki zyskała na popularności także w ekonomii, biologii czy medycynie. Dziś analiza czynnikowa jest narzędziem interdyscyplinarnym, stosowanym zarówno do eksploracji struktur danych, jak i do testowania teorii opartych na zmiennych latentnych.\nFormalna postać modelu eksploracyjnej analizy czynnikowej (EFA) zakłada, że zmienne obserwowalne \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)^\\top\\) można wyrazić jako kombinację liniową czynników latentnych oraz składników specyficznych. Model przyjmuje postać:\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\]\ngdzie:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#efa",
    "href": "fa.html#efa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) – wektor zmiennych obserwowalnych,\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) – wektor średnich,\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) – macierz ładunków czynnikowych, której element \\(\\lambda_{ij}\\) opisuje wpływ czynnika \\(j\\) na zmienną \\(i\\),\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) – wektor czynników latentnych (czynników wspólnych),\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) – wektor składników specyficznych (unikalnych, błędów pomiaru).\n\n\nZałożenia klasycznego modelu EFA\n\nRozkład czynników wspólnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne mają średnią zero i macierz kowariancji równą macierzy jednostkowej. To założenie oznacza, że czynniki są nieskorelowane i mają wariancję jednostkową (jest to standaryzacja wprowadzona dla identyfikowalności modelu).\nRozkład składników specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzą diagonalną o elementach dodatnich. Oznacza to, że błędy są nieskorelowane między sobą oraz niezależne od czynników \\(\\mathbf{f}\\).\nNiezależność czynników i błędów \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyższej konstrukcji wynika, że kowariancja zmiennych obserwowalnych jest sumą części wspólnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowód\n\n\n\nNiech losowy wektor obserwacji ma postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynników wspólnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor składników specyficznych. Zakładamy, że \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazać, że \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczególności przy \\(\\Phi=I_m\\), że mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a ponieważ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancję \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyrażamy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] Korzystając z liniowości kowariancji i tożsamości \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorów losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skończonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDzięki założeniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikają i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] Jeśli dodatkowo przyjmiemy standardyzację czynników \\(\\Phi=I_m\\) (co jest konwencją identyfikacyjną modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego należało dowieść.\nWarto odnotować, że dowód nie wymaga niezależności \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym — wystarcza nieskorelowanie, aby zniknęły składniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postać \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to można zastosować tzw. whitening czynników \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co równoważnie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), które prowadzą do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to związane z możliwością przeprowadzania różnych transformacji czynników bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogólnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeżeli dokonamy transformacji ortogonalnej czynników \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzą ortogonalną, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, że \\(\\Lambda\\) i \\(\\Phi\\) nie są jednoznacznie wyznaczone. Różne pary \\((\\Lambda, \\Phi)\\) mogą prowadzić do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczególności wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jedną z takich równoważnych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalną rolę, ponieważ jest miejscem, w którym spotykają się dwa składniki zmienności: wspólna i specyficzna. Rozkład \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, że całkowita wariancja i kowariancja obserwowanych zmiennych może być przedstawiona jako suma efektu wspólnych czynników oraz efektu specyficznego, indywidualnego dla każdej zmiennej.\nCzęść \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspólne źródło zmienności, czyli wariancję wyjaśnianą przez czynniki ukryte. To właśnie ta część umożliwia redukcję wymiaru – wiele zmiennych obserwowanych można sprowadzić do kilku czynników, które reprezentują główną strukturę zależności. Interpretacja czynników jako ukrytych wymiarów (np. inteligencja, poziom lęku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uprościć analizę, ale także nadać jej znaczenie teoretyczne w danej dziedzinie badań.\nZ kolei \\(\\Psi\\) odpowiada za wariancję unikalną, czyli tę część zmienności, która nie jest współdzielona z innymi zmiennymi. Obejmuje ona zarówno wariancję czysto specyficzną dla danej cechy, jak i wariancję błędu pomiarowego. Dzięki temu możliwe jest odróżnienie struktury głębokiej (czynnikowej) od elementów przypadkowych i indywidualnych.\nPodsumowując, znaczenie modelu czynnikowego polega na tym, że pozwala on wydzielić istotne, ukryte mechanizmy stojące za współzależnościami zmiennych i oddzielić je od szumów specyficznych dla pojedynczych obserwacji. W praktyce oznacza to możliwość redukcji liczby analizowanych zmiennych, uproszczenie opisu złożonych danych i pogłębienie interpretacji zjawisk społecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynników w praktyce opiera się przede wszystkim na analizie macierzy ładunków czynnikowych \\(\\Lambda\\). Każdy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile związku pomiędzy zmienną obserwowaną \\(x_i\\) a czynnikiem \\(f_j\\). Im wyższa wartość bezwzględna ładunku, tym większy udział danego czynnika w wyjaśnianiu zmienności konkretnej zmiennej. Na przykład w psychologii wysoki ładunek czynnika na zmiennej opisującej pamięć krótkotrwałą i na zmiennej opisującej zdolność rozwiązywania problemów matematycznych może sugerować, że obie cechy są przejawem wspólnego czynnika – inteligencji ogólnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#metody-estymacji-ładunków-czynnikowych",
    "href": "fa.html#metody-estymacji-ładunków-czynnikowych",
    "title": "Analiza czynnikowa",
    "section": "Metody estymacji ładunków czynnikowych",
    "text": "Metody estymacji ładunków czynnikowych\nMetoda największej wiarogodności (ang. Maximal Likelihood, ML)\nZałożenia\nZakładamy, że wektor zmiennych obserwowalnych\n\\[\n\\mathbf{x} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\Sigma),\n\\]\ngdzie kowariancja \\(\\Sigma\\) ma postać modelową \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nDla uproszczenia przyjmuje się często, że czynniki \\(\\mathbf{f}\\) są standaryzowane i nieskorelowane, czyli \\(\\Phi = I_m\\). Wówczas macierz kowariancji ma postać\n\\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nFunkcja wiarygodności\nDla próby \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) funkcja wiarygodności rozkładu normalnego wynosi\n\\[\nL(\\Lambda,\\Psi) = (2\\pi)^{-\\frac{np}{2}} |\\Sigma|^{-\\frac{n}{2}}\n\\exp\\left(-\\tfrac{1}{2}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{x}_i-\\mu)\\right).\n\\]\nczęściej wyrażana w postaci zlogarytmowanej\n\\[\n\\ell(\\Lambda,\\Psi) = -\\frac{n}{2} \\left[ \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) \\right] + C,\n\\]\ngdzie \\(S = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)(\\mathbf{x}_i-\\mu)^\\top\\) jest macierzą kowariancji z próby.\nEstymacja parametrów\nEstymatory \\(\\hat{\\Lambda}, \\hat{\\Psi}\\) dobiera się tak, aby maksymalizowały \\(\\ell(\\Lambda,\\Psi)\\), co odpowiada minimalizacji funkcji rozbieżności:\n\\[\nF(\\Lambda,\\Psi) = \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) - \\log |S| - p.\n\\]\nPowyższa miara rozbieżności powstaje z odległości Kullbacka-Leiblera między rozkładami normalnymi \\(\\mathcal{N}_p(\\mu, \\Sigma)\\) i \\(\\mathcal{N}_p(\\mu, S)\\) i jest równa dokładnie \\(2D_{KL}(S||\\Sigma)\\).\nProcedura obliczeniowa\nW praktyce:\n\nWybiera się liczbę \\(m\\) czynników2.\nUstala się początkowe wartości \\(\\Lambda, \\Psi\\)3.\nIteracyjnie poprawia się parametry, rozwiązując równania warunków pierwszego rzędu\n\n2 wybór liczby czynników zostanie przedstawiony nieco później3 spsoby ewstępnej estymacji zostaną omówione w dalszej części\\[\n\\frac{\\partial \\ell}{\\partial \\Lambda} = 0, \\quad \\frac{\\partial \\ell}{\\partial \\Psi} = 0.\n\\]\n\nTakie postępowanie iteracyjne prowadzi się aż do zbieżności funkcji wiarygodności.\nWłasności\n\nEstymatory ML są efektywne przy spełnieniu założenia o normalności wielowymiarowej danych pierwotnych.\nUmożliwiaja testy istotności liczby czynników:\n\nHipoteza \\(H_0: \\Sigma = \\Lambda\\Lambda^\\top + \\Psi\\) vs \\(H_1: \\Sigma\\) dowolna.\nStatystyka testowa ma w przybliżeniu rozkład \\(\\chi^2\\).\n\n\nPozwalają też konstruować przedziały ufności dla ładunków czynnikowych.\nOgraniczenia\n\nWymagaja dużej próby i spełnienia założenia normalności wielowymiarowej.\nMoże być numerycznie niestabilne, zwłaszcza gdy liczba czynników jest duża w stosunku do liczby zmiennych.\nPrzy małych próbach lub silnym naruszeniu normalności wyniki mogą być obciążone.\nMetoda osi głównych (ang. Principal Axis Factoring, PAF)\nIdea metody PAF\nW metodzie PAF znanej również jako metoda czynników głównych, zakładamy klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad \\mathrm{Cov}(\\mathbf{x}) = \\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nCelem jest znalezienie takiego \\(\\Lambda\\) i \\(\\Psi\\), aby zbliżyć się do macierzy kowariancji próbkowej \\(S\\). W odróżnieniu od ML, PAF nie opiera się na funkcji wiarygodności ani na rozbieżności Kullbacka–Leiblera, lecz maksymalizuje wariancję wspólną zmiennych, traktując część specyficzną \\((\\Psi)\\) jako resztę.\nMacierz zredukowanych korelacji\nW metodzie Principal Axis Factoring (PAF) kluczową rolę odgrywa macierz zredukowanych korelacji. Punktem wyjścia jest macierz korelacji \\(\\mathbf{R}\\) pomiędzy zmiennymi obserwowanymi \\(\\mathbf{x}\\). Na diagonali tej macierzy stoją jedynki, odzwierciedlające fakt, że każda zmienna jest w pełni skorelowana sama ze sobą. Jednak w modelu czynnikowym zakładamy, że całkowita wariancja zmiennej \\(x_j\\) może zostać podzielona na część wspólną (zasoby zmienności wspólnej - ang. communalities) i część swoistą (zasoby zmienności swoistej - ang. uniqness):\n\\[\n1 = h_j^2 + \\psi_j, \\quad j=1,\\dots,p,\n\\]\ngdzie \\(h_j^2\\) oznacza zasób zmienności wspólnej, a \\(\\psi_j\\) wariancję swoistą. W konstrukcji macierzy zredukowanych korelacji zamiast jedynek wstawia się w diagonali właśnie wartości \\(h_j^2\\). Otrzymujemy w ten sposób macierz\n\\[\n\\mathbf{R}^* = [r_{ij}^*], \\quad r_{jj}^* = h_j^2.\n\\]\nMacierz \\(\\mathbf{R}^*\\) ma więc charakter „zredukowany”, ponieważ na jej diagonali pozostaje tylko ta część wariancji zmiennej, którą model czynnikowy ma szansę wyjaśnić. Dzięki temu macierz ta może być przybliżana przez strukturę \\(\\Lambda \\Lambda^\\top\\), co odpowiada wspólnej wariancji wszystkich zmiennych.\n\n\n\n\n\n\nWstępne oszacowania zasobów zmienności wspólnej\n\n\n\nProblem polega na tym, że wartości \\(h_j^2\\) nie są znane a priori. Dlatego w praktyce stosuje się różne metody wstępnego ich wyznaczania, które mogą być następnie udoskonalane iteracyjnie w kolejnych krokach procedury PAF. Do najczęściej stosowanych metod należą:\n\nśrednia arytmetyczna współczynników korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\frac{1}{m}\\sum_{j'=1}^m r_{jj'},\\quad j\\ne j'\n\\]\nmaksymalna wartość bezwzględna współczynników korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\max_{j'}|r_{jj'}|, \\quad j\\ne j',\n\\]\nwspółczynnik determinacji wielokrotnej danej zmiennej z innymi zmiennymi (najczęściej stosowana i wykorzystywana przez R) \\[\nh_j^2=R^2_{j\\cdot 1,2,\\ldots,m},\n\\]\nformuła triad \\[\nh_j^2=\\frac{r_{jj'}r_{jj''}}{r_{j'j''}}, \\quad j\\ne j' \\ne j''\n\\] gdzie \\(r_{jj'}, r_{jj''}\\) - dwie najwyższe wartości współczynników korelacji \\(j\\)-tej zmiennej z innymi zmiennymi.\n\n\n\nRozkład na wartości własne\nW metodzie PAF zakładamy, że tylko część wariancji każdej zmiennej jest wspólna. Oznacza to, że zamiast pełnej macierzy korelacji \\(\\mathbf{R}\\), rozważamy macierz zredukowanych korelacji: \\[\n\\mathbf{R}^* = \\mathbf{R} - \\Psi,\n\\] gdzie na diagonali znajdują się oszacowane zasoby zmienności wspólnej \\(\\hat{h}_j^2\\), zamiast jedynek.\nNastępnie wykonujemy dekompozycję spektralną tej macierzy: \\[\n\\mathbf{R}^* = \\mathbf{Q}^* \\mathbf{D}^* {\\mathbf{Q}^*}^\\top,\n\\] gdzie \\(\\mathbf{Q}^*\\) i \\(\\mathbf{D}^*\\) są odpowiednio wektorami i wartościami własnymi macierzy \\(\\mathbf{R}^*\\).\nEstymator ładunków czynnikowych w PAF ma więc postać \\[\n\\hat{\\Lambda} = \\mathbf{Q}^*_m (\\mathbf{D}^*_m)^{1/2},\n\\]\nbazującą na zmodyfikowanej macierzy korelacji, w której uwzględniono oszacowane komunalności.\nPonieważ \\(\\hat{h}_j^2\\) same zależą od ładunków (są ich sumą kwadratów), w praktyce stosuje się procedurę iteracyjną: zaczynamy od pewnych wartości początkowych, obliczamy dekompozycję spektralną, aktualizujemy komunalności i powtarzamy procedurę aż do zbieżności.\nIteracyjna poprawa komunalności\nPonieważ początkowe komunalności są przybliżone, PAF stosuje procedurę iteracyjną:\n\nSzacujemy \\(\\Lambda\\) na podstawie bieżącego \\(\\mathbf{R}^*\\).\nObliczamy nowe zasoby zmienności wspólnej \\(h_j^2 = \\sum_{k=1}^m \\lambda_{jk}^2\\).\nWstawiamy je na przekątnej \\(\\mathbf{R}^*\\) zamiast starych wartości.\nPowtarzamy rozkład wartości własnych.\n\nProces powtarza się aż do zbieżności, czyli stabilizacji ładunków czynnikowych i zasobów zmienności wspólnej.\nWłasności\n\n\nDopasowanie do wariancji wspólnej – PAF minimalizuje różnice pomiędzy macierzą zredukowanych korelacji \\(\\mathbf{R}^*\\) a aproksymacją \\(\\Lambda \\Lambda^\\top\\). Sskupia się na wariancji wspólnej.\n\nIteracyjność oszacowań – estymatory w PAF powstają w procesie iteracyjnym, w którym kolejne przybliżenia komunalności są poprawiane na podstawie sumy kwadratów aktualnych ładunków czynnikowych. Dzięki temu metoda zbiega do rozwiązań lepiej oddających strukturę wspólną niż proste metody jednorazowe.\n\nNiestandaryzowana postać estymatorów – rozwiązania PAF mogą zależeć od przyjętych wartości początkowych \\(h_j^2\\). Różne wybory startowe mogą prowadzić do nieco innych estymatorów, choć w praktyce po kilku iteracjach zbieżność do stabilnego rozwiązania jest zazwyczaj dobra.\n\nInterpretowalność – ponieważ oszacowane ładunki czynnikowe odzwierciedlają wyłącznie część wspólną wariancji, interpretacja czynników uzyskanych metodą PAF jest bliższa teoretycznemu modelowi czynnikowemu niż w przypadku metod opartych na PCA.\nOgraniczenia\n\n\nBrak optymalności w sensie funkcji wiarygodności – w przeciwieństwie do metody największej wiarygodności (ML), estymatory PAF nie mają znanych własności asymptotycznych, takich jak efektywność czy zgodność w sensie probabilistycznym. Są bardziej heurystyczne niż ściśle statystyczne.\n\nZależność od wartości początkowych komunalności – oszacowania początkowe wpływają na przebieg iteracji i mogą prowadzić do lokalnych minimów. W praktyce wybór metody startowej (np. \\(R^2\\), średnia korelacja, …) ma znaczenie dla szybkości i stabilności algorytmu.\n\nMożliwość uzyskania ujemnych komunalności – w niektórych przypadkach iteracje mogą prowadzić do oszacowań \\(h_j^2 &lt; 0\\) (tzw. przypadek Haywooda), co jest sprzeczne z definicją wariancji wspólnej. Wówczas konieczne stosowanie innych metod estymacji ładunków.\n\nMniejsza przydatność przy małych próbach – ponieważ metoda nie opiera się na pełnym modelu statystycznym, jej własności są mniej stabilne przy niewielkich licznościach obserwacji. Wyniki mogą być wówczas silnie zależne od przypadkowych fluktuacji w danych.\n\nBrak testów statystycznych dopasowania modelu – w odróżnieniu od metody ML, PAF nie pozwala na formalne testowanie hipotez o liczbie czynników czy jakości dopasowania modelu do danych.\nMetoda sładowych głównych (ang. Principal Component Method)\nMetoda sładowych głównych należy do klasy metod wspólnotowych, czyli takich, które zakładają klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\quad \\Sigma = \\Lambda\\Lambda^\\top + \\Psi.\n\\]\nCelem jest oszacowanie macierzy ładunków \\(\\Lambda\\), tak aby jak najlepiej odtworzyć część wspólną wariancji.\nIdea metody\nW metodzie PCM zakładamy, że cała wariancja zmiennej jest wariancją wspólną, tzn. \\[\nh_j^2 = 1, \\quad j=1,\\ldots,p.\n\\]\nOznacza to, że macierz zredukowanych korelacji jest po prostu zwykłą macierzą korelacji \\(\\mathbf{R}\\): \\[\n\\mathbf{R} = \\Lambda \\Lambda^\\top + \\Psi,\n\\] przy czym w PCM przyjmujemy \\(\\Psi = \\mathbf{0}\\).\nNastępnie wykonujemy dekompozycję spektralną \\[\n\\mathbf{R} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^\\top,\n\\] gdzie:\n\n\n\\(\\mathbf{Q} = (q_1, q_2, \\ldots, q_p)\\) – to macierz ortonormalnych wektorów własnych,\n\n\\(\\mathbf{D} = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p)\\) – to macierz wartości własnych uporządkowanych malejąco.\n\nJeśli chcemy oszacować model z \\(m\\) czynnikami, to bierzemy największe \\(m\\) wartości własne i odpowiadające im wektory własne. Estymator ładunków czynnikowych jest wtedy równy \\[\n\\hat{\\Lambda} = \\mathbf{Q}_m \\mathbf{D}_m^{1/2},\n\\] gdzie \\(\\mathbf{Q}_m = (q_1,\\ldots,q_m)\\), a \\(\\mathbf{D}_m = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_m)\\).\nWidzimy więc, że w PCM ładunki są wprost pierwiastkami z największych wartości własnych pomnożonymi przez odpowiadające im wektory własne.\nProcedura estymacji4\n\n\nKonstruujemy macierz korelacji \\(\\mathbf{R}\\).\nObliczamy rozkład wartości i wektorów własnych macierzy \\(\\mathbf{R}\\).\nWybieramy \\(m\\) największych wartości własnych (odpowiadających liczbie czynników w modelu).\nNa tej podstawie konstruujemy macierz ładunków czynnikowych \\(\\Lambda\\).\n4 tu widać największą różnicę pomięcy PCM a PAF; w metodzie PCM występuję jedna iteracja estymacji ładunkówWłasności\n\n\nZgodność z modelem czynnikowym – metoda dąży do aproksymacji struktury wspólnej wariancji, a nie całkowitej wariancji.\n\nZbieżność do stabilnych oszacowań – iteracyjne poprawki komunalności pozwalają uzyskać estymatory spójne z założeniami modelu.\n\nŁatwość interpretacji – podobnie jak PCA, metoda bazuje na analizie spektralnej wartości własnych, co ułatwia intuicyjne rozumienie struktury danych.\nOgraniczenia\n\n\nBrak optymalności statystycznej – podobnie jak PAF, metoda nie ma własności estymatorów opartych na funkcji wiarygodności (ML).\n\nZależność od początkowych oszacowań komunalności – nieprawidłowy wybór startowy może utrudnić uzyskanie sensownych rozwiązań.\n\nHaywood case – zdarza się, że zasoby zmienności wspólnej mogą przyjmować wartości ujemne.\nMetoda minimalizacji reszt (ang. MINRES)\nIdea metody MINRES\nW modelu czynnikowym przyjmujemy, że macierz kowariancji (lub korelacji) ma postać \\[\n\\Sigma = \\Lambda \\Lambda' + \\Psi,\n\\] gdzie \\(\\Lambda\\) to macierz ładunków czynnikowych, a \\(\\Psi = \\mathrm{diag}(\\psi_1,\\ldots,\\psi_p)\\) to macierz wariancji swoistych.\nW metodzie MINRES nie próbujemy dokładnie odtworzyć całej macierzy \\(\\Sigma\\). Zamiast tego minimalizujemy reszty pozadiagonalne, czyli różnice między obserwowaną macierzą korelacji \\(\\mathbf{R}\\) a macierzą odtworzoną z modelu \\(\\Lambda \\Lambda^\\top + \\Psi\\), przy czym skupiamy się wyłącznie na elementach pozadiagonalnych.\nFunkcja kryterialna\nFormalnie minimalizowana jest suma kwadratów reszt poza przekątną \\[\nF(\\Lambda, \\Psi) = \\sum_{i \\neq j} \\Big( r_{ij} - \\hat{r}_{ij} \\Big)^2,\n\\] gdzie:\n\n\n\\(r_{ij}\\) to element macierzy korelacji empirycznej \\(\\mathbf{R}\\),\n\n\\(\\hat{r}_{ij}\\) to element macierzy odtworzonej \\(\\Lambda \\Lambda^\\top + \\Psi\\),\nelementy diagonalne nie są uwzględniane (bo zawsze odtwarzane są przez normalizację zmiennych).\n\nMożna to zapisać równoważnie jako \\[\nF(\\Lambda) = | \\mathbf{R} - (\\Lambda \\Lambda' + \\Psi)|^2_{off},\n\\] gdzie \\(|\\cdot|_{off}\\) oznacza normę Frobeniusa liczona tylko na częściach pozadiagonalnych macierzy.\nProcedura estymacyjna\n\nZaczynamy od przybliżonych wartości komunalności \\(\\hat{h}_j^2\\), tak jak w PAF.\nBudujemy macierz reszt \\[\n\\mathbf{U} = \\mathbf{R} - (\\Lambda \\Lambda^\\top + \\Psi).\n\\]\n\nSzukamy takich ładunków \\(\\Lambda\\), które minimalizują sumę kwadratów elementów \\(\\mathbf{U}\\) poza przekątną.\nW praktyce problem redukuje się do iteracyjnego rozwiązywania układów równań własnych, bardzo podobnie jak w PAF, ale z innym warunkiem minimalizacji (PAF dopasowuje wartości własne macierzy zredukowanych korelacji, MINRES – reszty pozadiagonalne).\nZwiązek z dekompozycją spektralną\nW przeciwieństwie do PCM czy PAF, metoda MINRES nie ma bezpośredniego prostego rozwiązania w postaci pierwiastków z wartości własnych. Wymaga zastosowania iteracyjnych algorytmów numerycznych, które szukają \\(\\Lambda\\) minimalizującej \\(F(\\Lambda)\\). Jednak podobnie jak w PAF, punktem startowym mogą być wektory własne macierzy zredukowanych korelacji. Następnie algorytm minimalizacji dopasowuje ładunki tak, by reszty pozadiagonalne były jak najmniejsze.\nWłaściwości i ograniczenia\n\n\nMINRES skupia się tylko na korelacjach pomiędzy zmiennymi, ignorując elementy diagonalne – co sprawia, że estymacja jest mniej wrażliwa na problem ujemnych komunalności (tzw. Heywood cases).\nMetoda jest relatywnie stabilna numerycznie i dobrze sprawdza się przy dużej liczbie zmiennych.\nOgraniczeniem jest to, że wynik zależy od jakości początkowych oszacowań zasobów zmienności wspólnej. Przy złym wyborze startu możliwa jest wolna zbieżność albo zbieżność do lokalnego minimum.\nMetoda uogólnionych najmniejszych kwadratów (ang. Generalized Least Squares, GLS)\nIdea metody\nGLS, podobnie jak MINRES czy ML, polega na porównaniu macierzy obserwowanej \\(\\mathbf{S}\\) (kowariancji lub korelacji) z macierzą odtworzoną przez model czynnikowy \\(\\hat{\\Sigma} = \\Lambda \\Lambda^\\top + \\Psi\\). Różnica w stosunku do MINRES polega na tym, że w GLS ważymy reszty, czyli błędy odwzorowania poszczególnych elementów macierzy \\(\\mathbf{S}\\).\nFormalnie kryterium minimalizacji ma postać \\[\nF_{\\text{GLS}}(\\Lambda, \\Psi) = \\mathrm{tr}\\Big[ \\big( S - \\hat{\\Sigma} \\big) W \\big( S - \\hat{\\Sigma} \\big) W \\Big],\n\\]\ngdzie \\(W\\) to macierz wag, zwykle przyjmowana jako odwrotność (lub pseudoodwrotność) wariancji estymatora elementów macierzy \\(\\mathbf{S}\\).\nW przeciwieństwie do MINRES (gdzie wszystkie reszty traktowane są jednakowo), w GLS różne elementy macierzy kowariancji otrzymują różne wagi. Wagi te wynikają z asymptotycznych własności estymatora macierzy kowariancji i uwzględniają fakt, że elementy macierzy nie są niezależne i mają różne wariancje. Dzięki temu GLS jest bardziej efektywny statystycznie niż MINRES, ale jednocześnie mniej wymagający niż ML (który zakłada pełną normalność wielowymiarową).\nWłasności\n\nEstymatory GLS są spójne i asymptotycznie efektywne w klasie metod najmniejszych kwadratów, przy założeniu poprawnej specyfikacji modelu.\nGLS, podobnie jak ML, uwzględnia strukturę wariancji elementów macierzy \\(\\mathbf{S}\\), co czyni go bardziej precyzyjnym niż MINRES.\nZ drugiej strony GLS jest mniej czuły na naruszenie założenia normalności niż ML, dlatego bywa rekomendowany przy większych odchyleniach od normalności.\nOgraniczenia\n\nProcedura GLS jest obliczeniowo trudniejsza niż MINRES, ponieważ wymaga oszacowania (lub przyjęcia) odpowiedniej macierzy wag.\nW praktyce GLS bywa niestabilny przy małych próbach lub przy silnych współliniowościach zmiennych.\nW implementacjach programowych często stosuje się GLS jako kompromis pomiędzy prostym MINRES a wymagającym ML.\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nIstnieją również inne metody estymacji ładunków czynnikowych, jak metody bayesowskie, czy metody z regularyzacją LASSO ale nie są one częścią tego opracowania.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynników",
    "href": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynników",
    "title": "Analiza czynnikowa",
    "section": "Oceny dopasowania modelu i kryteria doboru liczby czynników",
    "text": "Oceny dopasowania modelu i kryteria doboru liczby czynników\nOcena dopasowania modelu EFA opiera się na kilku uzupełniających się perspektywach: globalnym dopasowaniu implikowanej macierzy kowariancji do macierzy empirycznej, analizie reszt korelacyjnych, doborze liczby czynników, stabilności rozwiązania oraz jakości lokalnej (ładunki i zasoby zmienności wspólnej). Poniżej przedstawiam najważniejsze procedury wraz z ich interpretacją oraz typowymi pułapkami.\nProporcja wyjaśnionej wariancji przez czynniki\nProporcja wariancji wyjaśnionej przez model czynnikowy, czyli stosunek sumy wariancji wspólnej do całkowitej wariancji wszystkich zmiennych, stanowi podstawową miarę jakości dopasowania. W przypadku standaryzowanych zmiennych całkowita wariancja wynosi \\(p\\), więc proporcja ta ma postać \\[\n\\text{Proporcja wyjaśnionej wariancji} = \\frac{\\sum_{j=1}^p h_j^2}{p}.\n\\] Wyższe wartości (np. powyżej \\(0,6\\)) wskazują na dobrą reprezentację zmiennych przez czynniki, natomiast niskie wartości (np. poniżej \\(0,4\\)) sugerują, że model nie uchwytuje istotnej części struktury danych. Jednak sama proporcja nie uwzględnia liczby czynników ani złożoności modelu, dlatego powinna być interpretowana w kontekście innych wskaźników dopasowania.\nTest chi-kwadrat\nW metodzie ML został przedstawiony test dopasowania oparty na maximum likelihood. Przy założeniu normalności wielowymiarowej i zidentyfikowanym modelu postaci \\[\n\\Sigma=\\Lambda\\Lambda^\\top + \\Psi\n\\] testujemy hipotezę \\(H_0:\\ \\Sigma(\\Lambda,\\Psi)=S\\) w populacji, gdzie \\(S\\) oznacza macierz kowariancji (lub korelacji) z próby. Statystyka \\(\\chi^2\\) rośnie wraz z pogarszającym się dopasowaniem (niestety duże próby sprzyjają odrzucaniu nawet dobrze dopasowanych modeli, a naruszenia normalności mogą zawyżać lub zaniżać wynik).\nWskaźnik RMSEA\nWskaźnik root mean square error of approximation (RMSEA) mierzy błąd aproksymacji na jednostkę stopnia swobody i można go interpretować jako „błąd w populacji”, nie tylko w próbie. Definiujemy go jako \\[\n\\mathrm{RMSEA}=\\sqrt{\\max\\left\\{\\frac{\\chi^2-df}{df(n-1)},0\\right\\}},\n\\] a ocenę uzupełniamy o przedział ufności oparty na niecentralnym rozkładzie chi-kwadrat. Wartości rzędu \\(0,05-0,08\\) tradycyjnie uznawane są za akceptowalne, traktując progi orientacyjnie: wzrost liczby zmiennych i stopni swobody sprzyja niższym RMSEA, natomiast małe próby destabilizują oszacowanie.\nAnaliza reszt\nAnaliza reszt macierzy korelacji stanowi podstawową kontrolę lokalnego dopasowania, niezależnie od sposobu estymacji. Wyznaczamy reszty \\(r_{ij}-\\hat r_{ij}\\) i przeglądamy rozkład wartości bezwzględnych, a dokładnie odsetek przekraczających praktyczne progi (np. \\(0,05\\)). Wskaźniki zbiorcze, takie jak RMSR (root mean square residual) oraz SRMR (standardized RMSR), agregują wielkość reszt poza diagonalą - mniejsze wartości świadczą o lepszym dopasowaniu. Mapa ciepła reszt ułatwia wykrywanie klastrów niedopasowania sugerujących brakujący czynnik lub zbyt małą liczbę czynników.\nKryteria informacyjne\nKryteria informacyjne, takie jak AIC i BIC, służą do porównywania modeli o różnej liczbie czynników, karząc nadmierną złożoność. Definiujemy je przez logarytm funkcji wiarogodności i liczbę parametrów. BIC silniej faworyzuje prostsze modele przy dużych próbach. Bardzo ważne jest aby używać tych metod do porównywania modeli otrzymanych tą samą metodą.\nInne wskaźniki dopasowania\nWskaźniki „globalne” starszej generacji, takie jak GFI i AGFI (goodness of fit index, adjusted GFI), oceniają proporcję wariancji/kowariancji wyjaśnionej przez model. Są wrażliwe na rozmiar próby i liczbę zmiennych, skłonne do optymizmu w dużych modelach i do pesymizmu przy małej liczbie stopni swobody. Możemy je traktować pomocniczo, kładąc większy nacisk na RMSEA oraz analizę reszt.\nAnaliza wartości własnych macierzy reszt uzupełnia powyższe podejścia. Po wyodrębnieniu \\(m\\) czynników obliczamy resztową macierz korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) i badać jej wartości własne. Duże dodatnie wartości własne sygnalizują pozostawioną wspólną wariancję (niedomiar czynników) lub struktury lokalne.\nJakość lokalną rozwiązania oceniać przez zasoby zmienności wspólnej i swoistej. \\[\nh_j^2=\\sum_{k=1}^{m}\\lambda_{jk}^{2}\n\\] mierzą część wariancji zmiennej \\(x_j\\) wyjaśnioną przez czynniki, bardzo niskie \\(h_j^2\\) wskazują słabą reprezentację zmiennej, natomiast bardzo wysokie — wraz z ryzykiem ujemnych \\(\\Psi_j\\) (przypadki Haywooda) — mogą sygnalizować dopasowanie wymuszone lub niewłaściwą liczebność czynników. Sumy kwadratów ładunków per czynnik odzwierciedlają wyjaśnioną wspólną wariancję i służą do oceny równomierności wkładu czynników.\nW rozwiązaniach dopuszczajacych korelacje pomiedzy czynnikami dodatkowym aspektem dopasowania jest macierz korelacji czynników \\(\\Phi\\). Bardzo wysokie korelacje między czynnikami sugerują nadmiarowość i potencjalne przeparametryzowanie. Wówczas warto rozważyć redukcję liczby czynników lub alternatywne struktury.\nNajbardziej znane kryteria doboru liczby czynników to:\nKryterium wykresu osypiska (Scree plot, Cattell)\nNa osi poziomej odkładamy kolejne wartości własne, a na pionowej ich wielkość. Punktem granicznym jest miejsce, gdzie wykres „załamuje się” i przechodzi w „osypisko” – od tego miejsca czynniki interpretowane są jako szum.\n\nZalety: wizualna intuicja, łatwe zastosowanie.\nWady: często subiektywność w określeniu miejsca „łokcia”, szczególnie gdy krzywa nie ma wyraźnego załamania.\nAnaliza równoległa (Parallel analysis, Horn)\nPolega na porównaniu wartości własnych dla danych empirycznych z wartościami własnymi uzyskanymi dla danych losowych o tej samej strukturze (ta sama liczba zmiennych i obserwacji). Zatrzymuje się te czynniki, których wartości własne przewyższają np. 95. percentyl rozkładu wartości losowych.\n\nZalety: jedna z najbardziej rekomendowanych metod, dobrze sprawdza się w praktyce.\nWady: wymaga procedur symulacyjnych, większej mocy obliczeniowej.\nKryterium MAP (Minimum Average Partial, Velicer)\nOpiera się na analizie korelacji cząstkowych. Stopniowo usuwa się kolejne czynniki, a następnie oblicza średnią wartość kwadratu korelacji cząstkowych. Liczba czynników odpowiadająca minimum tej wartości uznawana jest za optymalną.\n\nZalety: metoda oparta na minimalizacji resztowych zależności, obiektywna.\nWady: wrażliwa na naruszenia założeń modelu, mniej intuicyjna dla początkujących.\nTesty statystyczne dopasowania (dla ML)\nPrzy estymacji metodą największej wiarygodności można zastosować test chi-kwadrat dla porównania modelu z \\(m\\) czynnikami z modelem pełnym. Sprawdza się, czy macierz implikowana przez model różni się istotnie od empirycznej. Liczbę czynników dobiera się tak, aby model był jeszcze akceptowalny, ale nie przeparametryzowany.\n\nZalety: formalne podejście statystyczne.\nWady: silna wrażliwość na liczność próby i założenie normalności wielowymiarowej; w dużych próbach nawet dobre modele mogą być odrzucane.\nKryteria informacyjne (AIC, BIC, CAIC)\nPorównują modele o różnej liczbie czynników, równoważąc dopasowanie (log-wiarygodność) i złożoność (liczbę parametrów). Optymalna liczba czynników to ta, dla której wartość kryterium jest minimalna.\n\nZalety: uwzględniają karę za nadmierną złożoność, dobrze sprawdzają się w porównaniach.\nWady: wartości kryteriów są zależne od metody estymacji, więc porównywać można tylko modele oszacowane tą samą metodą.\nAnaliza reszt i spektrum wartości własnych macierzy reszt\nPo przyjęciu liczby czynników oblicza się macierz reszt korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) . Jeśli w resztach (poza przekątną) pozostają duże (co do wartości bezwzględnej) wartości własne, oznacza to, że nie wszystkie wspólne zależności zostały uchwycone i potrzebne są dodatkowe czynniki.\n\nZalety: pozwala ocenić niedopasowanie „lokalne” i strukturalne.\nWady: wymaga bardziej zaawansowanej interpretacji.\nUdział wyjaśnionej wariancji\nW praktyce często wymaga się, aby całkowita wyjaśniona wariancja przekraczała określony próg (np. 50% w naukach społecznych). Dodatkowo analizuje się równomierność wkładu poszczególnych czynników.\n\nZalety: intuicyjne i łatwe do raportowania.\nWady: arbitralne progi, zależne od liczby zmiennych i kontekstu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#eksploracyjna-analiza-czynnikowa",
    "href": "fa.html#eksploracyjna-analiza-czynnikowa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) – wektor zmiennych obserwowalnych,\n\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) – wektor średnich,\n\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) – macierz ładunków czynnikowych, której element \\(\\lambda_{ij}\\) opisuje wpływ czynnika \\(j\\) na zmienną \\(i\\),\n\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) – wektor czynników latentnych (czynników wspólnych),\n\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) – wektor składników specyficznych (unikalnych, błędów pomiaru).\n\nZałożenia klasycznego modelu EFA\n\nRozkład czynników wspólnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne mają średnią zero i macierz kowariancji równą macierzy jednostkowej. To założenie oznacza, że czynniki są nieskorelowane i mają wariancję jednostkową (jest to standaryzacja wprowadzona dla identyfikowalności modelu).\nRozkład składników specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzą diagonalną o elementach dodatnich. Oznacza to, że błędy są nieskorelowane między sobą oraz niezależne od czynników \\(\\mathbf{f}\\).\nNiezależność czynników i błędów \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyższej konstrukcji wynika, że kowariancja zmiennych obserwowalnych jest sumą części wspólnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowód\n\n\n\nNiech losowy wektor obserwacji ma postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynników wspólnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor składników specyficznych. Zakładamy, że \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazać, że \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczególności przy \\(\\Phi=I_m\\), że mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a ponieważ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancję \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyrażamy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] Korzystając z liniowości kowariancji i tożsamości \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorów losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skończonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDzięki założeniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikają i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] Jeśli dodatkowo przyjmiemy standardyzację czynników \\(\\Phi=I_m\\) (co jest konwencją identyfikacyjną modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego należało dowieść.\nWarto odnotować, że dowód nie wymaga niezależności \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym — wystarcza nieskorelowanie, aby zniknęły składniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postać \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to można zastosować tzw. whitening czynników \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co równoważnie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), które prowadzą do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to związane z możliwością przeprowadzania różnych transformacji czynników bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogólnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeżeli dokonamy transformacji ortogonalnej czynników \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzą ortogonalną, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, że \\(\\Lambda\\) i \\(\\Phi\\) nie są jednoznacznie wyznaczone. Różne pary \\((\\Lambda, \\Phi)\\) mogą prowadzić do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczególności wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jedną z takich równoważnych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalną rolę, ponieważ jest miejscem, w którym spotykają się dwa składniki zmienności: wspólna i specyficzna. Rozkład \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, że całkowita wariancja i kowariancja obserwowanych zmiennych może być przedstawiona jako suma efektu wspólnych czynników oraz efektu specyficznego, indywidualnego dla każdej zmiennej.\nCzęść \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspólne źródło zmienności, czyli wariancję wyjaśnianą przez czynniki ukryte. To właśnie ta część umożliwia redukcję wymiaru – wiele zmiennych obserwowanych można sprowadzić do kilku czynników, które reprezentują główną strukturę zależności. Interpretacja czynników jako ukrytych wymiarów (np. inteligencja, poziom lęku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uprościć analizę, ale także nadać jej znaczenie teoretyczne w danej dziedzinie badań.\nZ kolei \\(\\Psi\\) odpowiada za wariancję unikalną, czyli tę część zmienności, która nie jest współdzielona z innymi zmiennymi. Obejmuje ona zarówno wariancję czysto specyficzną dla danej cechy, jak i wariancję błędu pomiarowego. Dzięki temu możliwe jest odróżnienie struktury głębokiej (czynnikowej) od elementów przypadkowych i indywidualnych.\nPodsumowując, znaczenie modelu czynnikowego polega na tym, że pozwala on wydzielić istotne, ukryte mechanizmy stojące za współzależnościami zmiennych i oddzielić je od szumów specyficznych dla pojedynczych obserwacji. W praktyce oznacza to możliwość redukcji liczby analizowanych zmiennych, uproszczenie opisu złożonych danych i pogłębienie interpretacji zjawisk społecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynników w praktyce opiera się przede wszystkim na analizie macierzy ładunków czynnikowych \\(\\Lambda\\). Każdy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile związku pomiędzy zmienną obserwowaną \\(x_i\\) a czynnikiem \\(f_j\\). Im wyższa wartość bezwzględna ładunku, tym większy udział danego czynnika w wyjaśnianiu zmienności konkretnej zmiennej. Na przykład w psychologii wysoki ładunek czynnika na zmiennej opisującej pamięć krótkotrwałą i na zmiennej opisującej zdolność rozwiązywania problemów matematycznych może sugerować, że obie cechy są przejawem wspólnego czynnika – inteligencji ogólnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#rotacje-czynników",
    "href": "fa.html#rotacje-czynników",
    "title": "Analiza czynnikowa",
    "section": "Rotacje czynników",
    "text": "Rotacje czynników\nRotacja czynników jest etapem analizy czynnikowej, którego celem jest poprawa interpretowalności rozwiązania poprzez uproszczenie struktury ładunków czynnikowych. Matematycznie polega ona na zastosowaniu transformacji liniowej do macierzy ładunków \\(\\Lambda\\). Jeśli \\(\\Lambda\\) jest macierzą \\(p \\times m\\) ładunków (gdzie \\(p\\) to liczba zmiennych, a \\(m\\) liczba czynników), to po rotacji otrzymujemy nową macierz ładunków \\[\n\\Lambda^* = \\Lambda T,\n\\] gdzie \\(T\\) jest macierzą transformacji rotacyjnej o wymiarach \\(m \\times m\\). W zależności od własności macierzy \\(T\\) wyróżnia się dwa główne typy rotacji: ortogonalne i skośne (oblique).\nRotacje ortogonalne\nW przypadku rotacji ortogonalnych macierz \\(T\\) jest macierzą ortogonalną, czyli spełnia warunek: \\[T^\\top T = TT^\\top = I_m.\\] Oznacza to, że czynniki po rotacji pozostają nieskorelowane (\\(\\Phi = I_m\\)).\nNajważniejsze rodzaje rotacji ortogonalnych:\n\nVarimax (Kaiser, 1958) - najczęściej stosowana rotacja ortogonalna. Maksymalizuje wariancję kwadratów ładunków w ramach każdego czynnika. Prowadzi do tego, że każda zmienna ma wysokie ładunki tylko na jednym czynniku, a bliskie zeru na pozostałych. Funkcja celu \\[\nV = \\sum_{j=1}^m \\left[ \\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*4} - \\left(\\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*2}\\right)^2 \\right].\n\\]\nQuartimax - minimalizuje liczbę czynników potrzebnych do opisania każdej zmiennej, upraszczając wiersze macierzy ładunków. Funkcja celu \\[\nQ = \\sum_{i=1}^p \\sum_{j=1}^m \\lambda_{ij}^{*4}.\n\\]\nEquamax - łączy idee varimax i quartimax. Celem jest równoważenie prostoty struktur wierszy i kolumn macierzy ładunków. Funkcja celu \\[\nE = \\frac12(Q + V).\n\\]\nBiquartimax - celem tej rotacji jest jednoczesne uproszczenie wierszy i kolumn macierzy ładunków. W praktyce łączy zalety varimax i quartimax. Funkcja celu \\[\nBQ = \\alpha \\, Q + (1 - \\alpha) \\, V,\n\\] z modyfikacją wag, które równoważą wpływ prostoty wierszy i kolumn. Zmienne mają tendencję do ładowania się mocno na jednym czynniku (jak w varimax), ale jednocześnie ogranicza się sytuacje, w których jedna zmienna ma średnie ładunki na wielu czynnikach (jak w quartimax).\nRotacje skośne (oblique)\nW przypadku rotacji skośnych macierz \\(T\\) nie musi być ortogonalna, więc \\[\nT^\\top T \\neq I_m.\n\\] W efekcie rotowane czynniki mogą być skorelowane, a macierz korelacji czynników \\(\\Phi\\) przyjmuje ogólną postać dodatnio określoną.\nPodstawowe rodzaje:\n\nOblimin (Jennrich & Sampson, 1966) - rodzina rotacji z parametrem \\(\\gamma\\), który reguluje stopień skośności. Dla \\(\\gamma = 0\\) rozwiązanie staje się quartimax, a większe \\(\\gamma\\) prowadzą do większej korelacji czynników. Funkcja celu \\[\nF(\\Lambda^*) = \\sum_{i=1}^p \\sum_{j=1}^m \\left(\\lambda_{ij}^{*2} - \\gamma \\frac{\\sum_{k=1}^m \\lambda_{ik}^{*2}}{m}\\right)^2.\n\\]\nPromax (Hendrickson & White, 1964) - rotacja skośna oparta na prostym podejściu dwustopniowym. Najpierw stosuje się rotację ortogonalną (najczęściej varimax), następnie ładunki są podnoszone do potęgi \\(k\\) (zwykle 3 lub 4), aby wymusić prostą strukturę, i ponownie dopasowywane przy użyciu metody najmniejszych kwadratów \\[\n\\tilde{\\lambda}{jk} = \\text{sign}(\\lambda^*_{jk}) \\cdot |\\lambda^*_{jk}|^p.\n\\] Rotacja promax pozwala uzyskać bardziej realistyczne struktury, gdy czynniki są rzeczywiście skorelowane.\nGeomin (Yates, 1987) - minimalizuje średnią geometryczną kwadratów ładunków, co prowadzi do sytuacji, w której każda zmienna ma niewiele istotnych ładunków. Funkcja celu \\[\nG(\\Lambda^*) = \\sum_{i=1}^p \\left( \\prod_{j=1}^m (\\lambda_{ij}^{*2} + \\epsilon) \\right)^{1/m},\n\\] gdzie \\(\\epsilon\\) to mały parametr stabilizujący.\nSimplimax (Kiers, 1994) - uogólnienie kryteriów prostoty, które minimalizuje liczbę dużych i małych ładunków w macierzy, pozwalając użytkownikowi sterować liczbą „prostych” elementów.\nWybór rodzaju rotacji\n\nRotacje ortogonalne są preferowane, gdy zakładamy, że czynniki powinny być niezależne teoretycznie.\nRotacje skośne stosuje się, gdy istnieje uzasadnienie, że czynniki mogą być skorelowane (co jest częste w naukach społecznych, psychologii czy biologii).\n\n\nNa potrzeby ilustracji budowy modelu EFA wykorzystamy dane z pakietu psych, które zawierają wyniki różnych testów poznawczych (Harman, 1976). Dane te są często używane jako przykład w literaturze dotyczącej analizy czynnikowej.\n\nKodlibrary(psych)\n\n# Dane: macierz korelacji testów poznawczych (Harman, 1976)\ndata(\"Harman74.cor\")\n\n\n\n\n\n\n\n\n\nZmienna\nOpis\nKategoria testu\n\n\n\nVisualPerception\nRozpoznawanie i analiza relacji przestrzennych w figurach\nZdolności przestrzenne / percepcyjne\n\n\nCubes\nManipulacja wyobrażeniowa brył, rotacje przestrzenne\nZdolności przestrzenne\n\n\nPaperFormBoard\nSkładanie i dopasowywanie elementów figur\nZdolności przestrzenne\n\n\nFlags\nRozpoznawanie wzorów i relacji symboli\nPercepcja wzrokowa / logiczne\n\n\nGeneralInformation\nOgólna wiedza faktograficzna\nZdolności werbalne\n\n\nPargraphComprehension\nRozumienie tekstów pisanych\nZdolności werbalne\n\n\nSentenceCompletion\nUzupełnianie zdań brakującymi słowami\nZdolności werbalne\n\n\nWordClassification\nGrupowanie słów według znaczenia\nZdolności werbalne / semantyczne\n\n\nWordMeaning\nZnajomość i rozumienie znaczeń słów\nZdolności werbalne\n\n\nAddition\nWykonywanie prostych działań arytmetycznych\nZdolności numeryczne\n\n\nCode\nDopasowywanie symboli do liczb według klucza\nSzybkość przetwarzania / percepcja\n\n\nCountingDots\nLiczenie elementów wzrokowych\nSzybkość percepcji / numeryczne\n\n\nStraightCurvedCapitals\nRozpoznawanie prostych i zakrzywionych liter\nPercepcja wizualna / szybkość\n\n\nWordRecognition\nRozpoznawanie słów z listy\nPamięć i zdolności werbalne\n\n\nNumberRecognition\nRozpoznawanie liczb z listy\nPamięć / percepcja numeryczna\n\n\nFigureRecognition\nRozpoznawanie i identyfikacja figur\nPamięć wizualna / percepcja\n\n\nObjectNumber\nDopasowywanie obiektów do liczb\nZłożone zdolności percepcyjno-num.\n\n\nNumberFigure\nDopasowywanie liczb do figur\nZłożone zdolności percepcyjno-num.\n\n\nFigureWord\nDopasowywanie figur do słów\nŁączenie informacji wizualno-werbalnych\n\n\nDeduction\nRozwiązywanie zadań logicznych, wnioskowanie\nRozumowanie logiczne\n\n\nNumericalPuzzles\nZadania numeryczne o charakterze problemowym\nZdolności numeryczne / logiczne\n\n\nProblemReasoning\nRozwiązywanie złożonych problemów\nRozumowanie ogólne\n\n\nSeriesCompletion\nUzupełnianie szeregów logicznych lub numerycznych\nRozumowanie abstrakcyjne / numeryczne\n\n\nArithmeticProblems\nRozwiązywanie zadań arytmetycznych o większej trudności\nZdolności numeryczne\n\n\n\nWidać, że testy można grupować w pięć głównych obszarów: przestrzenne/percepcyjne (np. Cubes, VisualPerception), werbalne (np. WordMeaning, SentenceCompletion), numeryczne (np. Addition, ArithmeticProblems), pamięciowe (np. WordRecognition, NumberRecognition), oraz rozumowania i logiczne (np. Deduction, SeriesCompletion). To właśnie takie powiązania w macierzy korelacji uzasadniają zastosowanie analizy czynnikowej w celu identyfikacji ukrytych wymiarów inteligencji.\nNajpierw sprawdzimy czy dane nadają się do analizy czynnikowej, obliczając test KMO i test sferyczności Bartletta.\n\nKodlibrary(tidyverse)\nlibrary(easystats)\n\ncheck_factorstructure(Harman74.cor$cov, n = 145) \n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(276) = 1545.86, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.88). The individual KMO scores are: VisualPerception (0.90), Cubes (0.84), PaperFormBoard (0.78), Flags (0.85), GeneralInformation (0.88), PargraphComprehension (0.89), SentenceCompletion (0.89), WordClassification (0.92), WordMeaning (0.88), Addition (0.81), Code (0.85), CountingDots (0.84), StraightCurvedCapitals (0.89), WordRecognition (0.85), NumberRecognition (0.88), FigureRecognition (0.89), ObjectNumber (0.85), NumberFigure (0.88), FigureWord (0.83), Deduction (0.93), NumericalPuzzles (0.91), ProblemReasoning (0.93), SeriesCompletion (0.91), ArithmeticProblems (0.92).\n\n\nTest sferyczności Bartletta dostarcza podstawowego potwierdzenia, że w zbiorze danych występują istotne statystycznie korelacje pomiędzy zmiennymi. Wynik \\(\\chi^2(276) = 1545.86,\\ p &lt; 0.001\\) oznacza, że hipoteza zerowa o macierzy korelacji równej macierzy jednostkowej zostaje odrzucona. Innymi słowy, zmienne nie są niezależne, a ich struktura korelacyjna uzasadnia dalsze poszukiwanie wspólnych czynników. Gdyby test okazał się nieistotny, sugerowałby brak uzasadnienia do stosowania analizy czynnikowej, ponieważ nie byłoby wystarczających zależności między zmiennymi.\nMiara adekwatności próby KMO (Kaiser–Meyer–Olkin) wskazuje, na ile obserwowane korelacje mogą być wyjaśnione przez czynniki wspólne w porównaniu z korelacjami cząstkowymi. Wynik ogólny KMO = 0.88 mieści się w przedziale uznawanym za „bardzo dobry” (powyżej 0.80). Oznacza to, że dane dobrze nadają się do analizy czynnikowej i możemy oczekiwać stabilnych, interpretowalnych rozwiązań. Wartości indywidualne dla poszczególnych zmiennych mieszczą się między 0.78 a 0.93, a więc wszystkie osiągają poziom „dobry” lub „bardzo dobry”. Najwyższe wartości, takie jak Deduction (0.93), ProblemReasoning (0.93) czy ArithmeticProblems (0.92), wskazują na wyjątkowo silną reprezentację tych testów w przestrzeni czynnikowej. Z kolei najniższe, jak PaperFormBoard (0.78), są nadal akceptowalne, ale sugerują nieco słabszą integrację tej zmiennej z pozostałymi. Całościowo zarówno wynik globalny, jak i rozkład wartości cząstkowych KMO jednoznacznie potwierdzają zasadność prowadzenia analizy czynnikowej na tym zbiorze danych.\n\nKod# Parallel analysis\nfa.parallel(Harman74.cor$cov, n.obs = 145, fa = \"fa\")\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\nSamo kryterium paralelne wskazuje na 4 czynniki, choć gdyby brać pod uwagę samo kryterium osypiska to rozwiązanie z 5 czynnikami też wydaje się być właściwe.\n\nKod# Kryterium MAP\nVSS(Harman74.cor$cov, n.obs = 145, plot = F)\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.8  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.85  with  2  factors\n\nThe Velicer MAP achieves a minimum of 0.02  with  4  factors \nBIC achieves a minimum of  -731.36  with  3  factors\nSample Size adjusted BIC achieves a minimum of  -112  with  5  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq    prob sqresid  fit RMSEA  BIC SABIC complex\n1 0.80 0.00 0.025 252   626 8.0e-34    16.8 0.80 0.101 -628   170     1.0\n2 0.55 0.85 0.022 229   428 3.1e-14    12.7 0.85 0.077 -711    13     1.5\n3 0.46 0.79 0.017 207   299 3.0e-05    10.0 0.88 0.055 -731   -76     1.8\n4 0.42 0.74 0.017 186   228 1.9e-02     8.0 0.90 0.039 -698  -109     1.9\n5 0.40 0.71 0.021 166   189 1.1e-01     7.2 0.91 0.030 -637  -112     2.0\n6 0.40 0.71 0.024 147   162 1.8e-01     6.3 0.92 0.026 -569  -104     2.0\n7 0.40 0.70 0.028 129   138 2.7e-01     5.6 0.93 0.021 -504   -95     2.2\n8 0.41 0.70 0.030 112   111 5.0e-01     5.0 0.94 0.000 -446   -92     2.3\n  eChisq  SRMR eCRMS eBIC\n1    748 0.097 0.101 -506\n2    422 0.073 0.080 -718\n3    240 0.055 0.063 -790\n4    133 0.041 0.050 -792\n5    105 0.036 0.047 -721\n6     81 0.032 0.044 -651\n7     62 0.028 0.041 -580\n8     44 0.023 0.037 -514\n\n\n\n\n\n\n\n\nWskaźnik\nInterpretacja\n\n\n\nvss1\nDopasowanie Very Simple Structure przy założeniu jednego czynnika na zmienną; wyższe = lepsze.\n\n\nvss2\nDopasowanie VSS przy założeniu maksymalnie dwóch czynników na zmienną; wyższe = lepsze.\n\n\nmap\nKryterium Velicera; minimum wskazuje optymalną liczbę czynników (eliminuje korelacje cząstkowe).\n\n\ndof\nStopnie swobody testu dopasowania chi-kwadrat.\n\n\nchisq\nWartość statystyki chi-kwadrat; niska w relacji do df sugeruje dobre dopasowanie.\n\n\nprob\nWartość p testu chi-kwadrat; wysoka oznacza brak podstaw do odrzucenia poprawnego dopasowania.\n\n\nsqresid\nSuma kwadratów reszt (różnice R − R̂); niższe wartości = lepsze odwzorowanie danych.\n\n\nfit\nProporcja wyjaśnionej wariancji w macierzy korelacji; wyższe wartości = lepsze dopasowanie.\n\n\nRMSEA\nBłąd aproksymacji w populacji; &lt; 0.05 bardzo dobre, 0.05–0.08 akceptowalne, &gt; 0.10 słabe.\n\n\nBIC\nKryterium informacyjne; niższe wartości = lepszy kompromis dopasowania i prostoty.\n\n\nSABIC\nWersja BIC korygowana o wielkość próby; lepsza przy mniejszych próbach.\n\n\ncomplex\nŚrednia liczba czynników na które ładują się zmienne; niższe = prostsza struktura.\n\n\neChisq\nEstymowana statystyka chi-kwadrat w alternatywnej estymacji; interpretacja analogiczna jak chisq.\n\n\nSRMR\nStandardized Root Mean Square Residual; niski poziom (&lt; 0.08) wskazuje dobre dopasowanie.\n\n\neCRMS\nEstymowany błąd resztowy analogiczny do RMSEA; mniejsze wartości = lepsze dopasowanie.\n\n\neBIC\nEstymowana wersja kryterium BIC; niższe wartości = lepszy model.\n\n\n\nKryterium MAP Velicera wskazuje, że minimalna wartość statystyki została osiągnięta przy czterech czynnikach (MAP = 0.017). Oznacza to, że w ujęciu tego kryterium, czynniki te najlepiej redukują korelacje cząstkowe między zmiennymi – czyli eliminują największą część wariancji niepowiązanej ze wspólną strukturą czynnikową. Innymi słowy, przy czterech czynnikach model najefektywniej odwzorowuje wspólne zależności bez pozostawiania nadmiernych reszt.\nWarto jednak zauważyć, że różne kryteria sugerują odmienne liczby czynników. Kryterium BIC wskazuje na trzy czynniki jako najbardziej oczekiwane rozwiązanie, natomiast skorygowany BIC (SABIC) preferuje pięć czynników. Z kolei wskaźniki VSS (Very Simple Structure) sugerują jedno– lub dwuczynnikowe rozwiązania, maksymalizujące prostotę struktury. Ostateczna decyzja wymaga zatem kompromisu: MAP sugeruje cztery czynniki jako najpełniej oddające wspólną strukturę zmiennych, BIC preferuje trzy jako prostsze, a SABIC wskazuje na pięć. Interpretacja powinna uwzględniać nie tylko statystyki, lecz także sensowność teoretyczną i interpretowalność uzyskanych czynników w kontekście badanego materiału.\n\nKod# Analiza czynnikowa\nfa_model &lt;- fa(Harman74.cor$cov, nfactors = 4, n.obs = 145, \n               fm = \"ml\", rotate = \"varimax\")\n\nfa_model\n\nFactor Analysis using method =  ml\nCall: fa(r = Harman74.cor$cov, nfactors = 4, n.obs = 145, rotate = \"varimax\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                        ML1   ML3   ML2  ML4   h2   u2 com\nVisualPerception       0.16  0.69  0.19 0.16 0.56 0.44 1.4\nCubes                  0.12  0.44  0.08 0.10 0.22 0.78 1.3\nPaperFormBoard         0.14  0.57 -0.02 0.11 0.36 0.64 1.2\nFlags                  0.23  0.53  0.10 0.08 0.35 0.65 1.5\nGeneralInformation     0.74  0.19  0.21 0.15 0.65 0.35 1.4\nPargraphComprehension  0.77  0.20  0.07 0.23 0.69 0.31 1.4\nSentenceCompletion     0.81  0.20  0.15 0.07 0.72 0.28 1.2\nWordClassification     0.57  0.34  0.24 0.13 0.51 0.49 2.2\nWordMeaning            0.81  0.20  0.04 0.23 0.74 0.26 1.3\nAddition               0.17 -0.12  0.83 0.17 0.76 0.24 1.2\nCode                   0.18  0.12  0.51 0.37 0.45 0.55 2.2\nCountingDots           0.02  0.21  0.72 0.09 0.56 0.44 1.2\nStraightCurvedCapitals 0.19  0.44  0.53 0.08 0.51 0.49 2.3\nWordRecognition        0.20  0.05  0.08 0.55 0.35 0.65 1.3\nNumberRecognition      0.12  0.12  0.07 0.52 0.30 0.70 1.3\nFigureRecognition      0.07  0.41  0.06 0.53 0.45 0.55 2.0\nObjectNumber           0.14  0.06  0.22 0.57 0.40 0.60 1.4\nNumberFigure           0.03  0.29  0.34 0.46 0.41 0.59 2.6\nFigureWord             0.15  0.24  0.16 0.37 0.24 0.76 2.6\nDeduction              0.38  0.40  0.12 0.30 0.41 0.59 3.0\nNumericalPuzzles       0.17  0.38  0.44 0.22 0.42 0.58 2.8\nProblemReasoning       0.37  0.40  0.12 0.30 0.40 0.60 3.1\nSeriesCompletion       0.37  0.50  0.24 0.24 0.50 0.50 2.9\nArithmeticProblems     0.37  0.16  0.50 0.30 0.50 0.50 2.8\n\n                       ML1  ML3  ML2  ML4\nSS loadings           3.65 2.87 2.66 2.29\nProportion Var        0.15 0.12 0.11 0.10\nCumulative Var        0.15 0.27 0.38 0.48\nProportion Explained  0.32 0.25 0.23 0.20\nCumulative Proportion 0.32 0.57 0.80 1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  276  with the objective function =  11.44 with Chi Square =  1545.86\ndf of  the model are 186  and the objective function was  1.71 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  145 with the empirical chi square  135.74  with prob &lt;  1 \nThe total n.obs was  145  with Likelihood Chi Square =  226.68  with prob &lt;  0.022 \n\nTucker Lewis Index of factoring reliability =  0.951\nRMSEA index =  0.038  and the 90 % confidence intervals are  0.016 0.056\nBIC =  -698.99\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2  ML4\nCorrelation of (regression) scores with factors   0.93 0.87 0.91 0.82\nMultiple R square of scores with factors          0.87 0.76 0.83 0.68\nMinimum correlation of possible factor scores     0.73 0.52 0.66 0.36\n\n\nModel czteroczynnikowy oszacowany metodą największej wiarygodności na macierzy korelacji Harman74.cor$cov dobrze odwzorowuje strukturę danych i dostarcza interpretowalnych wyników.\nPierwszy czynnik (ML1) skupia się na kompetencjach werbalnych i wiedzy ogólnej. Najwyższe ładunki uzyskano dla zmiennych takich jak WordMeaning (0.81), SentenceCompletion (0.81), ParagraphComprehension (0.77) czy GeneralInformation (0.74). Wskazuje to, że ML1 reprezentuje wymiar wiedzy językowej i rozumienia tekstu. Zasoby zmienności wspólej dla tych zmiennych są wysokie (powyżej 0.65), co oznacza, że znaczna część ich wariancji została uchwycona przez model.\nDrugi czynnik (ML2) odzwierciedla zdolności arytmetyczne i numeryczne. Najsilniejsze ładunki dotyczą zmiennych Addition (0.83), CountingDots (0.72) i ArithmeticProblems (0.50). Oznacza to, że ML2 reprezentuje wymiar obliczeniowy, obejmujący zarówno proste działania matematyczne, jak i bardziej złożone zadania wymagające operowania na liczbach. Wysokie wartości \\(h_j^2\\) (np. 0.76 dla Addition) sugerują dobrą reprezentację tych zmiennych.\nTrzeci czynnik (ML3) można interpretować jako zdolności wzrokowo-przestrzenne i percepcyjne. Najsilniejsze ładunki wystąpiły dla VisualPerception (0.69), PaperFormBoard (0.57), Flags (0.53) oraz SeriesCompletion (0.50). Grupa ta obejmuje zadania związane z manipulacją figurami, rozpoznawaniem wzorów i orientacją przestrzenną.\nCzwarty czynnik (ML4) wydaje się związany z rozpoznawaniem wzrokowym i pamięcią wzrokową. Największe ładunki dotyczą zmiennych takich jak WordRecognition (0.55), NumberRecognition (0.52), FigureRecognition (0.53) czy ObjectNumber (0.57). Sugeruje to wymiar rozpoznawania i szybkiego identyfikowania bodźców wzrokowych.\nŁącznie cztery czynniki wyjaśniają 48% wariancji całkowitej, co w psychometrii jest uznawane za wartość akceptowalną przy tego typu danych. Dopasowanie globalne modelu również jest dobre: RMSEA = 0.038 (z przedziałem ufności 0.016–0.056) wskazuje na bardzo dobre dopasowanie, a Tucker-Lewis Index wynosi 0.951, co również świadczy o wysokiej jakości modelu. Niskie wartości RMSR (0.04) oraz wysoka zgodność dopasowania poza przekątną (0.98) potwierdzają, że model trafnie odwzorowuje strukturę korelacji między zmiennymi.\nOstatecznie wyniki wskazują, że struktura czteroczynnikowa jest dobrze uzasadniona empirycznie i teoretycznie. Każdy czynnik odpowiada odmiennym zdolnościom poznawczym – werbalnym, numerycznym, przestrzennym i percepcyjno-pamięciowym – a ich interpretacje są zgodne z psychologicznymi ujęciami inteligencji wielowymiarowej.\nDla większej czytelności przedstawiamy ładunki czynnikowe po rotacji varimax w formie tabelarycznej, z wyciętymi ładunkami o niskich wartościach.\n\nKodmodel_parameters(fa_model, sort = TRUE, threshold = \"max\")\n\n# Rotated loadings from Factor Analysis (varimax-rotation)\n\nVariable               |  ML1 |  ML3 |  ML2 |  ML4 | Complexity | Uniqueness\n----------------------------------------------------------------------------\nWordMeaning            | 0.81 |      |      |      |       1.30 |       0.26\nSentenceCompletion     | 0.81 |      |      |      |       1.21 |       0.28\nPargraphComprehension  | 0.77 |      |      |      |       1.35 |       0.31\nGeneralInformation     | 0.74 |      |      |      |       1.39 |       0.35\nWordClassification     | 0.57 |      |      |      |       2.17 |       0.49\nVisualPerception       |      | 0.69 |      |      |       1.38 |       0.44\nPaperFormBoard         |      | 0.57 |      |      |       1.20 |       0.64\nFlags                  |      | 0.53 |      |      |       1.51 |       0.65\nSeriesCompletion       |      | 0.50 |      |      |       2.87 |       0.50\nCubes                  |      | 0.44 |      |      |       1.33 |       0.78\nDeduction              |      | 0.40 |      |      |       3.05 |       0.59\nProblemReasoning       |      | 0.40 |      |      |       3.08 |       0.60\nAddition               |      |      | 0.83 |      |       1.21 |       0.24\nCountingDots           |      |      | 0.72 |      |       1.21 |       0.44\nStraightCurvedCapitals |      |      | 0.53 |      |       2.27 |       0.49\nCode                   |      |      | 0.51 |      |       2.25 |       0.55\nArithmeticProblems     |      |      | 0.50 |      |       2.83 |       0.50\nNumericalPuzzles       |      |      | 0.44 |      |       2.84 |       0.58\nObjectNumber           |      |      |      | 0.57 |       1.45 |       0.60\nWordRecognition        |      |      |      | 0.55 |       1.32 |       0.65\nFigureRecognition      |      |      |      | 0.53 |       1.96 |       0.55\nNumberRecognition      |      |      |      | 0.52 |       1.26 |       0.70\nNumberFigure           |      |      |      | 0.46 |       2.62 |       0.59\nFigureWord             |      |      |      | 0.37 |       2.56 |       0.76\n\nThe 4 latent factors (varimax rotation) accounted for 47.78% of the total variance of the original data (ML1 = 15.20%, ML3 = 11.97%, ML2 = 11.07%, ML4 = 9.54%).\n\n\nMożemy też przedstawić model graficznie.\n\nKodfa.diagram(fa_model, marg = c(1,5,1,1), rsize = 2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  }
]