[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wielowymiarowa analiza danych",
    "section": "",
    "text": "Wstęp\nWielowymiarowa analiza danych stanowi jeden z filarów współczesnej statystyki i eksploracji danych, oferując metody pozwalające zrozumieć strukturę i zależności w zbiorach danych, w których każda obserwacja jest opisana wieloma zmiennymi jednocześnie. W dobie powszechnego dostępu do danych oraz rosnącego zapotrzebowania na ich zaawansowaną analizę, umiejętność stosowania metod wielowymiarowych staje się nieodzowna zarówno w badaniach naukowych, jak i w analizie danych stosowanej w przemyśle, finansach, biologii, medycynie czy naukach społecznych.\nNiniejsza książka została opracowana z myślą o dwóch kierunkach kształcenia akademickiego: matematyce oraz inżynierii i analizie danych. Jej celem jest zapewnienie solidnych podstaw teoretycznych oraz praktycznych umiejętności niezbędnych do stosowania metod wielowymiarowych w rzeczywistych problemach badawczych i aplikacyjnych. Zakres tematyczny książki został dobrany tak, aby uwzględniać zarówno klasyczne metody statystyczne, jak i techniki wykorzystywane we wspsółczesnej analizie danych.\nW pierwszej części książki omówione zostaną testy wielowymiarowe, które stanowią rozszerzenie klasycznych metod statystycznych na przypadki, w których każda obserwacja opisana jest wieloma zmiennymi. Szczególna uwaga zostanie poświęcona testowi Hotellinga T², będącemu odpowiednikiem testu t dla wielu zmiennych, oraz analizie wariancji dla wielu zmiennych (MANOVA), pozwalającej na badanie różnic między grupami z uwzględnieniem współzależności zmiennych. Celem tej części będzie zrozumienie podstaw inferencji w przestrzeni wielowymiarowej i interpretacji wyników testów z uwzględnieniem macierzy kowariancji.\nNastępnie przedstawiona zostanie analiza kanoniczna, która służy do badania zależności pomiędzy dwoma zestawami zmiennych. Czytelnik pozna konstrukcję zmiennych kanonicznych, sposoby ich interpretacji oraz znaczenie wag i korelacji kanonicznych. Analiza ta ma kluczowe znaczenie wszędzie tam, gdzie celem jest znalezienie skorelowanych struktur w dwóch grupach cech, np. w badaniach biologicznych, społecznych lub psychometrycznych.\nKolejna część książki będzie poświęcona analizie czynnikowej (FA), która umożliwia modelowanie współzmienności zestawu zmiennych za pomocą mniejszej liczby zmiennych ukrytych, zwanych czynnikami. Przedstawione zostaną metody estymacji, kryteria wyboru liczby czynników oraz techniki rotacji, które służą lepszej interpretacji wyników. Analiza czynnikowa jest często stosowana w badaniach ankietowych i psychometrycznych, ale znajduje również zastosowanie w analizie danych ekonomicznych i marketingowych.\nW dalszej kolejności wprowadzony zostanie model ścieżkowy oraz jego uogólnienie w postaci modeli równań strukturalnych (SEM). Modele te pozwalają na modelowanie zarówno obserwowalnych, jak i ukrytych zmiennych oraz relacji przyczynowych pomiędzy nimi. Czytelnik pozna strukturę modelu ścieżkowego, pojęcie identyfikowalności, miary dopasowania oraz techniki estymacji parametrów. Modele SEM są obecnie szeroko stosowane w naukach społecznych, biologii, psychologii i ekonomii.\nNastępnie omówione zostaną metody redukcji wymiarowości, których celem jest uproszczenie reprezentacji danych bez utraty istotnej informacji. Kluczową techniką będzie analiza składowych głównych (PCA), która pozwala na znalezienie nowych osi zmienności w danych. Kolejno zaprezentowana zostanie analiza niezależnych składowych (ICA), która poszukuje składników statystycznie niezależnych, co jest szczególnie użyteczne w analizie sygnałów. Obie metody znajdą zastosowanie zarówno w przygotowaniu danych, jak i w ich eksploracji.\nKolejna część książki poświęcona będzie metodom skalowania wielowymiarowego (Multidimensional Scaling, MDS), które umożliwiają odwzorowanie relacji odległościowych pomiędzy obiektami w przestrzeni o mniejszym wymiarze. Wariant metric zakłada zachowanie rzeczywistych wartości odległości, natomiast non-metric koncentruje się na porządku dystansów. Metody te pozwalają uzyskać intuicyjne wizualizacje struktur danych, szczególnie przydatne w psychologii, socjologii czy analizie rynku.\nW uzupełnieniu do klasycznych technik przedstawione zostaną nieliniowe metody redukcji wymiarowości, takie jak t-distributed Stochastic Neighbor Embedding (t-SNE) oraz Uniform Manifold Approximation and Projection (UMAP). Obie techniki pozwalają na odwzorowanie skomplikowanych struktur danych w przestrzeniach dwu- lub trójwymiarowych, zachowując lokalne sąsiedztwa. Choć są to metody przede wszystkim eksploracyjne i wizualizacyjne, ich wartość w analizie dużych zbiorów danych jest trudna do przecenienia.\nNastępnie przedstawiona zostanie analiza skupień, której celem jest odkrywanie naturalnych grup w zbiorze danych. Omówione zostaną zarówno metody hierarchiczne, jak i niehierarchiczne, w tym popularna metoda k-średnich. Poruszona zostanie problematyka doboru liczby skupień oraz oceny stabilności i jakości otrzymanych rozwiązań. Analiza skupień znajduje zastosowanie w segmentacji rynku, biologii molekularnej, diagnostyce medycznej i wielu innych dziedzinach.\nKolejna część książki poświęcona będzie analizie korespondencji, stosowanej do eksploracji związków pomiędzy zmiennymi jakościowymi przedstawionymi w postaci tablicy kontyngencji. Przedstawiona zostanie zarówno analiza korespondencji prosta (dla dwóch zmiennych), jak i złożona (dla więcej niż dwóch). Omówione zostaną interpretacja map percepcyjnych, odwzorowanie profili oraz związki z metodami takimi jak PCA czy MDS.\nOstatni rozdział poświęcony będzie analizie log-liniowej, która umożliwia modelowanie częstości w tablicach wielodzielczych na podstawie interakcji pomiędzy zmiennymi kategorycznymi. Zostaną zaprezentowane modele pełne i uproszczone, zasady testowania złożoności modeli oraz interpretacji parametrów. Analiza log-liniowa jest szczególnie przydatna przy badaniu wielowymiarowych zależności między zmiennymi kategorycznymi w badaniach społecznych, medycznych oraz w analizie zachowań konsumenckich.\nWszystkie metody zostaną zilustrowane przykładami praktycznymi, realizowanymi w języku R. Pozwoli to Czytelnikowi nie tylko zrozumieć teoretyczne podstawy omawianych technik, ale także nabyć umiejętność ich stosowania w praktyce analitycznej.",
    "crumbs": [
      "Wstęp"
    ]
  },
  {
    "objectID": "multi_tests.html",
    "href": "multi_tests.html",
    "title": "Testy wielowymiarowe",
    "section": "",
    "text": "Test Hotellinga dla znanej macierzy kowariancji (Anderson 1992)\nW tradycyjnej analizie statystycznej często koncentrujemy się na porównywaniu grup ze względu na jedną zmienną – np. porównujemy średni wzrost kobiet i mężczyzn, wykorzystując test t-Studenta. Jednak w rzeczywistości badawczej rzadko interesuje nas tylko jedna cecha. Przykładowo, porównując grupy pacjentów, możemy jednocześnie rozważać poziom ciśnienia, cholesterolu i BMI. Albo, analizując dane socjologiczne, chcemy porównać grupy pod względem dochodów, wykształcenia i poziomu zadowolenia z życia.\nUżycie wielu testów jednowymiarowych wydaje się kuszące – testujemy każdą zmienną osobno. Jednak prowadzi to do trzech istotnych problemów (Huberty i Morris 1989):\n\\[\n\\mathbb{P}(\\text{co najmniej jeden błąd I rodzaju}) = 1 - (1 - \\alpha)^p = 1 - 0.95^{10} \\approx 0.40\n\\]\nPowyższe problemy uzasadniają potrzebę stosowania testów wielowymiarowych – uwzględniających strukturę współzmienności między cechami oraz pozwalających na testowanie hipotez dotyczących całych wektorów średnich.\nRozważmy próbkę \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\ldots, \\boldsymbol{y}_n \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), gdzie \\(\\boldsymbol{\\Sigma}\\) jest znana. Oznacza to, że mamy do czynienia z ciągiem niezależnych losowych wektorów \\(\\boldsymbol{y}_i\\), z których każdy ma ten sam wielowymiarowy rozkład normalny o wymiarze \\(p\\), średniej \\(\\boldsymbol{\\mu}\\) i macierzy kowariancji \\(\\boldsymbol{\\Sigma}\\). Każdy wektor \\(\\boldsymbol{y}_i\\) można interpretować jako punkt w przestrzeni \\(\\mathbb{R}^p\\), opisujący \\(p\\) cech (zmiennych) dla jednej obserwacji. Formalnie jest to wektor kolumnowy postaci \\(\\boldsymbol{y}_i = [y_{i1}, y_{i2}, \\ldots, y_{ip}]^\\top\\), gdzie indeks \\(i\\) numeruje jednostki (np. osoby, obiekty pomiaru), a indeksy \\(j = 1, \\ldots, p\\) odpowiadają poszczególnym zmiennym. Wektor ten traktowany jest jako zmienna losowa, ponieważ jego wartości są wynikiem losowego procesu generującego dane. Rozkład \\(\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) jest wielowymiarową wersją rozkładu normalnego. Opisuje on sytuację, w której każda kombinacja liniowa zmiennych losowych w \\(\\boldsymbol{y}_i\\) również ma rozkład normalny, a funkcja gęstości prawdopodobieństwa ma postać zależną od wartości wektora średnich oraz struktury kowariancji. Jest to fundamentalne założenie w klasycznej analizie statystycznej, pozwalające na stosowanie wielu narzędzi statystycznych.\nParametr \\(\\boldsymbol{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_p]^\\top\\) to wektor wartości oczekiwanych każdej z analizowanych zmiennych. Oznacza on przeciętny poziom zmiennej \\(y_{ij}\\) w populacji dla każdej cechy \\(j\\). Jest to parametr istotny z punktu widzenia testowania hipotez, ponieważ wiele testów statystycznych dotyczy właśnie równości lub różnic wektorów średnich między grupami.\nZ kolei macierz \\(\\boldsymbol{\\Sigma}\\) to dodatnio określona, symetryczna macierz kowariancji o wymiarach \\(p \\times p\\). Jej elementy \\(\\sigma_{jj}\\) opisują wariancje poszczególnych zmiennych, natomiast elementy poza główną przekątną \\(\\sigma_{jk}\\) (dla \\(j \\ne k\\)) opisują kowariancje, czyli współzmienność pomiędzy zmiennymi \\(y_{ij}\\) i \\(y_{ik}\\). W analizie wielowymiarowej uwzględnienie tych zależności między cechami jest kluczowe, ponieważ pozwala lepiej zrozumieć strukturę danych i dokonywać bardziej trafnych wniosków statystycznych.\nO próbie \\(\\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n\\) zakładamy, że wszystkie obserwacje są niezależne oraz pochodzą z tego samego rozkładu. Oznacza to, że mamy do czynienia z próbą losową, niezależną o identycznych rozkładach (i.i.d.), co jest podstawowym założeniem wielu testów i metod estymacji. Całą próbkę można przedstawić jako macierz danych o wymiarach \\(n \\times p\\), w której wiersze odpowiadają jednostkom, a kolumny cechom.\nW przypadku, gdy macierz kowariancji \\(\\boldsymbol{\\Sigma}\\) jest znana, możemy zastosować uproszczone wersje testów statystycznych, takie jak klasyczny test Hotellinga \\(T^2\\) dla jednej próby. Jest to jednak sytuacja czysto teoretyczna, ponieważ w praktyce \\(\\boldsymbol{\\Sigma}\\) musi być zazwyczaj estymowana na podstawie danych. Pomimo tego, przypadek znanej macierzy jest użyteczny do budowania intuicji, zrozumienia ról poszczególnych parametrów i wyprowadzania własności statystyk testowych.\nChcemy przetestować hipotezę:\n\\[\nH_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu} \\neq \\boldsymbol{\\mu}_0\n\\]\nStatystyka testowa opiera się na uogólnionej odległości Mahalanobisa:\n\\[\nT^2 = n (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Sigma}^{-1} (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)\n\\]\nPod warunkiem spełnienia \\(H_0\\), statystyka \\(T^2 \\sim \\chi^2_p\\). Zatem możemy porównać wartość \\(T^2\\) z odpowiednim kwantylem rozkładu chi-kwadrat.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#założenia",
    "href": "multi_tests.html#założenia",
    "title": "Testy wielowymiarowe",
    "section": "Założenia",
    "text": "Założenia\n\nPróby są niezależne;\nObserwacje w każdej grupie pochodzą z rozkładu wielowymiarowego normalnego;\n\nMacierze kowariancji są równe \\(\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}\\). Jest to kluczowe założenie umożliwiające zbudowanie wspólnego estymatora kowariancji i zastosowanie rozkładu \\(T^2\\) Hotellinga. Choć może być ono naruszone w praktyce, to dla dużych prób test zachowuje swoje właściwości asymptotyczne (Rencher 1998).\n\nWektory średnich z próby wyrażamy jako:\n\\[\n\\bar{\\boldsymbol{y}}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} \\boldsymbol{y}_{1,i}, \\quad\n\\bar{\\boldsymbol{y}}_2 = \\frac{1}{n_2} \\sum_{i=1}^{n_2} \\boldsymbol{y}_{2,i}\n\\]\nNieobciążonym estymatorem macierzy kowariancji (\\(\\boldsymbol{\\Sigma}\\)) jest tzw. połączony estymator kowariancji:\n\\[\n\\mathbf{S} =\n\\frac{(n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2}{n_1 + n_2 - 2}\n\\] gdzie \\[\n\\mathbf{S}_1 = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top\n\\]\n\\[\n\\mathbf{S}_2 = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top\n\\]\nZatem:\n\\[\n\\mathbf{S} = \\frac{\\mathbf{W}_1 + \\mathbf{W}_2}{n_1 + n_2 - 2}\n\\] gdzie: \\[\n\\mathbf{W}_1 = \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top = (n_1 - 1)\\mathbf{S}_1\n\\]\n\\[\n\\mathbf{W}_2 = \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top = (n_2 - 1)\\mathbf{S}_2\n\\] Statystyka testowa Hotellinga wówczas ma postać:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)\n\\]\nA gdy \\(H_0\\) jest prawdziwa, to po przekształceniu: \\[\nF = \\frac{(n_1 + n_2 - p - 1)}{p(n_1 + n_2 - 2)} T^2 \\sim F_{p, n_1 + n_2 - p - 1}\n\\]\nAlternatywnie, można zapisać, że: \\[\nT^2 \\sim \\frac{p(n_1 + n_2 - 2)}{n_1 + n_2 - p - 1} F_{p, n_1 + n_2 - p - 1}\n\\]\nHipotezę zerową \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) odrzucamy na poziomie istotności \\(\\alpha\\), jeśli: \\[\nT^2 &gt; T^2_{\\alpha, p, n_1 + n_2 - 2}\n\\] lub równoważnie: \\[\nF &gt; F_{\\alpha, p, n_1 + n_2 - p - 1}\n\\]\nAby test był możliwy do przeprowadzenia, konieczne jest, aby \\(n_1 + n_2 - 2 &gt; p\\), czyli liczba stopni swobody w estymacji wspólnej kowariancji była większa niż wymiar przestrzeni cech.\n\n\n\n\n\n\nAdnotacja\n\n\n\nW praktyce istotne jest, aby przed zastosowaniem testu \\(T^2\\) Hotellinga dla dwóch prób zweryfikować założenie o równości macierzy kowariancji — np. za pomocą testu Boxa. Test M Boxa (ang. Box’s M test) służy do statystycznej weryfikacji hipotezy równości macierzy kowariancji w wielu grupach.\nZałóżmy, że mamy \\(G\\) niezależnych prób z wielowymiarowego rozkładu normalnego:\n\\[\n\\boldsymbol{y}_{g} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_g, \\boldsymbol{\\Sigma}_g), \\quad g = 1, \\ldots, G\n\\] Testujemy hipotezę: \\[\nH_0: \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\ldots = \\boldsymbol{\\Sigma}_G = \\boldsymbol{\\Sigma}\n\\] przeciwko alternatywie: \\[\nH_1: \\exists\\, g, h: \\boldsymbol{\\Sigma}_g \\ne \\boldsymbol{\\Sigma}_h\n\\] Niech\n\n\n\\(\\mathbf{S}_g\\) – macierz kowariancji w grupie \\(g\\),\n\n\\(n_g\\) – liczba obserwacji w grupie \\(g\\),\n\n\\(\\mathbf{S}_p\\) – połączony estymator macierzy kowariancji:\n\n\\[\n\\mathbf{S}_p = \\frac{1}{N - G} \\sum_{g=1}^G (n_g - 1)\\mathbf{S}_g\n\\] gdzie \\(N = \\sum_{g=1}^G n_g\\) – łączna liczba obserwacji. Wówczas, statystyka testowa Boxa ma postać: \\[\nM = (N - G) \\cdot \\ln|\\mathbf{S}_p| - \\sum_{g=1}^G (n_g - 1) \\cdot \\ln|\\mathbf{S}_g|\n\\] Poprawka na skończoną próbkę prowadzi do statystyki: \\[\nC = \\left(1 - c\\right) \\cdot M\n\\] gdzie: \\[\nc = \\frac{1}{3(p + 1)(G - 1)} \\left[ \\sum_{g=1}^G \\frac{1}{n_g - 1} - \\frac{1}{N - G} \\right]\n\\] Statystyka \\(C\\) jest asymptotycznie zbierzna do rozkładu \\(\\chi^2\\left(\\frac{p}{2}(p + 1)(G - 1)\\right)\\) liczbą stopni swobody.\nHipotezę \\(H_0\\) o równości macierzy kowariancji odrzuca się, jeśli: \\[\nC &gt; \\chi^2_{1 - \\alpha, df}\n\\] lub gdy \\(p\\) testu jest mniejsza od poziomu istotności \\(\\alpha\\).\nUwagi praktyczne\n\nTest M Boxa jest wrażliwy na odchylenia od normalności – jeśli dane nie są zbliżone do normalnych, test może dawać mylące wyniki.\nW dużych próbach nawet drobne różnice między macierzami kowariancji mogą prowadzić do odrzucenia \\(H_0\\), choć nie mają istotnego wpływu praktycznego.\nW małych próbach test może być niestabilny – zaleca się ostrożność przy interpretacji.\n\n\n\n\nPrzykład 4.1 (Porównanie dwóch grup za pomocą testu \\(T^2\\) Hotellinga)  \n\nKodlibrary(MASS)\n# Parametry symulacji\nset.seed(44)\np &lt;- 2          # liczba zmiennych\nn1 &lt;- 30        # liczba obserwacji w grupie 1\nn2 &lt;- 35        # liczba obserwacji w grupie 2\n\n# Parametry rozkładu\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1, 1)\nSigma &lt;- matrix(c(1, 0.5,\n                  0.5, 1), nrow = 2)\n\n# Generowanie danych\nY1 &lt;- mvrnorm(n1, mu = mu1, Sigma = Sigma)\nY2 &lt;- mvrnorm(n2, mu = mu2, Sigma = Sigma)\n\n# Średnie z próby\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymatory kowariancji\nS1 &lt;- cov(Y1)\nS2 &lt;- cov(Y2)\n\n# Wspólna kowariancja (połączona)\nSp &lt;- ((n1 - 1)*S1 + (n2 - 1)*S2) / (n1 + n2 - 2)\n\n# Statystyka testowa Hotellinga T^2\ndiff_mean &lt;- y1_bar - y2_bar\nT2 &lt;- (n1 * n2) / (n1 + n2) * t(diff_mean) %*% solve(Sp) %*% diff_mean\nT2 &lt;- as.numeric(T2)\n\n# Przekształcenie do F\ndf1 &lt;- p\ndf2 &lt;- n1 + n2 - p - 1\nF_stat &lt;- (df2 / (df1 * (n1 + n2 - 2))) * T2\n\n# Wartość krytyczna\nalpha &lt;- 0.05\nF_crit &lt;- qf(1 - alpha, df1, df2)\n\n# p-wartość\np_val &lt;- 1 - pf(F_stat, df1, df2)\n\n# Wynik testu\nsprintf(\"Statystyka T² = %.3f\", T2)\n\n[1] \"Statystyka T² = 17.340\"\n\nKodsprintf(\"Statystyka F = %.3f\", F_stat)\n\n[1] \"Statystyka F = 8.532\"\n\nKodsprintf(\"Wartość krytyczna F = %.3f\", F_crit)\n\n[1] \"Wartość krytyczna F = 3.145\"\n\nKodsprintf(\"p-value = %e\", p_val)\n\n[1] \"p-value = 5.330310e-04\"\n\nKod# Wizualizacja \ndf1 &lt;- as.data.frame(Y1) %&gt;%\n  mutate(grupa = \"Grupa 1\")\n\ndf2 &lt;- as.data.frame(Y2) %&gt;%\n  mutate(grupa = \"Grupa 2\")\n\ndf_all &lt;- bind_rows(df1, df2)\ncolnames(df_all)[1:2] &lt;- c(\"X1\", \"X2\")\n\n# Ramka danych ze średnimi\nmeans &lt;- data.frame(\n  X1 = c(y1_bar[1], y2_bar[1]),\n  X2 = c(y1_bar[2], y2_bar[2]),\n  grupa = c(\"Grupa 1\", \"Grupa 2\")\n)\n\n# Wykres\nggplot(df_all, aes(x = X1, y = X2, color = grupa, shape = grupa)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_point(data = means, aes(x = X1, y = X2),\n             shape = c(1, 2), size = 5, stroke = 1.2, show.legend = FALSE) +\n  scale_shape_manual(values = c(16, 17)) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  coord_equal() +\n  theme_minimal() +\n  labs(title = \"Porównanie dwóch grup\",\n       x = \"X1\", y = \"X2\", color = \"Grupa\", shape = \"Grupa\")\n\n\n\n\n\n\n\n\nKod# Alternatywnie, użycie gotowej funkcji z pakietu ICSNP\nlibrary(ICSNP)\nHotellingsT2(rbind(Y1, Y2)~factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Hotelling's two sample T2-test\n\ndata:  rbind(Y1, Y2) by factor(c(rep(1, n1), rep(2, n2)))\nT.2 = 8.5321, df1 = 2, df2 = 62, p-value = 0.000533\nalternative hypothesis: true location difference is not equal to c(0,0)\n\n\nW analizowanym przykładzie zdefiniowaliśmy macierze kowariancji identycznie ale w rzeczywistości należałoby testować hipotezę o równości macierzy kowariancji. Tylko dla celów ćwiczeniowych pokażę jak to zrobić.\n\nKod# Test Boxa na równość macierzy kowariancji\nlibrary(biotools)\nboxM(rbind(Y1, Y2), factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  rbind(Y1, Y2)\nChi-Sq (approx.) = 2.1261, df = 3, p-value = 0.5466\n\nKod# lub z wykorzystaniem pakietu rstatix\nlibrary(rstatix)\nbox_m(df_all[,-3], df_all[,3])\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      2.13   0.547         3 Box's M-test for Homogeneity of Covariance Matric…\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nW sytuacji, gdy założenie o równości macierzy kowariancji jest naruszone, można stosować alternatywne metody, takie jak:\n\nTesty permutacyjne (Hotelling::hotelling.test(Y~Group, perm = TRUE, B = 5000)).\nUogólniony test Hotellinga - test Jamesa (ang. James’s second-order test) lub czasami nazywany również testem Welch-type Hotelling test (Hotelling::hotelling.test(Y~Group, var.equal = FALSE)).\nW przypadku danych charakteryzujących się dużą liczbą zmiennych w stosunku do liczby obserwacji, można rozważyć użycie estymatora Jamesa-Steina do stabilizaji macierzy kowariancji (Hotelling::hotelling.test(Y~Group, shrinkage = TRUE)).\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nOdrzucenie hipotezy zerowej \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) w teście Hotellinga \\(T^2\\) oznacza, że mamy statystycznie istotny dowód na to, iż rozkłady średnich wektorów dwóch populacji wielowymiarowych różnią się, biorąc pod uwagę współzmienność między zmiennymi. W kontekście zastosowań praktycznych, oznacza to, że przynajmniej jedna zmienna (lub kombinacja zmiennych) odróżnia grupy, nawet jeśli nie da się tego wykazać za pomocą testów jednowymiarowych.\nW sytuacji, gdy mamy dane wielowymiarowe \\(\\boldsymbol{y}_{gi} \\in \\mathbb{R}^p\\) z dwóch niezależnych grup (\\(g = 1,2\\)), testujemy:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu}_1 \\neq \\boldsymbol{\\mu}_2.\n\\]\nOdrzucenie \\(H_0\\) sugeruje, że istnieje różnica pomiędzy średnimi wektorami, ale nie musi oznaczać, że którakolwiek ze średnich poszczególnych zmiennych \\(\\mu_{1j}, \\mu_{2j}\\) różni się istotnie — zwłaszcza jeśli uwzględnimy korelacje między zmiennymi.\nTesty jednowymiarowe ignorują te współzależności, dlatego mogą nie wykazać istotnych różnic, mimo że ogólny profil wielowymiarowy się różni. Innymi słowy, może nie istnieć żadna istotna różnica w poszczególnych zmiennych, ale pewna kombinacja liniowa tych zmiennych pozwala na rozróżnienie grup.\nRozważmy kombinację liniową:\n\\[\nz = \\boldsymbol{a}^\\top \\boldsymbol{y},\n\\]\ngdzie \\(\\boldsymbol{a} \\in \\mathbb{R}^p\\) to niezerowy wektor wag. Wówczas \\(z\\) jest jednowymiarową zmienną losową będącą projekcją obserwacji \\(\\boldsymbol{y}\\) na kierunek \\(\\boldsymbol{a}\\).\nJeśli hipoteza \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) jest fałszywa, to istnieje taki wektor \\(\\boldsymbol{a}\\), dla którego:\n\\[\nH_0: \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_1 = \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_2\n\\]\nzostanie odrzucona w teście jednowymiarowym. Dla takiej kombinacji liniowej możemy zdefiniować statystykę \\(t\\)-Studenta:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\bar{z}_1 - \\bar{z}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) s_z^2}},\n\\]\ngdzie:\n\n\n\\(\\bar{z}_g = \\boldsymbol{a}^\\top \\bar{\\boldsymbol{y}}_g\\) – średnia z projekcji grupy \\(g\\),\n\n\\(s_z^2\\) – nieobciążony estymator wariancji \\(z\\), czyli:\n\n\\[\ns_z^2 = \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a},\n\\]\na \\(\\mathbf{S}\\) to wspólna macierz kowariancji.\nStąd pełna postać statystyki:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\boldsymbol{a}^\\top (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a}}}.\n\\]\nPonieważ statystyka \\(t(\\boldsymbol{a})\\) może być zarówno dodatnia, jak i ujemna, stosuje się często jej kwadrat jako miarę istotności:\n\\[\nT^2 = t^2(\\boldsymbol{a}).\n\\]\nStatystyka Hotellinga \\(T^2\\) przyjmuje postać:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nJest to forma uogólnionej odległości Mahalanobisa między średnimi wektorami. Można pokazać, że istnieje taki wektor \\(\\boldsymbol{a}\\), który maksymalizuje różnicę \\(t(\\boldsymbol{a})\\) — to tzw. funkcja dyskryminacyjna:\n\\[\n\\boldsymbol{a} = \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nDla tej wartości wektora \\(\\boldsymbol{a}\\), statystyka \\(T^2\\) przyjmuje największą wartość i jest najbardziej czuła na różnice między grupami.\nOdrzucenie \\(H_0\\) oznacza więc, że w przestrzeni \\(\\mathbb{R}^p\\) istnieje kierunek \\(\\boldsymbol{a}\\), dla którego grupy mają różne średnie projekcje. To otwiera drogę do:\n\nkonstrukcji funkcji dyskryminacyjnych (jak w analizie dyskryminacyjnej),\nidentyfikacji zmiennych lub ich kombinacji odpowiedzialnych za różnicę,\ndalszych analiz jednowymiarowych dla projekcji \\(z = \\boldsymbol{a}^\\top \\boldsymbol{y}\\).\n\n\nKodlibrary(gridExtra)  # dla strzałki jako warstwy\n\nset.seed(42)\n\n# Parametry\nn1 &lt;- n2 &lt;- 100\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1.5, 0.5)\nSigma &lt;- matrix(c(1, 0.8, 0.8, 1), ncol = 2)\n\n# Dane\nY1 &lt;- mvrnorm(n1, mu1, Sigma)\nY2 &lt;- mvrnorm(n2, mu2, Sigma)\n\n# Średnie\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymacja wspólnej kowariancji\nS_pooled &lt;- ((n1 - 1) * cov(Y1) + (n2 - 1) * cov(Y2)) / (n1 + n2 - 2)\n\n# Kierunek dyskryminacyjny a\ndiff &lt;- y1_bar - y2_bar\na &lt;- solve(S_pooled, diff)\na_norm &lt;- a / sqrt(sum(a^2))  # normalizacja\n\n# Punkt startowy strzałki (środek między średnimi)\norigin &lt;- (y1_bar + y2_bar) / 2\nscale &lt;- 3  # długość strzałki\narrow_end &lt;- origin + scale * a_norm\n\n# Ramka danych do wykresu\ndf &lt;- rbind(\n  data.frame(X1 = Y1[,1], X2 = Y1[,2], Grupa = \"Grupa 1\"),\n  data.frame(X1 = Y2[,1], X2 = Y2[,2], Grupa = \"Grupa 2\")\n)\n\n# Wektory średnich i strzałka\nmeans_df &lt;- data.frame(rbind(y1_bar, y2_bar))\ncolnames(means_df) &lt;- c(\"X1\", \"X2\")\nmeans_df$Grupa &lt;- c(\"Grupa 1\", \"Grupa 2\")  # te same etykiety co w danych punktów\n\narrow_df &lt;- data.frame(\n  x = origin[1],\n  y = origin[2],\n  xend = arrow_end[1],\n  yend = arrow_end[2]\n)\n\n# Wykres\nggplot(df, aes(x = X1, y = X2, color = Grupa)) +\n  geom_point(alpha = 0.5) +\n  geom_point(data = means_df, aes(x = X1, y = X2),\n             size = 4, shape = 16, show.legend = FALSE) +  # średnie tym samym kolorem; bez legendy\n  geom_segment(data = arrow_df, \n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")), color = \"black\", size = 1, show.legend = FALSE) +\n  labs(\n    title = latex2exp::TeX(r\"(Ilustracja kierunku dyskryminacyjnego $a = S^{-1} (\\bar{y}_1 - \\bar{y}_2)$)\"),\n    x = latex2exp::TeX(r\"($X_1$)\"),\n    y = latex2exp::TeX(r\"($X_2$)\")\n  ) +\n  coord_equal() +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#założenia-i-testowane-hipotezy",
    "href": "multi_tests.html#założenia-i-testowane-hipotezy",
    "title": "Testy wielowymiarowe",
    "section": "Założenia i testowane hipotezy",
    "text": "Założenia i testowane hipotezy\nZakłada się, że obserwacje są niezależne i pochodzą z wielowymiarowego rozkładu normalnego w każdej grupie, tj.:\n\\[\n\\boldsymbol{y}_{ij} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}), \\quad i = 1, \\dots, g,\\ j = 1, \\dots, n_i,\n\\]\ngdzie:\n\n\n\\(\\boldsymbol{y}_{ij} \\in \\mathbb{R}^p\\) — wektor obserwacji w grupie \\(i\\),\n\n\\(\\boldsymbol{\\mu}_i\\) — wektor średnich dla grupy \\(i\\),\n\n\\(\\boldsymbol{\\Sigma}\\) — wspólna macierz kowariancji we wszystkich grupach (założenie homogeniczności).\n\nTestowana jest hipoteza zerowa:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\dots = \\boldsymbol{\\mu}_g\n\\]\nwobec alternatywy:\n\\[\nH_1: \\exists\\ i,j\\ \\text{takie, że}\\ \\boldsymbol{\\mu}_i \\ne \\boldsymbol{\\mu}_j.\n\\]\nModel MANOVA opiera się na kilku fundamentalnych założeniach dotyczących danych, których spełnienie warunkuje poprawność i wiarygodność uzyskanych wyników. Ich naruszenie może prowadzić do fałszywych wniosków, zbyt dużej liczby odrzuceń hipotezy zerowej lub do błędnych ocen efektów czynników. Poniżej przedstawiono szczegółowo każde z tych założeń.\n\nPierwszym kluczowym założeniem jest odpowiednia wielkość próby. Przyjmuje się, że liczba obserwacji w każdej grupie (komórce) powinna przekraczać liczbę zmiennych zależnych, które są jednocześnie analizowane. To praktyczne zalecenie pozwala uniknąć problemów z oszacowaniem macierzy kowariancji i zapewnia dostateczną moc statystyczną.\nKolejnym istotnym założeniem jest niezależność obserwacji. Oznacza to, że każda jednostka obserwacyjna (np. osoba) powinna przynależeć wyłącznie do jednej grupy. Obserwacje wewnątrz i pomiędzy grupami nie mogą być ze sobą powiązane. W szczególności, model MANOVA nie jest odpowiedni dla danych z pomiarami powtarzanymi u tych samych obiektów. Dobór próby powinien być dokonany w sposób losowy, bez systematycznych zależności.\nTrzecim wymogiem jest brak obserwacji odstających, zarówno w sensie jednowymiarowym (dla każdej zmiennej z osobna), jak i wielowymiarowym (dla kombinacji wszystkich zmiennych zależnych). Obserwacje odstające mogą silnie zniekształcać wartości średnich i macierzy kowariancji, przez co wyniki MANOVA stają się niestabilne.\nFundamentalnym założeniem MANOVA jest wielowymiarowa normalność rozkładu danych w każdej z grup. Oznacza to, że wektor zmiennych zależnych w każdej grupie powinien mieć rozkład wielowymiarowy normalny. W R można zastosować funkcję mshapiro_test() z pakietu rstatix, aby przeprowadzić test Shapiro–Wilka dla sprawdzenia normalności wielowymiarowej.\nKolejne założenie dotyczy braku współliniowości. Oczekuje się, że zmienne zależne będą ze sobą skorelowane w umiarkowany sposób, ale nie nadmiernie. Wartości współczynników korelacji przekraczające \\(r = 0,90\\) są uznawane za niepożądane i mogą powodować problemy numeryczne oraz błędną interpretację wyników. Jak podają Tabachnick i Fidell (2012), zmienne powinny wnosić unikalne informacje do modelu.\nWażnym wymogiem jest również liniowość zależności między zmiennymi zależnymi w każdej grupie. Oznacza to, że zależności pomiędzy każdą parą zmiennych muszą być dobrze opisane przez funkcję liniową — jest to konieczne, aby poprawnie oszacować strukturę kowariancji.\nDla poprawnego działania MANOVA zakłada się także jednorodność wariancji dla każdej zmiennej zależnej między grupami. Można to testować za pomocą testu Levene’a. Nieistotny wynik testu Levene’a sugeruje, że wariancje są porównywalne w grupach.\nOstatnie, ale bardzo istotne, jest założenie o jednorodności macierzy kowariancji (homogeniczności macierzy wariancji–kowariancji) pomiędzy grupami. Oznacza to, że struktura współzależności między zmiennymi powinna być podobna w każdej grupie. Weryfikację tego założenia umożliwia test Boxa (Box’s M test), który stanowi wielowymiarowy odpowiednik testu Levene’a. Ze względu na dużą czułość testu Boxa na odstępstwa od założeń, przyjmuje się konserwatywny poziom istotności \\(\\alpha = 0,001\\) dla weryfikacji jego wyniku.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "href": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "title": "Testy wielowymiarowe",
    "section": "Konstrukcja modelu i statystyki testowe",
    "text": "Konstrukcja modelu i statystyki testowe\nPodobnie jak w jednowymiarowym przypadku, w MANOVA analizuje się rozkład wariancji całkowitej na wariancję międzygrupową i wewnątrzgrupową, ale w postaci macierzy kowariancji:\n\nMacierz wariancji międzygrupowej (ang. between-group SSCP):\n\n\\[\n\\mathbf{B} = \\sum_{i=1}^{g} n_i (\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})(\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})^\\top\n\\]\n\nMacierz wariancji wewnątrzgrupowej (ang. within-group SSCP):\n\n\\[\n\\mathbf{W} = \\sum_{i=1}^{g} \\sum_{j=1}^{n_i} (\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)(\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)^\\top\n\\]\nMacierz wariancji całkowitej to: \\(\\mathbf{T} = \\mathbf{B} + \\mathbf{W}\\).\nW celu przeprowadzenia testu MANOVA, wykorzystuje się statystyki oparte na stosunku macierzy:\n\\[\n\\mathbf{W}^{-1} \\mathbf{B}\n\\]\nNajczęściej spotykane statystyki testowe to:\n\nWilks’ Lambda:\n\n\\[\n\\Lambda = \\frac{\\det(\\mathbf{W})}{\\det(\\mathbf{B} + \\mathbf{W})}\n\\]\n\nStatystyka Pillai-Bartletta (Trace):\n\n\\[\nV = \\mathrm{tr}\\left[(\\mathbf{B} + \\mathbf{W})^{-1} \\mathbf{B}\\right]\n\\]\n\nStatystyka Hotellinga–Lawleya (Trace):\n\n\\[\nT = \\mathrm{tr}(\\mathbf{W}^{-1} \\mathbf{B})\n\\]\n\nNajwiększy pierwiastek Roy’a:\n\n\\[\n\\theta_{\\text{max}} = \\text{największa wartość własna}\\ (\\mathbf{W}^{-1} \\mathbf{B})\n\\]\nWybór konkretnej statystyki zależy od liczebności prób, wymiaru przestrzeni i liczby grup. W praktyce Wilks’ Lambda jest najczęściej stosowana.\n\nPrzykład 5.1 Na poziomie istotności \\(\\alpha=0.05\\) zweryfikuj hipotezę, że czynnik grupujący (Group) istotnie różnicuje zmienne Actions i Thoughts jednocześnie.\n\nKodlibrary(gtsummary)\nlibrary(rstatix)\nlibrary(easystats)\nlibrary(gt)\ndane &lt;- rio::import(\"data/OCD.dat\")\n\n\nStatystyki opisowe grup\n\nKoddane %&gt;% \n  tbl_summary(by = Group,\n              statistic = list(where(is.numeric) ~ \"{mean} ({sd})\"),\n              type = list(Actions ~ \"continuous\"),\n              digits = list(everything() ~ 2))\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nBT\nN = 101\n\n\nCBT\nN = 101\n\n\nNo Treatment Control\nN = 101\n\n\n\n\nActions\n3.70 (1.77)\n4.90 (1.20)\n5.00 (1.05)\n\n\nThoughts\n15.20 (2.10)\n13.40 (1.90)\n15.00 (2.36)\n\n\n\n\n1 Mean (SD)\n\n\n\n\nKodp &lt;- dane %&gt;% \n  select(-Group) %&gt;% \n  correlation()\n\np %&gt;% print_html()\n\n\n\n\n\n\nCorrelation Matrix (pearson-method)\n\n\nParameter1\nParameter2\nr\n95% CI\nt(28)\np\n\n\n\nActions\nThoughts\n0.06\n(-0.31, 0.41)\n0.31\n0.758\n\n\np-value adjustment method: Holm (1979); Observations: 30\n\n\n\n\n\nW kontekście zmiennej Actions widzimy najwyższy poziom w grupie No treatment, natomiast najniższy w grupie BT. Dla zmiennej Thoughts najwyższy poziom osiągnięto w grupie BT a najniższy w grupie CBT. Grupy różnią się również zmiennością obu cech. Związek pomiędzy zmiennymi Actions i Thoughts jest niemal niezauważalny. Korelacja pomiędzy tymi cechami jest nieistotnie różna od zera.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  ggplot(aes(x = Group, y = value, fill = name)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(fill = \"Variable\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\nPowyższe wykresy potwierdzają znaczne różnice pomiędzy grupami w kontekście analizowanych cech.\nZałożenia\n\nKoddane %&gt;% \n  group_split(Group) %&gt;% \n  map_df(~mshapiro_test(.x[,2:3])) %&gt;% \n  mutate(Group = dane %&gt;% \n           group_keys(Group) %&gt;% \n           pull(Group),\n         .before = statistic) %&gt;%\n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nstatistic\np.value\n\n\n\nBT\n0.891\n0.175\n\n\nCBT\n0.959\n0.777\n\n\nNo Treatment Control\n0.826\n0.030\n\n\n\n\n\n\nJedynie w grupie No treatment nie jest zachowana wielowymiarowa normalność rozkładu analizowanych cech. Można też przeprowadzić testy normalności poszczególnych zmiennych, ale należy pamiętać, że brak podstaw do odrzucenia hipotezy o normalności brzegowych zmiennych nie jest warunkiem dostatecznym, a jedynie koniecznym.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Actions) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nActions\n0.872\n0.106\n\n\nCBT\nActions\n0.952\n0.691\n\n\nNo Treatment Control\nActions\n0.859\n0.074\n\n\n\n\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nThoughts\n0.877\n0.120\n\n\nCBT\nThoughts\n0.914\n0.310\n\n\nNo Treatment Control\nThoughts\n0.826\n0.030\n\n\n\n\n\n\nPodobnie jak w przypadku wielowymiarowym brak normalności zarysował się w grupie No treatment i to tylko dla zmiennej Thoughts. Teraz przechodzimy do testowania jednorodności kowariancji.\n\nKodbox_m(dane[,2:3], dane$Group)  %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n8.893\n0.180\n6.000\nBox's M-test for Homogeneity of Covariance Matrices\n\n\n\n\n\nNa podstawie powyższego testu można stwierdzić, iż nie ma podstaw do odrzucenia hipotezy o jednorodności macierzy kowariancji.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Actions) \n\n[1] Group      Actions    Thoughts   is.outlier is.extreme\n&lt;0 wierszy&gt; (lub 'row.names' o zerowej długości)\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nActions\nThoughts\nis.outlier\nis.extreme\n\n\nNo Treatment Control\n4\n20\nTRUE\nFALSE\n\n\n\n\nKodwhich(dane$Actions == 4 & dane$Thoughts == 20)\n\n[1] 26\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  mahalanobis_distance() %&gt;% \n  filter(is.outlier==TRUE)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: Actions &lt;int&gt;, Thoughts &lt;int&gt;, mahal.dist &lt;dbl&gt;,\n#   is.outlier &lt;lgl&gt;\n\n\nIstnieje jedna obserwacja odstająca w grupie No treatment (obserwacja nr 26). Test wielowymiarowy nie wykrył żadnego elementu odstającego.\nPomimo niespełnienia założenia o wielowymiarowej normalności cech w grupach, zastosujemy test MANOVA.\nManova\n\nKodmod &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane)\nManova(mod) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.32\n4\n54\n2.56\n0.049\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Wilk\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.70\n4\n52\n2.55\n0.050\n\n\nWilks test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Roy\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.33\n2\n27\n4.52\n0.020\n\n\nRoy test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Hotelling\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.41\n4\n50\n2.55\n0.051\n\n\nHotelling-Lawley test statistic Anova Table (Type 2 tests)\n\n\n\n\nKod# model bez obserawcji odstającej\nmod2 &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane[-26,])\nManova(mod2) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.36\n4\n52\n2.87\n0.032\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\n\nAnalizują wszystkie rodzaje testów Manova, widzimy, że jedynie test Hotellinga-Laweya nie daje podstaw do odrzucenia hipotezy o równości wektorów średnich. Natomiast ponieważ co najmniej jeden z nich wskazał istotność różnic, to przyjmujemy, że są podstawy aby odrzucić hipotezę o równości wektorów średnich pomiędzy grupami. Test wykluczający obserwację odstającą również każe odrzucić hipotezę \\(H_0\\).\nPrzeprowadzimy zatem analizę brzegową.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  anova_test(value~Group) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\nActions\nGroup\n2.000\n27.000\n2.771\n0.080\n\n0.170\n\n\nThoughts\nGroup\n2.000\n27.000\n2.154\n0.136\n\n0.138\n\n\n\n\n\n\nAnaliza brzegowa pokazuje ciekawy wynik, mianowicie, dla żadnej z analizowanych cech testy brzegowe nie wykazały istotnych różnic. To pokazuje jak ważne jest stosowanie testów wielowymiarowych w kontekście porównań grup.\nPost-hoc\nPonieważ testy brzegowe ANOVA nie wykazały różnic, to testów post-hoc nie powinno się wykonywać, ale dla celów ćwiczeniowych pokażę jak je wykonać.\n\nKodpwc &lt;- dane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  games_howell_test(value~Group)\npwc %&gt;% \n  select(-.y.) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\ngroup1\ngroup2\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\nActions\nBT\nCBT\n1.200\n−0.544\n2.944\n0.209\nns\n\n\nActions\nBT\nNo Treatment Control\n1.300\n−0.394\n2.994\n0.148\nns\n\n\nActions\nCBT\nNo Treatment Control\n0.100\n−1.189\n1.389\n0.979\nns\n\n\nThoughts\nBT\nCBT\n−1.800\n−4.085\n0.485\n0.138\nns\n\n\nThoughts\nBT\nNo Treatment Control\n−0.200\n−2.749\n2.349\n0.978\nns\n\n\nThoughts\nCBT\nNo Treatment Control\n1.600\n−0.852\n4.052\n0.244\nns\n\n\n\n\n\n\nTesty post-hoc potwierdzają wyniki testów brzegowych ANOVA, ponieważ brakuje różnic pomiędzy poziomami zmiennych grupujących.\n\n\n\n\n\nAnderson, T. W. 1992. „Introduction to Hotelling (1931) The Generalization of Student’s Ratio”. W, 45–53. Springer New York. https://doi.org/10.1007/978-1-4612-0919-5_3.\n\n\nHotelling, Harold. 1992. „The generalization of Student’s ratio”. W, 54–65. Springer.\n\n\nHuberty, Carl J., i John D. Morris. 1989. „Multivariate Analysis Versus Multiple Univariate Analyses.” Psychological Bulletin 105 (2): 302–8. https://doi.org/10.1037/0033-2909.105.2.302.\n\n\n„Omnibus MANOVA Tests”. 1985. W, 14–39. SAGE Publications, Inc. https://doi.org/10.4135/9781412985222.d16.\n\n\nRencher, Alvin C. 1998. Multivariate statistical inference and applications. T. 635. Wiley New York.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Anderson, T. W. 1992. “Introduction to Hotelling (1931) the\nGeneralization of Student’s Ratio.” In, 45–53.\nSpringer New York. https://doi.org/10.1007/978-1-4612-0919-5_3.\n\n\nBartlett, M. S. 1951. “The Effect of Standardization on a χ 2\nApproximation in Factor Analysis.” Biometrika 38 (3/4):\n337. https://doi.org/10.2307/2332580.\n\n\nBollen, Kenneth A. 1989. “Structural Equation Models with Observed\nVariables.” Structural Equations with Latent Variables,\nApril, 80–150. https://doi.org/10.1002/9781118619179.ch4.\n\n\nCattell, Raymond B. 1966. “The Scree Test For The Number Of\nFactors.” Multivariate Behavioral Research 1 (2):\n245–76. https://doi.org/10.1207/s15327906mbr0102_10.\n\n\nEveritt, B. S., and A. Yates. 1989. “Multivariate Exploratory Data\nAnalysis: A Perspective on Exploratory Factor Analysis.”\nBiometrics 45 (1): 342. https://doi.org/10.2307/2532065.\n\n\nGrieder, Silvia, and Markus D. Steiner. 2020. “Algorithmic Jingle\nJungle: A Comparison of Implementations of Principal Axis Factoring and\nPromax Rotation in r and SPSS.” http://dx.doi.org/10.31234/osf.io/7hwrm.\n\n\nHarman, Harry H., and Wayne H. Jones. 1966. “Factor Analysis by\nMinimizing Residuals (Minres).” Psychometrika 31 (3):\n351–68. https://doi.org/10.1007/bf02289468.\n\n\nHendrickson, Alan E., and Paul Owen White. 1964. “PROMAX: A QUICK\nMETHOD FOR ROTATION TO OBLIQUE SIMPLE STRUCTURE.” British\nJournal of Statistical Psychology 17 (1): 65–70. https://doi.org/10.1111/j.2044-8317.1964.tb00244.x.\n\n\nHorn, John L. 1965. “A Rationale and Test for the Number of\nFactors in Factor Analysis.” Psychometrika 30 (2):\n179–85. https://doi.org/10.1007/bf02289447.\n\n\n———. 1969. “Harry H. Harman Modern Factor Analysis (Second\nEdition, Revised). Chicago and London: University of Chicago Press,\n1967. Pp. Xx + 474. $12.50.” Psychometrika\n34 (1): 134–38. https://doi.org/10.1017/s0033312300004580.\n\n\nHotelling, Harold. 1936. “Relations Between Two Sets of\nVariates.” Biometrika 28 (3/4): 321. https://doi.org/10.2307/2333955.\n\n\n———. 1992. “The Generalization of Student’s\nRatio.” In, 54–65. Springer.\n\n\nHuang, Yafei, and Peter M. Bentler. 2015. “Behavior of\nAsymptotically Distribution Free Test Statistics in Covariance Versus\nCorrelation Structure Analysis.” Structural Equation\nModeling: A Multidisciplinary Journal 22 (4): 489–503. https://doi.org/10.1080/10705511.2014.954078.\n\n\nHuberty, Carl J., and John D. Morris. 1989. “Multivariate Analysis\nVersus Multiple Univariate Analyses.” Psychological\nBulletin 105 (2): 302–8. https://doi.org/10.1037/0033-2909.105.2.302.\n\n\n“Introduction to Factor Analysis.” 2020. In, 1–12. SAGE\nPublications, Inc. https://doi.org/10.4135/9781544339900.n4.\n\n\nJacobucci, Ross, and Kevin J. Grimm. 2018. “Comparison of\nFrequentist and Bayesian Regularization in Structural Equation\nModeling.” Structural Equation Modeling: A Multidisciplinary\nJournal 25 (4): 639–49. https://doi.org/10.1080/10705511.2017.1410822.\n\n\nJennrich, R. I., and P. F. Sampson. 1966. “Rotation for Simple\nLoadings.” Psychometrika 31 (3): 313–23. https://doi.org/10.1007/bf02289465.\n\n\nJöreskog, Karl G., and Arthur S. Goldberger. 1972. “Factor\nAnalysis by Generalized Least Squares.” Psychometrika 37\n(3): 243–60. https://doi.org/10.1007/bf02306782.\n\n\nKaiser, Henry F. 1958. “The Varimax Criterion for Analytic\nRotation in Factor Analysis.” Psychometrika 23 (3):\n187–200. https://doi.org/10.1007/bf02289233.\n\n\n———. 1970. “A Second Generation Little Jiffy.”\nPsychometrika 35 (4): 401–15. https://doi.org/10.1007/bf02291817.\n\n\nKiers, Henk A. L. 1994. “Simplimax: Oblique Rotation to an Optimal\nTarget with Simple Structure.” Psychometrika 59 (4):\n567–79. https://doi.org/10.1007/bf02294392.\n\n\nKILIÇ, Abdullah, İbrahim UYSAL, and Burcu ATAR. 2020. “Comparison\nof Confirmatory Factor Analysis Estimation Methods on Binary\nData.” International Journal of Assessment Tools in\nEducation 7 (3): 451–87. https://doi.org/10.21449/ijate.660353.\n\n\nKyriazos, Theodoros, and Mary Poga-Kyriazou. 2023. “Applied\nPsychometrics: Estimator Considerations in Commonly Encountered\nConditions in CFA, SEM, and EFA Practice.” Psychology 14\n(05): 799–828. https://doi.org/10.4236/psych.2023.145043.\n\n\nLatan, Hengky, and Richard Noonan, eds. 2017. Partial Least Squares\nPath Modeling. Springer International Publishing. https://doi.org/10.1007/978-3-319-64069-3.\n\n\nLawley, D. N. 1940. “VI.The Estimation of Factor\nLoadings by the Method of Maximum Likelihood.” Proceedings of\nthe Royal Society of Edinburgh 60 (1): 64–82. https://doi.org/10.1017/s037016460002006x.\n\n\nLi, Cheng-Hsien. 2015. “Confirmatory Factor Analysis with Ordinal\nData: Comparing Robust Maximum Likelihood and Diagonally Weighted Least\nSquares.” Behavior Research Methods 48 (3): 936–49. https://doi.org/10.3758/s13428-015-0619-7.\n\n\n———. 2021. “Statistical Estimation of Structural Equation Models\nwith a Mixture of Continuous and Categorical Observed Variables.”\nBehavior Research Methods 53 (5): 2191–2213. https://doi.org/10.3758/s13428-021-01547-z.\n\n\nLu, Zhao-Hua, Sy-Miin Chow, and Eric Loken. 2016. “Bayesian Factor\nAnalysis as a Variable-Selection Problem: Alternative Priors and\nConsequences.” Multivariate Behavioral Research 51 (4):\n519–39. https://doi.org/10.1080/00273171.2016.1168279.\n\n\nMarriott, F. H. C., and R. Gittins. 1986. “Canonical Analysis: A\nReview with Applications in Ecology; Biomathematics, Vol. 12.”\nBiometrics 42 (1): 222. https://doi.org/10.2307/2531264.\n\n\n“Omnibus MANOVA Tests.” 1985. In, 14–39. SAGE Publications,\nInc. https://doi.org/10.4135/9781412985222.d16.\n\n\nRencher, Alvin C. 1998. Multivariate Statistical Inference and\nApplications. Vol. 635. Wiley New York.\n\n\nSchweizer, Karl, and Christine DiStefano, eds. 2016. Principles and\nMethods of Test Construction. Hogrefe Publishing. https://doi.org/10.1027/00449-000.\n\n\nSpearman, C. 1961. “\"General\nIntelligence\" Objectively Determined and Measured.”\nIn, 59–73. Appleton-Century-Crofts. https://doi.org/10.1037/11491-006.\n\n\n“Supplemental Material for The Performance of ML, DWLS, and ULS\nEstimation With Robust Corrections in Structural Equation Models With\nOrdinal Variables.” 2016. Psychological Methods. https://doi.org/10.1037/met0000093.supp.\n\n\nTarka, Piotr. 2017. “An Overview of Structural Equation Modeling:\nIts Beginnings, Historical Development, Usefulness and Controversies in\nthe Social Sciences.” Quality & Quantity 52 (1):\n313–54. https://doi.org/10.1007/s11135-017-0469-8.\n\n\nThurstone, L. L. 1931. “Multiple Factor Analysis.”\nPsychological Review 38 (5): 406–27. https://doi.org/10.1037/h0069792.\n\n\nTurney, A. H. 1939. “Factor Analysis Makes ProgressA\nStudy in Factor Analysis: The Stability of a Bi-Factor\nSolution. Karl J. Holzinger , Frances Swineford.”\nThe School Review 47 (9): 709–11. https://doi.org/10.1086/440440.\n\n\nVelicer, Wayne F. 1976. “Determining the Number of Components from\nthe Matrix of Partial Correlations.” Psychometrika 41\n(3): 321–27. https://doi.org/10.1007/bf02293557.\n\n\nWang, Xiaojing, Candace M. Kammerer, Stewart Anderson, Jiang Lu, and\nEleanor Feingold. 2008. “A Comparison of Principal Component\nAnalysis and Factor Analysis Strategies for Uncovering Pleiotropic\nFactors.” Genetic Epidemiology 33 (4): 325–31. https://doi.org/10.1002/gepi.20384.\n\n\nWright, Sewall. 1934. “The Method of Path Coefficients.”\nThe Annals of Mathematical Statistics 5 (3): 161–215. https://doi.org/10.1214/aoms/1177732676.",
    "crumbs": [
      "Literatura"
    ]
  },
  {
    "objectID": "cca.html",
    "href": "cca.html",
    "title": "Analiza kanoniczna",
    "section": "",
    "text": "Przypomnienie z algebry 😉\nAnaliza kanoniczna (ang. Canonical Correlation Analysis, CCA) jest klasyczną techniką statystyczną służącą do badania związków pomiędzy dwoma zestawami zmiennych wielowymiarowych. Jej podstawowym celem jest znalezienie takich kombinacji liniowych zmiennych z obu zestawów, które maksymalizują wzajemną korelację – są to tzw. kanoniczne zmienne lub kanoniczne składniki. Technika ta została wprowadzona przez Harolda Hotellinga w roku 1936, a więc w okresie intensywnego rozwoju metod statystycznych opartych na algebrze macierzy (Hotelling 1936).\nW tym samym czasie powstawały także inne fundamenty analizy wielowymiarowej, takie jak analiza składowych głównych (PCA) czy dyskryminacja liniowa (LDA)1. Analiza kanoniczna stanowi zatem jeden z filarów klasycznej statystyki wielowymiarowej i do dziś pozostaje istotnym narzędziem eksploracji i modelowania złożonych zależności.\nW odróżnieniu od regresji wielorakiej, która przewiduje zestaw zmiennych zależnych na podstawie zestawu predyktorów, analiza kanoniczna traktuje obie grupy zmiennych symetrycznie – nie zakłada istnienia wyraźnego kierunku przyczynowego. Dlatego stosuje się ją w sytuacjach, gdy celem jest ogólna analiza współzależności pomiędzy dwoma zbiorami zmiennych, a nie przewidywanie jednego zestawu na podstawie drugiego.\nTypowe zastosowania analizy kanonicznej obejmują:\nNa potrzeby definicji modeli kanonicznego potrzebne będą nam pewne twierdzenia z zakresu algebry.\nOto matematyczna definicja modelu CCA oraz dowód istnienia rozwiązania, sformułowana ściśle w duchu Twojego tekstu:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#sformułowanie-problemu-własnego",
    "href": "cca.html#sformułowanie-problemu-własnego",
    "title": "Analiza kanoniczna",
    "section": "Sformułowanie problemu własnego",
    "text": "Sformułowanie problemu własnego\nPrzekształćmy zmienne \\[\nc=\\Sigma_{XX}^{1/2}a,\\quad d=\\Sigma_{YY}^{1/2}b.\n\\]\nWówczas \\[\n\\rho(a,b)=\\frac{c^\\top\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}d}{\\sqrt{c^\\top c}\\sqrt{d^\\top d}}.\n\\]\nZ lematu Cauchy’ego–Buniakowskiego–Schwarza mamy \\[\n\\left|c^\\top \\mathbf{M} d\\right| \\le\n\\bigl(c^\\top \\mathbf{M}\\mathbf{M}^\\top c\\bigr)^{1/2}\\bigl(d^\\top d\\bigr)^{1/2},\n\\quad \\text{gdzie }\\mathbf{M}=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}.\n\\tag{5.1}\\]\nZatem \\[\n\\rho(a,b)^2 \\le\n\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}.\n\\]\nPonieważ \\(\\mathbf{M}\\mathbf{M}^\\top=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\) jest macierzą symetryczną dodatnio określoną, z lematu Rayleigha–Ritza otrzymujemy \\[\n\\max_{c\\neq 0}\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}=\\lambda_1,\n\\] gdzie \\(\\lambda_1\\) to największa wartość własna tej macierzy, osiągana dla \\(c=e_1\\) – jej wektora własnego.\nJeśli \\[\nd \\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1\n\\] to Równanie 5.1 staje się równością.\nWracając do oryginalnych współczynników \\[\na_1=\\Sigma_{XX}^{-1/2}e_1,\\quad\nb_1\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1.\n\\]\nPierwsza korelacja kanoniczna wynosi wówczas \\[\n\\rho_1=\\sqrt{\\lambda_1}.\n\\]\nAnalogicznie dla kolejnych par, przy założeniu \\(c\\perp e_1,\\dots,e_{k-1}\\), mamy \\[\n\\rho_k=\\sqrt{\\lambda_k},\n\\] gdzie \\(\\lambda_k\\) to kolejne wartości własne macierzy \\(\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\), a odpowiadające im wektory własne \\(e_k\\) definiują kolejne wektory kanoniczne \\[\nU_k=e_k^\\top\\Sigma_{XX}^{-1/2}X,\\quad\nV_k=f_k^\\top\\Sigma_{YY}^{-1/2}Y,\\quad\nf_k\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_k.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#własności-rozwiązań",
    "href": "cca.html#własności-rozwiązań",
    "title": "Analiza kanoniczna",
    "section": "Własności rozwiązań",
    "text": "Własności rozwiązań\nDla każdej pary \\((U_k,V_k)\\) zachodzi \\[\n\\mathrm{Var}(U_k)=\\mathrm{Var}(V_k)=1,\\quad\n\\mathrm{Cov}(U_k,U_l)=\\mathrm{Cov}(V_k,V_l)=\\mathrm{Cov}(U_k,V_l)=0\\quad (k\\neq l).\n\\]\n\n\n\n\n\n\nDowód powyższych równości\n\n\n\nDla danej pary wektorów kanonicznych mamy\n\\[\nU_k = a_k^\\top X = e_k^\\top \\Sigma_{XX}^{-1/2} X,\n    \\qquad\n    V_k = b_k^\\top Y = f_k^\\top \\Sigma_{YY}^{-1/2} Y,\n\\] gdzie:\n\n\n\\(e_k\\) jest ortonormalnym6 wektorem własnym macierzy \\(\\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} \\Sigma_{XX}^{-1/2}\\)\n\n\n\\(f_k \\propto \\Sigma_{YY}^{-1/2} \\Sigma_{YX} \\Sigma_{XX}^{-1/2} e_k\\),\n\n\\(\\rho_k = \\sqrt{\\lambda_k}\\), gdzie \\(\\lambda_k\\) to odpowiadająca wartość własna.\n\nMacierze \\(\\Sigma_{XX}\\), \\(\\Sigma_{YY}\\) są dodatnio określone, więc można wprowadzić transformacje \\[\n\\tilde{X} = \\Sigma_{XX}^{-1/2}X, \\quad \\tilde{Y} = \\Sigma_{YY}^{-1/2}Y.\n\\] Zatem \\[\nU_k = e_k^\\top \\tilde{X},\\quad V_k = f_k^\\top \\tilde{Y}.\n\\]\nNajpierw udowodnimy, że \\(\\operatorname{Var}(U_k) = \\operatorname{Var}(V_k) = 1\\).\nZmienna \\(U_k = e_k^\\top \\tilde{X}\\), więc \\[\n\\operatorname{Var}(U_k) = \\operatorname{Var}(e_k^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_k.\n\\] Zauważmy, że \\[\n\\operatorname{Var}(\\tilde{X}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XX} \\Sigma_{XX}^{-1/2} = I,\n\\] więc \\[\n\\operatorname{Var}(U_k) = e_k^\\top I e_k = e_k^\\top e_k = 1.\n\\] Analogicznie \\[\n\\operatorname{Var}(V_k) = f_k^\\top \\operatorname{Var}(\\tilde{Y}) f_k = f_k^\\top f_k = 1,\n\\] ponieważ \\(\\tilde{Y}\\) ma jednostkową macierz kowariancji, a \\(f_k\\) są znormalizowane.\nTeraz dowiedziemy, że \\(\\operatorname{Cov}(U_k, U_l) = 0\\) dla \\(k \\neq l\\)\n\\[\n\\operatorname{Cov}(U_k, U_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, e_l^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_l = e_k^\\top e_l.\n\\] Ponieważ \\(e_k\\), \\(e_l\\) są ortonormalnymi wektorami własnymi symetrycznej macierzy, to \\[\ne_k^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, U_l) = 0.\n\\] Podobnie \\[\n\\operatorname{Cov}(V_k, V_l) = f_k^\\top f_l = 0 \\quad \\text{dla } k \\neq l.\n\\]\nNa koniec dowiedźmy, że \\(\\operatorname{Cov}(U_k, V_l) = 0\\) dla \\(k \\neq l\\) \\[\n\\operatorname{Cov}(U_k, V_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, f_l^\\top \\tilde{Y}) = e_k^\\top \\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) f_l.\n\\] Z definicji \\[\n\\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1/2} =: M.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = e_k^\\top M f_l.\n\\] Z poprzednich wyprowadzeń \\[\nf_l \\propto M^\\top e_l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) \\propto e_k^\\top M M^\\top e_l.\n\\] Ale macierz \\(MM^\\top\\) jest symetryczna, a \\(e_k\\) są jej ortonormalnymi wektorami własnymi, więc \\[\ne_k^\\top MM^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = 0.\n\\]\n\n\n6 ortonormalność wynika z niezmienniczości korelacji względem długości wektorówPonadto korelacje kanoniczne są niezmiennicze względem odwracalnych przekształceń liniowych \\(X\\) i \\(Y\\): \\[\nX^*=\\mathcal{U}^TX+u,\\quad Y^*=\\mathcal{V}^TY+v \\implies\n\\rho_i(X^*,Y^*)=\\rho_i(X,Y).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wniosek",
    "href": "cca.html#wniosek",
    "title": "Analiza kanoniczna",
    "section": "Wniosek",
    "text": "Wniosek\nPonieważ macierze kowariancji \\(\\Sigma_{XX}\\) i \\(\\Sigma_{YY}\\) są dodatnio określone, ich odwrotności istnieją. Macierze: \\[\n\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2},\\quad\n\\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}\n\\] są symetryczne i dodatnio określone, więc mają rzeczywiste, dodatnie wartości własne i ortonormalne wektory własne. Z lematu Rayleigha–Ritza otrzymujemy maksymalizację ilorazu Rayleigha oraz gwarancję istnienia rozwiązania. Tym samym wykazano, że pary \\((a_k,b_k)\\) istnieją, a odpowiadające im \\(\\rho_k=\\sqrt{\\lambda_k}\\) są kanonicznymi korelacjami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#hipoteza-zerowa-i-alternatywna",
    "href": "cca.html#hipoteza-zerowa-i-alternatywna",
    "title": "Analiza kanoniczna",
    "section": "Hipoteza zerowa i alternatywna",
    "text": "Hipoteza zerowa i alternatywna\nDla zbiorów zmiennych \\(X \\in \\mathbb{R}^p\\), \\(Y \\in \\mathbb{R}^q\\), testujemy\n\n\n\\(H_0: \\rho_1 = \\rho_2 = \\cdots = \\rho_s = 0\\) – brak istotnych korelacji kanonicznych (pierwiastków),\n\n\\(H_1\\): istnieje co najmniej jedna istotna korelacja kanoniczna, tj. \\(\\exists i \\leq s \\ \\text{takie, że } \\rho_i \\ne 0\\).\n\ngdzie \\(s = \\min(p, q)\\), a \\(\\rho_i\\) to \\(i\\)-ta korelacja kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#statystyka-testowa-test-wilka",
    "href": "cca.html#statystyka-testowa-test-wilka",
    "title": "Analiza kanoniczna",
    "section": "Statystyka testowa – test Wilka",
    "text": "Statystyka testowa – test Wilka\nW celu przetestowania tej hipotezy, wykorzystuje się statystykę Wilka, która bazuje na iloczynie składników postaci (\\(1 - \\lambda_i\\)), gdzie \\(\\lambda_i\\) to wartości własne odpowiadające kwadratom korelacji kanonicznych \\[\n\\lambda_i = \\rho_i^2.\n\\] Statystyka Wilkas jest zdefiniowana jako \\[\n\\Lambda = \\prod_{i=1}^s (1 - \\lambda_i).\n\\]\nInterpretacja - im mniejsze wartości \\(\\Lambda\\), tym większa zależność między zbiorami \\(X\\) i \\(Y\\). Duże wartości \\(\\lambda_i\\) (czyli silne korelacje kanoniczne) powodują, że \\(\\Lambda\\) dąży do zera.\nW praktyce, dla próby \\(n\\)-elementowej, stosujemy wersję testu bazującą na macierzach kowariancji estymowanych z próby \\(S_{XX}, S_{XY}, S_{YX}, S_{YY})\\) – odpowiedniki \\(\\Sigma_{XX}, \\Sigma_{XY}, \\Sigma_{YX}, \\Sigma_{YY}\\).\nWówczas \\[\nT^2/n = \\left|I - S_{YY}^{-1} S_{YX} S_{XX}^{-1} S_{XY} \\right| = \\prod_{i=1}^s (1 - \\hat{\\lambda}_i),\n\\] gdzie \\(\\hat{\\lambda}_i\\) to próbkowe wartości własne (szacunki \\(\\lambda_i\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#rozkład-asymptotyczny-i-transformacja-do-rozkładu-chi2",
    "href": "cca.html#rozkład-asymptotyczny-i-transformacja-do-rozkładu-chi2",
    "title": "Analiza kanoniczna",
    "section": "Rozkład asymptotyczny i transformacja do rozkładu \\(\\chi^2\\)\n",
    "text": "Rozkład asymptotyczny i transformacja do rozkładu \\(\\chi^2\\)\n\nWielu autorów (np. Marriott i Gittins (1986)) sugeruje przekształcenie statystyki Wilksa do postaci asymptotycznie zgodnej z rozkładem \\(\\chi^2\\), np. za pomocą transformacji\n\\[\n-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda) \\sim \\chi^2_{pq}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#procedura-testowania",
    "href": "cca.html#procedura-testowania",
    "title": "Analiza kanoniczna",
    "section": "Procedura testowania",
    "text": "Procedura testowania\n\nOszacuj wszystkie korelacje kanoniczne \\(\\hat{\\rho}_1, \\ldots, \\hat{\\rho}_p\\).\nOd \\(k = 0\\) do \\(p-1\\) oblicz \\(\\Lambda_k = \\prod_{i=k+1}^{p}(1 - \\hat{\\rho}_i^2)\\).\nOblicz transformację \\(\\chi^2_k=-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda_k)\\)\n\nPorównaj z odpowiednim kwantylem rozkładu \\(\\chi^2\\) z \\((q - k)(r - k)\\) stopniami swobody.\nJeśli wartość statystyki przekracza ten kwantyl (\\(p&lt;\\alpha\\)), odrzuć \\(H_0^{(k)}\\) i przejdź do \\(k+1\\). Jeśli nie, zatrzymaj się – kolejne korelacje uznajemy za nieistotne.\n\nOcena dopasowania modelu w analizie kanonicznej (CCA – Canonical Correlation Analysis) obejmuje kilka istotnych wskaźników diagnostycznych, które pozwalają zrozumieć siłę i strukturę relacji między dwoma zbiorami zmiennych. Poniżej omówione zostały trzy kluczowe miary: ładunki czynnikowe, wariancja wyjaśniona oraz redundancja.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#ładunki-czynnikowe-ang.-canonical-loadings",
    "href": "cca.html#ładunki-czynnikowe-ang.-canonical-loadings",
    "title": "Analiza kanoniczna",
    "section": "Ładunki czynnikowe (ang. canonical loadings)",
    "text": "Ładunki czynnikowe (ang. canonical loadings)\n\n\nDefinicja - korelacje pomiędzy zmiennymi kanonicznymi (czyli kombinacjami liniowymi wektorów \\(a_k'X\\) i \\(b_k'Y\\)) a oryginalnymi zmiennymi ze zbiorów \\(X\\) i \\(Y\\).\n\nInterpretacja:\n\nPokazują, które konkretne zmienne pierwotne w największym stopniu „ładują się” (czyli kontrybuują) na daną zmienną kanoniczną.\nWysoka wartość (np. &gt; 0.7) wskazuje na silną zależność między zmienną oryginalną a daną zmienną kanoniczną.\nZnaki dodatnie/ujemne pozwalają wnioskować o kierunku związku.\n\n\n\nWzór:\n\nDla zbioru \\(X\\): \\[\n\\text{loadings}_X = \\mathrm{Corr}(X, U_k) = \\Sigma_{XX} a_k\n\\]\n\nDla zbioru \\(Y\\): \\[\n\\text{loadings}_Y = \\mathrm{Corr}(Y, V_k) = \\Sigma_{YY} b_k\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wariancja-wyjaśniona-ang.-variance-explained",
    "href": "cca.html#wariancja-wyjaśniona-ang.-variance-explained",
    "title": "Analiza kanoniczna",
    "section": "Wariancja wyjaśniona (ang. variance explained)",
    "text": "Wariancja wyjaśniona (ang. variance explained)\n\n\nDefinicja - średnia kwadratów ładunków czynnikowych dla każdej zmiennej kanonicznej i każdego zbioru danych.\n\nInterpretacja:\n\nInformuje, jaką część wariancji oryginalnych zmiennych w danym zbiorze (\\(X\\) lub \\(Y\\)) wyjaśnia dana zmienna kanoniczna.\nMożna traktować ten wskaźnik jako odpowiednik współczynnika determinacji \\(R^2\\) dla pojedynczej zmiennej kanonicznej.\nWysoka wartość oznacza, że dana zmienna kanoniczna dobrze reprezentuje zbiór, z którego została utworzona.\n\n\n\nWzór: \\[\n\\text{Explained variance} = \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k)\n\\] gdzie \\(p\\) to liczba zmiennych w zbiorze \\(X\\), a \\(U_k\\) to \\(k\\)-ta zmienna kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#redundancja-ang.-redundancy-index",
    "href": "cca.html#redundancja-ang.-redundancy-index",
    "title": "Analiza kanoniczna",
    "section": "Redundancja (ang. redundancy index)",
    "text": "Redundancja (ang. redundancy index)\n\n\nDefinicja - iloczyn kwadratu korelacji kanonicznej \\(\\rho_k^2\\) oraz wariancji wyjaśnionej przez daną zmienną kanoniczną we własnym zbiorze.\n\nInterpretacja:\n\nInformuje, jaka część przeciętnej wariancji jednej grupy zmiennych jest wyjaśniana przez zmienną kanoniczną utworzoną na podstawie drugiego zbioru.\nMiara ta pokazuje, czy dany zbiór zmiennych wnosi unikalną informację o drugim zbiorze.\nWysoka redundancja oznacza, że istnieje istotny związek między strukturami dwóch zbiorów zmiennych.\n\n\n\nWzór: \\[\n\\text{Redundancy}_X = \\rho_k^2 \\cdot \\left( \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k) \\right)\n\\] Analogicznie definiujemy redundancję względem \\(Y\\).\n\n\n\n\n\n\n\n\nMiara\nCo opisuje\nInterpretacja praktyczna\n\n\n\nŁadunki czynnikowe\nSiłę powiązania zmiennej oryginalnej z kanoniczną\nWysoka wartość ⇒ silna reprezentacja zmiennej\n\n\nWariancja wyjaśniona\nŚrednia siła reprezentacji zbioru przez zm. kanoniczną\nMiara dopasowania struktury do zbioru\n\n\nRedundancja\nIlość informacji o jednym zbiorze zawarta w drugim\nMiara istotności relacji między zbiorami",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#normalność-wielowymiarowa",
    "href": "cca.html#normalność-wielowymiarowa",
    "title": "Analiza kanoniczna",
    "section": "Normalność wielowymiarowa",
    "text": "Normalność wielowymiarowa\nZakłada się, że obydwa zbiory zmiennych losowych – \\(X\\) i \\(Y\\) – są wspólnie rozkładem normalnym wielowymiarowym jak podano w Równanie 4.1. Normalność umożliwia stosowanie testów statystycznych (np. testu Wilksa) do oceny liczby istotnych korelacji kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-wartości-odstających-outliers",
    "href": "cca.html#brak-wartości-odstających-outliers",
    "title": "Analiza kanoniczna",
    "section": "Brak wartości odstających (outliers)",
    "text": "Brak wartości odstających (outliers)\nZarówno obserwacje odstające jednowymiarowe, jak i wielowymiarowe mogą istotnie zaburzać wynik analizy kanonicznej. Odstające wartości mogą wpływać na macierze kowariancji, zmieniając kierunki i siły relacji między zbiorami zmiennych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wystarczająca-liczba-obserwacji",
    "href": "cca.html#wystarczająca-liczba-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "Wystarczająca liczba obserwacji",
    "text": "Wystarczająca liczba obserwacji\nLiczba obserwacji powinna znacząco przekraczać liczbę zmiennych w każdym zbiorze. Liczba obserwacji \\(n\\) w każdej grupie powinna być większa niż suma liczby zmiennych w \\(X\\) i \\(Y\\) \\[\nn &gt; p + q\n\\] Zapewnia odwracalność macierzy kowariancji oraz stabilność estymatorów.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#liniowość-zależności",
    "href": "cca.html#liniowość-zależności",
    "title": "Analiza kanoniczna",
    "section": "Liniowość zależności",
    "text": "Liniowość zależności\nZakłada się, że związki między wszystkimi parami zmiennych są liniowe. Ponieważ CCA opiera się na maksymalizacji liniowych kombinacji, nieliniowe zależności mogą pozostać niewykryte.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-nadmiernej-współliniowości-multikolinearności",
    "href": "cca.html#brak-nadmiernej-współliniowości-multikolinearności",
    "title": "Analiza kanoniczna",
    "section": "Brak nadmiernej współliniowości (multikolinearności)",
    "text": "Brak nadmiernej współliniowości (multikolinearności)\nZmienne wewnątrz każdego zbioru (w \\(X\\) lub w \\(Y\\)) nie powinny być nadmiernie skorelowane. Wysoka współliniowość może prowadzić do niestabilnych i trudnych do interpretacji wektorów kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#niezależność-obserwacji",
    "href": "cca.html#niezależność-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "Niezależność obserwacji",
    "text": "Niezależność obserwacji\nKażda obserwacja powinna pochodzić od innej jednostki (brak powtórzeń pomiarów). Niezależność warunkuje poprawność estymatorów kowariancji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "fa.html",
    "href": "fa.html",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "Eksploracyjna analiza czynnikowa\nAnaliza czynnikowa należy do klasy metod wielowymiarowych, których celem jest odkrywanie ukrytych struktur stojących za obserwowanymi zmiennymi. W odróżnieniu od metod takich jak analiza głównych składowych1, które opierają się na czysto algebraicznych przekształceniach danych, analiza czynnikowa ma wyraźne odniesienie do modeli statystycznych i psychometrycznych, w których zakłada się istnienie czynników latentnych – czyli zmiennych ukrytych, niewidocznych bezpośrednio, ale wpływających na wartości zmiennych obserwowalnych. Przykładem może być konstrukt „inteligencja”, który przejawia się w wynikach testów logicznych, pamięciowych czy językowych. Głównym celem analizy czynnikowej jest redukcja wymiarowości poprzez reprezentację wielu zmiennych w postaci mniejszej liczby czynników oraz lepsze zrozumienie powiązań między zmiennymi poprzez ujawnienie wspólnych źródeł ich zmienności.\nMożna wyróżnić dwa podstawowe podejścia do analizy czynnikowej. Eksploracyjna analiza czynnikowa (EFA, Exploratory Factor Analysis) jest stosowana, gdy badacz nie ma wcześniej zdefiniowanych hipotez co do liczby czynników czy struktury powiązań między nimi. Jej celem jest odkrycie potencjalnych układów zależności i zidentyfikowanie liczby czynników najlepiej opisujących dane. Konfirmacyjna analiza czynnikowa (CFA, Confirmatory Factor Analysis) jest natomiast podejściem dedukcyjnym – badacz z góry formułuje model teoretyczny (np. że pewne zmienne mierzą „pamięć roboczą”, a inne „myślenie abstrakcyjne”) i testuje jego zgodność z danymi empirycznymi. CFA jest szczególnie istotna w kontekście walidacji narzędzi badawczych, np. kwestionariuszy psychologicznych, i stanowi fundament bardziej zaawansowanych modeli strukturalnych (SEM).\nHistoria analizy czynnikowej sięga początków XX wieku i jest ściśle związana z psychometrią. Jej pionierem był Charles Spearman, który w 1904 roku zaproponował model jednoczynnikowy, interpretując zmienne poznawcze jako przejawy ogólnego czynnika inteligencji. W kolejnych dekadach metoda była rozwijana przez psychologów, takich jak Thurstone, który wprowadził koncepcję wieloczynnikową oraz przez statystyków, którzy rozwijali formalne podstawy estymacji czynników i rotacji macierzy ładunków. W latach 60. i 70. analiza czynnikowa stała się jedną z najczęściej stosowanych metod w badaniach psychologicznych i społecznych, a wraz z rozwojem informatyki zyskała na popularności także w ekonomii, biologii czy medycynie. Dziś analiza czynnikowa jest narzędziem interdyscyplinarnym, stosowanym zarówno do eksploracji struktur danych, jak i do testowania teorii opartych na zmiennych latentnych.\nFormalna postać modelu eksploracyjnej analizy czynnikowej (EFA) zakłada, że zmienne obserwowalne \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)^\\top\\) można wyrazić jako kombinację liniową czynników latentnych oraz składników specyficznych. Model przyjmuje postać („Introduction to Factor Analysis” 2020):\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\]\ngdzie:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#efa",
    "href": "fa.html#efa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) – wektor zmiennych obserwowalnych,\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) – wektor średnich,\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) – macierz ładunków czynnikowych, której element \\(\\lambda_{ij}\\) opisuje wpływ czynnika \\(j\\) na zmienną \\(i\\),\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) – wektor czynników latentnych (czynników wspólnych),\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) – wektor składników specyficznych (unikalnych, błędów pomiaru).\n\n\nZałożenia klasycznego modelu EFA\n\nRozkład czynników wspólnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne mają średnią zero i macierz kowariancji równą macierzy jednostkowej. To założenie oznacza, że czynniki są nieskorelowane i mają wariancję jednostkową (jest to standaryzacja wprowadzona dla identyfikowalności modelu).\nRozkład składników specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzą diagonalną o elementach dodatnich. Oznacza to, że błędy są nieskorelowane między sobą oraz niezależne od czynników \\(\\mathbf{f}\\).\nNiezależność czynników i błędów \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyższej konstrukcji wynika, że kowariancja zmiennych obserwowalnych jest sumą części wspólnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowód\n\n\n\nNiech losowy wektor obserwacji ma postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynników wspólnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor składników specyficznych. Zakładamy, że \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazać, że \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczególności przy \\(\\Phi=I_m\\), że mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a ponieważ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancję \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyrażamy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] Korzystając z liniowości kowariancji i tożsamości \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorów losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skończonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDzięki założeniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikają i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] Jeśli dodatkowo przyjmiemy standardyzację czynników \\(\\Phi=I_m\\) (co jest konwencją identyfikacyjną modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego należało dowieść.\nWarto odnotować, że dowód nie wymaga niezależności \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym — wystarcza nieskorelowanie, aby zniknęły składniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postać \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to można zastosować tzw. whitening czynników \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co równoważnie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), które prowadzą do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to związane z możliwością przeprowadzania różnych transformacji czynników bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogólnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeżeli dokonamy transformacji ortogonalnej czynników \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzą ortogonalną, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, że \\(\\Lambda\\) i \\(\\Phi\\) nie są jednoznacznie wyznaczone. Różne pary \\((\\Lambda, \\Phi)\\) mogą prowadzić do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczególności wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jedną z takich równoważnych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalną rolę, ponieważ jest miejscem, w którym spotykają się dwa składniki zmienności: wspólna i specyficzna. Rozkład \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, że całkowita wariancja i kowariancja obserwowanych zmiennych może być przedstawiona jako suma efektu wspólnych czynników oraz efektu specyficznego, indywidualnego dla każdej zmiennej.\nCzęść \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspólne źródło zmienności, czyli wariancję wyjaśnianą przez czynniki ukryte. To właśnie ta część umożliwia redukcję wymiaru – wiele zmiennych obserwowanych można sprowadzić do kilku czynników, które reprezentują główną strukturę zależności. Interpretacja czynników jako ukrytych wymiarów (np. inteligencja, poziom lęku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uprościć analizę, ale także nadać jej znaczenie teoretyczne w danej dziedzinie badań.\nZ kolei \\(\\Psi\\) odpowiada za wariancję unikalną, czyli tę część zmienności, która nie jest współdzielona z innymi zmiennymi. Obejmuje ona zarówno wariancję czysto specyficzną dla danej cechy, jak i wariancję błędu pomiarowego. Dzięki temu możliwe jest odróżnienie struktury głębokiej (czynnikowej) od elementów przypadkowych i indywidualnych.\nPodsumowując, znaczenie modelu czynnikowego polega na tym, że pozwala on wydzielić istotne, ukryte mechanizmy stojące za współzależnościami zmiennych i oddzielić je od szumów specyficznych dla pojedynczych obserwacji. W praktyce oznacza to możliwość redukcji liczby analizowanych zmiennych, uproszczenie opisu złożonych danych i pogłębienie interpretacji zjawisk społecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynników w praktyce opiera się przede wszystkim na analizie macierzy ładunków czynnikowych \\(\\Lambda\\). Każdy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile związku pomiędzy zmienną obserwowaną \\(x_i\\) a czynnikiem \\(f_j\\). Im wyższa wartość bezwzględna ładunku, tym większy udział danego czynnika w wyjaśnianiu zmienności konkretnej zmiennej. Na przykład w psychologii wysoki ładunek czynnika na zmiennej opisującej pamięć krótkotrwałą i na zmiennej opisującej zdolność rozwiązywania problemów matematycznych może sugerować, że obie cechy są przejawem wspólnego czynnika – inteligencji ogólnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#metody-estymacji-ładunków-czynnikowych",
    "href": "fa.html#metody-estymacji-ładunków-czynnikowych",
    "title": "Analiza czynnikowa",
    "section": "Metody estymacji ładunków czynnikowych",
    "text": "Metody estymacji ładunków czynnikowych\nMetoda największej wiarogodności (ang. Maximal Likelihood, ML) (Lawley 1940)\n\nZałożenia\nZakładamy, że wektor zmiennych obserwowalnych\n\\[\n\\mathbf{x} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\Sigma),\n\\]\ngdzie kowariancja \\(\\Sigma\\) ma postać modelową \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nDla uproszczenia przyjmuje się często, że czynniki \\(\\mathbf{f}\\) są standaryzowane i nieskorelowane, czyli \\(\\Phi = I_m\\). Wówczas macierz kowariancji ma postać\n\\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nFunkcja wiarygodności\nDla próby \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) funkcja wiarygodności rozkładu normalnego wynosi\n\\[\nL(\\Lambda,\\Psi) = (2\\pi)^{-\\frac{np}{2}} |\\Sigma|^{-\\frac{n}{2}}\n\\exp\\left(-\\tfrac{1}{2}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{x}_i-\\mu)\\right).\n\\]\nczęściej wyrażana w postaci zlogarytmowanej\n\\[\n\\ell(\\Lambda,\\Psi) = -\\frac{n}{2} \\left[ \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) \\right] + C,\n\\]\ngdzie \\(S = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)(\\mathbf{x}_i-\\mu)^\\top\\) jest macierzą kowariancji z próby.\nEstymacja parametrów\nEstymatory \\(\\hat{\\Lambda}, \\hat{\\Psi}\\) dobiera się tak, aby maksymalizowały \\(\\ell(\\Lambda,\\Psi)\\), co odpowiada minimalizacji funkcji rozbieżności:\n\\[\nF(\\Lambda,\\Psi) = \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) - \\log |S| - p.\n\\]\nPowyższa miara rozbieżności powstaje z odległości Kullbacka-Leiblera między rozkładami normalnymi \\(\\mathcal{N}_p(\\mu, \\Sigma)\\) i \\(\\mathcal{N}_p(\\mu, S)\\) i jest równa dokładnie \\(2D_{KL}(S||\\Sigma)\\).\nProcedura obliczeniowa\nW praktyce:\n\nWybiera się liczbę \\(m\\) czynników2.\nUstala się początkowe wartości \\(\\Lambda, \\Psi\\)3.\nIteracyjnie poprawia się parametry, rozwiązując równania warunków pierwszego rzędu\n\n2 wybór liczby czynników zostanie przedstawiony nieco później3 spsoby ewstępnej estymacji zostaną omówione w dalszej części\\[\n\\frac{\\partial \\ell}{\\partial \\Lambda} = 0, \\quad \\frac{\\partial \\ell}{\\partial \\Psi} = 0.\n\\]\n\nTakie postępowanie iteracyjne prowadzi się aż do zbieżności funkcji wiarygodności.\nWłasności\n\nEstymatory ML są efektywne przy spełnieniu założenia o normalności wielowymiarowej danych pierwotnych.\nUmożliwiaja testy istotności liczby czynników:\n\nHipoteza \\(H_0: \\Sigma = \\Lambda\\Lambda^\\top + \\Psi\\) vs \\(H_1: \\Sigma\\) dowolna.\nStatystyka testowa ma w przybliżeniu rozkład \\(\\chi^2\\).\n\n\nPozwalają też konstruować przedziały ufności dla ładunków czynnikowych.\nOgraniczenia\n\nWymagaja dużej próby i spełnienia założenia normalności wielowymiarowej.\nMoże być numerycznie niestabilne, zwłaszcza gdy liczba czynników jest duża w stosunku do liczby zmiennych.\nPrzy małych próbach lub silnym naruszeniu normalności wyniki mogą być obciążone.\nMetoda osi głównych (ang. Principal Axis Factoring, PAF) (Grieder i Steiner 2020)\n\nIdea metody PAF\nW metodzie PAF znanej również jako metoda czynników głównych, zakładamy klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad \\mathrm{Cov}(\\mathbf{x}) = \\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nCelem jest znalezienie takiego \\(\\Lambda\\) i \\(\\Psi\\), aby zbliżyć się do macierzy kowariancji próbkowej \\(S\\). W odróżnieniu od ML, PAF nie opiera się na funkcji wiarygodności ani na rozbieżności Kullbacka–Leiblera, lecz maksymalizuje wariancję wspólną zmiennych, traktując część specyficzną \\((\\Psi)\\) jako resztę.\nMacierz zredukowanych korelacji\nW metodzie Principal Axis Factoring (PAF) kluczową rolę odgrywa macierz zredukowanych korelacji. Punktem wyjścia jest macierz korelacji \\(\\mathbf{R}\\) pomiędzy zmiennymi obserwowanymi \\(\\mathbf{x}\\). Na diagonali tej macierzy stoją jedynki, odzwierciedlające fakt, że każda zmienna jest w pełni skorelowana sama ze sobą. Jednak w modelu czynnikowym zakładamy, że całkowita wariancja zmiennej \\(x_j\\) może zostać podzielona na część wspólną (zasoby zmienności wspólnej - ang. communalities) i część swoistą (zasoby zmienności swoistej - ang. uniqness):\n\\[\n1 = h_j^2 + \\psi_j, \\quad j=1,\\dots,p,\n\\]\ngdzie \\(h_j^2\\) oznacza zasób zmienności wspólnej, a \\(\\psi_j\\) wariancję swoistą. W konstrukcji macierzy zredukowanych korelacji zamiast jedynek wstawia się w diagonali właśnie wartości \\(h_j^2\\). Otrzymujemy w ten sposób macierz\n\\[\n\\mathbf{R}^* = [r_{ij}^*], \\quad r_{jj}^* = h_j^2.\n\\]\nMacierz \\(\\mathbf{R}^*\\) ma więc charakter „zredukowany”, ponieważ na jej diagonali pozostaje tylko ta część wariancji zmiennej, którą model czynnikowy ma szansę wyjaśnić. Dzięki temu macierz ta może być przybliżana przez strukturę \\(\\Lambda \\Lambda^\\top\\), co odpowiada wspólnej wariancji wszystkich zmiennych.\n\n\n\n\n\n\nWstępne oszacowania zasobów zmienności wspólnej\n\n\n\nProblem polega na tym, że wartości \\(h_j^2\\) nie są znane a priori. Dlatego w praktyce stosuje się różne metody wstępnego ich wyznaczania, które mogą być następnie udoskonalane iteracyjnie w kolejnych krokach procedury PAF. Do najczęściej stosowanych metod należą:\n\nśrednia arytmetyczna współczynników korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\frac{1}{m}\\sum_{j'=1}^m r_{jj'},\\quad j\\ne j'\n\\]\nmaksymalna wartość bezwzględna współczynników korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\max_{j'}|r_{jj'}|, \\quad j\\ne j',\n\\]\nwspółczynnik determinacji wielokrotnej danej zmiennej z innymi zmiennymi (najczęściej stosowana i wykorzystywana przez R) \\[\nh_j^2=R^2_{j\\cdot 1,2,\\ldots,m},\n\\]\nformuła triad \\[\nh_j^2=\\frac{r_{jj'}r_{jj''}}{r_{j'j''}}, \\quad j\\ne j' \\ne j''\n\\] gdzie \\(r_{jj'}, r_{jj''}\\) - dwie najwyższe wartości współczynników korelacji \\(j\\)-tej zmiennej z innymi zmiennymi.\n\n\n\nRozkład na wartości własne\nW metodzie PAF zakładamy, że tylko część wariancji każdej zmiennej jest wspólna. Oznacza to, że zamiast pełnej macierzy korelacji \\(\\mathbf{R}\\), rozważamy macierz zredukowanych korelacji: \\[\n\\mathbf{R}^* = \\mathbf{R} - \\Psi,\n\\] gdzie na diagonali znajdują się oszacowane zasoby zmienności wspólnej \\(\\hat{h}_j^2\\), zamiast jedynek.\nNastępnie wykonujemy dekompozycję spektralną tej macierzy: \\[\n\\mathbf{R}^* = \\mathbf{Q}^* \\mathbf{D}^* {\\mathbf{Q}^*}^\\top,\n\\] gdzie \\(\\mathbf{Q}^*\\) i \\(\\mathbf{D}^*\\) są odpowiednio wektorami i wartościami własnymi macierzy \\(\\mathbf{R}^*\\).\nEstymator ładunków czynnikowych w PAF ma więc postać \\[\n\\hat{\\Lambda} = \\mathbf{Q}^*_m (\\mathbf{D}^*_m)^{1/2},\n\\]\nbazującą na zmodyfikowanej macierzy korelacji, w której uwzględniono oszacowane komunalności.\nPonieważ \\(\\hat{h}_j^2\\) same zależą od ładunków (są ich sumą kwadratów), w praktyce stosuje się procedurę iteracyjną: zaczynamy od pewnych wartości początkowych, obliczamy dekompozycję spektralną, aktualizujemy komunalności i powtarzamy procedurę aż do zbieżności.\nIteracyjna poprawa komunalności\nPonieważ początkowe komunalności są przybliżone, PAF stosuje procedurę iteracyjną:\n\nSzacujemy \\(\\Lambda\\) na podstawie bieżącego \\(\\mathbf{R}^*\\).\nObliczamy nowe zasoby zmienności wspólnej \\(h_j^2 = \\sum_{k=1}^m \\lambda_{jk}^2\\).\nWstawiamy je na przekątnej \\(\\mathbf{R}^*\\) zamiast starych wartości.\nPowtarzamy rozkład wartości własnych.\n\nProces powtarza się aż do zbieżności, czyli stabilizacji ładunków czynnikowych i zasobów zmienności wspólnej.\nWłasności\n\n\nDopasowanie do wariancji wspólnej – PAF minimalizuje różnice pomiędzy macierzą zredukowanych korelacji \\(\\mathbf{R}^*\\) a aproksymacją \\(\\Lambda \\Lambda^\\top\\). Sskupia się na wariancji wspólnej.\n\nIteracyjność oszacowań – estymatory w PAF powstają w procesie iteracyjnym, w którym kolejne przybliżenia komunalności są poprawiane na podstawie sumy kwadratów aktualnych ładunków czynnikowych. Dzięki temu metoda zbiega do rozwiązań lepiej oddających strukturę wspólną niż proste metody jednorazowe.\n\nNiestandaryzowana postać estymatorów – rozwiązania PAF mogą zależeć od przyjętych wartości początkowych \\(h_j^2\\). Różne wybory startowe mogą prowadzić do nieco innych estymatorów, choć w praktyce po kilku iteracjach zbieżność do stabilnego rozwiązania jest zazwyczaj dobra.\n\nInterpretowalność – ponieważ oszacowane ładunki czynnikowe odzwierciedlają wyłącznie część wspólną wariancji, interpretacja czynników uzyskanych metodą PAF jest bliższa teoretycznemu modelowi czynnikowemu niż w przypadku metod opartych na PCA.\nOgraniczenia\n\n\nBrak optymalności w sensie funkcji wiarygodności – w przeciwieństwie do metody największej wiarygodności (ML), estymatory PAF nie mają znanych własności asymptotycznych, takich jak efektywność czy zgodność w sensie probabilistycznym. Są bardziej heurystyczne niż ściśle statystyczne.\n\nZależność od wartości początkowych komunalności – oszacowania początkowe wpływają na przebieg iteracji i mogą prowadzić do lokalnych minimów. W praktyce wybór metody startowej (np. \\(R^2\\), średnia korelacja, …) ma znaczenie dla szybkości i stabilności algorytmu.\n\nMożliwość uzyskania ujemnych komunalności – w niektórych przypadkach iteracje mogą prowadzić do oszacowań \\(h_j^2 &lt; 0\\) (tzw. przypadek Haywooda), co jest sprzeczne z definicją wariancji wspólnej. Wówczas konieczne stosowanie innych metod estymacji ładunków.\n\nMniejsza przydatność przy małych próbach – ponieważ metoda nie opiera się na pełnym modelu statystycznym, jej własności są mniej stabilne przy niewielkich licznościach obserwacji. Wyniki mogą być wówczas silnie zależne od przypadkowych fluktuacji w danych.\n\nBrak testów statystycznych dopasowania modelu – w odróżnieniu od metody ML, PAF nie pozwala na formalne testowanie hipotez o liczbie czynników czy jakości dopasowania modelu do danych.\nMetoda sładowych głównych (ang. Principal Component Method) (Wang i in. 2008)\n\nMetoda sładowych głównych należy do klasy metod wspólnotowych, czyli takich, które zakładają klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\quad \\Sigma = \\Lambda\\Lambda^\\top + \\Psi.\n\\]\nCelem jest oszacowanie macierzy ładunków \\(\\Lambda\\), tak aby jak najlepiej odtworzyć część wspólną wariancji.\nIdea metody\nW metodzie PCM zakładamy, że cała wariancja zmiennej jest wariancją wspólną, tzn. \\[\nh_j^2 = 1, \\quad j=1,\\ldots,p.\n\\]\nOznacza to, że macierz zredukowanych korelacji jest po prostu zwykłą macierzą korelacji \\(\\mathbf{R}\\): \\[\n\\mathbf{R} = \\Lambda \\Lambda^\\top + \\Psi,\n\\] przy czym w PCM przyjmujemy \\(\\Psi = \\mathbf{0}\\).\nNastępnie wykonujemy dekompozycję spektralną \\[\n\\mathbf{R} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^\\top,\n\\] gdzie:\n\n\n\\(\\mathbf{Q} = (q_1, q_2, \\ldots, q_p)\\) – to macierz ortonormalnych wektorów własnych,\n\n\\(\\mathbf{D} = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p)\\) – to macierz wartości własnych uporządkowanych malejąco.\n\nJeśli chcemy oszacować model z \\(m\\) czynnikami, to bierzemy największe \\(m\\) wartości własne i odpowiadające im wektory własne. Estymator ładunków czynnikowych jest wtedy równy \\[\n\\hat{\\Lambda} = \\mathbf{Q}_m \\mathbf{D}_m^{1/2},\n\\] gdzie \\(\\mathbf{Q}_m = (q_1,\\ldots,q_m)\\), a \\(\\mathbf{D}_m = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_m)\\).\nWidzimy więc, że w PCM ładunki są wprost pierwiastkami z największych wartości własnych pomnożonymi przez odpowiadające im wektory własne.\nProcedura estymacji4\n\n\nKonstruujemy macierz korelacji \\(\\mathbf{R}\\).\nObliczamy rozkład wartości i wektorów własnych macierzy \\(\\mathbf{R}\\).\nWybieramy \\(m\\) największych wartości własnych (odpowiadających liczbie czynników w modelu).\nNa tej podstawie konstruujemy macierz ładunków czynnikowych \\(\\Lambda\\).\n4 tu widać największą różnicę pomięcy PCM a PAF; w metodzie PCM występuję jedna iteracja estymacji ładunkówWłasności\n\n\nZgodność z modelem czynnikowym – metoda dąży do aproksymacji struktury wspólnej wariancji, a nie całkowitej wariancji.\n\nZbieżność do stabilnych oszacowań – iteracyjne poprawki komunalności pozwalają uzyskać estymatory spójne z założeniami modelu.\n\nŁatwość interpretacji – podobnie jak PCA, metoda bazuje na analizie spektralnej wartości własnych, co ułatwia intuicyjne rozumienie struktury danych.\nOgraniczenia\n\n\nBrak optymalności statystycznej – podobnie jak PAF, metoda nie ma własności estymatorów opartych na funkcji wiarygodności (ML).\n\nZależność od początkowych oszacowań komunalności – nieprawidłowy wybór startowy może utrudnić uzyskanie sensownych rozwiązań.\n\nHaywood case – zdarza się, że zasoby zmienności wspólnej mogą przyjmować wartości ujemne.\nMetoda minimalizacji reszt (ang. MINRES) (Harman i Jones 1966)\n\nIdea metody MINRES\nW modelu czynnikowym przyjmujemy, że macierz kowariancji (lub korelacji) ma postać \\[\n\\Sigma = \\Lambda \\Lambda' + \\Psi,\n\\] gdzie \\(\\Lambda\\) to macierz ładunków czynnikowych, a \\(\\Psi = \\mathrm{diag}(\\psi_1,\\ldots,\\psi_p)\\) to macierz wariancji swoistych.\nW metodzie MINRES nie próbujemy dokładnie odtworzyć całej macierzy \\(\\Sigma\\). Zamiast tego minimalizujemy reszty pozadiagonalne, czyli różnice między obserwowaną macierzą korelacji \\(\\mathbf{R}\\) a macierzą odtworzoną z modelu \\(\\Lambda \\Lambda^\\top + \\Psi\\), przy czym skupiamy się wyłącznie na elementach pozadiagonalnych.\nFunkcja kryterialna\nFormalnie minimalizowana jest suma kwadratów reszt poza przekątną \\[\nF(\\Lambda, \\Psi) = \\sum_{i \\neq j} \\Big( r_{ij} - \\hat{r}_{ij} \\Big)^2,\n\\] gdzie:\n\n\n\\(r_{ij}\\) to element macierzy korelacji empirycznej \\(\\mathbf{R}\\),\n\n\\(\\hat{r}_{ij}\\) to element macierzy odtworzonej \\(\\Lambda \\Lambda^\\top + \\Psi\\),\nelementy diagonalne nie są uwzględniane (bo zawsze odtwarzane są przez normalizację zmiennych).\n\nMożna to zapisać równoważnie jako \\[\nF(\\Lambda) = | \\mathbf{R} - (\\Lambda \\Lambda' + \\Psi)|^2_{off},\n\\] gdzie \\(|\\cdot|_{off}\\) oznacza normę Frobeniusa liczona tylko na częściach pozadiagonalnych macierzy.\nProcedura estymacyjna\n\nZaczynamy od przybliżonych wartości komunalności \\(\\hat{h}_j^2\\), tak jak w PAF.\nBudujemy macierz reszt \\[\n\\mathbf{U} = \\mathbf{R} - (\\Lambda \\Lambda^\\top + \\Psi).\n\\]\n\nSzukamy takich ładunków \\(\\Lambda\\), które minimalizują sumę kwadratów elementów \\(\\mathbf{U}\\) poza przekątną.\nW praktyce problem redukuje się do iteracyjnego rozwiązywania układów równań własnych, bardzo podobnie jak w PAF, ale z innym warunkiem minimalizacji (PAF dopasowuje wartości własne macierzy zredukowanych korelacji, MINRES – reszty pozadiagonalne).\nZwiązek z dekompozycją spektralną\nW przeciwieństwie do PCM czy PAF, metoda MINRES nie ma bezpośredniego prostego rozwiązania w postaci pierwiastków z wartości własnych. Wymaga zastosowania iteracyjnych algorytmów numerycznych, które szukają \\(\\Lambda\\) minimalizującej \\(F(\\Lambda)\\). Jednak podobnie jak w PAF, punktem startowym mogą być wektory własne macierzy zredukowanych korelacji. Następnie algorytm minimalizacji dopasowuje ładunki tak, by reszty pozadiagonalne były jak najmniejsze.\nWłaściwości i ograniczenia\n\n\nMINRES skupia się tylko na korelacjach pomiędzy zmiennymi, ignorując elementy diagonalne – co sprawia, że estymacja jest mniej wrażliwa na problem ujemnych komunalności (tzw. Heywood cases).\nMetoda jest relatywnie stabilna numerycznie i dobrze sprawdza się przy dużej liczbie zmiennych.\nOgraniczeniem jest to, że wynik zależy od jakości początkowych oszacowań zasobów zmienności wspólnej. Przy złym wyborze startu możliwa jest wolna zbieżność albo zbieżność do lokalnego minimum.\nMetoda uogólnionych najmniejszych kwadratów (ang. Generalized Least Squares, GLS) (Jöreskog i Goldberger 1972)\n\nIdea metody\nGLS, podobnie jak MINRES czy ML, polega na porównaniu macierzy obserwowanej \\(\\mathbf{S}\\) (kowariancji lub korelacji) z macierzą odtworzoną przez model czynnikowy \\(\\hat{\\Sigma} = \\Lambda \\Lambda^\\top + \\Psi\\). Różnica w stosunku do MINRES polega na tym, że w GLS ważymy reszty, czyli błędy odwzorowania poszczególnych elementów macierzy \\(\\mathbf{S}\\).\nFormalnie kryterium minimalizacji ma postać \\[\nF_{\\text{GLS}}(\\Lambda, \\Psi) = \\mathrm{tr}\\Big[ \\big( S - \\hat{\\Sigma} \\big) W \\big( S - \\hat{\\Sigma} \\big) W \\Big],\n\\]\ngdzie \\(W\\) to macierz wag, zwykle przyjmowana jako odwrotność (lub pseudoodwrotność) wariancji estymatora elementów macierzy \\(\\mathbf{S}\\).\nW przeciwieństwie do MINRES (gdzie wszystkie reszty traktowane są jednakowo), w GLS różne elementy macierzy kowariancji otrzymują różne wagi. Wagi te wynikają z asymptotycznych własności estymatora macierzy kowariancji i uwzględniają fakt, że elementy macierzy nie są niezależne i mają różne wariancje. Dzięki temu GLS jest bardziej efektywny statystycznie niż MINRES, ale jednocześnie mniej wymagający niż ML (który zakłada pełną normalność wielowymiarową).\nWłasności\n\nEstymatory GLS są spójne i asymptotycznie efektywne w klasie metod najmniejszych kwadratów, przy założeniu poprawnej specyfikacji modelu.\nGLS, podobnie jak ML, uwzględnia strukturę wariancji elementów macierzy \\(\\mathbf{S}\\), co czyni go bardziej precyzyjnym niż MINRES.\nZ drugiej strony GLS jest mniej czuły na naruszenie założenia normalności niż ML, dlatego bywa rekomendowany przy większych odchyleniach od normalności.\nOgraniczenia\n\nProcedura GLS jest obliczeniowo trudniejsza niż MINRES, ponieważ wymaga oszacowania (lub przyjęcia) odpowiedniej macierzy wag.\nW praktyce GLS bywa niestabilny przy małych próbach lub przy silnych współliniowościach zmiennych.\nW implementacjach programowych często stosuje się GLS jako kompromis pomiędzy prostym MINRES a wymagającym ML.\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nIstnieją również inne metody estymacji ładunków czynnikowych, jak metody bayesowskie (Lu, Chow, i Loken 2016), czy metody z regularyzacją LASSO ale nie są one częścią tego opracowania (Jacobucci i Grimm 2018).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynników",
    "href": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynników",
    "title": "Analiza czynnikowa",
    "section": "Oceny dopasowania modelu i kryteria doboru liczby czynników",
    "text": "Oceny dopasowania modelu i kryteria doboru liczby czynników\nOcena dopasowania modelu EFA opiera się na kilku uzupełniających się perspektywach: globalnym dopasowaniu implikowanej macierzy kowariancji do macierzy empirycznej, analizie reszt korelacyjnych, doborze liczby czynników, stabilności rozwiązania oraz jakości lokalnej (ładunki i zasoby zmienności wspólnej). Poniżej przedstawiam najważniejsze procedury wraz z ich interpretacją oraz typowymi pułapkami.\nProporcja wyjaśnionej wariancji przez czynniki\nProporcja wariancji wyjaśnionej przez model czynnikowy, czyli stosunek sumy wariancji wspólnej do całkowitej wariancji wszystkich zmiennych, stanowi podstawową miarę jakości dopasowania. W przypadku standaryzowanych zmiennych całkowita wariancja wynosi \\(p\\), więc proporcja ta ma postać \\[\n\\text{Proporcja wyjaśnionej wariancji} = \\frac{\\sum_{j=1}^p h_j^2}{p}.\n\\] Wyższe wartości (np. powyżej \\(0,6\\)) wskazują na dobrą reprezentację zmiennych przez czynniki, natomiast niskie wartości (np. poniżej \\(0,4\\)) sugerują, że model nie uchwytuje istotnej części struktury danych. Jednak sama proporcja nie uwzględnia liczby czynników ani złożoności modelu, dlatego powinna być interpretowana w kontekście innych wskaźników dopasowania.\nTest chi-kwadrat\nW metodzie ML został przedstawiony test dopasowania oparty na maximum likelihood. Przy założeniu normalności wielowymiarowej i zidentyfikowanym modelu postaci \\[\n\\Sigma=\\Lambda\\Lambda^\\top + \\Psi\n\\] testujemy hipotezę \\(H_0:\\ \\Sigma(\\Lambda,\\Psi)=S\\) w populacji, gdzie \\(S\\) oznacza macierz kowariancji (lub korelacji) z próby. Statystyka \\(\\chi^2\\) rośnie wraz z pogarszającym się dopasowaniem (niestety duże próby sprzyjają odrzucaniu nawet dobrze dopasowanych modeli, a naruszenia normalności mogą zawyżać lub zaniżać wynik).\nWskaźnik RMSEA\nWskaźnik root mean square error of approximation (RMSEA) mierzy błąd aproksymacji na jednostkę stopnia swobody i można go interpretować jako „błąd w populacji”, nie tylko w próbie. Definiujemy go jako \\[\n\\mathrm{RMSEA}=\\sqrt{\\max\\left\\{\\frac{\\chi^2-df}{df(n-1)},0\\right\\}},\n\\] a ocenę uzupełniamy o przedział ufności oparty na niecentralnym rozkładzie chi-kwadrat. Wartości rzędu \\(0,05-0,08\\) tradycyjnie uznawane są za akceptowalne, traktując progi orientacyjnie: wzrost liczby zmiennych i stopni swobody sprzyja niższym RMSEA, natomiast małe próby destabilizują oszacowanie.\nAnaliza reszt\nAnaliza reszt macierzy korelacji stanowi podstawową kontrolę lokalnego dopasowania, niezależnie od sposobu estymacji. Wyznaczamy reszty \\(r_{ij}-\\hat r_{ij}\\) i przeglądamy rozkład wartości bezwzględnych, a dokładnie odsetek przekraczających praktyczne progi (np. \\(0,05\\)). Wskaźniki zbiorcze, takie jak RMSR (root mean square residual) oraz SRMR (standardized RMSR), agregują wielkość reszt poza diagonalą - mniejsze wartości świadczą o lepszym dopasowaniu. Mapa ciepła reszt ułatwia wykrywanie klastrów niedopasowania sugerujących brakujący czynnik lub zbyt małą liczbę czynników.\nKryteria informacyjne\nKryteria informacyjne, takie jak AIC i BIC, służą do porównywania modeli o różnej liczbie czynników, karząc nadmierną złożoność. Definiujemy je przez logarytm funkcji wiarogodności i liczbę parametrów. BIC silniej faworyzuje prostsze modele przy dużych próbach. Bardzo ważne jest aby używać tych metod do porównywania modeli otrzymanych tą samą metodą.\nInne wskaźniki dopasowania\nWskaźniki „globalne” starszej generacji, takie jak GFI i AGFI (goodness of fit index, adjusted GFI), oceniają proporcję wariancji/kowariancji wyjaśnionej przez model. Są wrażliwe na rozmiar próby i liczbę zmiennych, skłonne do optymizmu w dużych modelach i do pesymizmu przy małej liczbie stopni swobody. Możemy je traktować pomocniczo, kładąc większy nacisk na RMSEA oraz analizę reszt.\nAnaliza wartości własnych macierzy reszt uzupełnia powyższe podejścia. Po wyodrębnieniu \\(m\\) czynników obliczamy resztową macierz korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) i badać jej wartości własne. Duże dodatnie wartości własne sygnalizują pozostawioną wspólną wariancję (niedomiar czynników) lub struktury lokalne.\nJakość lokalną rozwiązania oceniać przez zasoby zmienności wspólnej i swoistej. \\[\nh_j^2=\\sum_{k=1}^{m}\\lambda_{jk}^{2}\n\\] mierzą część wariancji zmiennej \\(x_j\\) wyjaśnioną przez czynniki, bardzo niskie \\(h_j^2\\) wskazują słabą reprezentację zmiennej, natomiast bardzo wysokie — wraz z ryzykiem ujemnych \\(\\Psi_j\\) (przypadki Haywooda) — mogą sygnalizować dopasowanie wymuszone lub niewłaściwą liczebność czynników. Sumy kwadratów ładunków per czynnik odzwierciedlają wyjaśnioną wspólną wariancję i służą do oceny równomierności wkładu czynników.\nW rozwiązaniach dopuszczajacych korelacje pomiedzy czynnikami dodatkowym aspektem dopasowania jest macierz korelacji czynników \\(\\Phi\\). Bardzo wysokie korelacje między czynnikami sugerują nadmiarowość i potencjalne przeparametryzowanie. Wówczas warto rozważyć redukcję liczby czynników lub alternatywne struktury.\nNajbardziej znane kryteria doboru liczby czynników to:\nKryterium wykresu osypiska (Scree plot, Cattell (1966))\nNa osi poziomej odkładamy kolejne wartości własne, a na pionowej ich wielkość. Punktem granicznym jest miejsce, gdzie wykres „załamuje się” i przechodzi w „osypisko” – od tego miejsca czynniki interpretowane są jako szum.\n\nZalety: wizualna intuicja, łatwe zastosowanie.\nWady: często subiektywność w określeniu miejsca „łokcia”, szczególnie gdy krzywa nie ma wyraźnego załamania.\nAnaliza równoległa (Parallel analysis, Horn (1965))\nPolega na porównaniu wartości własnych dla danych empirycznych z wartościami własnymi uzyskanymi dla danych losowych o tej samej strukturze (ta sama liczba zmiennych i obserwacji). Zatrzymuje się te czynniki, których wartości własne przewyższają np. 95. percentyl rozkładu wartości losowych.\n\nZalety: jedna z najbardziej rekomendowanych metod, dobrze sprawdza się w praktyce.\nWady: wymaga procedur symulacyjnych, większej mocy obliczeniowej.\nKryterium MAP (Minimum Average Partial, Velicer (1976))\nOpiera się na analizie korelacji cząstkowych. Stopniowo usuwa się kolejne czynniki, a następnie oblicza średnią wartość kwadratu korelacji cząstkowych. Liczba czynników odpowiadająca minimum tej wartości uznawana jest za optymalną.\n\nZalety: metoda oparta na minimalizacji resztowych zależności, obiektywna.\nWady: wrażliwa na naruszenia założeń modelu, mniej intuicyjna dla początkujących.\nTesty statystyczne dopasowania (dla ML)\nPrzy estymacji metodą największej wiarygodności można zastosować test chi-kwadrat dla porównania modelu z \\(m\\) czynnikami z modelem pełnym. Sprawdza się, czy macierz implikowana przez model różni się istotnie od empirycznej. Liczbę czynników dobiera się tak, aby model był jeszcze akceptowalny, ale nie przeparametryzowany.\n\nZalety: formalne podejście statystyczne.\nWady: silna wrażliwość na liczność próby i założenie normalności wielowymiarowej; w dużych próbach nawet dobre modele mogą być odrzucane.\nKryteria informacyjne (AIC, BIC, CAIC)\nPorównują modele o różnej liczbie czynników, równoważąc dopasowanie (log-wiarygodność) i złożoność (liczbę parametrów). Optymalna liczba czynników to ta, dla której wartość kryterium jest minimalna.\n\nZalety: uwzględniają karę za nadmierną złożoność, dobrze sprawdzają się w porównaniach.\nWady: wartości kryteriów są zależne od metody estymacji, więc porównywać można tylko modele oszacowane tą samą metodą.\nAnaliza reszt i spektrum wartości własnych macierzy reszt\nPo przyjęciu liczby czynników oblicza się macierz reszt korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) . Jeśli w resztach (poza przekątną) pozostają duże (co do wartości bezwzględnej) wartości własne, oznacza to, że nie wszystkie wspólne zależności zostały uchwycone i potrzebne są dodatkowe czynniki.\n\nZalety: pozwala ocenić niedopasowanie „lokalne” i strukturalne.\nWady: wymaga bardziej zaawansowanej interpretacji.\nUdział wyjaśnionej wariancji\nW praktyce często wymaga się, aby całkowita wyjaśniona wariancja przekraczała określony próg (np. 50% w naukach społecznych). Dodatkowo analizuje się równomierność wkładu poszczególnych czynników.\n\nZalety: intuicyjne i łatwe do raportowania.\nWady: arbitralne progi, zależne od liczby zmiennych i kontekstu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#eksploracyjna-analiza-czynnikowa",
    "href": "fa.html#eksploracyjna-analiza-czynnikowa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) – wektor zmiennych obserwowalnych,\n\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) – wektor średnich,\n\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) – macierz ładunków czynnikowych, której element \\(\\lambda_{ij}\\) opisuje wpływ czynnika \\(j\\) na zmienną \\(i\\),\n\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) – wektor czynników latentnych (czynników wspólnych),\n\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) – wektor składników specyficznych (unikalnych, błędów pomiaru).\n\nZałożenia klasycznego modelu EFA\n\nRozkład czynników wspólnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne mają średnią zero i macierz kowariancji równą macierzy jednostkowej. To założenie oznacza, że czynniki są nieskorelowane i mają wariancję jednostkową (jest to standaryzacja wprowadzona dla identyfikowalności modelu).\nRozkład składników specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzą diagonalną o elementach dodatnich. Oznacza to, że błędy są nieskorelowane między sobą oraz niezależne od czynników \\(\\mathbf{f}\\).\nNiezależność czynników i błędów \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyższej konstrukcji wynika, że kowariancja zmiennych obserwowalnych jest sumą części wspólnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowód\n\n\n\nNiech losowy wektor obserwacji ma postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynników wspólnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor składników specyficznych. Zakładamy, że \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazać, że \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczególności przy \\(\\Phi=I_m\\), że mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a ponieważ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancję \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyrażamy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] Korzystając z liniowości kowariancji i tożsamości \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorów losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skończonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDzięki założeniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikają i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] Jeśli dodatkowo przyjmiemy standardyzację czynników \\(\\Phi=I_m\\) (co jest konwencją identyfikacyjną modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego należało dowieść.\nWarto odnotować, że dowód nie wymaga niezależności \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym — wystarcza nieskorelowanie, aby zniknęły składniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postać \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to można zastosować tzw. whitening czynników \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co równoważnie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), które prowadzą do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to związane z możliwością przeprowadzania różnych transformacji czynników bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogólnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeżeli dokonamy transformacji ortogonalnej czynników \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzą ortogonalną, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, że \\(\\Lambda\\) i \\(\\Phi\\) nie są jednoznacznie wyznaczone. Różne pary \\((\\Lambda, \\Phi)\\) mogą prowadzić do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczególności wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jedną z takich równoważnych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalną rolę, ponieważ jest miejscem, w którym spotykają się dwa składniki zmienności: wspólna i specyficzna. Rozkład \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, że całkowita wariancja i kowariancja obserwowanych zmiennych może być przedstawiona jako suma efektu wspólnych czynników oraz efektu specyficznego, indywidualnego dla każdej zmiennej.\nCzęść \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspólne źródło zmienności, czyli wariancję wyjaśnianą przez czynniki ukryte. To właśnie ta część umożliwia redukcję wymiaru – wiele zmiennych obserwowanych można sprowadzić do kilku czynników, które reprezentują główną strukturę zależności. Interpretacja czynników jako ukrytych wymiarów (np. inteligencja, poziom lęku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uprościć analizę, ale także nadać jej znaczenie teoretyczne w danej dziedzinie badań.\nZ kolei \\(\\Psi\\) odpowiada za wariancję unikalną, czyli tę część zmienności, która nie jest współdzielona z innymi zmiennymi. Obejmuje ona zarówno wariancję czysto specyficzną dla danej cechy, jak i wariancję błędu pomiarowego. Dzięki temu możliwe jest odróżnienie struktury głębokiej (czynnikowej) od elementów przypadkowych i indywidualnych.\nPodsumowując, znaczenie modelu czynnikowego polega na tym, że pozwala on wydzielić istotne, ukryte mechanizmy stojące za współzależnościami zmiennych i oddzielić je od szumów specyficznych dla pojedynczych obserwacji. W praktyce oznacza to możliwość redukcji liczby analizowanych zmiennych, uproszczenie opisu złożonych danych i pogłębienie interpretacji zjawisk społecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynników w praktyce opiera się przede wszystkim na analizie macierzy ładunków czynnikowych \\(\\Lambda\\). Każdy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile związku pomiędzy zmienną obserwowaną \\(x_i\\) a czynnikiem \\(f_j\\). Im wyższa wartość bezwzględna ładunku, tym większy udział danego czynnika w wyjaśnianiu zmienności konkretnej zmiennej. Na przykład w psychologii wysoki ładunek czynnika na zmiennej opisującej pamięć krótkotrwałą i na zmiennej opisującej zdolność rozwiązywania problemów matematycznych może sugerować, że obie cechy są przejawem wspólnego czynnika – inteligencji ogólnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#rotacje-czynników",
    "href": "fa.html#rotacje-czynników",
    "title": "Analiza czynnikowa",
    "section": "Rotacje czynników",
    "text": "Rotacje czynników\nRotacja czynników jest etapem analizy czynnikowej, którego celem jest poprawa interpretowalności rozwiązania poprzez uproszczenie struktury ładunków czynnikowych. Matematycznie polega ona na zastosowaniu transformacji liniowej do macierzy ładunków \\(\\Lambda\\). Jeśli \\(\\Lambda\\) jest macierzą \\(p \\times m\\) ładunków (gdzie \\(p\\) to liczba zmiennych, a \\(m\\) liczba czynników), to po rotacji otrzymujemy nową macierz ładunków \\[\n\\Lambda^* = \\Lambda T,\n\\] gdzie \\(T\\) jest macierzą transformacji rotacyjnej o wymiarach \\(m \\times m\\). W zależności od własności macierzy \\(T\\) wyróżnia się dwa główne typy rotacji: ortogonalne i skośne (oblique).\nRotacje ortogonalne\nW przypadku rotacji ortogonalnych macierz \\(T\\) jest macierzą ortogonalną, czyli spełnia warunek: \\[T^\\top T = TT^\\top = I_m.\\] Oznacza to, że czynniki po rotacji pozostają nieskorelowane (\\(\\Phi = I_m\\)).\nNajważniejsze rodzaje rotacji ortogonalnych:\n\nVarimax (Kaiser 1958) - najczęściej stosowana rotacja ortogonalna. Maksymalizuje wariancję kwadratów ładunków w ramach każdego czynnika. Prowadzi do tego, że każda zmienna ma wysokie ładunki tylko na jednym czynniku, a bliskie zeru na pozostałych. Funkcja celu \\[\nV = \\sum_{j=1}^m \\left[ \\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*4} - \\left(\\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*2}\\right)^2 \\right].\n\\]\nQuartimax - minimalizuje liczbę czynników potrzebnych do opisania każdej zmiennej, upraszczając wiersze macierzy ładunków. Funkcja celu \\[\nQ = \\sum_{i=1}^p \\sum_{j=1}^m \\lambda_{ij}^{*4}.\n\\]\nEquamax - łączy idee varimax i quartimax. Celem jest równoważenie prostoty struktur wierszy i kolumn macierzy ładunków. Funkcja celu \\[\nE = \\frac12(Q + V).\n\\]\nBiquartimax - celem tej rotacji jest jednoczesne uproszczenie wierszy i kolumn macierzy ładunków. W praktyce łączy zalety varimax i quartimax. Funkcja celu \\[\nBQ = \\alpha \\, Q + (1 - \\alpha) \\, V,\n\\] z modyfikacją wag, które równoważą wpływ prostoty wierszy i kolumn. Zmienne mają tendencję do ładowania się mocno na jednym czynniku (jak w varimax), ale jednocześnie ogranicza się sytuacje, w których jedna zmienna ma średnie ładunki na wielu czynnikach (jak w quartimax).\nRotacje skośne (oblique)\nW przypadku rotacji skośnych macierz \\(T\\) nie musi być ortogonalna, więc \\[\nT^\\top T \\neq I_m.\n\\] W efekcie rotowane czynniki mogą być skorelowane, a macierz korelacji czynników \\(\\Phi\\) przyjmuje ogólną postać dodatnio określoną.\nPodstawowe rodzaje:\n\nOblimin (Jennrich i Sampson 1966) - rodzina rotacji z parametrem \\(\\gamma\\), który reguluje stopień skośności. Dla \\(\\gamma = 0\\) rozwiązanie staje się quartimax, a większe \\(\\gamma\\) prowadzą do większej korelacji czynników. Funkcja celu \\[\nF(\\Lambda^*) = \\sum_{i=1}^p \\sum_{j=1}^m \\left(\\lambda_{ij}^{*2} - \\gamma \\frac{\\sum_{k=1}^m \\lambda_{ik}^{*2}}{m}\\right)^2.\n\\]\nPromax (Hendrickson i White 1964) - rotacja skośna oparta na prostym podejściu dwustopniowym. Najpierw stosuje się rotację ortogonalną (najczęściej varimax), następnie ładunki są podnoszone do potęgi \\(k\\) (zwykle 3 lub 4), aby wymusić prostą strukturę, i ponownie dopasowywane przy użyciu metody najmniejszych kwadratów \\[\n\\tilde{\\lambda}{jk} = \\text{sign}(\\lambda^*_{jk}) \\cdot |\\lambda^*_{jk}|^p.\n\\] Rotacja promax pozwala uzyskać bardziej realistyczne struktury, gdy czynniki są rzeczywiście skorelowane.\nGeomin (Everitt i Yates 1989) - minimalizuje średnią geometryczną kwadratów ładunków, co prowadzi do sytuacji, w której każda zmienna ma niewiele istotnych ładunków. Funkcja celu \\[\nG(\\Lambda^*) = \\sum_{i=1}^p \\left( \\prod_{j=1}^m (\\lambda_{ij}^{*2} + \\epsilon) \\right)^{1/m},\n\\] gdzie \\(\\epsilon\\) to mały parametr stabilizujący.\nSimplimax (Kiers 1994) - uogólnienie kryteriów prostoty, które minimalizuje liczbę dużych i małych ładunków w macierzy, pozwalając użytkownikowi sterować liczbą „prostych” elementów.\nWybór rodzaju rotacji\n\nRotacje ortogonalne są preferowane, gdy zakładamy, że czynniki powinny być niezależne teoretycznie.\nRotacje skośne stosuje się, gdy istnieje uzasadnienie, że czynniki mogą być skorelowane (co jest częste w naukach społecznych, psychologii czy biologii).\n\n\nPrzykład 3.1 Na potrzeby ilustracji budowy modelu EFA wykorzystamy dane z pakietu psych, które zawierają wyniki różnych testów poznawczych (Horn 1969). Dane te są często używane jako przykład w literaturze dotyczącej analizy czynnikowej.\n\nKodlibrary(psych)\n\n# Dane: macierz korelacji testów poznawczych\ndata(\"Harman74.cor\")\n\n\n\n\n\n\n\n\n\nZmienna\nOpis\nKategoria testu\n\n\n\nVisualPerception\nRozpoznawanie i analiza relacji przestrzennych w figurach\nZdolności przestrzenne / percepcyjne\n\n\nCubes\nManipulacja wyobrażeniowa brył, rotacje przestrzenne\nZdolności przestrzenne\n\n\nPaperFormBoard\nSkładanie i dopasowywanie elementów figur\nZdolności przestrzenne\n\n\nFlags\nRozpoznawanie wzorów i relacji symboli\nPercepcja wzrokowa / logiczne\n\n\nGeneralInformation\nOgólna wiedza faktograficzna\nZdolności werbalne\n\n\nPargraphComprehension\nRozumienie tekstów pisanych\nZdolności werbalne\n\n\nSentenceCompletion\nUzupełnianie zdań brakującymi słowami\nZdolności werbalne\n\n\nWordClassification\nGrupowanie słów według znaczenia\nZdolności werbalne / semantyczne\n\n\nWordMeaning\nZnajomość i rozumienie znaczeń słów\nZdolności werbalne\n\n\nAddition\nWykonywanie prostych działań arytmetycznych\nZdolności numeryczne\n\n\nCode\nDopasowywanie symboli do liczb według klucza\nSzybkość przetwarzania / percepcja\n\n\nCountingDots\nLiczenie elementów wzrokowych\nSzybkość percepcji / numeryczne\n\n\nStraightCurvedCapitals\nRozpoznawanie prostych i zakrzywionych liter\nPercepcja wizualna / szybkość\n\n\nWordRecognition\nRozpoznawanie słów z listy\nPamięć i zdolności werbalne\n\n\nNumberRecognition\nRozpoznawanie liczb z listy\nPamięć / percepcja numeryczna\n\n\nFigureRecognition\nRozpoznawanie i identyfikacja figur\nPamięć wizualna / percepcja\n\n\nObjectNumber\nDopasowywanie obiektów do liczb\nZłożone zdolności percepcyjno-num.\n\n\nNumberFigure\nDopasowywanie liczb do figur\nZłożone zdolności percepcyjno-num.\n\n\nFigureWord\nDopasowywanie figur do słów\nŁączenie informacji wizualno-werbalnych\n\n\nDeduction\nRozwiązywanie zadań logicznych, wnioskowanie\nRozumowanie logiczne\n\n\nNumericalPuzzles\nZadania numeryczne o charakterze problemowym\nZdolności numeryczne / logiczne\n\n\nProblemReasoning\nRozwiązywanie złożonych problemów\nRozumowanie ogólne\n\n\nSeriesCompletion\nUzupełnianie szeregów logicznych lub numerycznych\nRozumowanie abstrakcyjne / numeryczne\n\n\nArithmeticProblems\nRozwiązywanie zadań arytmetycznych o większej trudności\nZdolności numeryczne\n\n\n\nWidać, że testy można grupować w pięć głównych obszarów: przestrzenne/percepcyjne (np. Cubes, VisualPerception), werbalne (np. WordMeaning, SentenceCompletion), numeryczne (np. Addition, ArithmeticProblems), pamięciowe (np. WordRecognition, NumberRecognition), oraz rozumowania i logiczne (np. Deduction, SeriesCompletion). To właśnie takie powiązania w macierzy korelacji uzasadniają zastosowanie analizy czynnikowej w celu identyfikacji ukrytych wymiarów inteligencji.\nNajpierw sprawdzimy czy dane nadają się do analizy czynnikowej, obliczając test KMO i test sferyczności Bartletta.\n\nKodlibrary(tidyverse)\nlibrary(easystats)\n\ncheck_factorstructure(Harman74.cor$cov, n = 145) \n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(276) = 1545.86, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.88). The individual KMO scores are: VisualPerception (0.90), Cubes (0.84), PaperFormBoard (0.78), Flags (0.85), GeneralInformation (0.88), PargraphComprehension (0.89), SentenceCompletion (0.89), WordClassification (0.92), WordMeaning (0.88), Addition (0.81), Code (0.85), CountingDots (0.84), StraightCurvedCapitals (0.89), WordRecognition (0.85), NumberRecognition (0.88), FigureRecognition (0.89), ObjectNumber (0.85), NumberFigure (0.88), FigureWord (0.83), Deduction (0.93), NumericalPuzzles (0.91), ProblemReasoning (0.93), SeriesCompletion (0.91), ArithmeticProblems (0.92).\n\n\nTest sferyczności Bartletta dostarcza podstawowego potwierdzenia, że w zbiorze danych występują istotne statystycznie korelacje pomiędzy zmiennymi. Wynik \\(\\chi^2(276) = 1545.86,\\ p &lt; 0.001\\) oznacza, że hipoteza zerowa o macierzy korelacji równej macierzy jednostkowej zostaje odrzucona. Innymi słowy, zmienne nie są niezależne, a ich struktura korelacyjna uzasadnia dalsze poszukiwanie wspólnych czynników. Gdyby test okazał się nieistotny, sugerowałby brak uzasadnienia do stosowania analizy czynnikowej, ponieważ nie byłoby wystarczających zależności między zmiennymi.\nMiara adekwatności próby KMO (Kaiser–Meyer–Olkin) wskazuje, na ile obserwowane korelacje mogą być wyjaśnione przez czynniki wspólne w porównaniu z korelacjami cząstkowymi. Wynik ogólny KMO = 0.88 mieści się w przedziale uznawanym za „bardzo dobry” (powyżej 0.80). Oznacza to, że dane dobrze nadają się do analizy czynnikowej i możemy oczekiwać stabilnych, interpretowalnych rozwiązań. Wartości indywidualne dla poszczególnych zmiennych mieszczą się między 0.78 a 0.93, a więc wszystkie osiągają poziom „dobry” lub „bardzo dobry”. Najwyższe wartości, takie jak Deduction (0.93), ProblemReasoning (0.93) czy ArithmeticProblems (0.92), wskazują na wyjątkowo silną reprezentację tych testów w przestrzeni czynnikowej. Z kolei najniższe, jak PaperFormBoard (0.78), są nadal akceptowalne, ale sugerują nieco słabszą integrację tej zmiennej z pozostałymi. Całościowo zarówno wynik globalny, jak i rozkład wartości cząstkowych KMO jednoznacznie potwierdzają zasadność prowadzenia analizy czynnikowej na tym zbiorze danych.\n\nKod# Parallel analysis\nfa.parallel(Harman74.cor$cov, n.obs = 145, fa = \"fa\")\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\nSamo kryterium paralelne wskazuje na 4 czynniki, choć gdyby brać pod uwagę samo kryterium osypiska to rozwiązanie z 5 czynnikami też wydaje się być właściwe.\n\nKod# Kryterium MAP\nVSS(Harman74.cor$cov, n.obs = 145, plot = F)\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.8  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.85  with  2  factors\n\nThe Velicer MAP achieves a minimum of 0.02  with  4  factors \nBIC achieves a minimum of  -731.36  with  3  factors\nSample Size adjusted BIC achieves a minimum of  -112  with  5  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq    prob sqresid  fit RMSEA  BIC SABIC complex\n1 0.80 0.00 0.025 252   626 8.0e-34    16.8 0.80 0.101 -628   170     1.0\n2 0.55 0.85 0.022 229   428 3.1e-14    12.7 0.85 0.077 -711    13     1.5\n3 0.46 0.79 0.017 207   299 3.0e-05    10.0 0.88 0.055 -731   -76     1.8\n4 0.42 0.74 0.017 186   228 1.9e-02     8.0 0.90 0.039 -698  -109     1.9\n5 0.40 0.71 0.021 166   189 1.1e-01     7.2 0.91 0.030 -637  -112     2.0\n6 0.40 0.71 0.024 147   162 1.8e-01     6.3 0.92 0.026 -569  -104     2.0\n7 0.40 0.70 0.028 129   138 2.7e-01     5.6 0.93 0.021 -504   -95     2.2\n8 0.41 0.70 0.030 112   111 5.0e-01     5.0 0.94 0.000 -446   -92     2.3\n  eChisq  SRMR eCRMS eBIC\n1    748 0.097 0.101 -506\n2    422 0.073 0.080 -718\n3    240 0.055 0.063 -790\n4    133 0.041 0.050 -792\n5    105 0.036 0.047 -721\n6     81 0.032 0.044 -651\n7     62 0.028 0.041 -580\n8     44 0.023 0.037 -514\n\n\n\n\n\n\n\n\nWskaźnik\nInterpretacja\n\n\n\nvss1\nDopasowanie Very Simple Structure przy założeniu jednego czynnika na zmienną; wyższe = lepsze.\n\n\nvss2\nDopasowanie VSS przy założeniu maksymalnie dwóch czynników na zmienną; wyższe = lepsze.\n\n\nmap\nKryterium Velicera; minimum wskazuje optymalną liczbę czynników (eliminuje korelacje cząstkowe).\n\n\ndof\nStopnie swobody testu dopasowania chi-kwadrat.\n\n\nchisq\nWartość statystyki chi-kwadrat; niska w relacji do df sugeruje dobre dopasowanie.\n\n\nprob\nWartość p testu chi-kwadrat; wysoka oznacza brak podstaw do odrzucenia poprawnego dopasowania.\n\n\nsqresid\nSuma kwadratów reszt (różnice R − R̂); niższe wartości = lepsze odwzorowanie danych.\n\n\nfit\nProporcja wyjaśnionej wariancji w macierzy korelacji; wyższe wartości = lepsze dopasowanie.\n\n\nRMSEA\nBłąd aproksymacji w populacji; &lt; 0.05 bardzo dobre, 0.05–0.08 akceptowalne, &gt; 0.10 słabe.\n\n\nBIC\nKryterium informacyjne; niższe wartości = lepszy kompromis dopasowania i prostoty.\n\n\nSABIC\nWersja BIC korygowana o wielkość próby; lepsza przy mniejszych próbach.\n\n\ncomplex\nŚrednia liczba czynników na które ładują się zmienne; niższe = prostsza struktura.\n\n\neChisq\nEstymowana statystyka chi-kwadrat w alternatywnej estymacji; interpretacja analogiczna jak chisq.\n\n\nSRMR\nStandardized Root Mean Square Residual; niski poziom (&lt; 0.08) wskazuje dobre dopasowanie.\n\n\neCRMS\nEstymowany błąd resztowy analogiczny do RMSEA; mniejsze wartości = lepsze dopasowanie.\n\n\neBIC\nEstymowana wersja kryterium BIC; niższe wartości = lepszy model.\n\n\n\nKryterium MAP Velicera wskazuje, że minimalna wartość statystyki została osiągnięta przy czterech czynnikach (MAP = 0.017). Oznacza to, że w ujęciu tego kryterium, czynniki te najlepiej redukują korelacje cząstkowe między zmiennymi – czyli eliminują największą część wariancji niepowiązanej ze wspólną strukturą czynnikową. Innymi słowy, przy czterech czynnikach model najefektywniej odwzorowuje wspólne zależności bez pozostawiania nadmiernych reszt.\nWarto jednak zauważyć, że różne kryteria sugerują odmienne liczby czynników. Kryterium BIC wskazuje na trzy czynniki jako najbardziej oczekiwane rozwiązanie, natomiast skorygowany BIC (SABIC) preferuje pięć czynników. Z kolei wskaźniki VSS (Very Simple Structure) sugerują jedno– lub dwuczynnikowe rozwiązania, maksymalizujące prostotę struktury. Ostateczna decyzja wymaga zatem kompromisu: MAP sugeruje cztery czynniki jako najpełniej oddające wspólną strukturę zmiennych, BIC preferuje trzy jako prostsze, a SABIC wskazuje na pięć. Interpretacja powinna uwzględniać nie tylko statystyki, lecz także sensowność teoretyczną i interpretowalność uzyskanych czynników w kontekście badanego materiału.\n\nKod# Analiza czynnikowa\nfa_model &lt;- fa(Harman74.cor$cov, nfactors = 4, n.obs = 145, \n               fm = \"ml\", rotate = \"varimax\")\n\nfa_model\n\nFactor Analysis using method =  ml\nCall: fa(r = Harman74.cor$cov, nfactors = 4, n.obs = 145, rotate = \"varimax\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                        ML1   ML3   ML2  ML4   h2   u2 com\nVisualPerception       0.16  0.69  0.19 0.16 0.56 0.44 1.4\nCubes                  0.12  0.44  0.08 0.10 0.22 0.78 1.3\nPaperFormBoard         0.14  0.57 -0.02 0.11 0.36 0.64 1.2\nFlags                  0.23  0.53  0.10 0.08 0.35 0.65 1.5\nGeneralInformation     0.74  0.19  0.21 0.15 0.65 0.35 1.4\nPargraphComprehension  0.77  0.20  0.07 0.23 0.69 0.31 1.4\nSentenceCompletion     0.81  0.20  0.15 0.07 0.72 0.28 1.2\nWordClassification     0.57  0.34  0.24 0.13 0.51 0.49 2.2\nWordMeaning            0.81  0.20  0.04 0.23 0.74 0.26 1.3\nAddition               0.17 -0.12  0.83 0.17 0.76 0.24 1.2\nCode                   0.18  0.12  0.51 0.37 0.45 0.55 2.2\nCountingDots           0.02  0.21  0.72 0.09 0.56 0.44 1.2\nStraightCurvedCapitals 0.19  0.44  0.53 0.08 0.51 0.49 2.3\nWordRecognition        0.20  0.05  0.08 0.55 0.35 0.65 1.3\nNumberRecognition      0.12  0.12  0.07 0.52 0.30 0.70 1.3\nFigureRecognition      0.07  0.41  0.06 0.53 0.45 0.55 2.0\nObjectNumber           0.14  0.06  0.22 0.57 0.40 0.60 1.4\nNumberFigure           0.03  0.29  0.34 0.46 0.41 0.59 2.6\nFigureWord             0.15  0.24  0.16 0.37 0.24 0.76 2.6\nDeduction              0.38  0.40  0.12 0.30 0.41 0.59 3.0\nNumericalPuzzles       0.17  0.38  0.44 0.22 0.42 0.58 2.8\nProblemReasoning       0.37  0.40  0.12 0.30 0.40 0.60 3.1\nSeriesCompletion       0.37  0.50  0.24 0.24 0.50 0.50 2.9\nArithmeticProblems     0.37  0.16  0.50 0.30 0.50 0.50 2.8\n\n                       ML1  ML3  ML2  ML4\nSS loadings           3.65 2.87 2.66 2.29\nProportion Var        0.15 0.12 0.11 0.10\nCumulative Var        0.15 0.27 0.38 0.48\nProportion Explained  0.32 0.25 0.23 0.20\nCumulative Proportion 0.32 0.57 0.80 1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  276  with the objective function =  11.44 with Chi Square =  1545.86\ndf of  the model are 186  and the objective function was  1.71 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  145 with the empirical chi square  135.74  with prob &lt;  1 \nThe total n.obs was  145  with Likelihood Chi Square =  226.68  with prob &lt;  0.022 \n\nTucker Lewis Index of factoring reliability =  0.951\nRMSEA index =  0.038  and the 90 % confidence intervals are  0.016 0.056\nBIC =  -698.99\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2  ML4\nCorrelation of (regression) scores with factors   0.93 0.87 0.91 0.82\nMultiple R square of scores with factors          0.87 0.76 0.83 0.68\nMinimum correlation of possible factor scores     0.73 0.52 0.66 0.36\n\n\nModel czteroczynnikowy oszacowany metodą największej wiarygodności na macierzy korelacji Harman74.cor$cov dobrze odwzorowuje strukturę danych i dostarcza interpretowalnych wyników.\nPierwszy czynnik (ML1) skupia się na kompetencjach werbalnych i wiedzy ogólnej. Najwyższe ładunki uzyskano dla zmiennych takich jak WordMeaning (0.81), SentenceCompletion (0.81), ParagraphComprehension (0.77) czy GeneralInformation (0.74). Wskazuje to, że ML1 reprezentuje wymiar wiedzy językowej i rozumienia tekstu. Zasoby zmienności wspólej dla tych zmiennych są wysokie (powyżej 0.65), co oznacza, że znaczna część ich wariancji została uchwycona przez model.\nDrugi czynnik (ML2) odzwierciedla zdolności arytmetyczne i numeryczne. Najsilniejsze ładunki dotyczą zmiennych Addition (0.83), CountingDots (0.72) i ArithmeticProblems (0.50). Oznacza to, że ML2 reprezentuje wymiar obliczeniowy, obejmujący zarówno proste działania matematyczne, jak i bardziej złożone zadania wymagające operowania na liczbach. Wysokie wartości \\(h_j^2\\) (np. 0.76 dla Addition) sugerują dobrą reprezentację tych zmiennych.\nTrzeci czynnik (ML3) można interpretować jako zdolności wzrokowo-przestrzenne i percepcyjne. Najsilniejsze ładunki wystąpiły dla VisualPerception (0.69), PaperFormBoard (0.57), Flags (0.53) oraz SeriesCompletion (0.50). Grupa ta obejmuje zadania związane z manipulacją figurami, rozpoznawaniem wzorów i orientacją przestrzenną.\nCzwarty czynnik (ML4) wydaje się związany z rozpoznawaniem wzrokowym i pamięcią wzrokową. Największe ładunki dotyczą zmiennych takich jak WordRecognition (0.55), NumberRecognition (0.52), FigureRecognition (0.53) czy ObjectNumber (0.57). Sugeruje to wymiar rozpoznawania i szybkiego identyfikowania bodźców wzrokowych.\nŁącznie cztery czynniki wyjaśniają 48% wariancji całkowitej, co w psychometrii jest uznawane za wartość akceptowalną przy tego typu danych. Dopasowanie globalne modelu również jest dobre: RMSEA = 0.038 (z przedziałem ufności 0.016–0.056) wskazuje na bardzo dobre dopasowanie, a Tucker-Lewis Index wynosi 0.951, co również świadczy o wysokiej jakości modelu. Niskie wartości RMSR (0.04) oraz wysoka zgodność dopasowania poza przekątną (0.98) potwierdzają, że model trafnie odwzorowuje strukturę korelacji między zmiennymi.\nOstatecznie wyniki wskazują, że struktura czteroczynnikowa jest dobrze uzasadniona empirycznie i teoretycznie. Każdy czynnik odpowiada odmiennym zdolnościom poznawczym – werbalnym, numerycznym, przestrzennym i percepcyjno-pamięciowym – a ich interpretacje są zgodne z psychologicznymi ujęciami inteligencji wielowymiarowej.\nDla większej czytelności przedstawiamy ładunki czynnikowe po rotacji varimax w formie tabelarycznej, z wyciętymi ładunkami o niskich wartościach.\n\nKodmodel_parameters(fa_model, sort = TRUE, threshold = \"max\") %&gt;% \n  print_html()\n\n\n\n\n\n\nRotated loadings from Factor Analysis (varimax-rotation)\n\n\nVariable\nML1\nML3\nML2\nML4\nComplexity\nUniqueness\n\n\n\n\nWordMeaning\n0.81\n\n\n\n1.30\n0.26\n\n\nSentenceCompletion\n0.81\n\n\n\n1.21\n0.28\n\n\nPargraphComprehension\n0.77\n\n\n\n1.35\n0.31\n\n\nGeneralInformation\n0.74\n\n\n\n1.39\n0.35\n\n\nWordClassification\n0.57\n\n\n\n2.17\n0.49\n\n\nVisualPerception\n\n0.69\n\n\n1.38\n0.44\n\n\nPaperFormBoard\n\n0.57\n\n\n1.20\n0.64\n\n\nFlags\n\n0.53\n\n\n1.51\n0.65\n\n\nSeriesCompletion\n\n0.50\n\n\n2.87\n0.50\n\n\nCubes\n\n0.44\n\n\n1.33\n0.78\n\n\nDeduction\n\n0.40\n\n\n3.05\n0.59\n\n\nProblemReasoning\n\n0.40\n\n\n3.08\n0.60\n\n\nAddition\n\n\n0.83\n\n1.21\n0.24\n\n\nCountingDots\n\n\n0.72\n\n1.21\n0.44\n\n\nStraightCurvedCapitals\n\n\n0.53\n\n2.27\n0.49\n\n\nCode\n\n\n0.51\n\n2.25\n0.55\n\n\nArithmeticProblems\n\n\n0.50\n\n2.83\n0.50\n\n\nNumericalPuzzles\n\n\n0.44\n\n2.84\n0.58\n\n\nObjectNumber\n\n\n\n0.57\n1.45\n0.60\n\n\nWordRecognition\n\n\n\n0.55\n1.32\n0.65\n\n\nFigureRecognition\n\n\n\n0.53\n1.96\n0.55\n\n\nNumberRecognition\n\n\n\n0.52\n1.26\n0.70\n\n\nNumberFigure\n\n\n\n0.46\n2.62\n0.59\n\n\nFigureWord\n\n\n\n0.37\n2.56\n0.76\n\n\n\nThe 4 latent factors (varimax rotation) accounted for 47.78% of the total variance of the original data (ML1 = 15.20%, ML3 = 11.97%, ML2 = 11.07%, ML4 = 9.54%).\n\n\n\n\n\nMożemy też przedstawić model graficznie.\n\nKodfa.diagram(fa_model, marg = c(1,5,1,1), rsize = 2)\n\n\n\n\n\n\n\nMożna spróbować estymować model z pięcioma czynnikami, co odpowiadałoby pierwotnemu rozpoznaniu obszarów.\n\nKodfa_model_5 &lt;- fa(Harman74.cor$cov, nfactors = 5, n.obs = 145, \n                 fm = \"ml\", rotate = \"varimax\")\nmodel_parameters(fa_model_5, sort = TRUE, threshold = \"max\") %&gt;% \n  print_html()\n\n\n\n\n\n\nRotated loadings from Factor Analysis (varimax-rotation)\n\n\nVariable\nML1\nML3\nML2\nML4\nML5\nComplexity\nUniqueness\n\n\n\n\nSentenceCompletion\n0.81\n\n\n\n\n1.21\n0.28\n\n\nWordMeaning\n0.80\n\n\n\n\n1.31\n0.26\n\n\nPargraphComprehension\n0.77\n\n\n\n\n1.39\n0.29\n\n\nGeneralInformation\n0.74\n\n\n\n\n1.40\n0.36\n\n\nWordClassification\n0.57\n\n\n\n\n2.17\n0.49\n\n\nVisualPerception\n\n0.66\n\n\n\n1.59\n0.45\n\n\nPaperFormBoard\n\n0.56\n\n\n\n1.30\n0.64\n\n\nSeriesCompletion\n\n0.55\n\n\n\n2.72\n0.44\n\n\nFlags\n\n0.53\n\n\n\n1.47\n0.65\n\n\nDeduction\n\n0.45\n\n\n\n3.33\n0.52\n\n\nCubes\n\n0.44\n\n\n\n1.32\n0.78\n\n\nProblemReasoning\n\n0.42\n\n\n\n3.10\n0.58\n\n\nAddition\n\n\n0.84\n\n\n1.21\n0.21\n\n\nCountingDots\n\n\n0.69\n\n\n1.34\n0.44\n\n\nArithmeticProblems\n\n\n0.50\n\n\n2.94\n0.48\n\n\nNumericalPuzzles\n\n\n0.44\n\n\n2.85\n0.56\n\n\nObjectNumber\n\n\n\n0.56\n\n1.46\n0.61\n\n\nWordRecognition\n\n\n\n0.56\n\n1.34\n0.64\n\n\nFigureRecognition\n\n\n\n0.53\n\n1.95\n0.55\n\n\nNumberRecognition\n\n\n\n0.51\n\n1.29\n0.71\n\n\nNumberFigure\n\n\n\n0.45\n\n2.65\n0.60\n\n\nCode\n\n\n\n0.45\n\n3.36\n0.39\n\n\nFigureWord\n\n\n\n0.36\n\n2.56\n0.76\n\n\nStraightCurvedCapitals\n\n\n\n\n0.56\n3.16\n0.26\n\n\n\nThe 5 latent factors (varimax rotation) accounted for 50.25% of the total variance of the original data (ML1 = 15.13%, ML3 = 12.35%, ML2 = 10.23%, ML4 = 9.77%, ML5 = 2.76%).\n\n\n\n\n\nChoć wzrósł nieco poziom wyjaśnionej wariancji przez czynniki, to jednak rozwiązanie, w którym występują pojedyncze zmienne jako czynnik nie są porządane. Dlatego pozostaniemy przy rozwiązaniu z czterema czynnikami.\n\n\n\n\n\n\n\nWskazówka\n\n\n\nLiczba czynników, może być szacowana na podstawie różnych kryteriów, z których każde eksponuje inny aspekt, ale ostateczna decyzja o wyborze liczby czynników powinna być podyktowana głównie zgodnością otrzymany wyników z teorią oraz interpretowalnością. Oczywiście nie powinno się to dziać kosztem znacznego obniżenia poziomu dopasowania modelu.\n\n\nNa temat konfiramcyjnej analizy czynnikowej (CFA) zostnie poświęcony kolejny rozdział.\n\n\n\n\nBartlett, M. S. 1951. „The Effect of Standardization on a χ 2 Approximation in Factor Analysis”. Biometrika 38 (3/4): 337. https://doi.org/10.2307/2332580.\n\n\nCattell, Raymond B. 1966. „The Scree Test For The Number Of Factors”. Multivariate Behavioral Research 1 (2): 245–76. https://doi.org/10.1207/s15327906mbr0102_10.\n\n\nEveritt, B. S., i A. Yates. 1989. „Multivariate Exploratory Data Analysis: A Perspective on Exploratory Factor Analysis.” Biometrics 45 (1): 342. https://doi.org/10.2307/2532065.\n\n\nGrieder, Silvia, i Markus D. Steiner. 2020. „Algorithmic Jingle Jungle: A Comparison of Implementations of Principal Axis Factoring and Promax Rotation in R and SPSS”. http://dx.doi.org/10.31234/osf.io/7hwrm.\n\n\nHarman, Harry H., i Wayne H. Jones. 1966. „Factor Analysis by Minimizing Residuals (Minres)”. Psychometrika 31 (3): 351–68. https://doi.org/10.1007/bf02289468.\n\n\nHendrickson, Alan E., i Paul Owen White. 1964. „PROMAX: A QUICK METHOD FOR ROTATION TO OBLIQUE SIMPLE STRUCTURE”. British Journal of Statistical Psychology 17 (1): 65–70. https://doi.org/10.1111/j.2044-8317.1964.tb00244.x.\n\n\nHorn, John L. 1965. „A Rationale and Test for the Number of Factors in Factor Analysis”. Psychometrika 30 (2): 179–85. https://doi.org/10.1007/bf02289447.\n\n\n———. 1969. „Harry H. Harman Modern Factor Analysis (Second Edition, Revised). Chicago and London: University of Chicago Press, 1967. Pp. Xx + 474. $12.50”. Psychometrika 34 (1): 134–38. https://doi.org/10.1017/s0033312300004580.\n\n\n„Introduction to Factor Analysis”. 2020. W, 1–12. SAGE Publications, Inc. https://doi.org/10.4135/9781544339900.n4.\n\n\nJacobucci, Ross, i Kevin J. Grimm. 2018. „Comparison of Frequentist and Bayesian Regularization in Structural Equation Modeling”. Structural Equation Modeling: A Multidisciplinary Journal 25 (4): 639–49. https://doi.org/10.1080/10705511.2017.1410822.\n\n\nJennrich, R. I., i P. F. Sampson. 1966. „Rotation for Simple Loadings”. Psychometrika 31 (3): 313–23. https://doi.org/10.1007/bf02289465.\n\n\nJöreskog, Karl G., i Arthur S. Goldberger. 1972. „Factor Analysis by Generalized Least Squares”. Psychometrika 37 (3): 243–60. https://doi.org/10.1007/bf02306782.\n\n\nKaiser, Henry F. 1958. „The Varimax Criterion for Analytic Rotation in Factor Analysis”. Psychometrika 23 (3): 187–200. https://doi.org/10.1007/bf02289233.\n\n\n———. 1970. „A Second Generation Little Jiffy”. Psychometrika 35 (4): 401–15. https://doi.org/10.1007/bf02291817.\n\n\nKiers, Henk A. L. 1994. „Simplimax: Oblique Rotation to an Optimal Target with Simple Structure”. Psychometrika 59 (4): 567–79. https://doi.org/10.1007/bf02294392.\n\n\nLawley, D. N. 1940. „VI.The Estimation of Factor Loadings by the Method of Maximum Likelihood”. Proceedings of the Royal Society of Edinburgh 60 (1): 64–82. https://doi.org/10.1017/s037016460002006x.\n\n\nLu, Zhao-Hua, Sy-Miin Chow, i Eric Loken. 2016. „Bayesian Factor Analysis as a Variable-Selection Problem: Alternative Priors and Consequences”. Multivariate Behavioral Research 51 (4): 519–39. https://doi.org/10.1080/00273171.2016.1168279.\n\n\nVelicer, Wayne F. 1976. „Determining the Number of Components from the Matrix of Partial Correlations”. Psychometrika 41 (3): 321–27. https://doi.org/10.1007/bf02293557.\n\n\nWang, Xiaojing, Candace M. Kammerer, Stewart Anderson, Jiang Lu, i Eleanor Feingold. 2008. „A Comparison of Principal Component Analysis and Factor Analysis Strategies for Uncovering Pleiotropic Factors”. Genetic Epidemiology 33 (4): 325–31. https://doi.org/10.1002/gepi.20384.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#założenia-dotyczące-danych",
    "href": "fa.html#założenia-dotyczące-danych",
    "title": "Analiza czynnikowa",
    "section": "Założenia dotyczące danych",
    "text": "Założenia dotyczące danych\nAby estymacja modelu eksploracyjnej analizy czynnikowej (EFA) była uzasadniona, dane powinny spełniać szereg założeń teoretycznych i praktycznych.\n\nPo pierwsze, podstawą jest istnienie istotnej struktury korelacyjnej pomiędzy zmiennymi obserwowalnymi. Jeżeli zmienne są w zasadzie nieskorelowane, nie da się wydzielić wspólnych czynników. Warunek ten weryfikuje się wstępnie testem sferyczności Bartletta (Bartlett 1951) oraz miarą adekwatności próby KMO (Kaiser 1970).\nPo drugie, zakłada się odpowiednią wielkość próby. Choć w literaturze nie istnieje jednoznaczna reguła, rekomenduje się co najmniej 5–10 obserwacji na zmienną oraz łączną liczebność rzędu ≥100–200 jednostek, aby uzyskać stabilne rozwiązania i wiarygodne oszacowania ładunków czynnikowych.\nPo trzecie, dane powinny pochodzić z rozkładu wielowymiarowego normalnego, szczególnie jeżeli korzysta się z estymacji metodą największej wiarygodności. Naruszenia normalności mogą prowadzić do zawyżenia błędów standardowych, problemów z testami istotności oraz błędnych przedziałów ufności.\nPo czwarte, model EFA wymaga, aby zmienne były ciągłe lub co najmniej traktowane jako przybliżenie zmiennych ciągłych. W przypadku zmiennych kategorycznych należy sięgnąć po odpowiednie uogólnienia (np. analizy czynnikowe dla danych porządkowych).\nPo piąte, zakłada się addytywność wariancji. Oznacza to, że wariancja każdej zmiennej obserwowanej rozkłada się na część wspólną, wyjaśnianą przez czynniki latentne, oraz część swoistą (unikalną dla danej zmiennej), zgodnie z postacią: \\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\nI na koniec, należy zadbać o brak nadmiernej współliniowości oraz o to, by liczba czynników nie przekraczała liczby zmiennych – w przeciwnym wypadku model byłby nieidentyfikowalny.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "sam.html",
    "href": "sam.html",
    "title": "Modele ścieżkowe",
    "section": "",
    "text": "Modele ścieżkowe (ang. path modeling)",
    "crumbs": [
      "Modele ścieżkowe"
    ]
  },
  {
    "objectID": "sem.html",
    "href": "sem.html",
    "title": "Modele strukturalne",
    "section": "",
    "text": "Konfirmacyjna analiza czynnikowa\nModele strukturalne (ang. Structural Equation Models) stanowią uogólnienie klasycznych modeli regresyjnych do układów wielu równań z jednoczesnymi zależnościami między zmiennymi. W najprostszym wariancie, zwanym path analysis (PA), wszystkie zmienne są obserwowalne, a celem jest estymacja współczynników ścieżek, dekompozycja efektów na bezpośrednie i pośrednie oraz wyjaśnienie współzmienności. Konfirmacyjna analiza czynnikowa (CFA) rozszerza to ujęcie o niewidoczne wprost czynniki latentne, modelując relację wskaźnik–czynnik i separując wariancję wspólną od swoistej. Modele strukturalne (SEM) integrują oba poziomy: pomiarowy (jak w CFA) i strukturalny (jak w path analysis), tworząc jedną ramę, w której czynniki latentne i zmienne obserwowalne łączą się w sieć równań opisujących zależności przyczynowo-interpretacyjne.\nRys historyczny sięga prac Sewalla Wrighta z lat 1918–1934, który wprowadził path analysis i reguły śledzenia ścieżek, pozwalające dekomponować kowariancje na sumy iloczynów współczynników (Wright 1934). Równolegle rozwijała się analiza czynnikowa: Spearman (1961), który postulował czynnik ogólny, a Thurstone (1931) wprowadził czynniki wielowymiarowe. Przełomem był formalny opis CFA i ujęcie SEM przez Jöreskoga (koniec lat 60.), który połączył model pomiarowy i strukturalny w system LISREL (Tarka 2017). Lata 80. i 90. przyniosły rozwój estymacji, wskaźników dopasowania i oprogramowania (m.in. EQS, AMOS), a podręcznikowa synteza Bollen’a (1989) ugruntowała teorię (Bollen 1989). W kolejnych dekadach pojawiały się metody odporne i dla zmiennych porządkowych oraz uogólnienia dla danych longitudalnych i wielopoziomowych, co uczyniło z SEM uniwersalną ramę modelowania.\nKonfirmacyjna analiza czynnikowa (ang. Confirmatory Factor Analysis, CFA), stanowi ujęcie modelu pomiarowego, w którym a priori narzuca strukturę zależności między zmiennymi obserwowalnymi a czynnikami ukrytymi. W odróżnieniu od eksploracyjnej analizy czynnikowej, gdzie pozwala danym „odkrywać” wzorzec ładunków, w CFA określamy, które zmienne ładują się na których czynnikach, które ładunki są równe zeru, a które mogą się różnić, oraz czy dopuszczamy korelacje błędów pomiaru. Celem jest weryfikacja hipotezy o poprawnej budowie narzędzia pomiarowego i o liczbie oraz treści czynników, a następnie oceniamy dopasowanie modelu do macierzy kowariancji/średnich w populacji.\nFormalnie przyjmujemy ten sam model pomiarowy co w EFA, lecz z nałożonymi ograniczeniami strukturalnymi na macierz ładunków. Niech \\(\\mathbf{x}\\in\\mathbb{R}^p\\) oznacza wektor zmiennych obserwowalnych, \\(\\mathbf{f}\\in\\mathbb{R}^m\\) wektor czynników, \\(\\Lambda\\in\\mathbb{R}^{p\\times m}\\) macierz ładunków, \\(\\boldsymbol{\\epsilon}\\in\\mathbb{R}^p\\) wektor składników swoistych. Model przyjmuje wówczas postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\,\\mathbf{f}+\\boldsymbol{\\epsilon},\\qquad\n\\mathbb{E}(\\mathbf{f})=\\mathbf{0},\\ \\ \\mathbb{E}(\\boldsymbol{\\epsilon})=\\mathbf{0},\\ \\ \\mathrm{Cov}(\\mathbf{f})=\\Phi,\\ \\ \\mathrm{Cov}(\\boldsymbol{\\epsilon})=\\Psi,\n\\] gdzie \\(\\Phi\\) jest dodatnio określoną macierzą kowariancji czynników (dla rotacji skośnych) lub macierzą jednostkową (dla czynników ortogonalnych), a \\(\\Psi\\) z reguły jest macierzą diagonalną, co odpowiada nieskorelowanym błędom pomiaru. Macierz kowariancji implikowana przez model ma postać \\[\n\\Sigma(\\theta)=\\Lambda\\,\\Phi\\,\\Lambda^\\top + \\Psi,\n\\] gdzie \\(\\theta\\) reprezentuje wszystkie parametry modelu.\nKlucz identyfikacji w CFA polegać na tym, że rotacyjna nieoznaczoność znika dzięki z góry zdefiniowanemu wzorcowi zer w \\(\\Lambda\\) (każdy wskaźnik ładujący się wyłącznie na „własnym” czynniku). Aby skalować czynniki, przyjmować jedną z równoważnych konwencji: ustalamy wariancję czynnika na 1 i estymujemy wszystkie ładunki, albo ustalamy po jednym ładunku na 1 w każdej kolumnie \\(\\Lambda\\) i estymujemy wariancje czynników. Praktycznie zapewniamy co najmniej trzy sensowne wskaźniki na czynnik; dwa wskaźniki bywają wystarczające przy dodatkowych ograniczeniach równości lub znanych błędach pomiaru.\nEstymujemy parametry zwykle metodą największej wiarygodności, co przy normalności wielowymiarowej oznacza minimalizowanie rozbieżność między \\(S\\) a \\(\\Sigma(\\theta)\\) i umożliwia wprowadzenie testu globalnego dopasowania \\(\\chi^2\\). Przy naruszeniach normalności stosujemy wersje odporne lub ważone metody najmniejszych kwadratów dla danych porządkowych (WLSMV) (Li 2015).\nInterpretacja CFA opieramy na ładunkach w \\(\\Lambda\\) jako czułościach wskaźników na czynniki, wariancjach i korelacjach czynników w \\(\\Phi\\) jako sile i współwystępowaniu wymiarów latentnych oraz na resztach i modyfikacjach jako sygnałach lokalnego niedopasowania. Siła CFA polega na tym, że pozwala wprost testować hipotezy o narzędziu pomiarowym, porównywać modele teoretycznie motywowane i zapewniać podstawę do dalszych modeli strukturalnych, SEM, w których czynniki stają się zmiennymi wyjaśniającymi i wyjaśnianymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#konfirmacyjna-analiza-czynnikowa",
    "href": "sem.html#konfirmacyjna-analiza-czynnikowa",
    "title": "Modele strukturalne",
    "section": "",
    "text": "Przykład 4.1 Przeprowadzimy CFA na danych pochodzących z badania PISA 2009, dotyczących strategii uczenia się uczniów. Wybierzemy 13 pozycji z kwestionariusza ucznia, które mają odzwierciedlać trzy strategie: zapamiętywania (M), opracowywania (E) i kontroli (C). Sprawdzimy, czy dane z Wielkiej Brytanii potwierdzają tę strukturę trójczynnikową.\n\nKod#devtools::install_github(\"talbano/epmr\")\nlibrary(epmr)\nlibrary(gt)\nlibrary(lavaan)\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(sjPlot)\n\n# wybór itemów z testu (łącznie 13), podzielonych wg założonej struktury, \n# którą będziemy weryfikować\n# strategie zapamiętywania, opracowywania i kontroli\nmitems &lt;- c(\"st27q01\", \"st27q03\", \"st27q05\", \"st27q07\")\neitems &lt;- c(\"st27q04\", \"st27q08\", \"st27q10\", \"st27q12\")\ncitems &lt;- c(\"st27q02\", \"st27q06\", \"st27q09\", \"st27q11\", \n  \"st27q13\")\nalitems &lt;- c(mitems, eitems, citems)\n\n# Zawęzimy badania tylko go Wielkiej Brytanii\npisagbr &lt;- PISA09[PISA09$cnt == \"GBR\", alitems]\npisagbr &lt;- pisagbr[complete.cases(pisagbr[, c(mitems, \n  eitems, citems)]), ]\nplot_likert(pisagbr, groups = c(rep(\"Zapamiętywanie\", \n  length(mitems)), rep(\"Opracowywanie\", length(eitems)), \n  rep(\"Kontrola\", length(citems))))\n\n\n\n\n\n\n\nNa potrzeby budowy modelu konfirmacyjnego użyjemy pakietu lavaan. Definiujemy model z trzema czynnikami, gdzie każdy czynnik jest ładowany przez odpowiednie pozycje kwestionariusza. Następnie estymujemy model metodą największej wiarygodności i sprawdzamy dopasowanie modelu do danych.\n\nKod# Definicja modelu CFA\nmodel_cfa &lt;- '\n  # Definicja czynników\n  Zapamiętywanie =~ st27q01 + st27q03 + st27q05 + st27q07\n  Opracowywanie =~ st27q04 + st27q08 + st27q10 + st27q12\n  Kontrola =~ st27q02 + st27q06 + st27q09 + st27q11 + st27q13\n'\n\n# Estymacja modelu CFA\nfit_cfa &lt;- cfa(model_cfa, data = pisagbr, auto.var = TRUE, auto.cov.lv.x = TRUE, std.lv = TRUE)\n\n\n\n\nauto.var = TRUE - estymuje wariancje czynników i błędów pomiarowych, bez potrzeby ręcznego ich dodawania.\n\nauto.cov.lv.x = TRUE - estymuje kowariancje pomiędzy wszystkimi czynnikami latentnymi.\n\nstd.lv = TRUE - ustawia wariancję czynników latentnych na 1, co daje bezpośrednio interpretowalne ładunki czynnikowe jako korelacje.\n\n\nKod# Podsumowanie dopasowania\nmodel_performance(fit_cfa, metrics = c(\"p_Chi2\", \"GFI\", \"AGFI\", \"NFI\", \"NNFI\", \"CFI\", \"RMSEA\", \"RMR\", \"SRMR\", \"RFI\")) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\np_Chi2\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMR\nSRMR\nRFI\n\n\n0.000\n0.936\n0.907\n0.881\n0.856\n0.885\n0.081\n0.042\n0.057\n0.850\n\n\n\n\n\nNajpierw ocenimy dopasowanie modelu:\n\nPo pierwsze, test chi-kwadrat (p_Chi2 = 0.000) jest istotny, co formalnie sugeruje, że model nie odtwarza idealnie macierzy kowariancji w populacji. Jednakże, przy większych próbach test ten jest nadwrażliwy i często prowadzi do odrzucenia nawet dobrze dopasowanych modeli, dlatego nie należy go traktować jako jedynego kryterium oceny.\nJeśli chodzi o wskaźniki dopasowania absolutnego, wartości GFI = 0.936 oraz AGFI = 0.907 wskazują na przyzwoite dopasowanie – oba mieszczą się powyżej progu 0.90, choć nie osiągają poziomu bardzo dobrego (≥ 0.95). Podobnie RMR = 0.042 i SRMR = 0.057 sugerują, że przeciętne reszty między obserwowaną a implikowaną macierzą są umiarkowanie niskie – SRMR &lt; 0.08 jest zwykle uznawane za akceptowalne.\nW przypadku wskaźników dopasowania przyrostowego (NFI = 0.881, NNFI = 0.856, CFI = 0.885, RFI = 0.850), wszystkie wartości są poniżej konwencjonalnego progu 0.90, co wskazuje na pewne niedopasowanie.\nSzczególnie ważny jest RMSEA = 0.081, który mieści się w strefie dopuszczalnej, ale nie idealnej (0.05–0.08 uznaje się za akceptowalne dopasowanie, a &gt; 0.10 za słabe). Wartość 0.081 wskazuje na model na granicy akceptowalności – można go uznać za umiarkowanie dopasowany, ale istnieją przesłanki do jego ulepszania (np. rozważenie korelacji błędów pomiarowych, dodanie lub modyfikacja pozycji).\n\nTeraz przejdźmy do interpretacji parametrów modelu:\n\nKodmodel_parameters(fit_cfa, component = \"loading\", standardize = T) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nTo\nOperator\nFrom\nCoefficient\nSE\nCI_low\nCI_high\nz\np\nComponent\n\n\n\nZapamiętywanie\n=~\nst27q01\n0.585\n0.014\n0.557\n0.613\n40.467\n0.000\nLoading\n\n\nZapamiętywanie\n=~\nst27q03\n0.644\n0.014\n0.618\n0.671\n47.357\n0.000\nLoading\n\n\nZapamiętywanie\n=~\nst27q05\n0.597\n0.014\n0.569\n0.625\n41.809\n0.000\nLoading\n\n\nZapamiętywanie\n=~\nst27q07\n0.601\n0.014\n0.574\n0.629\n42.299\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q04\n0.534\n0.015\n0.505\n0.562\n36.319\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q08\n0.644\n0.013\n0.618\n0.669\n49.774\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q10\n0.706\n0.012\n0.682\n0.729\n58.794\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q12\n0.725\n0.012\n0.702\n0.748\n61.801\n0.000\nLoading\n\n\nKontrola\n=~\nst27q02\n0.550\n0.014\n0.523\n0.578\n39.524\n0.000\nLoading\n\n\nKontrola\n=~\nst27q06\n0.662\n0.012\n0.639\n0.685\n55.488\n0.000\nLoading\n\n\nKontrola\n=~\nst27q09\n0.657\n0.012\n0.633\n0.680\n54.562\n0.000\nLoading\n\n\nKontrola\n=~\nst27q11\n0.672\n0.012\n0.649\n0.695\n57.236\n0.000\nLoading\n\n\nKontrola\n=~\nst27q13\n0.588\n0.013\n0.562\n0.614\n44.259\n0.000\nLoading\n\n\n\n\n\nKodmodel_parameters(fit_cfa, component = \"correlation\") %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nTo\nOperator\nFrom\nCoefficient\nSE\nCI_low\nCI_high\nz\np\nComponent\n\n\n\nZapamiętywanie\n~~\nOpracowywanie\n0.368\n0.021\n0.327\n0.409\n17.565\n0.000\nCorrelation\n\n\nZapamiętywanie\n~~\nKontrola\n0.714\n0.015\n0.684\n0.744\n46.863\n0.000\nCorrelation\n\n\nOpracowywanie\n~~\nKontrola\n0.576\n0.017\n0.543\n0.609\n34.248\n0.000\nCorrelation\n\n\n\n\n\n\n\nW przypadku strategii zapamiętywania, wszystkie pozycje (st27q01, st27q03, st27q05, st27q07) ładują się umiarkowanie silnie na czynniku, z wartościami współczynników standaryzowanych w przedziale 0.59–0.64. Oznacza to, że zmienne te są spójnymi wskaźnikami tego konstruktu i wnoszą podobny wkład w jego pomiar. Wysokie istotności statystyczne (p &lt; .001) potwierdzają, że każda z tych zmiennych odgrywa istotną rolę w budowie czynnika.\nDla strategii opracowywania obserwujemy wyższe wartości ładunków czynnikowych – od 0.53 dla st27q04 do 0.73 dla st27q12. Oznacza to, że ta grupa pytań jest silnie związana z konstruktem emocjonalnych strategii uczenia się, a zwłaszcza pozycje st27q10 i st27q12 okazują się najbardziej reprezentatywne. Interpretować to można jako silną spójność wskaźników i dużą trafność pomiarową tego czynnika.\nJeśli chodzi o strategie kontrolne, wszystkie pozycje mają dość wysokie ładunki, od 0.55 do 0.67, co sugeruje dobrą konsystencję wewnętrzną tego konstruktu. Szczególnie istotne są pozycje st27q06, st27q09 i st27q11, które mają najwyższe wartości współczynników, a więc najlepiej odzwierciedlają mechanizmy związane z kontrolą uczenia się.\nWyniki korelacji między trzema strategiami uczenia się wskazują, że są one ze sobą istotnie powiązane, choć w różnym stopniu. Najsilniejszy związek występuje między Zapamiętywaniem a Kontrolą (r = 0.714, p &lt; 0.001), co sugeruje, że monitorowanie procesu uczenia się jest ściśle powiązane ze stosowaniem technik zapamiętywania. Nieco słabsze, ale nadal istotne powiązania obserwuje się między Opracowywaniem a Kontrolą (r = 0.576, p &lt; 0.001) oraz między Zapamiętywaniem a Opracowywaniem (r = 0.368, p &lt; 0.001), co wskazuje, że przetwarzanie materiału oraz kontrola uczenia się są umiarkowanie powiązane z technikami pamięciowymi. Łącznie wyniki te potwierdzają, że strategie tworzą spójny, ale zróżnicowany zbiór powiązanych ze sobą podejść do uczenia się.\n\nModel możemy również przedstawić graficznie.\n\nKodlibrary(semPlot)\nlibrary(RColorBrewer)\n\n# wybieramy pastelową paletę\npastel_cols &lt;- brewer.pal(3, \"Pastel2\")\n\nsemPaths(fit_cfa,\n         whatLabels = \"std\",\n         what = 'std',\n         layout = \"tree2\",\n         groups = \"latents\",\n         sizeMan = 6,\n         sizeLat = 8,\n         nCharNodes = 0,\n         style = \"lisrel\",\n         # kolory pastelowe dla zmiennych latentnych i obserwowanych\n         color = pastel_cols,\n         colorLat = pastel_cols[1],\n         colorMan = pastel_cols[2],\n         edge.color = \"grey70\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#path-analysis",
    "href": "sem.html#path-analysis",
    "title": "Modele strukturalne",
    "section": "Path analysis",
    "text": "Path analysis\nAnaliza ścieżkowa (ang. path analysis) jest jedną z najwcześniejszych form modelowania strukturalnego i stanowi naturalne rozwinięcie regresji wielokrotnej. Jej głównym celem jest badanie złożonych układów zależności przyczynowo-skutkowych między zmiennymi obserwowalnymi, w tym układów obejmujących zmienne pośredniczące (mediatory). Została zaproponowana przez Sewalla Wrighta w latach 20. XX wieku jako narzędzie do formalizacji równań przyczynowych w biologii, a następnie rozwinęła się jako fundament współczesnych modeli SEM.\nFormalnie model analizy ścieżkowej można zapisać jako system równań liniowych \\[\n\\mathbf{y} = B\\mathbf{y} + \\Gamma \\mathbf{x} + \\zeta,\n\\] gdzie:\n\n\n\\(\\mathbf{y}\\) to wektor zmiennych endogenicznych (wyjaśnianych w modelu),\n\n\\(\\mathbf{x}\\) to wektor zmiennych egzogenicznych (traktowanych jako dane, nieobjaśniane w modelu),\n\n\\(B\\) to macierz współczynników regresji pomiędzy zmiennymi endogenicznymi,\n\n\\(\\Gamma\\) to macierz współczynników regresji łączących zmienne egzogeniczne z endogenicznymi,\n\n\\(\\zeta\\) to wektor zakłóceń (błędów strukturalnych).\n\nZałożenia analizy ścieżkowej są w dużej mierze zbieżne z klasycznymi założeniami regresji liniowej. Obejmują one liniowość zależności, brak silnej współliniowości między predyktorami, nieskorelowanie błędów \\(\\zeta\\) z egzogenicznymi zmiennymi \\(\\mathbf{x}\\) oraz odpowiednio dużą próbę, aby zapewnić stabilność estymacji. Dodatkowo zakłada się poprawność teoretyczną modelu – to badacz definiuje strukturę ścieżek na podstawie teorii lub wcześniejszych wyników, a analiza ma na celu jej statystyczną weryfikację.\nEstymacja parametrów w analizie ścieżkowej opiera się najczęściej na metodzie największej wiarygodności (maximum likelihood, ML), która minimalizuje różnicę między macierzą kowariancji obserwowanej a macierzą kowariancji implikowaną przez model. Alternatywnie stosuje się metody oparte na najmniejszych kwadratach (generalized least squares, GLS, ordinary least squares, OLS) (Schweizer i DiStefano 2016), a w przypadku naruszenia normalności rozkładu dostępne są warianty odporne, takie jak robust ML (Schweizer i DiStefano 2016) czy estymacja asymptotycznie niezależna (asymptotically distribution free, ADF) (Huang i Bentler 2015). W nowszych zastosowaniach wykorzystuje się również metody bayesowskie, które pozwalają wprowadzić rozkłady a priori dla parametrów i prowadzić wnioskowanie probabilistyczne o strukturze zależności.\n\nPrzykład 4.2 Wykonamy prostą analizę ścieżkową na danych mtcars, aby zbadać wpływ masy pojazdu (wt) na jego zużycie paliwa (mpg), za pośrednictwem mocy silnika (hp). Hipoteza zakłada, że masa wpływa na moc, która z kolei wpływa na zużycie paliwa.\n\nKod# Model ścieżkowy (wyłącznie zmienne obserwowalne)\n# hp jest mediatorem między wt a mpg\nmodel_pa &lt;- '\n  # równania regresji (część strukturalna)\n  mpg ~ c*wt + b*hp\n  hp  ~ a*wt\n\n  # efekty pośrednie i całkowite\n  ind := a*b\n  tot := c + (a*b)\n'\n\nfit_pa &lt;- sem(model_pa, data = mtcars,\n              estimator = \"MLR\")                         # estymator odporny (robust ML)\n\n# Podsumowanie wyników (standaryzacja, istotności, efekty zdefiniowane)\nsummary(fit_pa, standardized = TRUE, ci = TRUE, rsquare = TRUE)\n\nlavaan 0.6-20 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                            32\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  mpg ~                                                                 \n    wt         (c)   -3.878    0.620   -6.255    0.000   -5.093   -2.663\n    hp         (b)   -0.032    0.007   -4.781    0.000   -0.045   -0.019\n  hp ~                                                                  \n    wt         (a)   46.160    5.734    8.051    0.000   34.922   57.398\n   Std.lv  Std.all\n                  \n   -3.878   -0.630\n   -0.032   -0.361\n                  \n   46.160    0.659\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .mpg               6.095    1.645    3.705    0.000    2.870    9.320\n   .hp             2577.777  996.624    2.587    0.010  624.430 4531.125\n   Std.lv  Std.all\n    6.095    0.173\n 2577.777    0.566\n\nR-Square:\n                   Estimate\n    mpg               0.827\n    hp                0.434\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind              -1.467    0.351   -4.179    0.000   -2.155   -0.779\n    tot              -5.344    0.634   -8.434    0.000   -6.587   -4.102\n   Std.lv  Std.all\n   -1.467   -0.238\n   -5.344   -0.868\n\nKod# Wizualizacja diagramu ścieżek\nsemPaths(fit_pa,\n         what = \"std\", \n         whatLabels = \"std\",\n         layout = \"circle\",\n         style = \"lisrel\",\n         residuals = FALSE, intercepts = FALSE,\n         nCharNodes = 0, sizeMan = 7,\n         groups = \"manifests\",\n         color = brewer.pal(3, \"Pastel2\"),\n         edge.color = \"grey60\")\n\n\n\n\n\n\n\nModel zapisujemy jako: \\[\n\\mathbf{y} = B\\mathbf{y} + \\Gamma \\mathbf{x} + \\zeta,\n\\] gdzie:\n\n\n\\(\\mathbf{y} = \\begin{bmatrix} mpg \\\\ hp \\end{bmatrix}\\) to zmienne endogeniczne,\n\n\\(\\mathbf{x} = wt\\) to zmienna egzogeniczna,\n\n\\(B\\) to macierz regresji pomiędzy zmiennymi endogenicznymi,\n\n\\(\\Gamma\\) to macierz efektów zmiennych egzogenicznych na endogeniczne,\n\n\\(\\zeta\\) to wektor błędów strukturalnych.\n\nEstymowane równania \\[\n\\begin{aligned}\nmpg &= c \\cdot wt + b \\cdot hp + \\zeta_{mpg}, \\\\\nhp  &= a \\cdot wt + \\zeta_{hp},\n\\end{aligned}\n\\] gdzie:\n\n\\(a = 46.160\\) (standaryzowane \\(0.659\\)) – wpływ masy (wt) na moc (hp),\n\\(b = -0.032\\) (standaryzowane \\(-0.361\\)) – wpływ mocy (hp) na spalanie (mpg),\n\\(c = -3.878\\) (standaryzowane \\(-0.630\\)) – bezpośredni wpływ masy (wt) na spalanie (mpg).\nMacierz \\(B\\) (zależności między endogenicznymi): \\[\nB =\n\\begin{bmatrix}\n0 & b \\\\\n0 & 0\n\\end{bmatrix},\n\\quad b = -0.032.\n\\]\nMacierz \\(\\Gamma\\) (wpływy egzogenicznej zmiennej \\(wt\\)): \\[\n\\Gamma =\n\\begin{bmatrix}\nc \\\\\na\n\\end{bmatrix},\n\\quad c = -3.878, \\quad a = 46.160.\n\\]\nWariancje resztowe (błędy strukturalne): \\[\n\\mathrm{Var}(\\zeta_{mpg}) = 6.095 \\; (17.3\\%),\n\\quad \\mathrm{Var}(\\zeta_{hp}) = 2577.777 \\; (56.6\\%).\n\\]\n\nOznacza to, że model wyjaśnia 82.7% wariancji spalania i 43.4% wariancji mocy.\n\nEfekt pośredni masy na spalanie przez moc \\[\nind = a \\cdot b = 46.160 \\cdot (-0.032) = -1.467\n\\] istotny statystycznie (\\(p &lt; 0.001\\)).\nEfekt całkowity masy na spalanie: \\[\ntot = c + a \\cdot b = -3.878 + (-1.467) = -5.344.\n\\] Oznacza to, że wzrost masy samochodu (o jednostkę standaryzowaną) zmniejsza spalanie o 0.868 jednostki standardowej – w dużej części bezpośrednio, a w mniejszej poprzez wzrost mocy.\n\nModel ścieżkowy wskazuje, że masa samochodu (wt) ma silny negatywny wpływ na oszczędność paliwa (mpg), zarówno bezpośrednio, jak i pośrednio poprzez zwiększanie mocy silnika (hp). Moc natomiast sama w sobie pogarsza spalanie. Wartości \\(R^2\\) potwierdzają, że model bardzo dobrze wyjaśnia zmienność mpg (83%), ale umiarkowanie słabiej radzi sobie z hp (43%).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#założenia-modeli-sem",
    "href": "sem.html#założenia-modeli-sem",
    "title": "Modele strukturalne",
    "section": "Założenia modeli SEM",
    "text": "Założenia modeli SEM\n\nOparcie na teorii - SEM z definicji służy testowaniu i potwierdzaniu modelu teoretycznego. Dlatego punktem wyjścia musi być koncepcja badawcza oparta na wcześniejszych badaniach i spójnej teorii. Model powinien odzwierciedlać hipotezy dotyczące relacji między konstruktami latentnymi i zmiennymi obserwowalnymi.\n\nWielkość próby - zaleca się próby liczące co najmniej około 200 obserwacji, choć ostateczny wymóg zależy od trzech czynników:\n\nrozkładu zmiennych,\nzłożoności modelu,\nmetody estymacji.\n\nDuże próby zwiększają stabilność wyników i odporność na naruszenia założeń.\n\nNormalność rozkładu - ponieważ SEM opiera się na macierzy kowariancji, standardowo zakłada się wielowymiarową normalność rozkładu zmiennych. W praktyce odchylenia od normalności można kompensować, stosując estymatory odporne, np. robust ML czy WLSMV.\nLiniowość związków - zakłada się, że relacje między konstruktami latentnymi a wskaźnikami obserwowalnymi oraz między zmiennymi latentnymi mają charakter liniowy.\nBrak silnej współliniowości - predyktory w modelu powinny być możliwie niezależne. Choć umiarkowana współliniowość zwykle nie jest problemem, silne korelacje mogą prowadzić do trudności w estymacji i interpretacji ścieżek.\nKompletność danych - modele SEM wymagają pełnych danych. Można to osiągnąć poprzez imputację (np. średnią, regresję) albo stosując metody wykorzystujące pełną informację przy brakach danych, jak FIML (Full Information Maximum Likelihood).\nNiezależność błędów pomiarowych - standardowe założenie głosi, że błędy pomiarowe są nieskorelowane. W praktyce jednak niekiedy dopuszcza się ich korelacje, zwłaszcza gdy sugerują to indeksy modyfikacyjne i uzasadnia teoria.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#ocena-dopasowania-modelu-sem",
    "href": "sem.html#ocena-dopasowania-modelu-sem",
    "title": "Modele strukturalne",
    "section": "Ocena dopasowania modelu SEM",
    "text": "Ocena dopasowania modelu SEM\n\n\n\n\n\n\n\nWskaźnik\nWartość idealna\nWartość akceptowalna\n\n\n\nChi-kwadrat (CMIN) *\np &gt; 0,05 (przy α = 0,05)\np &lt; 0,05 (przy α = 0,05)\n\n\nStandaryzowany chi-kwadrat (CMIN/df) *\n&lt; 3\n&lt; 5\n\n\nGFI (Goodness of Fit Index)\n&gt; 0,95\n&gt; 0,90\n\n\nAGFI (Adjusted GFI)\n&gt; 0,90\n&gt; 0,85\n\n\nCFI (Comparative Fit Index) *\n&gt; 0,95\n&gt; 0,90\n\n\nTLI (Tucker-Lewis Index, NNFI)\n&gt; 0,90\n&gt; 0,85\n\n\nNFI (Normed Fit Index)\n&gt; 0,95\n&gt; 0,90\n\n\nPGFI (Parsimonious GFI)\n&gt; 0,50\nbrak sztywnych progów\n\n\nPNFI (Parsimonious NFI)\n&gt; 0,50\nbrak sztywnych progów\n\n\nPCFI (Parsimonious CFI)\n&gt; 0,50\nbrak sztywnych progów\n\n\nSRMR (Standardized RMR) *\n&lt; 0,05\n&lt; 0,08\n\n\nRMSEA (Root Mean Square Error of Approximation) *\n&lt; 0,05 [90% CI]\n&lt; 0,10 [90% CI]\n\n\n\n\nPrzykład 4.3 Zbiór HolzingerSwineford1939 (pakietu lavaan) zawiera wyniki uczniów w dziewięciu testach poznawczych oraz podstawowe cechy demograficzne i szkolne (Turney 1939). Dziewięć pozycji testowych tworzy trzy klasyczne domeny poznawcze: visual (postrzeganie wzrokowe), textual (kompetencje werbalne) i speed (szybkość przetwarzania), po trzy wskaźniki w każdej domenie. Oryginalne zmienne testowe oznaczone są jako x1–x9 i w literaturze przypisuje się je do czynników: - x1, x2, x3 → czynnik Visual, - x4, x5, x6 → czynnik Textual, - x7, x8, x9 → czynnik Speed.\nW danych można znaleźć też zmienne: ageyr (wiek w latach), agemo (nadwyżka miesięcy), sex (płeć, kod 1 = chłopiec, 2 = dziewczynka), school (szkoła), grade (klasa). Do modelu wprowadzony zostanie wiek w latach ciągłych: age = ageyr + agemo/12, a płeć zostanie przekodowana binarnie (sex01: 0 = dziewczynka, 1 = chłopiec) dla przejrzystości interpretacji.\nHipotezy badawcze (wpływy bezpośrednie i pośrednie)\nPrzyjmiemy klasyczną trójczynnikową strukturę pomiarową (Visual, Textual, Speed), a w części strukturalnej założymy wpływy wieku i płci na latentne zdolności oraz zależność między zdolnościami:\n\n\n\\(H_0^1:\\) Wiek dodatnio wpływa na Visual i Textual oraz – pośrednio – na Speed.\n\n\\(H_0^2:\\) Płeć (kod 1 = chłopiec) różnicuje profile: dodatnio wpływa na Visual, natomiast słabiej lub ujemnie na Textual; wpływ na Speed występuje pośrednio poprzez Visual i Textual.\n\n\\(H_0^3:\\) Czynnik Speed zależy wprost od Visual i Textual; efekty wieku i płci na Speed będą zatem częściowo pośredniczone przez Visual i Textual.\n\n\nKoddata(\"HolzingerSwineford1939\")\n\n# Przygotowanie zmiennych egzogenicznych\nhs &lt;- within(HolzingerSwineford1939, {\n  age &lt;- ageyr + agemo/12\n  sex01 &lt;- as.numeric(sex == 1)  # 1=boy, 0=girl (w razie innego kodowania dostosować)\n})\n\n# Specyfikacja modelu SEM: część pomiarowa (CFA) + część strukturalna\nmodel_sem &lt;- '\n  # Część pomiarowa (CFA)\n  Visual  =~ x1 + x2 + x3\n  Textual =~ x4 + x5 + x6\n  Speed   =~ x7 + x8 + x9\n\n  # Część strukturalna (path analysis na latentach)\n  Speed   ~ b1*Visual + b2*Textual\n  Visual  ~ a1*age + a2*sex01\n  Textual ~ a3*age + a4*sex01\n\n  # Efekty pośrednie wieku i płci na Speed\n  ind_age  := a1*b1 + a3*b2\n  ind_sex  := a2*b1 + a4*b2\n\n  # Efekty całkowite wieku i płci na Speed\n  tot_age  := ind_age\n  tot_sex  := ind_sex\n'\n\nfit_sem &lt;- sem(model_sem, data = hs,\n               estimator = \"MLR\",      # robust ML\n               meanstructure = TRUE)\n\n\nW części pomiarowej zdefiniowano trzy czynniki pierwszego rzędu z klasycznym mapowaniem wskaźników. W części strukturalnej założono, że Visual i Textual determinują Speed, a age i sex01 oddziałują na Visual i Textual. Zdefiniowano także etykiety ścieżek, aby policzyć efekty pośrednie i całkowite (:=). Parametry raportowane są w skalach surowych i standaryzowanych.\n\nKodmodel_performance(fit_sem, c(\"Chi2\",\"Chi2_df\",\"p_Chi2\",\"CFI\",\"TLI\",\"RMSEA\",\"RMSEA_CI_low\",\"RMSEA_CI_high\",\"SRMR\")) %&gt;% \n  print_html()\n\n\n\n\n\nChi2\nChi2_df\np_Chi2\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nSRMR\n\n\n172.77\n39\n0\n0.86\n0.11\n0.09\n0.12\n0.11\n\n\n\n\n\nOcena dopasowania modelu SEM zawsze powinna być przeprowadzona z kilku perspektyw: testu chi-kwadrat, wskaźników dopasowania przyrostowych (incremental fit indices) oraz wskaźników błędu aproksymacji. Wyniki uzyskane w analizie wskazują na istotne sygnały niedopasowania modelu.\nTest chi-kwadrat dla modelu dał wartość \\(\\chi^2 = 172,77\\) przy df = 39, co przy dużej liczności prowadzi do p &lt; 0.001. Oznacza to, że w sensie dosłownym odrzucamy hipotezę o pełnym zgodnym odwzorowaniu macierzy kowariancji w populacji przez model. Jednak test chi-kwadrat jest bardzo wrażliwy zarówno na rozmiar próby, jak i złożoność modelu, dlatego wynik ten traktuje się raczej jako punkt wyjścia niż rozstrzygające kryterium.\nWskaźniki przyrostowe pokazują umiarkowanie słabe dopasowanie. Wartości CFI = 0.858 i TLI = 0.803 są wyraźnie poniżej rekomendowanego poziomu 0.90, a tym bardziej 0.95, które zwykle przyjmuje się jako granicę bardzo dobrego dopasowania. To sugeruje, że model w obecnej postaci nie wyjaśnia wystarczająco dobrze struktury zależności obserwowanych w danych i potencjalnie wymaga modyfikacji – np. dodania powiązań reszt, rewizji struktury ścieżek lub przemyślenia samego modelu pomiarowego.\nWskaźnik błędu aproksymacji RMSEA = 0.107 (90% CI: 0.091–0.123) jest stosunkowo wysoki i wykracza poza granicę akceptowalności (zwykle &lt; 0.08, a najlepiej &lt; 0.05). Taki wynik sugeruje, że model charakteryzuje się zauważalnym błędem przybliżenia w stosunku do danych populacyjnych. Z kolei wskaźnik SRMR = 0.108 jest powyżej standardowego progu akceptowalności 0.08, co dodatkowo wskazuje na problemy z odwzorowaniem korelacji obserwowanych przez model.\nChcąc poprawić dopasowanie modelu można zaproponować następujące modyfikacje:\n\nPo pierwsze, dopuścić kowariancję zaburzeń zmiennych latentnych Visual i Textual. W praktyce te dwa konstrukty współdzielą wariancję nie w pełni wyjaśnioną przez wiek i płeć. Dodanie Visual ~~ Textual nie zmienia hipotez o wpływach na Speed, a często istotnie obniża błąd aproksymacji.\nPo drugie, w części pomiarowej można dopuścić wyłącznie te kowariancje reszt wskaźników, które mają jednoznaczne uzasadnienie treściowe. W HolzingerSwineford1939 typowe pary to x1–x2, x2–x3 (ten sam kanał wizualny), x4–x5 (werbalne), x7–x8 (szybkość). Te powiązania korygują lokalne niedopasowania bez naruszania sensu hipotez strukturalnych.\nPo trzecie, skontrolować jakość wskaźników. Jeżeli którykolwiek ładunek w CFA jest niski (np. &lt; 0.40) i generuje duże reszty, rozważyć jego usunięcie lub zamianę (jeśli masz silne uzasadnienie teoretyczne). Usunięcie pojedynczego, słabego wskaźnika często stabilizuje model.\nPo czwarte, uwzględnić współzmienność zmiennych egzogenicznych (age ~~ sex01). To technicznie poprawne i zapobiega „wpychaniu” ich korelacji w część strukturalną.\nPo piąte, upewniać się, że estymacja odpowiada naturze danych. Jeśli wskaźniki są porządkowe, stosować estymację DWLS i macierz polichoryczną; przy ciągłych pozostawić MLR (w naszym przypadku wyniki nie są ze skali Likerta, oryginalne dane zawierały zmienne z różnego zakresu).\nPo szóste, można rozważyć słabą nieliniowość wieku (age^2) tylko wtedy, gdy wskazują na to reszty i teoria; nie zmienia to sensu głównych hipotez (dalej wiek → Visual/Textual → Speed), ale bywa, że poprawia dopasowanie.\n\nPoniżej wariant modelu z minimalnymi, teoretycznie uzasadnionymi modyfikacjami.\n\nKodmodel_sem_refined &lt;- '\n  # CFA\n  Visual  =~ x1 + x2 + x3\n  Textual =~ x4 + x5 + x6\n  Speed   =~ x7 + x8 + x9\n\n  # Strukturalny (bez zmian hipotez)\n  Speed   ~ b1*Visual + b2*Textual\n  Visual  ~ a1*age + a2*sex01\n  Textual ~ a3*age + a4*sex01\n\n  # Dodatkowe kowariancje zgodne z teorią\n  Visual ~~ Textual          # współdzielona wariancja latentów\n  age ~~ sex01               # współzmienność egzogenicznych\n\n  # Skorelowane unikalności (tylko wewnątrz domen i z uzasadnieniem treściowym)\n  x1 ~~ x2\n  x2 ~~ x3\n  x4 ~~ x5\n  x7 ~~ x8\n\n  # Efekty pośrednie i całkowite (jak dotąd)\n  ind_age  := a1*b1 + a3*b2\n  ind_sex  := a2*b1 + a4*b2\n  tot_age  := ind_age\n  tot_sex  := ind_sex\n'\n\nfit_sem_refined &lt;- sem(model_sem_refined, data = hs,\n                       estimator = \"MLR\", \n                       meanstructure = TRUE)\n\nmodel_performance(fit_sem_refined, c(\"Chi2\",\"Chi2_df\",\"p_Chi2\",\"CFI\",\"TLI\",\"RMSEA\",\"RMSEA_CI_low\",\"RMSEA_CI_high\",\"SRMR\")) %&gt;% \n  print_html()\n\n\n\n\n\nChi2\nChi2_df\np_Chi2\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nSRMR\n\n\n97.03\n34\n5.64e-08\n0.93\n0.08\n0.06\n0.10\n0.06\n\n\n\n\nKod# Pomocniczo: gdzie są największe niedopasowania?\nmodindices(fit_sem, sort.=TRUE, minimum.value = 10)[1:12, c(\"lhs\",\"op\",\"rhs\",\"mi\",\"epc\",\"sepc.all\")] %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = c(\"mi\",\"epc\",\"sepc.all\"), decimals = 3) \n\n\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.all\n\n\n\nTextual\n~\nSpeed\n47.445\n1.782\n1.118\n\n\nx7\n~~\nx8\n36.699\n0.649\n1.063\n\n\nTextual\n=~\nx1\n34.020\n0.374\n0.316\n\n\nVisual\n~~\nTextual\n30.719\n0.282\n0.419\n\n\nTextual\n~\nVisual\n30.719\n0.567\n0.414\n\n\nVisual\n~\nTextual\n30.719\n0.310\n0.423\n\n\nVisual\n=~\nx9\n26.943\n0.558\n0.401\n\n\nage\n~\nSpeed\n25.940\n0.694\n0.421\n\n\nSpeed\n~\nage\n22.920\n0.200\n0.330\n\n\nVisual\n~\nSpeed\n20.687\n1.558\n1.337\n\n\nTextual\n~~\nSpeed\n18.965\n0.696\n1.307\n\n\nVisual\n=~\nx7\n18.063\n−0.483\n−0.321\n\n\n\n\n\n\nObecny model po modyfikacjach prezentuje już znacznie lepsze dopasowanie niż pierwotny, choć nadal nie jest idealny. Wskaźniki globalne wskazują, że dopasowanie można uznać za umiarkowanie dobre. Statystyka \\(\\chi^2\\) (97.03, df = 34, p &lt; 0.001) nadal jest istotna, co przy relatywnie małej liczbie stopni swobody sygnalizuje pewne niedopasowanie modelu do danych. Jednak należy pamiętać, że test \\(\\chi^2\\) jest bardzo czuły i w praktyce często odrzuca modele nawet przy akceptowalnym dopasowaniu. Lepszą informację dają indeksy: CFI = 0.933 mieści się w strefie „akceptowalnej”, ale jeszcze poniżej progu 0.95 sugerującego bardzo dobre dopasowanie. TLI = 0.892 jest blisko progu 0.90 i również wskazuje na umiarkowane dopasowanie. RMSEA = 0.078 (90% CI: 0.060–0.097) mieści się w przedziale akceptowalnym (&lt; 0.10), a dolna granica jest blisko 0.05, co sugeruje, że model jest względnie bliski dobrego dopasowania. SRMR = 0.055 jest niewiele powyżej granicy 0.05 i można go uznać za dość dobry wynik.\nAnaliza indeksów modyfikacyjnych1 pokazuje, że największe niedopasowania koncentrują się w kilku obszarach. Po pierwsze, sugerowane są dodatkowe powiązania strukturalne między czynnikami latentnymi a zmienną Speed (np. Textual ~ Speed, Visual ~ Speed), które jednak wykraczałyby poza pierwotnie założone hipotezy mediacyjne. Po drugie, wskazywane są silne powiązania między wskaźnikami tego samego czynnika (np. x7 ~~ x8), co można interpretować jako efekty metody lub nadmierne podobieństwo treściowe pozycji testowych. Po trzecie, pojawiają się sugestie dotyczące alternatywnych ładunków wskaźników (np. Textual =~ x1, Visual =~ x9), co wskazuje na pewne problemy z czystością czynników, ale ich wprowadzenie mogłoby zmienić interpretację teoretyczną czynników.\n1 Indeksy modyfikacyjne (modification indices, MI) wskazują, o ile zmniejszyłby się chi-kwadrat modelu, gdyby wprowadzono daną modyfikację (np. dodano ścieżkę lub skorelowano błędy). Wysokie wartości MI sugerują potencjalne obszary niedopasowania modelu do danych i mogą być punktem wyjścia do rozważań nad jego ulepszeniem. Należy jednak podchodzić do nich ostrożnie i zawsze w kontekście teorii, aby uniknąć nadmiernego dopasowania modelu do konkretnego zbioru danych.Godząc się na nieidealne dopasowanie, można uznać, że model w obecnej formie jest wystarczająco dobry do testowania głównych hipotez badawczych. Wprowadzenie pewnych zmian sugerowanych przez indeksy modyfikacyjne mogłoby utrudnić weryfikację postawionych hipotez.\n\nKodmodel_parameters(fit_sem_refined, standardized = TRUE, ci = TRUE) \n\n# Loading\n\nLink          | Coefficient |   SE |     100% CI |     z |      p\n-----------------------------------------------------------------\nVisual =~ x1  |        1.00 | 0.00 |             |       | &lt; .001\nVisual =~ x2  |        0.54 | 0.12 | [-Inf, Inf] |  4.52 | &lt; .001\nVisual =~ x3  |        0.69 | 0.12 | [-Inf, Inf] |  5.77 | &lt; .001\nTextual =~ x4 |        1.00 | 0.00 |             |       | &lt; .001\nTextual =~ x5 |        1.11 | 0.07 | [-Inf, Inf] | 16.57 | &lt; .001\nTextual =~ x6 |        0.94 | 0.13 | [-Inf, Inf] |  7.20 | &lt; .001\nSpeed =~ x7   |        1.00 | 0.00 |             |       | &lt; .001\nSpeed =~ x8   |        1.26 | 0.21 | [-Inf, Inf] |  6.03 | &lt; .001\nSpeed =~ x9   |        2.42 | 0.58 | [-Inf, Inf] |  4.19 | &lt; .001\n\n# Regression\n\nLink                 | Coefficient |   SE |     100% CI |     z |      p\n------------------------------------------------------------------------\nSpeed ~ Visual (b1)  |        0.23 | 0.08 | [-Inf, Inf] |  2.78 | 0.005 \nSpeed ~ Textual (b2) |    8.44e-03 | 0.03 | [-Inf, Inf] |  0.27 | 0.786 \nVisual ~ age (a1)    |   -9.65e-03 | 0.07 | [-Inf, Inf] | -0.13 | 0.897 \nVisual ~ sex01 (a2)  |        0.24 | 0.13 | [-Inf, Inf] |  1.78 | 0.074 \nTextual ~ age (a3)   |       -0.23 | 0.06 | [-Inf, Inf] | -3.79 | &lt; .001\nTextual ~ sex01 (a4) |       -0.07 | 0.14 | [-Inf, Inf] | -0.50 | 0.615 \n\n# Correlation\n\nLink              | Coefficient |   SE |     100% CI |     z |      p\n---------------------------------------------------------------------\nVisual ~~ Textual |        0.43 | 0.09 | [-Inf, Inf] |  4.91 | &lt; .001\nage ~~ sex01      |        0.08 | 0.03 | [-Inf, Inf] |  2.79 | 0.005 \nx1 ~~ x2          |       -0.04 | 0.10 | [-Inf, Inf] | -0.40 | 0.690 \nx2 ~~ x3          |        0.14 | 0.09 | [-Inf, Inf] |  1.52 | 0.129 \nx4 ~~ x5          |        0.03 | 0.12 | [-Inf, Inf] |  0.25 | 0.805 \nx7 ~~ x8          |        0.34 | 0.07 | [-Inf, Inf] |  5.22 | &lt; .001\n\n# Defined\n\nTo        | Coefficient |   SE |     100% CI |     z |     p\n------------------------------------------------------------\n(ind_age) |   -4.15e-03 | 0.02 | [-Inf, Inf] | -0.22 | 0.828\n(ind_sex) |        0.05 | 0.03 | [-Inf, Inf] |  1.69 | 0.091\n(tot_age) |   -4.15e-03 | 0.02 | [-Inf, Inf] | -0.22 | 0.828\n(tot_sex) |        0.05 | 0.03 | [-Inf, Inf] |  1.69 | 0.091\n\n\nModel pomiarowy\nWyniki dla czynników latentnych (Visual, Textual, Speed) pokazują, że wszystkie wskaźniki (x1–x9) istotnie ładują się na odpowiednich czynnikach (p &lt; 0.001), co potwierdza poprawność konstrukcji pomiarowej. Ładunki są zróżnicowane: np. dla Visual zmienne x2 i x3 mają umiarkowane wartości (0.54, 0.69), natomiast dla Textual wskaźniki x5 i x6 są mocno związane z czynnikiem (ok. 1.1 i 0.94). Czynnik Speed jest silnie określony przez x7–x9, przy czym x9 ma wyjątkowo wysoki ładunek (2.42), co może sugerować, że ta zmienna dominuje w definiowaniu konstruktu. Generalnie jednak wszystkie zmienne obserwowalne wnoszą istotny wkład, co wspiera trafność pomiarową.\nModel strukturalny\nHipotezy dotyczące wpływów bezpośrednich częściowo się potwierdziły. Czynnik Visual istotnie przewiduje Speed (b1 = 0.23, p = 0.005), co oznacza, że im wyższe zdolności wizualne, tym lepsze wyniki w zadaniach szybkościowych. Z kolei wpływ Textual na Speed okazał się nieistotny (b2 ≈ 0, p = 0.786), co przeczy hipotezie o jego znaczącym wkładzie. Wśród zmiennych egzogenicznych, wiek istotnie i negatywnie wpływa na Textual (a3 = -0.23, p &lt; 0.001), co można interpretować tak, że starsze osoby mają gorsze wyniki w zadaniach tekstowych. Płeć (sex01) nie odgrywa istotnej roli ani w Visual, ani w Textual (p &gt; 0.05), choć dla Visual efekt był bliski istotności (a2 = 0.24, p = 0.074), sugerując potencjalny trend.\nZależności między czynnikami\nIstnieje istotna korelacja między Visual i Textual (0.43, p &lt; .001), co wskazuje, że obie zdolności współwystępują, ale nie są tożsame. Korelacja ta wspiera tezę o współzależności różnych typów zdolności poznawczych. Dodatkowo, zidentyfikowano korelację między zmiennymi resztowymi x7 i x8 (0.34, p &lt; .001), co można interpretować jako częściowo wspólny czynnik specyficzny dla tych zadań szybkościowych.\nEfekty pośrednie i całkowite\nŚcieżki pośrednie przez Visual i Textual nie były istotne w przypadku wieku (ind_age ≈ 0, p = 0.828), natomiast dla płci (ind_sex = 0.05, p = 0.091) pojawił się trend w kierunku efektu pośredniego, choć bez pełnej istotności. Efekty całkowite (tot_age, tot_sex) powtarzają te same wnioski – brak efektów dla wieku i jedynie potencjalny, słaby wpływ płci.\nPodsumowanie\nModel po modyfikacjach w poprawny sposób mierzy czynniki i potwierdza istotną rolę zdolności wizualnych w wyjaśnianiu szybkości, podczas gdy zdolności tekstowe odgrywają mniejszą rolę. Wiek ma wyraźny negatywny wpływ na zdolności tekstowe, natomiast płeć nie wpływa istotnie na żaden z czynników, choć jej rola dla zdolności wizualnych może wymagać dalszej analizy. Wyniki wskazują, że hipotezy dotyczące bezpośredniego wpływu Visual na Speed oraz wieku na Textual znajdują potwierdzenie, natomiast hipotezy dotyczące Textual → Speed i sex01 → czynniki nie znajdują mocnego wsparcia.\n\nKod# Wizualizacja modelu SEM\nsemPaths(fit_sem_refined,\n         what = \"std\", \n         whatLabels = \"std\",\n         layout = \"tree2\",\n         style = \"lisrel\",\n         residuals = T, intercepts = F,\n         nCharNodes = 0, sizeMan = 5,\n         groups = \"latent\",\n         color = brewer.pal(3, \"Pastel2\"),\n         edge.color = \"grey60\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#modele-strukturalne-sem",
    "href": "sem.html#modele-strukturalne-sem",
    "title": "Modele strukturalne",
    "section": "Modele strukturalne (SEM)",
    "text": "Modele strukturalne (SEM)\nModele typu covariance-based structural equation modeling (CB-SEM), określane po prostu jako SEM, stanowią rozwinięcie i uogólnienie dwóch podejść: analizy czynnikowej (CFA) oraz analizy ścieżkowej (path analysis). Istota SEM polega na tym, że pozwala ono jednocześnie badamy trafność pomiaru zmiennych latentnych oraz testujemy hipotezy dotyczące relacji między tymi zmiennymi. Dzięki temu SEM stanowi narzędzie integrujące w sobie modelowanie pomiarowe i strukturalne, umożliwiając analizę złożonych układów zależności obserwowalnych i nieobserwowalnych.\nFormalnie model SEM zapisuje się jako system równań macierzowych. Model pomiarowy dla zmiennych egzogenicznych ma postać \\[\n\\mathbf{x} = \\Lambda_x \\boldsymbol{\\xi} + \\boldsymbol{\\delta},\n\\] gdzie \\(\\mathbf{x}\\) to wektor zmiennych obserwowalnych, \\(\\boldsymbol{\\xi}\\) – wektor latentnych zmiennych egzogenicznych, \\(\\Lambda_x\\) – macierz ładunków czynnikowych, a \\(\\boldsymbol{\\delta}\\) – błędy pomiarowe. Analogicznie model pomiarowy dla zmiennych endogenicznych przyjmuje formę \\[\n\\mathbf{y} = \\Lambda_y \\boldsymbol{\\eta} + \\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{y}\\) oznacza obserwowalne zmienne endogeniczne, \\(\\boldsymbol{\\eta}\\) – latentne zmienne endogeniczne, \\(\\Lambda_y\\) – macierz ładunków, a \\(\\boldsymbol{\\epsilon}\\) – błędy pomiaru. Trzecim elementem jest model strukturalny \\[\n\\boldsymbol{\\eta} = B \\boldsymbol{\\eta} + \\Gamma \\boldsymbol{\\xi} + \\boldsymbol{\\zeta},\n\\] który opisuje relacje pomiędzy zmiennymi latentnymi endogenicznymi \\((B)\\) oraz wpływ zmiennych egzogenicznych na endogeniczne \\((\\Gamma)\\), z uwzględnieniem zakłóceń strukturalnych \\((\\boldsymbol{\\zeta})\\).\nW modelach SEM kluczowe znaczenie mają zmienne latentne \\((\\xi, \\eta)\\), które reprezentują konstrukty teoretyczne trudne do bezpośredniego pomiaru, np. satysfakcję z życia czy strategie uczenia się. Zmienne obserwowalne \\((x, y)\\) stanowią wskaźniki tych konstruktów. Ładunki czynnikowe \\(\\Lambda\\) wskazują, jak silnie dana zmienna obserwowalna powiązana jest z konstruktem latentnym. Macierze \\(B\\) i \\(\\Gamma\\) opisują odpowiednio zależności między konstruktami oraz ich uwarunkowania przez zmienne egzogeniczne. Błędy pomiarowe \\((\\delta, \\epsilon)\\) i zakłócenia strukturalne (\\(\\zeta\\)) odzwierciedlają niewyjaśnioną wariancję.\nParametry SEM mogą być estymowane różnymi metodami. Najczęściej stosuje się metodę największej wiarygodności (ML), która minimalizuje rozbieżność między macierzą kowariancji modelową a empiryczną. Alternatywą są metody najmniejszych kwadratów: GLS (ang. Generalized Least Squares), ULS (ang. Unweighted Least Squares, mniej wymagająca co do rozkładów, lecz bez klasycznych testów istotności) oraz DWLS (ang. Diagonally Weighted Least Squares), szczególnie polecana przy danych porządkowych. W przypadku naruszeń normalności rozkładu stosuje się wersje odporne, takie jak MLR (ang. Maximum Likelihood Robust) czy MLM (ang. Maximum Likelihood Mean-adjusted), które korygują wariancje i błędy standardowe („Supplemental Material for The Performance of ML, DWLS, and ULS Estimation With Robust Corrections in Structural Equation Models With Ordinal Variables” 2016; KILIÇ, UYSAL, i ATAR 2020; Li 2021; Kyriazos i Poga-Kyriazou 2023).\nZnaczenie SEM polega na tym, że łączy ono analizę czynnikową i analizę ścieżkową w jeden spójny model. W części pomiarowej pozwala sprawdzić, czy narzędzie badawcze dobrze odwzorowuje zamierzone konstrukty, natomiast w części strukturalnej umożliwia testowanie hipotez o związkach między zmiennymi ukrytymi. Dzięki temu SEM jest traktowane jako złoty standard w psychometrii, naukach społecznych i zarządzaniu, oferując zarówno rzetelną ocenę jakości pomiaru, jak i analizę zależności przyczynowych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#cb-sem-vs.-pls-sem",
    "href": "sem.html#cb-sem-vs.-pls-sem",
    "title": "Modele strukturalne",
    "section": "CB-SEM vs. PLS-SEM",
    "text": "CB-SEM vs. PLS-SEM\nAnaliza równań strukturalnych (SEM) rozwija się w dwóch głównych nurtach: CB-SEM (ang. Covariance Based SEM) oraz PLS-SEM (ang. Partial Least Squares SEM). Różnica pomiędzy CB-SEM a PLS-SEM sprowadza się przede wszystkim do podejścia badawczego i celu analizy. CB-SEM koncentruje się na globalnym dopasowaniu całego modelu do danych i jest metodą konfirmacyjną – służy do testowania hipotez wyprowadzonych z teorii. Wymaga dobrze zdefiniowanego modelu, dużych prób i spełnienia klasycznych założeń statystycznych, a w zamian dostarcza bogaty zestaw wskaźników dopasowania i rzetelnych testów statystycznych. Z kolei PLS-SEM opiera się na minimalizacji reszt na poziomie zależności między zmiennymi i traktuje model bardziej jako narzędzie predykcyjne niż potwierdzające (Latan i Noonan 2017). Jest elastyczniejszy, lepiej sprawdza się przy małych próbach i nienormalnych danych, ale oferuje mniej rozwinięte kryteria dopasowania i bywa obciążony w sensie statystycznym. W praktyce CB-SEM wybiera się do badań potwierdzających teorię, a PLS-SEM – do badań eksploracyjnych i predykcyjnych.\nCB-SEM stosuje się głównie w sytuacjach, gdy badacz chce przetestować ugruntowany model teoretyczny, wymagający dużych prób i danych o normalnym rozkładzie. PLS-SEM natomiast okazuje się przydatny w badaniach eksploracyjnych, przy mniejszych próbach i danych odchylających się od normalności. Trzeba jednak pamiętać, że wyniki PLS-SEM mogą być bardziej obciążone, a sama metoda wciąż jest rozwijana. Najnowsze podejścia, takie jak PLSc-SEM, starają się połączyć zalety obu nurtów.\n\n\n\n\n\n\n\nKryterium\nCB-SEM\nPLS-SEM\n\n\n\nCel analizy\nPotwierdzanie całościowego modelu i dobrze zdefiniowanej teorii\nEksploracja i predykcja, rozwój teorii w początkowej fazie\n\n\nMetoda estymacji\nNajczęściej największa wiarygodność (ML), wymagająca normalności\nMetoda najmniejszych kwadratów (Partial Least Squares), odporna na nienormalność\n\n\nZmienne latentne\nModele czynnikowe (factor-based), akcent na konstrukty latentne\nModele kompozytowe (composite-based), akcent na wskaźniki i prognozowanie\n\n\nDopasowanie modelu\nBogaty zestaw wskaźników globalnego dopasowania (χ², RMSEA, CFI, GFI)\nOgraniczony zestaw wskaźników – głównie R², AVE, α Cronbacha\n\n\nElastyczność modelu\nMniej elastyczny, restrykcyjny, wymaga dokładnego określenia teorii\nBardziej elastyczny, radzi sobie z małymi próbami i brakiem normalności\n\n\nWymagania co do próby\nZwykle duża próba (≥200), dane normalne\nMoże być stosowany dla mniejszych prób, brak wymogu normalności\n\n\nTyp badań\nPotwierdzające, weryfikacja teorii\nEksploracyjne, poszukujące nowych zależności\n\n\nRozwój metody\nStabilna, ugruntowana tradycja\nWciąż rozwijana (np. PLSc-SEM), wyniki mogą być obciążone",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#tworzenie-i-adaptacja-narzędzi-pomiarowych-w-sem",
    "href": "sem.html#tworzenie-i-adaptacja-narzędzi-pomiarowych-w-sem",
    "title": "Modele strukturalne",
    "section": "Tworzenie i adaptacja narzędzi pomiarowych w SEM",
    "text": "Tworzenie i adaptacja narzędzi pomiarowych w SEM\nPoniżej zostanie przedstawiona pełna, uporządkowana ścieżka tworzenia oraz adaptacji narzędzia pomiarowego (np. psychometrycznego) – od konceptualizacji do finalnego podręcznika – wraz z kluczowymi statystykami, ich wzorami, sposobem liczenia i interpretacją. Całość formułować w duchu klasycznej teorii testów, uzupełniając o elementy analizy czynnikowej oraz SEM.\n\nKonceptualizacja konstruktu i specyfikacja treści\n\nRozpoczynać od precyzyjnego zdefiniowania konstruktu (dziedzina, zakres, wymiary) na podstawie literatury. Tworzymy mapę treści (tzw. blueprint), która łączy wymiary teoretyczne z planowanymi obszarami itemów i formatem odpowiedzi. Już na tym etapie określamy typ modelu pomiarowego (refleksyjny czy formatywny)2, bo determinuje to dalszą metodologię.\n2 Model refleksyjny zakłada, że konstrukt latentny wywołuje pewien poziom odpowiedzi na pozycje: zmienne obserwowalne są efektami wspólnej przyczyny, ich błędy są specyficzne i niepowiązane, a wysoka współzależność pozycji jest oczekiwana. Model formatywny zakłada przeciwny kierunek przyczynowy: wskaźniki „tworzą” konstrukt (kompozyt), więc itemy nie muszą być skorelowane, a miary spójności wewnętrznej nie mają zastosowania.\n\nGenerowanie puli pozycji i weryfikacja treści\n\nBudujemy szeroką pulę itemów o zróżnicowanej trudności/poziomie (w testach osiągnięć) lub „natężeniu” treści (w skalach postaw). Zapewniać jednoznaczność językową, unikać sformułowań double-barreled („Czy jest Pan zadowolony z obsługi i ceny usługi?” - pytanie jest jednocześnie o obsługę i cenę) i niepotrzebnych zaprzeczeń („Nie zgadzam się z tym, że nie powinno się zabraniać palenia w restauracjach”).\nDla trafności treściowej stosuje się ocenę ekspertów. W praktyce używa się współczynnika Aikena \\(V\\) dla ocen skali porządkowej (np. 1–4) \\[\nV \\;=\\; \\frac{\\sum_{i=1}^{N} (s_i - s_{\\min})}{N\\,(s_{\\max}-s_{\\min})},\n\\] gdzie \\(s_i\\) to ocena \\(i\\)-tego eksperta, a \\(N\\) liczba ekspertów. Wysokie \\(V\\) (np. \\(\\ge 0,70\\)) sugeruje dobrą zgodność co do trafności treści.\n\nAdaptacja językowo-kulturowa\n\nPrzy przenoszeniu narzędzia między językami stosować forward translation (2 niezależne tłumaczenia), back-translation, konsensus zespołu i decentering (ew. korekta źródła). Wykonuje się cognitive interviews (parafrazy - powtarzanie pytań własnymi słowami, think-aloud - respondent mówi na głos, jak rozumie pytanie i dlaczego wybiera daną odpowiedź) w grupie docelowej, aby sprawdzić proces odpowiedzi. Wersję pilotażową poprzedza się audytem językowym i kulturowym przykładów/skal.\n\nPilotaż i analiza pozycji\n\nW badaniu pilotażowym szacuje się własności pozycji. W klasycznej teorii testów kluczowe są:\n\nkorelacja pozycja–wynik całkowity (skorygowana o daną pozycję); wartości \\(\\ge 0.30\\) wskazuje satysfakcjonującą dyskryminację;\ntrudność pozycji (w testach osiągnięć) jako średni wynik lub odsetek poprawnych odpowiedzi \\(p\\in[0,1]\\); pożądany rozkład trudności dla zakresu zdolności badanych;\nwpływ usunięcia pozycji na rzetelność (alpha if item deleted).\n\n\nWstępna struktura czynnikowa (EFA)\n\nNa oddzielnej próbie wykonuje się eksploracyjną analizę czynnikową (EFA). Ustala się liczbę czynników stosując* parallel analysis* i kryterium MAP Velicera. Następnie stosuje się estymatjcę modelu za pomocą PAF lub ML, z rotacją ortogonalną (np. varimax) lub ukośną (np. oblimin), zależnie od oczekiwanej korelacji czynników. Wartości ładunków \\(|\\lambda| \\ge 0.40\\) zwykle uznaje się za użyteczne; diagnozuje się ewentualne ładunki krzyżowe i jeśli takie wystąpią starami się je eliminować.\n\nKonfirmacja analiza czynniowa (CFA) i model pomiarowy\n\nUstalamy model \\[\n\\mathbf{x} \\;=\\; \\Lambda \\mathbf{f} \\;+\\; \\boldsymbol{\\epsilon},\n\\qquad \\mathrm{Cov}(\\mathbf{f})=\\Phi,\\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon})=\\Psi,\n\\] co implikuje macierz kowariancji \\[\n\\Sigma(\\theta) \\;=\\; \\Lambda \\,\\Phi\\, \\Lambda^\\top \\;+\\; \\Psi.\n\\] Estymujemy parametry metodą ML lub odporną (np. MLR), dla danych porządkowych – DWLS. Ocena globalna dopasowania opierać na:\n\nstatystyce \\(\\chi^2\\) rozbieżności;\nRMSEA z 90% PU;\nCFI i TLI (przyrostowe w stosunku do modelu niezależnego):\n\n\\(\\mathrm{CFI} = 1 - \\frac{\\max(\\chi^2_{\\text{model}}-df_{\\text{model}},\\,0)}{\\max(\\chi^2_{\\text{baseline}}-df_{\\text{baseline}},\\,0)},\\)\n\\(\\mathrm{TLI} = \\frac{\\chi^2_{\\text{baseline}}/df_{\\text{baseline}} - \\chi^2_{\\text{model}}/df_{\\text{model}}}{\\chi^2_{\\text{baseline}}/df_{\\text{baseline}} - 1};\\)\n\n\nSRMR jako średni moduł reszt standaryzowanych.\n\n\nRzetelność skali\n\nW klasycznym ujęciu rzetelność skali \\(\\rho_{XX’}\\) to udział wariancji prawdziwej w wariancji obserwowanej: \\[\n\\rho_{XX’} \\;=\\; \\frac{\\sigma_{T}^2}{\\sigma_{X}^2}.\n\\] Najczęściej używane miary do oceny rzetelności to:\n\nalfa Cronbacha (spójność wewnętrzna), dla \\(k\\) pozycji \\[\n\\alpha \\;=\\; \\frac{k}{k-1}\\left(1 - \\frac{\\sum_{i=1}^{k}\\sigma_i^2}{\\sigma_X^2}\\right)\\!,\n\\] gdzie \\(\\sigma_i^2\\) to wariancja pozycji, a \\(\\sigma_X^2\\) wariancja sumy skali. \\(\\alpha \\ge 0.70\\) często uznawane jest za akceptowalne (zależnie od celu).\nomega McDonalda \\[\n\\omega \\;=\\; \\frac{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2}{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2 + \\sum_{i=1}^{k}\\psi_{ii}},\n\\] gdzie \\(\\lambda_i\\) to ładunki czynnika ogólnego, a \\(\\psi_{ii}\\) wariancje unikalne. Dla rozwiązań hierarchicznych używamy \\(\\omega_h\\) (udział czynnika ogólnego).\nmetoda split-half i korekta Spearmana–Browna dla dwóch równoległych połówek z korelacją r: _{} ;=; .\n\n\nTrafność\n\nW CFA/SEM oceniamy trafność zbieżną i rozbieżną:\n\n\ncomposite reliability (CR) \\[\n\\mathrm{CR} = \\frac{\\left(\\sum \\lambda_i\\right)^2}{\\left(\\sum \\lambda_i\\right)^2 + \\sum \\theta_i},\n\\] gdzie \\(\\theta_i\\) to wariancje błędu; wartości \\(\\ge 0.70\\) pożądane;\n\naverage variance extracted (AVE) \\[\n\\mathrm{AVE} = \\frac{\\sum \\lambda_i^2}{\\sum \\lambda_i^2 + \\sum \\theta_i},\n\\] \\(\\mathrm{AVE} \\ge 0.50\\) sugeruje trafność zbieżną;\nkryterium Fornella–Larckera - \\(\\sqrt{\\mathrm{AVE}}\\) czynnika powinna przekraczać jego korelacje z innymi czynnikami;\nHTMT (heterotrait–monotrait ratio) - \\(\\mathrm{HTMT} \\;=\\; \\frac{\\text{średnia korelacja między pozycjami z różnych konstruktów}}{\\text{średnia korelacja między pozycjami w obrębie konstruktów}},\\) wartości \\(&lt; 0.85{-}0.90\\) wskazują rozróżnialność konstruktów.\n\n\nSkalowanie, punktacja i normy\n\nDecydujemy o sposobie punktowania: suma/średnia pozycji (po ewentualnym odwróceniu kodowania) czy punktacja czynnikowa (regresyjna/ Bartlett’a3). Ustalamy również normy na podstawie np. siatki stenowej (niski, przeciętny i wysoki poziom skali).\n3 Bartlett scores są nieobciążonymi estymatorami czynników latentnych, ale mogą być mniej stabilne w małych próbach i przy słabych ładunkach.\nPrzykład 4.4 Dla ilustracji adaptacji narzędzia pomiarowego, wykorzystamy oszacowaną już strukturę czynnikową z Przykład 4.1. Mieliśmy tam 13 pozycji opisujących trzy strategie: zapamiętywania, opracowywania i kontroli dla osób z Wielkiej Brytanii. Załóżmy, że tą samą strukturę chcemy przenieść na rynek Hiszpański. W tym celu sprawdzimy dopasowanie modelu konfirmacyjnego na danych hiszpańskich.\n\nKodpisaspa &lt;- PISA09[PISA09$cnt == \"ESP\", c(alitems, \"sex\")]\npisaspa &lt;- pisaspa[complete.cases(pisaspa[, c(mitems, \n  eitems, citems)]), ]\n\n# Definicja modelu CFA\nmodel_cfa &lt;- '\n  # Definicja czynników\n  Zapamiętywanie =~ st27q01 + st27q03 + st27q05 + st27q07\n  Opracowywanie =~ st27q04 + st27q08 + st27q10 + st27q12\n  Kontrola =~ st27q02 + st27q06 + st27q09 + st27q11 + st27q13\n'\n\n# Estymacja modelu CFA\nfit_cfa_spa &lt;- cfa(model_cfa, data = pisaspa, auto.var = TRUE, auto.cov.lv.x = TRUE, std.lv = TRUE)\n\n\n\nKodcompare_performance(fit_cfa, fit_cfa_spa, metrics = c(\"p_Chi2\", \"GFI\", \"AGFI\", \"NFI\", \"NNFI\", \"CFI\", \"RMSEA\", \"RMR\", \"SRMR\", \"RFI\")) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nName\nModel\np_Chi2\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMR\nSRMR\nRFI\n\n\n\nfit_cfa\nlavaan\n0.000\n0.936\n0.907\n0.881\n0.856\n0.885\n0.081\n0.042\n0.057\n0.850\n\n\nfit_cfa_spa\nlavaan\n0.000\n0.940\n0.912\n0.882\n0.854\n0.884\n0.079\n0.053\n0.059\n0.851\n\n\n\n\n\n\nOcena jakości dopasowania modelu pokazuje, że przyjęta struktura dla Wielkiej Brytanii sprawdza się również dla Hiszpanii. To dopiero pierwszy (oczywiście pominąwszy wszystkie wcześniejsze kroki jak tłumaczenie, czy analiza eksploracyjna) krok do adaptacji narzędzia do nowych warunków.\nTeraz ocenimy rzetelność skali.\n\nKodtab_itemscale(df = pisaspa, factor.groups = c(rep(\"Zapamiętywanie\", 4), rep(\"Opracowanie\", 4), rep(\"Kontrola\", 5)), factor.groups.titles = c(\"Kontrola\", \"Opracowanie\", \"Zapamiętywanie\")) \n\n\nKontrola\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nst27q02\n0.00 %\n2.93\n0.91\n-0.44\n0.73\n0.46\n0.72\n\n\n\nst27q06\n0.00 %\n3.07\n0.92\n-0.6\n0.77\n0.57\n0.68\n\n\n\nst27q09\n0.00 %\n2.71\n0.89\n-0.14\n0.68\n0.56\n0.68\n\n\n\nst27q11\n0.00 %\n3.17\n0.87\n-0.78\n0.79\n0.54\n0.69\n\n\n\nst27q13\n0.00 %\n2.45\n1.02\n0.12\n0.61\n0.43\n0.73\n\n\n\nMean inter-item-correlation=0.371 · Cronbach's α=0.744\n\n\n\n \n\nOpracowanie\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nst27q04\n0.00 %\n2.49\n1.01\n0.05\n0.62\n0.47\n0.72\n\n\n\nst27q08\n0.00 %\n2.01\n0.94\n0.62\n0.50\n0.53\n0.68\n\n\n\nst27q10\n0.00 %\n2.33\n0.95\n0.21\n0.58\n0.56\n0.66\n\n\n\nst27q12\n0.00 %\n2.17\n0.93\n0.39\n0.54\n0.57\n0.66\n\n\n\nMean inter-item-correlation=0.416 · Cronbach's α=0.738\n\n\n\n \n\nZapamiętywanie\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nst27q01\n0.00 %\n2.6\n0.98\n0.02\n0.65\n0.54\n0.64\n\n\n\nst27q03\n0.00 %\n2.85\n0.95\n-0.33\n0.71\n0.52\n0.65\n\n\n\nst27q05\n0.00 %\n2.35\n1.03\n0.21\n0.59\n0.54\n0.64\n\n\n\nst27q07\n0.00 %\n2.92\n0.96\n-0.43\n0.73\n0.44\n0.70\n\n\n\nMean inter-item-correlation=0.390 · Cronbach's α=0.719\n\n\n\n \n\n\n\n\n\n\n\n\n\n \nComponent 1\nComponent 2\nComponent 3\n\n\nComponent 1\nα=0.744\n \n \n\n\nComponent 2\n0.521(&lt;.001)\n\nα=0.738\n \n\n\nComponent 3\n0.376(&lt;.001)\n\n0.222(&lt;.001)\n\nα=0.719\n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nSpójność wewnętrzna poszczególnych komponentów\nKażda z trzech skal (komponentów) osiąga akceptowalny poziom rzetelności wewnętrznej. Wartości współczynnika Cronbacha \\(\\alpha\\) wynoszą odpowiednio 0.738 dla strategii opracowywania, 0.744 dla kontroli oraz 0.719 dla zapamiętywania. Są to wartości przekraczające próg 0.70, co w badaniach psychometrycznych uznaje się za wystarczające dla narzędzi we wczesnym etapie walidacji. Wskazuje to, że pozycje w każdej ze skal mierzą spójny konstrukt.\nWartości korelacji między pozycjami (mean inter-item correlation) mieszczą się w zakresie 0.37–0.42, co uznaje się za optymalne (wartości zbyt niskie &lt;0.20 sugerują brak spójności, natomiast zbyt wysokie &gt;0.70 nadmiarowość). Oznacza to, że pozycje są ze sobą skorelowane w stopniu umiarkowanym, zachowując jednocześnie różnorodność treściową.\nAnaliza jakości pozycji\nWszystkie pozycje mają trudność (item difficulty) w przedziale 0.50–0.79, co oznacza, że średnie odpowiedzi respondentów oscylują wokół środka skali, bez efektów podłogowych czy sufitowych. Pozycje mają także umiarkowane lub wysokie wartości dyskryminacji (item discrimination w przedziale 0.43–0.57), wskazujące, że dobrze różnicują osoby z wyższymi i niższymi wynikami ogólnymi. Żadna z pozycji nie obniża znacząco rzetelności całej skali (wszystkie wartości „α if deleted” pozostają na poziomie podobnym lub niższym niż pełne α).\nZwiązki między komponentami\nKorelacje pomiędzy skalami są wszystkie istotne statystycznie, ale mają zróżnicowaną siłę. Najsilniejszy związek występuje pomiędzy Kontrolą a Opracowaniem (r = 0.521, p &lt; .001). Można to interpretować tak, że osoby, które dbają o planowanie i monitorowanie swojego uczenia się, częściej stosują także strategie głębszego opracowywania materiału. Skala Zapamiętywanie koreluje umiarkowanie z Opracowaniem (r = 0.376, p &lt; .001), co jest zgodne z intuicją: aby skutecznie zapamiętać, często trzeba wcześniej przetworzyć materiał. Najsłabszy, choć istotny związek obserwujemy między Kontrolą a Zapamiętywaniem (r = 0.222, p &lt; .001), co sugeruje, że te dwie strategie są bardziej odrębne, a ich powiązanie jest ograniczone.\nPodsumowanie\nOtrzymane wyniki sugerują, że narzędzie jest psychometrycznie poprawne: ma akceptowalną spójność wewnętrzną, zrównoważony poziom trudności pozycji, dobre wskaźniki dyskryminacji oraz pozwala rozróżniać trzy powiązane, ale odrębne strategie uczenia się. Komponent 2 wydaje się pełnić rolę centralną, ponieważ jest najwyraźniej powiązany zarówno z komponentem 1, jak i 3. To może sugerować jego bardziej ogólny charakter lub funkcję „pomostu” między dwoma innymi wymiarami.\nPo wykonaniu analizy CFA i ocenie rzetelności kolejnym etapem adaptacji narzędzia jest przeprowadzenie rozszerzonych analiz stabilności, takich jak test–retest czy współczynnik ICC, które pozwalają ocenić powtarzalność wyników (tych nie wykonamy, ponieważ do tego potrzeba przeprowadzenia ankiety w dwóch momentach czasowych). Należy także zweryfikować trafność – konwergencyjną, dyskryminacyjną i kryterialną – aby upewnić się, że narzędzie mierzy to, co zakładano teoretycznie.\n\nKodlibrary(semTools)\nAVE(fit_cfa_spa) \n\nZapamiętywanie  Opracowywanie       Kontrola \n         0.395          0.418          0.373 \n\nKodcompRelSEM(fit_cfa_spa)\n\nZapamiętywanie  Opracowywanie       Kontrola \n         0.722          0.744          0.752 \n\nKodhtmt(model_cfa, data = pisaspa)\n\n               Zpmęty Oprcwy Kontrl\nZapamiętywanie  1.000              \nOpracowywanie   0.288  1.000       \nKontrola        0.482  0.680  1.000\n\n\nWyniki można interpretować na trzech poziomach: rzetelności wewnętrznej (CR), trafności konwergencyjnej (AVE) oraz trafności dyskryminacyjnej (HTMT).\n\nWspółczynniki Composite Reliability (CR) mieszczą się w przedziale od 0.72 do 0.75. Są to wartości powyżej progu 0.70, co sugeruje akceptowalną spójność wewnętrzną każdej ze skal. Oznacza to, że wskaźniki w ramach czynnika zapamiętywania, opracowywania i kontroli dostarczają relatywnie stabilnej informacji o zmiennej latentnej.\nŚrednia wyjaśniona wariancja (AVE) dla wszystkich trzech czynników jest niska: 0.395, 0.418 i 0.373. Kryterium akceptowalne to zwykle AVE ≥ 0.50, co oznacza, że czynnik powinien wyjaśniać przynajmniej połowę wariancji swoich wskaźników. Tutaj wartości poniżej 0.5 wskazują, że wyjaśniona część wariancji jest mniejsza niż ta przypisana błędowi. Może to oznaczać, że wskaźniki są dość zróżnicowane, a ich wspólna treść (latentna) nie jest wystarczająco silnie uchwycona. W praktyce oznacza to ograniczoną trafność konwergencyjną – czyli wskaźniki nie „zbiegają się” wystarczająco na wspólny konstrukt.\nMacierz HTMT wskazuje na poziom rozróżnialności czynników (trafności dyskryminacyjnej). Przyjmuje się, że wartości HTMT &lt; 0.85 (lub bardziej liberalnie &lt; 0.90) oznaczają satysfakcjonującą rozróżnialność. W tym przypadku: • Zapamiętywanie–Opracowywanie = 0.288 – bardzo niski współczynnik, dobra rozróżnialność, • Zapamiętywanie–Kontrolne = 0.482 – umiarkowany, nadal bezpieczny, • Opracowywanie–Kontrolne = 0.680 – wyższy, ale poniżej progu 0.85, więc rozróżnialność jest zachowana.\n\nPodsumowując, model charakteryzuje się akceptowalną rzetelnością i zadowalającą trafnością dyskryminacyjną, ale ograniczoną trafnością konwergencyjną. W praktyce oznacza to, że choć skale mierzą różne konstrukty i są spójne wewnętrznie, to konstrukty te nie są jeszcze w pełni „czysto” uchwycone przez zestaw wskaźników – być może potrzebna byłaby rewizja niektórych pozycji, ich dodanie lub modyfikacja.\nIstotnym krokiem jest również badanie równoważności pomiaru (measurement invariance) przy użyciu CFA wielogrupowej, co umożliwia porównywanie wyników między grupami, np. ze względu na płeć czy wiek. My wykonamy analizę w podziale ze względu na płeć, aby dowiedzieć się czy narzędzie mierzy badane konstrukty podobnie w obu grupach.\n\nKod# równoważność konfiguracyjna \nfit_config &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", std.lv = TRUE)\n\n# równoważność metryczna (równe ładunki czynnikowe)\nfit_metric &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = \"loadings\", std.lv = TRUE)\n\n# równoważność skalowa (równe ładunki i przecięcia)\nfit_scalar &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = c(\"loadings\", \"intercepts\"), std.lv = TRUE)\n\n# równoważność ścisła (ładunki, przecięcia i błędy pomiarowe)\nfit_strict &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = c(\"loadings\", \"intercepts\", \"residuals\"), std.lv = TRUE)\n\n# Porównania dopasowania\nanova(fit_config, fit_metric, fit_scalar, fit_strict)\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nfit_config 124 246657 247240 3035.8                                           \nfit_metric 134 246700 247214 3099.0     63.216 0.037427      10  8.880e-10 ***\nfit_scalar 144 246922 247366 3340.7    241.727 0.078100      10  &lt; 2.2e-16 ***\nfit_strict 157 246967 247321 3411.6     70.901 0.034240      13  5.477e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nKodfitMeasures(fit_config, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.886 0.079 0.054 \n\nKodfitMeasures(fit_metric, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.884 0.076 0.056 \n\nKodfitMeasures(fit_scalar, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.875 0.076 0.058 \n\nKodfitMeasures(fit_strict, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.873 0.074 0.059 \n\n\nWyniki analizy grupowej CFA wskazują, że proces sprawdzania równoważności pomiaru ze względu na płeć w tym narzędziu napotyka istotne ograniczenia.\nModel konfiguracyjny (czyli sam układ czynników i wskaźników, bez dodatkowych ograniczeń) osiąga umiarkowane dopasowanie: CFI = 0.886, RMSEA = 0.079, SRMR = 0.054. Oznacza to, że struktura czynnikowa jest w obu grupach podobna, choć model nie jest idealnie dopasowany.\nDodanie ograniczenia równości ładunków czynnikowych (równoważność metryczna) nie powoduje znaczącego pogorszenia dopasowania – CFI spada minimalnie (0.886 → 0.884), RMSEA poprawia się nieznacznie (0.079 → 0.076), a SRMR pozostaje na porównywalnym poziomie. Pomimo istotnego testu różnicy chi-kwadrat (Δχ² = 63.2, p &lt; 0.001), kryteria praktyczne (ΔCFI &lt; 0.01, ΔRMSEA &lt; 0.015) sugerują, że równoważność metryczna jest akceptowalna. Można więc uznać, że wskaźniki w równym stopniu odzwierciedlają konstrukty w obu grupach.\nPrzejście do równoważność skalowej (dodanie równości przecięć) wyraźnie pogarsza dopasowanie – spadek CFI do 0.875 oraz brak poprawy w RMSEA i SRMR, przy bardzo dużym i istotnym przyroście χ² (Δχ² = 241.7, p &lt; 0.001). To oznacza, że przecięcia nie są równoważne między płciami, czyli nie można bezpośrednio porównywać średnich czynników latentnych.\nW modelu ścisłym (zrównanie dodatkowo wariancji błędów) dopasowanie pozostaje na podobnym poziomie (CFI = 0.873, RMSEA = 0.074, SRMR = 0.059), a test chi-kwadrat znów wskazuje istotne pogorszenie.\nPodsumowując, udało się uzyskać równoważność konfiguracyjną i metryczną, ale nie skalową. Oznacza to, że konstrukty są podobnie reprezentowane w obu grupach, lecz nie można ich średnich bezpośrednio porównywać, bo przecięcia różnią się w zależności od płci. W praktyce, w takim przypadku można rozważyć testowanie częściowej równoważność (partial invariance), czyli identyfikowanie tych pozycji, które rzeczywiście spełniają warunek równości przecięć i opieranie porównań tylko na nich.\nNa koniec utworzymy skale, które można łatwo intepretować przez osoby, które nie znają narzędzia i jego konstrukcji dobrze.\n\nKod# --- KLUCZE SKAL (dokładnie jak w modelu) ---\nzap_items &lt;- c(\"st27q01\",\"st27q03\",\"st27q05\",\"st27q07\")                 # Zapamiętywanie\nopr_items &lt;- c(\"st27q04\",\"st27q08\",\"st27q10\",\"st27q12\")                 # Opracowywanie\nkon_items &lt;- c(\"st27q02\",\"st27q06\",\"st27q09\",\"st27q11\",\"st27q13\")       # Kontrola\n\n# Uwaga: jeśli jakieś pozycje wymagają odwrócenia, zastosować recoding przed agregacją.\n\nscores_simple &lt;- pisaspa %&gt;%\n  mutate(\n    Zapam_mean = rowMeans(across(all_of(zap_items)), na.rm = TRUE),\n    Oprac_mean = rowMeans(across(all_of(opr_items)), na.rm = TRUE),\n    Kontr_mean = rowMeans(across(all_of(kon_items)), na.rm = TRUE),\n    Zapam_sum  = rowSums(across(all_of(zap_items)), na.rm = TRUE),\n    Oprac_sum  = rowSums(across(all_of(opr_items)), na.rm = TRUE),\n    Kontr_sum  = rowSums(across(all_of(kon_items)), na.rm = TRUE)\n  )\n\n# Wyniki czynnikowe – metoda Bartletta (bardziej \"czyste\" wobec błędów specyficznych)\nfs_bartlett &lt;- lavPredict(fit_cfa_spa, method = \"Bartlett\")\n\n# Złożyć do wspólnej ramki (zachowując ewentualne zmienne grupujące)\nscores_latent &lt;- cbind(\n  pisaspa %&gt;% select(any_of(c(\"sex\",\"age\"))),\n  as.data.frame(fs_bartlett)\n)\n\n\n\nKodto_sten &lt;- function(z){\n  # transformacja przybliżona; wyniki przycięte do 1..10\n  s &lt;- round(2*z + 5.5)\n  pmin(10, pmax(1, s))\n}\n\n# Percentyle z ECDF\nperc_ecdf &lt;- function(x) round(ecdf(x)(x)*100, 1)\n\n# --- NORMY GLOBALNE dla wyników latentnych Bartletta ---\nlatent_names &lt;- c(\"Zapamiętywanie\",\"Opracowywanie\",\"Kontrola\")\n\nnorms_global &lt;- scores_latent %&gt;%\n  mutate(\n    across(all_of(latent_names), scale, .names = \"{.col}_z\") %&gt;% as.data.frame()\n  )\n\n# Dla wygody wylicz stens, percentyle dla każdej skali latentnej\nfor(lat in latent_names){\n  zcol &lt;- paste0(lat, \"_z\")\n  norms_global[[paste0(lat,\"_sten\")]]    &lt;- to_sten(norms_global[[zcol]])\n  norms_global[[paste0(lat,\"_pct\")]]     &lt;- perc_ecdf(scores_latent[[lat]])\n}\n\n# Podgląd wybranych kolumn\nhead(norms_global %&gt;% select(any_of(c(\"sex\",\"age\")),\n                             ends_with(\"_z\"),\n                             ends_with(\"_sten\"),\n                             ends_with(\"_pct\")), n = 20) %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 2) %&gt;% \n  tab_options(\n    table.font.size = px(10), \n  )\n\n\n\n\n\nsex\nZapamiętywanie_z\nOpracowywanie_z\nKontrola_z\nZapamiętywanie_sten\nOpracowywanie_sten\nKontrola_sten\nZapamiętywanie_pct\nOpracowywanie_pct\nKontrola_pct\n\n\n\nf\n0.86\n−0.65\n0.20\n7.00\n4.00\n6.00\n78.60\n27.10\n55.00\n\n\nm\n−0.51\n−0.38\n−0.69\n4.00\n5.00\n4.00\n31.90\n34.80\n22.90\n\n\nm\n0.24\n0.74\n0.26\n6.00\n7.00\n6.00\n59.00\n77.50\n57.70\n\n\nm\n−1.23\n2.44\n1.69\n3.00\n10.00\n9.00\n11.50\n98.30\n96.50\n\n\nm\n−0.18\n0.35\n1.09\n5.00\n6.00\n8.00\n43.80\n65.40\n87.10\n\n\nm\n−0.25\n−0.34\n0.30\n5.00\n5.00\n6.00\n40.80\n39.10\n58.40\n\n\nf\n−0.70\n−1.04\n0.55\n4.00\n3.00\n7.00\n24.90\n17.40\n67.70\n\n\nf\n0.24\n1.07\n−0.93\n6.00\n8.00\n4.00\n58.90\n86.10\n17.30\n\n\nf\n0.31\n0.63\n1.69\n6.00\n7.00\n9.00\n61.60\n72.10\n97.40\n\n\nf\n0.70\n1.74\n0.16\n7.00\n9.00\n6.00\n73.90\n95.10\n54.30\n\n\nf\n−1.06\n−1.73\n−1.36\n3.00\n2.00\n3.00\n14.60\n0.80\n10.30\n\n\nm\n0.12\n2.16\n1.69\n6.00\n10.00\n9.00\n54.10\n98.20\n98.20\n\n\nf\n0.96\n0.99\n1.69\n7.00\n7.00\n9.00\n81.00\n81.80\n96.40\n\n\nm\n0.03\n1.32\n0.95\n6.00\n8.00\n7.00\n51.20\n88.70\n81.90\n\n\nm\n−0.25\n0.04\n0.11\n5.00\n6.00\n6.00\n40.80\n53.30\n50.10\n\n\nm\n−1.32\n−0.03\n−1.37\n3.00\n5.00\n3.00\n9.50\n51.60\n8.40\n\n\nm\n−1.74\n−0.72\n−0.88\n2.00\n4.00\n4.00\n4.40\n25.10\n18.30\n\n\nm\n−0.94\n−0.34\n−1.37\n4.00\n5.00\n3.00\n18.60\n39.10\n9.60\n\n\nf\n−1.79\n−0.76\n−0.24\n2.00\n4.00\n5.00\n4.20\n23.30\n37.30\n\n\nf\n−0.56\n0.46\n−0.09\n4.00\n6.00\n5.00\n29.00\n69.10\n43.70\n\n\n\n\n\nKod# --- NORMY WEDŁUG PŁCI (grupowe) ---\nnorms_by_sex &lt;- scores_latent %&gt;%\n  group_by(sex) %&gt;%\n  mutate(\n    # z-score wewnątrz płci\n    across(all_of(latent_names),\n           ~ as.numeric(scale(.x)), .names = \"{.col}_z_g\"),\n    # stens wewnątrz płci\n    across(ends_with(\"_z_g\"),\n           ~ to_sten(.x), .names = \"{.col}_sten\"),\n  ) %&gt;%\n  # percentyle z ECDF wewnątrz płci\n  mutate(\n    across(all_of(latent_names),\n           ~ perc_ecdf(.x), .names = \"{.col}_pct_g\")\n  ) %&gt;%\n  ungroup()\n\n# Podgląd\nhead(norms_by_sex %&gt;%\n       select(sex, starts_with(\"Zapamiętywanie_\"),\n                    starts_with(\"Opracowywanie_\"),\n                    starts_with(\"Kontrola_\")), n = 20)%&gt;% \n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 2) %&gt;% \n  tab_options(\n    table.font.size = px(10), \n  )\n\n\n\n\n\nsex\nZapamiętywanie_z_g\nZapamiętywanie_z_g_sten\nZapamiętywanie_pct_g\nOpracowywanie_z_g\nOpracowywanie_z_g_sten\nOpracowywanie_pct_g\nKontrola_z_g\nKontrola_z_g_sten\nKontrola_pct_g\n\n\n\nf\n0.82\n7.00\n77.40\n−0.61\n4.00\n29.10\n0.06\n6.00\n49.30\n\n\nm\n−0.42\n5.00\n35.10\n−0.42\n5.00\n32.20\n−0.53\n4.00\n27.50\n\n\nm\n0.31\n6.00\n61.70\n0.69\n7.00\n76.60\n0.38\n6.00\n63.20\n\n\nm\n−1.12\n3.00\n13.50\n2.39\n10.00\n97.90\n1.76\n9.00\n97.10\n\n\nm\n−0.10\n5.00\n46.20\n0.31\n6.00\n63.70\n1.19\n8.00\n89.40\n\n\nm\n−0.17\n5.00\n43.50\n−0.38\n5.00\n37.00\n0.43\n6.00\n63.80\n\n\nf\n−0.80\n4.00\n21.50\n−1.00\n3.00\n18.70\n0.43\n6.00\n63.20\n\n\nf\n0.18\n6.00\n56.00\n1.12\n8.00\n86.40\n−1.15\n3.00\n13.40\n\n\nf\n0.25\n6.00\n59.00\n0.67\n7.00\n73.50\n1.64\n9.00\n97.30\n\n\nf\n0.65\n7.00\n72.00\n1.79\n9.00\n95.40\n0.02\n6.00\n48.50\n\n\nf\n−1.18\n3.00\n12.30\n−1.70\n2.00\n1.10\n−1.61\n2.00\n7.00\n\n\nm\n0.19\n6.00\n56.80\n2.11\n10.00\n97.90\n1.76\n9.00\n98.50\n\n\nf\n0.92\n7.00\n79.90\n1.03\n8.00\n82.50\n1.64\n9.00\n95.80\n\n\nm\n0.10\n6.00\n53.60\n1.28\n8.00\n88.40\n1.05\n8.00\n85.30\n\n\nm\n−0.17\n5.00\n43.50\n0.00\n5.00\n51.60\n0.24\n6.00\n55.50\n\n\nm\n−1.21\n3.00\n11.30\n−0.07\n5.00\n49.80\n−1.19\n3.00\n10.70\n\n\nm\n−1.62\n2.00\n6.00\n−0.76\n4.00\n23.30\n−0.71\n4.00\n22.10\n\n\nm\n−0.84\n4.00\n21.30\n−0.38\n5.00\n37.00\n−1.19\n3.00\n12.60\n\n\nf\n−1.94\n2.00\n2.70\n−0.72\n4.00\n25.20\n−0.41\n5.00\n31.80\n\n\nf\n−0.66\n4.00\n25.80\n0.50\n7.00\n70.60\n−0.25\n5.00\n38.10\n\n\n\n\n\n\n\n\n\n\n\nBollen, Kenneth A. 1989. „Structural Equation Models with Observed Variables”. Structural Equations with Latent Variables, kwiecień, 80–150. https://doi.org/10.1002/9781118619179.ch4.\n\n\nHuang, Yafei, i Peter M. Bentler. 2015. „Behavior of Asymptotically Distribution Free Test Statistics in Covariance Versus Correlation Structure Analysis”. Structural Equation Modeling: A Multidisciplinary Journal 22 (4): 489–503. https://doi.org/10.1080/10705511.2014.954078.\n\n\nKILIÇ, Abdullah, İbrahim UYSAL, i Burcu ATAR. 2020. „Comparison of Confirmatory Factor Analysis Estimation Methods on Binary Data”. International Journal of Assessment Tools in Education 7 (3): 451–87. https://doi.org/10.21449/ijate.660353.\n\n\nKyriazos, Theodoros, i Mary Poga-Kyriazou. 2023. „Applied Psychometrics: Estimator Considerations in Commonly Encountered Conditions in CFA, SEM, and EFA Practice”. Psychology 14 (05): 799–828. https://doi.org/10.4236/psych.2023.145043.\n\n\nLatan, Hengky, i Richard Noonan, red. 2017. Partial Least Squares Path Modeling. Springer International Publishing. https://doi.org/10.1007/978-3-319-64069-3.\n\n\nLi, Cheng-Hsien. 2015. „Confirmatory Factor Analysis with Ordinal Data: Comparing Robust Maximum Likelihood and Diagonally Weighted Least Squares”. Behavior Research Methods 48 (3): 936–49. https://doi.org/10.3758/s13428-015-0619-7.\n\n\n———. 2021. „Statistical Estimation of Structural Equation Models with a Mixture of Continuous and Categorical Observed Variables”. Behavior Research Methods 53 (5): 2191–2213. https://doi.org/10.3758/s13428-021-01547-z.\n\n\nSchweizer, Karl, i Christine DiStefano, red. 2016. Principles and Methods of Test Construction. Hogrefe Publishing. https://doi.org/10.1027/00449-000.\n\n\nSpearman, C. 1961. „\"General Intelligence\" Objectively Determined and Measured.” W, 59–73. Appleton-Century-Crofts. https://doi.org/10.1037/11491-006.\n\n\n„Supplemental Material for The Performance of ML, DWLS, and ULS Estimation With Robust Corrections in Structural Equation Models With Ordinal Variables”. 2016. Psychological Methods. https://doi.org/10.1037/met0000093.supp.\n\n\nTarka, Piotr. 2017. „An Overview of Structural Equation Modeling: Its Beginnings, Historical Development, Usefulness and Controversies in the Social Sciences”. Quality & Quantity 52 (1): 313–54. https://doi.org/10.1007/s11135-017-0469-8.\n\n\nThurstone, L. L. 1931. „Multiple Factor Analysis.” Psychological Review 38 (5): 406–27. https://doi.org/10.1037/h0069792.\n\n\nTurney, A. H. 1939. „Factor Analysis Makes ProgressA Study in Factor Analysis: The Stability of a Bi-Factor Solution. Karl J. Holzinger , Frances Swineford”. The School Review 47 (9): 709–11. https://doi.org/10.1086/440440.\n\n\nWright, Sewall. 1934. „The Method of Path Coefficients”. The Annals of Mathematical Statistics 5 (3): 161–215. https://doi.org/10.1214/aoms/1177732676.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Metody redukcji wymiarowości",
    "section": "",
    "text": "PCA (Pearson 1901)\nHistoria metod redukcji wymiarowości jest ściśle związana z rozwojem statystyki, psychometrii, a następnie uczenia maszynowego i eksploracji danych. Już na początku XX wieku zaczęto poszukiwać narzędzi pozwalających na uproszczenie złożonych zbiorów danych, w których liczba zmiennych była zbyt duża, aby dało się je analizować bezpośrednio. Głównym celem było uchwycenie istotnych wzorców i zależności przy zachowaniu możliwie dużej ilości informacji.\nJednym z pierwszych i do dziś najczęściej stosowanych podejść jest analiza głównych składowych (Principal Component Analysis, PCA). Jej początki sięgają pracy Karla Pearsona z 1901 roku, który zaproponował metodę znajdowania „linii najlepszego dopasowania” w przestrzeni wielowymiarowej. Została ona następnie rozwinięta przez Harolda Hotellinga w latach 30. XX wieku, który sformalizował PCA jako metodę przekształcania skorelowanych zmiennych w nowy zbiór nieskorelowanych składowych, uporządkowanych według wariancji. PCA szybko znalazła zastosowanie w psychometrii i naukach społecznych, a następnie w genetyce, obrazowaniu i ekonomii.\nW latach powojennych, wraz z rozwojem psychologii eksperymentalnej i neuronauk, pojawiła się potrzeba metod lepiej uchwytujących niezależne źródła sygnału. Doprowadziło to do opracowania analizy niezależnych składowych (Independent Component Analysis, ICA). Choć koncepcje matematyczne stojące za ICA sięgają teorii informacji z połowy XX wieku, to metoda została sformalizowana dopiero w latach 80. i 90. XX wieku, m.in. dzięki pracom Jeana-Françoisa Cardoso czy Aapa Hyvärinena. ICA stała się niezwykle użyteczna w problemach takich jak separacja źródeł w sygnałach biomedycznych (np. EEG, fMRI), odszumianie danych czy analiza obrazów.\nRównolegle rozwijały się metody oparte na odległościach i podobieństwach, takie jak skalowanie wielowymiarowe (Multidimensional Scaling, MDS). Pierwsze idee pojawiły się w psychometrii w latach 50., a szczególnie w pracach Torgersona i Kruskala. Celem MDS było odwzorowanie obiektów opisanych macierzą podobieństw lub odległości w przestrzeni niskowymiarowej w taki sposób, aby zachować relacje strukturalne. Metoda ta znalazła szerokie zastosowanie w badaniach percepcji, marketingu, biologii oraz w analizie sieci społecznych.\nOd końca XX wieku rozwój metod redukcji wymiarowości przyspieszył, co było związane z eksplozją danych wysokowymiarowych w biologii molekularnej, informatyce czy analizie obrazów. Oprócz klasycznych metod liniowych zaczęto rozwijać techniki nieliniowe, takie jak t-SNE (2008, Laurens van der Maaten i Geoffrey Hinton) czy UMAP (2018, McInnes, Healy i Melville), które pozwalają zachować lokalne struktury danych w niskowymiarowej przestrzeni wizualizacji. Metody te zrewolucjonizowały analizę danych w uczeniu maszynowym i biologii obliczeniowej, np. w analizie danych pojedynczych komórek.\nDziś redukcja wymiarowości jest nie tylko techniką wspomagającą wizualizację danych, lecz także kluczowym elementem przetwarzania wstępnego w wielu modelach uczenia maszynowego. Od klasycznych metod PCA i MDS po nowoczesne techniki oparte na sieciach neuronowych, takie jak autoenkodery, rozwój tego obszaru odzwierciedla rosnącą potrzebę uproszczenia i interpretacji złożoności współczesnych danych.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#pca",
    "href": "pca.html#pca",
    "title": "Metody redukcji wymiarowości",
    "section": "",
    "text": "Matematyczna definicja modelu\nPunktem wyjścia analizy głównych składowych jest problem odwzorowania wielowymiarowego zbioru danych w przestrzeni o mniejszej liczbie wymiarów przy możliwie minimalnej stracie informacji. W praktyce dąży się do kompresji i odszumiania sygnału, usuwania współliniowości, stabilizacji dalszych modeli (np. regresji), a także do wizualizacji struktur klasowych i gradientów zmienności. Przykładowo, dla dwóch silnie skorelowanych cech pierwsza składowa główna jest skierowana wzdłuż linii największego rozrzutu (blisko prostej \\(y \\approx x\\)), a redukcja do jednego wymiaru zachowuje większą część wariancji niż dowolna inna projekcja.\nMatematyczna definicja poprzez maksymalizację wariancji i dekompozycję spektralną polega na transformacji scentralizowanej macierzy danych \\(X \\in \\mathbb{R}^{n\\times p}\\) (każdą kolumnę odjąć o jej średnią). Niech \\(\\Sigma=\\frac{1}{n-1}X^\\top X\\) oznacza empiryczną macierz kowariancji. Pierwszą składową wyznaczamy jako kierunek \\(w\\in\\mathbb{R}^{p}\\) rozwiązujący zadanie maksymalizacji wariancji projekcji, czyli maksymalizacji \\(\\mathrm{Var}(Xw)=w^\\top\\Sigma w\\) przy ograniczeniu \\(\\|w\\|_{2}=1\\). Zastosowanie mnożników Lagrange’a prowadzi do warunku stacjonarności \\(\\Sigma w=\\lambda w\\), a więc \\(w\\) jest wektorem własnym \\(\\Sigma\\), zaś \\(\\lambda\\) jest odpowiadającą mu wartością własną. Wybieramy największą wartość własną \\(\\lambda_{1}\\) i jej wektor \\(w_{1}\\), wówczas wariancja pierwszych wyników projekcji \\(z_{1}=Xw_{1}\\) równa się \\(\\lambda_{1}\\). Kolejne składowe otrzymujemy analogicznie jako rozwiązania tego samego problemu z dodatkowymi ograniczeniami ortogonalności \\(w_{j}^\\top w_{k}=0\\) dla \\(k&lt;j\\), co ustawia kolejne wektory własne \\(\\Sigma\\) w porządku malejących wartości własnych \\(\\lambda_{1}\\ge \\lambda_{2}\\ge \\dots \\ge \\lambda_{p}\\). Wektor wyników projekcji \\(z_{j}\\) nazywamy w praktyce scores, a \\(w_{j}\\) — wektorem ładunków (loadings). Kumulatywny udział wariancji wyjaśnianej przez pierwsze \\(k\\) składowych wynosi wówczas \\(\\sum_{j=1}^{k}\\lambda_{j}\\big/\\sum_{j=1}^{p}\\lambda_{j}\\) i służy na często do doboru \\(k\\).\nRównoważne wyprowadzenie modelu przez rozkład na wartości osobliwe, czyli SVD (ang. Singular Value Decomposition), opiera się na faktoryzacji \\(X=UDV^\\top\\), gdzie \\(U\\in\\mathbb{R}^{n\\times r}\\) i \\(V\\in\\mathbb{R}^{p\\times r}\\) mają ortonormalne kolumny, \\(D=\\mathrm{diag}(d_{1},\\dots,d_{r})\\) zawiera uporządkowane wartości osobliwe \\(d_{1}\\ge \\dots \\ge d_{r}&gt;0\\), a \\(r=\\mathrm{rank}(X)\\). Wówczas kolumny \\(V\\) pokrywają się (co do znaku) z wektorami ładunków \\(w_{j}\\), zaś macierz wyników projekcji \\(T=XV\\) równa się \\(UD\\). Związek między oboma podejściami jest ścisły: \\(\\lambda_{j}=d_{j}^{2}/(n-1)\\), a więc wariancje składowych odwzorowuje się przez kwadraty wartości osobliwych przeskalowane czynnikiem \\(1/(n-1)\\). Projekcja do \\(k\\) wymiarów przyjmuje wówczas postać \\(X\\mapsto T_{k}=UD_{k}\\), a rekonstrukcja rzędu \\(k\\) ma postać \\[\nX_{k}=T_{k}V_{k}^\\top=U_{k}D_{k}V_{k}^\\top.\n\\] Z twierdzenia Eckarta–Younga–Mirsky’ego wynika, że \\(X_{k}\\) minimalizuje błąd Frobeniusa \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\) o rządzie co najwyżej \\(k\\), czyli PCA daje najlepszą aproksymację niskorangową w sensie średniokwadratowym (jest to tzw. obcięte SVD). Ta równoważność łączyć dwie intuicje: maksymalizacja przechwyconej wariancji i minimalizacja błędu rekonstrukcji.\n\nTwierdzenie 5.1 (Twierdzenie Eckarta–Younga–Mirsky) Niech \\(X\\in\\mathbb{R}^{n\\times p}\\) i niech \\(r=\\mathrm{rank}(X)\\). Dla \\(k&lt;r\\) niech \\(X_{k}=U_{k}D_{k}V_{k}^\\top\\) będzie obciętym rozkładem SVD rzędu \\(k\\). Wówczas \\(X_{k}\\) jest jedyną macierzą o \\(\\mathrm{rank}(X_{k})=k\\), która minimalizuje błąd Frobeniusa1 \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\in\\mathbb{R}^{n\\times p}\\) o \\(\\mathrm{rank}(Y)\\le k\\). Ponadto zachodzi równość \\(\\|X-X_{k}\\|_{F}^{2}=\\sum_{j=k+1}^{r}d_{j}^{2}\\).\n1 Błąd Frobeniusa \\(\\|A\\|_{F}\\) macierzy \\(A\\) definiujemy jako \\(\\|A\\|_{F}=\\sqrt{\\sum_{i,j}a_{ij}^{2}}=\\sqrt{\\mathrm{tr}(A^\\top A)}.\\)\nZadania optymalizacyjne wyraża się zarówno w wersji wektorowej, jak i macierzowej. Dla pierwszej składowej rozwiązujemy problem maksymalizacji \\(w^\\top\\Sigma\\) w przy \\(\\|w\\|_{2}=1\\), co prowadzi do największej wartości własnej. Dla \\(k\\) składowych poszukujemy macierzy \\(W\\in\\mathbb{R}^{p\\times k}\\) o kolumnach ortonormalnych, która maksymalizuje \\(\\mathrm{tr}(W^\\top\\Sigma W)\\), skąd wynika wybór \\(k\\) wektorów własnych \\(\\Sigma\\). Równoważnie, szukamy projekcji \\(P=WW^\\top\\) minimalizującej błąd rekonstrukcji \\(\\|X-XWW^\\top\\|_{F}^{2}\\). W notacji SVD rozwiązanie ma postać \\(W=V_{k}\\), a więc projekcja działa przez mnożenie przez \\(V_{k}V_{k}^\\top\\).\nGdy cechy mierzymy w różnych jednostkach i skalach, zaleca się stosować macierz korelacji zamiast kowariancji, co jest równoważne standaryzacji kolumn \\(X\\) do wariancji 1. Wiele implementacji (np. w R funkcja prcomp) wykorzystuje SVD na scentralizowanych i ewentualnie standaryzowanych danych, co zapewniać numeryczną stabilność, zwłaszcza gdy \\(p\\gg n\\). W sytuacji \\(p\\gg n\\) korzystniejsze bywa liczenie mniejszych rozkładów: albo dual PCA2 na macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\), albo bezpośrednio obciętego SVD. Dla danych zaburzonych wartościami odstającymi rozważa się wersje odporne, np. zastępuje się \\(\\Sigma\\) estymatorem odpornym (ang. Minimum Covariance Determinant, MCD)3 lub stosuje się robust PCA i dekompozycje oparte na normie jądra i normie \\(L_{1}\\)4.\n2 Wówczas wektory własne \\(\\tilde{w}_{j}\\) macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\) (która jest niższego wymiaru niż \\(X^\\top X\\) a co za tym idzie lepiej się zachowuje numerycznie) przekształca się w wektory własne \\(\\Sigma\\) przez \\(w_{j}=X^\\top \\tilde{w}_{j}/\\sqrt{(n-1)\\tilde{\\lambda}_{j}}\\), gdzie \\(\\tilde{\\lambda}_{j}\\) jest odpowiadającą wartością własną.3 Estymator MCD polega na znalezieniu podzbioru \\(h\\) obserwacji (zwykle \\(h \\approx 0.75 n\\)) o najmniejszym wyznaczniku macierzy kowariancji, a następnie obliczeniu średniej i kowariancji na tym podzbiorze. Jest odporny na wartości odstające, ponieważ ignoruje obserwacje, które znacznie zwiększają wyznacznik.4 Metoda ta zakłada, że macierz danych \\(X\\) ma postać \\(X=L+S+E\\), gdzie \\(L\\) jest macierzą niskorangową (sygnał), \\(S\\) jest macierzą rzadką (wartości odstające), a \\(E\\) jest szumem o małej wariancji. Celem jest odzyskanie \\(L\\) poprzez minimalizację funkcji celu \\(\\|L\\|_{*}+\\lambda\\|S\\|_{1}\\) przy ograniczeniu \\(X=L+S\\), gdzie \\(\\|L\\|_{*}\\) jest normą jądra (suma wartości osobliwych \\(L\\)), a \\(\\|S\\|_{1}\\) jest normą \\(L_{1}\\) macierzy \\(S\\).Podsumowując, PCA można sformułować trojako: jako maksymalizację wariancji projekcji przy ograniczeniach ortogonalności, jako dekompozycję spektralną macierzy kowariancji oraz jako obcięte SVD zapewniające najlepszą aproksymację niskorangową.\nZałożenia modelu\nZałożenia dotyczące danych wejściowych do analizy głównych składowych (PCA) są stosunkowo słabe, ale mają istotny wpływ na jakość wyników i interpretację. Można je podzielić na kilka grup:\n\nStruktura danych\n\nLiniowość – PCA zakłada, że główne wzorce zmienności w danych można uchwycić przez liniowe kombinacje zmiennych wejściowych. Jeśli zależności są silnie nieliniowe (np. dane leżą na zakrzywionej rozmaitości), PCA nie odwzoruje ich poprawnie – lepiej wtedy stosować kernel PCA albo metody sąsiedztwa (np. t-SNE, UMAP).\nWspółzależność zmiennych – metoda ma sens tylko wtedy, gdy między cechami istnieją korelacje. Jeśli wszystkie zmienne są niezależne, PCA nie zredukuje wymiarów i każda składowa odpowiadać będzie jednej zmiennej.\n\n\nJednostki i skale pomiarowe\n\nPCA jest wrażliwa na skalę zmiennych, ponieważ opiera się na wariancji.\nJeśli cechy mierzone są w różnych jednostkach (np. temperatura w °C i masa w kg), należy je standaryzować (np. do średniej 0 i wariancji 1).\nGdy wszystkie cechy są w tej samej skali, można pracować na macierzy kowariancji; w przeciwnym razie lepiej korzystać z macierzy korelacji.\n\n\nRozkład danych\n\nNormalność wielowymiarowa nie jest wymagana, ale jeżeli dane mają rozkład wielowymiarowo normalny, to składowe główne są niezależne (nie tylko nieskorelowane), co upraszcza interpretację. Naruszenie założenia o normalności nie sprawia, że PCA nie działa, lecz niezależność składowych nie jest zagwarantowana.\nBrak wartości odstających – PCA jest bardzo wrażliwa na outliery, które mogą wpłynąć na kierunki głównych składowych, bo opiera się na kowariancji. Dlatego dane powinny być oczyszczone lub należy stosować wersje metody odporne (robust PCA).\n\n\nLiczebność próby\n\nAby oszacować macierz kowariancji, liczba obserwacji \\(n\\) powinna być odpowiednio duża względem liczby zmiennych \\(p\\).\nGdy \\(p \\gg n\\), klasyczna PCA bywa niestabilna i stosuje się wtedy dual PCA albo obcięte SVD.\n\n\nBraki danych - PCA wymaga pełnej macierzy danych (bez braków). W przypadku braków stosuje się najczęściej imputację (np. metodą średnich czy metody oparte na modelach).\nInterpretacja graficzna i praktyczna\n\nKod# Pakiety\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(scales)\n\nset.seed(44)\n\n# 1) Dane 2D o eliptycznym rozkładzie (silna współzmienność)\nn  &lt;- 300\nmu &lt;- c(0, 0)\nsd1 &lt;- 2\nsd2 &lt;- 1\nrho &lt;- 0.8\nSigma &lt;- matrix(c(sd1^2, rho*sd1*sd2,\n                  rho*sd1*sd2, sd2^2), nrow = 2, byrow = TRUE)\n\nX &lt;- MASS::mvrnorm(n, mu = mu, Sigma = Sigma) %&gt;%\n  as_tibble(.name_repair = ~c(\"x1\",\"x2\"))\n\n# 2) PCA na danych scentralizowanych (bez standaryzacji)\npca &lt;- prcomp(X, center = TRUE, scale. = FALSE)\n\n# Wartości własne i wektory (ładunki)\nlambda &lt;- pca$sdev^2\nV &lt;- pca$rotation    # kolumny: PC1, PC2\ncenter &lt;- colMeans(X)\n\n# 3) Punkty końcowe wektorów PC1 i PC2 (skalować długością ~ odchylenie wzdłuż składowej)\n# Skala wektora: k * sd wzdłuż danej składowej (tu k = 2 dla czytelności)\nk &lt;- 2\npc1_end &lt;- center + k * pca$sdev[1] * V[,1]\npc2_end &lt;- center + k * pca$sdev[2] * V[,2]\n\n# 4) Ramy wykresu i linie osi oryginalnego układu\nxr &lt;- range(X$x1); yr &lt;- range(X$x2)\n\n# 5) Dane pomocnicze do geometrii\narrows_df &lt;- tribble(\n  ~x,          ~y,          ~xend,        ~yend,     ~label,\n  center[1],   center[2],   pc1_end[1],   pc1_end[2], \"PC1\",\n  center[1],   center[2],   pc2_end[1],   pc2_end[2], \"PC2\"\n)\n\n# Opisy udziału wariancji\nexpl &lt;- percent(lambda / sum(lambda), accuracy = 0.1)\n\n# 6) Wykres\nggplot(X, aes(x = x1, y = x2)) +\n  # chmura punktów\n  geom_point(alpha = 0.5, size = 1.6) +\n  # elipsa rozrzutu (1 odchylenie standardowe ~ poziom 0.68)\n  stat_ellipse(type = \"norm\", level = 0.68, linewidth = 0.8) +\n  # oryginalne osie układu współrzędnych (przez (0,0))\n  geom_hline(yintercept = 0, linetype = 3, linewidth = 0.5) +\n  geom_vline(xintercept = 0, linetype = 3, linewidth = 0.5) +\n  # wektory składowych głównych (wychodzące ze środka danych)\n  geom_segment(data = arrows_df,\n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")),\n               linewidth = 1) +\n  # etykiety PC z udziałem wariancji\n  geom_text(data = arrows_df %&gt;%\n              mutate(txt = ifelse(label==\"PC1\",\n                                  paste0(\"PC1 (\", expl[1], \")\"),\n                                  paste0(\"PC2 (\", expl[2], \")\"))),\n            aes(x = xend, y = yend, label = txt),\n            nudge_x = 0.05, nudge_y = 0.05, hjust = 0, vjust = 0,\n            size = 3.5) +\n  # punkt środka\n  geom_point(aes(x = center[1], y = center[2]), color = \"black\", size = 2) +\n  coord_fixed() +\n  labs(x = \"x1 (oś oryginalna)\",\n       y = \"x2 (oś oryginalna)\",\n       title = \"Oryginalne osie, dane oraz dwie składowe główne (2D)\",\n       subtitle = paste0(\"Udział wariancji: PC1 = \", expl[1], \", PC2 = \", expl[2])) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"plain\"),\n        plot.subtitle = element_text(face = \"plain\"))\n\n\n\n\n\n\n\nDla dwóch wymiarów elipsa rozrzutu danych ma osie ustawione dokładnie wzdłuż \\(w_{1}\\) i \\(w_{2}\\), a ich długości proporcjonalne do \\(\\sqrt{\\lambda_{1}}\\) i \\(\\sqrt{\\lambda_{2}}\\). Transformacja do przestrzeni składowych odpowiada obrotowi układu współrzędnych tak, by oś \\(X_{1}'\\) leżała w kierunku największego rozrzutu, a \\(X_{2}'\\) — w kierunku pozostałej zmienności. Projekcja do \\(k&lt;p\\) wymiarów działa jak rzut ortogonalny na podprzestrzeń rozpiętą przez pierwsze \\(k\\) osi i „spłaszczenie” w pominiętych kierunkach, co minimalizuje błąd rekonstrukcji w sensie średniokwadratowym. Wykresy scores prezentują obiekty w przestrzeni składowych głównych i często ujawniają skupiska lub obserwacje odstające. Wektory loadings są przedstawiane na tzw. kole korelacji, gdzie końce strzałek leżą na okręgu jednostkowym, a ich długości i kąty odzwierciedlają korelacje zmiennych oryginalnych ze składowymi. Zmienne wskazujące podobne kierunki tworzą grupy, co pomaga rozumieć współzmienność. Wykres biplot łączy obie perspektywy: punkty obiektów i kierunki zmiennych w tej samej płaszczyźnie, dzięki czemu można podejrzeć jednocześnie relacje między obiektami i kontrybucje cech. Dodatkowo wykres udziału wariancji, czyli scree plot, porządkuje \\(\\lambda_{j}\\) i pomagać wyznaczyć \\(k\\) przez identyfikację „łokcia” krzywej lub przez osiągnięcie założonego poziomu wariancji kumulatywnej.\nŁadunek \\(w_{jk}\\) to współczynnik liniowej kombinacji \\(j\\)-tej składowej dla \\(k\\)-tej zmiennej; jego znak i wartość bezwzględna informują o kierunku i sile związku. Korelację zmiennej z \\(j\\)-tą składową szacujemy jako cosinus kąta między wektorem zmiennej a osią składowej na kole korelacji; duże wartości sugeruję dużą kontrybucję tej cechy do składowej. Rekonstrukcja obiektu \\(i\\)-tego z \\(k\\) składowych ma postać \\(\\hat{x}_{i}=\\sum_{j=1}^{k} t_{ij} \\, w_{j}^\\top\\), co pozwala na analizę błędów rekonstrukcji i odszumianie przez odcięcie składowych o małych \\(d_{j}\\). W regresji, gdy predyktory są współliniowe, stosuje się regresję na składowych głównych albo regresję grzbietową w przestrzeni scores, co poprawia stronę obliczeniową i zmniejszać wariancję estymatorów.\nKryteria doboru liczby składowych głównych\nDobór liczby składowych głównych (\\(k\\)) jest jednym z kluczowych etapów analizy PCA, ponieważ decyduje o tym, ile informacji (wariancji) zostanie zachowane przy redukcji wymiarowości. Zbyt mała liczba składowych prowadzi do utraty istotnych informacji, a zbyt duża – do utrzymania szumu i nadmiarowej redundancji. W praktyce stosuje się zestaw kryteriów ilościowych i jakościowych, które można podzielić na kilka grup.\nKryteria oparte na wariancji wyjaśnianej\nNajbardziej klasyczne podejście polega na analizie udziału wariancji przechwyconej przez pierwsze \\(k\\) składowych. Dla każdej składowej liczy się wartość własną \\(\\lambda_j\\) macierzy kowariancji, a udział wariancji wyjaśnianej przez pierwsze \\(k\\) składowych to \\[\n\\eta(k) = \\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n\\] Stosowane reguły:\n\nReguła progu wariancji - wybiera się najmniejsze \\(k\\), dla którego \\(\\eta(k)\\) przekracza ustalony próg, np. 80%, 90% lub 95% (w literaturze nie ma jednego progu). Gdy dane silnie skorelowane – wystarczą 2–3 składowe, a gdy dane są bardziej złożone – potrzeba więcej (5–10 i więcej).\nWykres osypiska (scree plot) – wykres wartości własnych \\(\\lambda_j\\) uporządkowanych malejąco. Wybiera się punkt, w którym tempo spadku gwałtownie maleje („łokieć krzywej”).\nWskaźnik udziału marginalnego - \\(\\Delta \\eta_j = \\eta(j) - \\eta(j-1)\\). Gdy przyrost staje się znikomy, dalsze składowe nie wnoszą istotnej informacji.\nKryteria algebraiczne\nKryterium wartości własnej (Kaisera–Guttmana) oparte jest na macierzy korelacji, które mówi, że zachowuje się tylko te składowe, których wartości własne \\(\\lambda_j &gt; 1\\). Oznacza to, że dana składowa wyjaśnia więcej wariancji niż pojedyncza standaryzowana zmienna. Reguła ta jest prosta, ale często zbyt konserwatywna (tendencja do wyboru zbyt wielu składowych).\nKryteria statystyczne i walidacyjne\n\nAnaliza równoległa (Parallel Analysis) - polega na porównaniu wartości własnych uzyskanych z danych rzeczywistych z wartościami własnymi uzyskanymi z wielu symulowanych zestawów danych o tych samych wymiarach, ale z losowym szumem. Zachowuje się tylko te składowe, których wartości własne przekraczają średnią (lub kwantyl) z rozkładu symulowanego. Ta metoda ogranicza ryzyko wyboru składowych wynikających z przypadku.\nWalidacja krzyżowa (Cross-Validation) - gdy PCA wykorzystuje się w kontekście modelowania predykcyjnego (np. PCA regression). Wybiera się takie \\(k\\), które minimalizuje błąd predykcji (np. RMSE) obliczany metodą walidacji krzyżowej.\n\nBartlett’s Test of Sphericity sprawdza, czy korelacje są wystarczająco silne, by PCA miała sens.\n\nBroken Stick Model porównuje udział wariancji każdej składowej z oczekiwaną wartością przy losowym rozkładzie wariancji – zachowuje się tylko te składowe, które przekraczają tę wartość (\\(E_k=\\frac{1}{p}\\sum_{j=k}^{p}\\frac{1}{j}\\)).\nKryteria interpretacyjne i dziedzinowe\nCzasami najważniejszy jest nie wynik numeryczny, lecz użyteczność interpretacyjna:\n\nWybiera się tyle składowych, ile da się sensownie zinterpretować (np. odpowiadających znanym procesom fizycznym, ekonomicznym, biologicznym).\nW analizie wizualnej (np. w eksploracji danych) często wybiera się 2 lub 3 pierwsze składowe, które umożliwiają wykresy 2D lub 3D.\n\n\n\n\n\n\n\n\n\nKryterium\nOpis\nZalety\nOgraniczenia\n\n\n\nUdział wariancji (np. ≥90%)\nZachowaj tyle składowych, by wyjaśnić określony procent całkowitej wariancji\nProste i intuicyjne\nWybór progu bywa arbitralny\n\n\nWykres osypiska (scree plot)\nWybór punktu „kolana” na krzywej wartości własnych\nWizualnie czytelne\nSubiektywne, zależy od interpretacji obserwatora\n\n\nWartość własna &gt; 1 (Kaiser–Guttman)\nZachowaj składowe, których wartości własne przekraczają 1 (dla macierzy korelacji)\nŁatwe obliczeniowo\nCzęsto zbyt liberalne – wybiera zbyt wiele składowych\n\n\nAnaliza równoległa (Parallel Analysis)\nPorównanie wartości własnych z rozkładem uzyskanym z danych losowych\nStatystycznie uzasadnione, ogranicza wybór przypadkowych komponentów\nWymaga symulacji lub dedykowanego oprogramowania\n\n\nWalidacja krzyżowa (Cross-Validation)\nWybór liczby składowych minimalizującej błąd predykcji (np. RMSE)\nNajlepsza w kontekście modeli predykcyjnych\nKosztowna obliczeniowo, wymaga podziału danych\n\n\nModel Broken Stick\nPorównanie udziału wariancji składowych z oczekiwanym rozkładem losowym\nUzasadnione teoretycznie, ogranicza przeuczenie\nMniej intuicyjne, rzadziej używane\n\n\nKryterium interpretacyjne\nWybór liczby składowych możliwych do sensownej interpretacji\nPraktyczne i kontekstowe\nSubiektywne i zależne od wiedzy dziedzinowej\n\n\n\n\nPrzykład 5.1 (PCA na danych irysów)  \n\nKodlibrary(factoextra)\nlibrary(easystats)\nlibrary(gt)\n\npca_iris &lt;- prcomp(iris[,-5], center = TRUE, scale. = TRUE) \n# albo\npca &lt;- principal_components(iris, n = 4, rotate = \"none\") # domyślnie standaryzuje zmienne\n\n# Wykres osypiska\nfviz_eig(pca_iris, addlabels = TRUE)\n\n\n\n\n\n\n\nJak widać z powyższego wykresu osypiska pierwsza składowa wyjaśnia około 73% całkowitej wariancji, a druga 23%. Jeśli chcieć opierać wybór liczby składowych głównych na kryteriach (również takich, które nie były prezentowane powyżej), to można użyć funkcji n_components() pakietu parameters w ekosystemie easystats.\n\nKodk &lt;- n_components(iris[,-5])\nas.data.frame(k)\n\n   n_Factors              Method       Family\n1          0          Scree (R2)     Scree_SE\n2          1             Bentler      Bentler\n3          1 Optimal coordinates        Scree\n4          1 Acceleration factor        Scree\n5          1   Parallel analysis        Scree\n6          1    Kaiser criterion        Scree\n7          1       Velicer's MAP Velicers_MAP\n8          2          Scree (SE)     Scree_SE\n9          2    VSS complexity 1          VSS\n10         2    VSS complexity 2          VSS\n11         3            Bartlett      Barlett\n12         3            Anderson      Barlett\n13         3              Lawley      Barlett\n\nKodplot(k)\n\n\n\n\n\n\n\nChoć większość kryteriów wskazuje na 1 składową, to na potrzeby przykładu wykorzystamy dwie składowe. Wyjaśniają one blisko 96% całkowitej wariancji (patrz poniżej). Możemy teraz przejrzeć wyniki PCA, czyli macierz ładunków (wektorów własnych) i macierz wyników projekcji (scores).\n\nKod# Udział wariancji\nsummary(pca_iris)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\nKod# Ładunki (wektory własne)\npca_iris$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\nKod# scores\nhead(pca_iris$x)\n\n           PC1        PC2         PC3          PC4\n[1,] -2.257141 -0.4784238  0.12727962  0.024087508\n[2,] -2.074013  0.6718827  0.23382552  0.102662845\n[3,] -2.356335  0.3407664 -0.04405390  0.028282305\n[4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n[5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n[6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\nPierwsza składowa główna (PC1) jest kombinacją liniową wszystkich czterech zmiennych, przy czym trzy z nich — Sepal.Length, Petal.Length oraz Petal.Width — mają dodatnie ładunki, natomiast Sepal.Width ma ładunek ujemny. Oznacza to, że składowa ta rośnie, gdy długość działki kielicha oraz długość i szerokość płatków są duże, a maleje, gdy szerokość działki jest duża. Można zatem interpretować PC1 jako wymiar opisujący ogólny rozmiar kwiatu: kwiaty o większych płatkach i węższych działkach uzyskują wyższe wartości tej składowej. W zbiorze iris PC1 bardzo dobrze rozdziela gatunki – setosa charakteryzuje się niskimi wartościami tej składowej (krótkie płatki, szerokie działki), natomiast versicolor i virginica mają wartości wysokie, co odpowiada większym rozmiarom kwiatów.\nDruga składowa główna (PC2) ma zupełnie inną strukturę ładunków. Zdominowana jest przez bardzo silny ujemny współczynnik dla Sepal.Width oraz mniejszy, również ujemny, dla Sepal.Length. Wpływ płatków na tę składową jest niewielki. PC2 odzwierciedla zatem zmienność w obrębie kształtu działki kielicha, a zwłaszcza jej proporcji długości do szerokości. Kwiaty o węższych działkach mają wyższe wartości PC2, natomiast te o szerszych – niższe.\nInterpretując wspólnie obie składowe, można stwierdzić, że PC1 opisuje rozmiar kwiatu, natomiast PC2 – proporcje i kształt działki. W przestrzeni PC1–PC2 dane tworzą układ, w którym setosa jest wyraźnie oddzielona od pozostałych gatunków poprzez niskie wartości PC1 i wysokie PC2, a versicolor i virginica różnią się między sobą głównie wzdłuż drugiej osi. W rezultacie te dwie składowe pozwalają na niemal pełne odwzorowanie i wizualne rozdzielenie gatunków, przy czym PC1 odpowiada za wymiar wielkościowy, a PC2 – za wymiar kształtowy. Na potrzeby wizualizacji możemy narysować wykres biplot łączący obiekty i zmienne w przestrzeni dwóch pierwszych składowych.\n\nKodfviz_pca_biplot(pca_iris, repel = TRUE,\n                col.var = \"blue\", # kolor zmiennych\n                col.ind = iris$Species) + # kolor obiektów wg gatunku\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#mds",
    "href": "pca.html#mds",
    "title": "Metody redukcji wymiarowości",
    "section": "MDS",
    "text": "MDS\nMetoda Multidimensional Scaling (MDS), czyli skalowanie wielowymiarowe, stanowi rodzinę technik służących do odwzorowania danych opisanych za pomocą macierzy odległości (lub podobieństw) w przestrzeń o niskim wymiarze — najczęściej dwuwymiarową lub trójwymiarową — w taki sposób, aby relacje między obiektami zostały zachowane możliwie wiernie. Celem jest więc konstrukcja wektorowej reprezentacji obiektów, która odtwarza zadane odległości.\nIstnieją dwie podstawowe wersje MDS: metryczna i niemetryczna, różniące się sposobem odwzorowania wartości wejściowych i kryterium dopasowania.\nWersja metryczna (klasyczna MDS) (Torgerson 1952)\n\nZałożenia\nDane wejściowe stanowi ma macierz odległości euklidesowych \\[\n\\Delta = [\\delta_{ij}]{n\\times n}, \\quad \\delta{ij} \\ge 0, \\quad \\delta_{ii}=0, \\quad \\delta_{ij}=\\delta_{ji}.\n\\] Celem jest znalezienie konfiguracji punktów \\(X \\in \\mathbb{R}^{n\\times p}\\), takiej że odległości między punktami \\(d_{ij}(X) = \\|x_i - x_j\\|\\) są jak najbardziej zbliżone do odległości zadanych \\(\\delta_{ij}.\\)\nWyprowadzenie modelu\nZ klasycznej geometrii euklidesowej wynika, że iloczyn skalarny między wektorami można zapisać przez odległości \\[\nx_i^\\top x_j = \\frac{1}{2}\\bigl(\\|x_i\\|^2 + \\|x_j\\|^2 - \\|x_i - x_j\\|^2 \\bigr).\n\\] Jeżeli dane są wyrażone przez odległości \\(\\delta_{ij}\\), to można odtworzyć tzw. macierz iloczynów skalarnych \\(B = XX^\\top\\), która określa współrzędne punktów po odpowiednim scentralizowaniu układu współrzędnych. Operacja ta nazywa się podwójnym centrowaniem (double centering) \\[\nB = -\\frac{1}{2} J \\Delta^{(2)} J,\n\\] gdzie \\(\\Delta^{(2)} = [\\delta_{ij}^2]\\) to macierz kwadratów odległości, a \\(J = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top\\) to macierz centrowania (usuwa środek ciężkości układu).\nMacierz \\(B\\) powinna być dodatnio półokreślona (w przypadku euklidesowym). Następnie wykonuje się jej rozkład spektralny \\[\nB = V \\Lambda V^\\top,\n\\] gdzie \\(\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_n)\\) zawiera wartości własne uporządkowane malejąco, a \\(V\\) to odpowiadające im wektory własne. Współrzędne punktów w przestrzeni o wymiarze \\(p\\) wyznacza się przez \\[\nX_p = V_p \\Lambda_p^{1/2},\n\\] gdzie \\(V_p\\) zawiera \\(p\\) pierwszych wektorów własnych, a \\(\\Lambda_p\\) odpowiadające im największe wartości własne. Otrzymana konfiguracja \\(X_p\\) odwzorowuje relacje odległości w sposób minimalizujący błąd w sensie średniokwadratowym.\nKryterium dopasowania\nW wersji metrycznej minimalizuje się błąd rekonstrukcji odległości \\[\n\\min_{X} \\sum_{i&lt;j} \\bigl( d_{ij}(X) - \\delta_{ij} \\bigr)^2.\n\\] Rozwiązanie klasycznej wersji powyższego problemu daje wprost powyższa dekompozycja macierzy \\(B\\), dlatego często określa się ją jako Classical Scaling lub Principal Coordinates Analysis (PCoA). Metoda ta jest w pełni analityczna i odpowiada PCA zastosowanej do macierzy odległości.\nWersja niemetryczna (Non-metric MDS) (Kruskal 1964)\n\nZałożenia\nW wersji niemetrycznej nie wymaga się, aby wartości \\(\\delta_{ij}\\) były dokładnymi odległościami — mogą być dowolnymi miarami niepodobieństwa, niekoniecznie metrycznymi (np. o charakterze porządkowym). Celem jest znalezienie konfiguracji \\(X\\), dla której rangi odległości \\(d_{ij}(X)\\) są możliwie zgodne z rangami danych \\(\\delta_{ij}\\).\nModel i funkcja stresu\nPonieważ nie zakłada się liniowego związku między \\(\\delta_{ij}\\) a \\(d_{ij}(X)\\), wprowadza się monotoniczną funkcję przekształcenia \\(f(\\cdot)\\), która dopasowuje skalę \\[\n\\hat{\\delta}_{ij} = f(d_{ij}(X)),\n\\] przy czym \\(f\\) zachowuje monotoniczność rangową (jeżeli \\(\\delta_{ij} &gt; \\delta_{kl}\\), to \\(f(\\delta_{ij}) &gt; f(\\delta_{kl})\\)).\nOptymalizuje się wtedy miarę stresu Kruskala (STRESS - STandardized REsidual Sum of Squares) \\[\nS(X) = \\sqrt{\\frac{\\sum_{i&lt;j} \\bigl(f(\\delta_{ij}) - d_{ij}(X)\\bigr)^2}{\\sum_{i&lt;j} d_{ij}(X)^2}}.\n\\] Minimalizacja stresu odbywa się iteracyjnie — zmienia się położenie punktów \\(x_i\\), aby zmniejszyć różnicę między rangami obserwowanych i odwzorowanych odległości.\n\n\nWartość STRESS\nOcena dopasowania\n\n\n\n&lt; 0.05\nDoskonałe\n\n\n0.05–0.10\nBardzo dobre\n\n\n0.10–0.20\nUmiarkowane\n\n\n0.20–0.30\nSłabe\n\n\n&gt; 0.30\nBardzo słabe (odwzorowanie nieadekwatne)\n\n\nInterpretacja\nW niemetrycznym MDS nie dąży się do zachowania dokładnych odległości, lecz do zachowania porządku relacji niepodobieństwa, obiekty podobne mają być blisko siebie, a niepodobne — daleko. Dzięki temu metoda jest odporna na nieliniowe zniekształcenia skali w danych wejściowych.\nPorównanie metod MDS\n\n\n\n\n\n\n\nCecha\nMetryczny MDS\nNiemetryczny MDS\n\n\n\nTyp danych wejściowych\nOdległości euklidesowe\nDowolne niepodobieństwa, także porządkowe\n\n\nZależność między danymi a odległościami\nLiniowa\nMonotoniczna (dowolna funkcja porządkowa)\n\n\nAlgorytm\nDekompozycja własna macierzy centrowanej\nIteracyjna optymalizacja stresu\n\n\nKryterium dopasowania\nMinimalizacja błędu kwadratowego odległości\nMinimalizacja stresu Kruskala\n\n\nInterpretacja\nOdtwarza dokładne relacje geometryczne\nOdtwarza relacje rangowe (porządek podobieństw)\n\n\n\nW praktyce metryczny MDS jest szybszy, prostszy i równoważny klasycznemu podejściu PCA, gdy dane mają charakter metryczny. Niemetryczny MDS natomiast jest bardziej elastyczny — pozwala odwzorować struktury nieliniowe, zachowując tylko relacje porządkowe między obiektami, co czyni go odpowiednim do danych percepcyjnych, preferencyjnych lub ankietowych.\n\nPrzykład 5.3 (MDS na danych o odległościach między miastami)  \n\nKodlibrary(MASS)        # isoMDS, UScitiesD\nlibrary(maps)        # zarysy map\nlibrary(ggrepel)\n\n\n# Dane: odległości drogowe między miastami w USA (w milach)\ndata(\"UScitiesD\")  # obiekt klasy 'dist' z pakietu MASS\n\n# 1) Metryczny MDS\nmds_metric &lt;- cmdscale(UScitiesD, k = 2)\nmds_metric\n\n                    [,1]       [,2]\nAtlanta        -718.7594  142.99427\nChicago        -382.0558 -340.83962\nDenver          481.6023  -25.28504\nHouston        -161.4663  572.76991\nLosAngeles     1203.7380  390.10029\nMiami         -1133.5271  581.90731\nNewYork       -1072.2357 -519.02423\nSanFrancisco   1420.6033  112.58920\nSeattle        1341.7225 -579.73928\nWashington.DC  -979.6220 -335.47281\n\nKod# 2) Niemetryczny MDS\nmds_nonmetric &lt;- isoMDS(UScitiesD, k = 2)$points\n\ninitial  value 0.049975 \niter   5 value 0.049265\niter  10 value 0.048377\niter  15 value 0.047490\niter  20 value 0.046603\niter  25 value 0.045715\niter  30 value 0.044828\niter  35 value 0.043941\niter  40 value 0.043053\niter  45 value 0.042166\niter  50 value 0.041278\nfinal  value 0.041278 \nstopped after 50 iterations\n\nKodmds_nonmetric\n\n                    [,1]       [,2]\nAtlanta        -718.7595  142.99430\nChicago        -382.0558 -340.83969\nDenver          481.6024  -25.28505\nHouston        -161.1399  572.76696\nLosAngeles     1203.7842  389.77723\nMiami         -1133.8537  581.91049\nNewYork       -1072.2359 -519.02433\nSanFrancisco   1420.6036  112.58922\nSeattle        1341.6768 -579.41626\nWashington.DC  -979.6222 -335.47288\n\nKod# 3) Ramy danych do wykresu MDS\nmds_df &lt;- data.frame(\n  City = rownames(mds_metric),\n  Metric_X = mds_metric[,1],\n  Metric_Y = mds_metric[,2],\n  Nonmetric_X = mds_nonmetric[,1],\n  Nonmetric_Y = mds_nonmetric[,2]\n)\n\n# 4) Wykresy MDS (po symetrii względem obu osi dla lepszej czytelności)\np1 &lt;- ggplot(mds_df, aes(x = -Metric_X, y = -Metric_Y, label = City)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_text_repel(size = 3) +\n  labs(title = \"Metryczny MDS na danych o odległościach między miastami\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal()\n\np2 &lt;- ggplot(mds_df, aes(x = -Nonmetric_X, y = -Nonmetric_Y, label = City)) +\n  geom_point(color = \"red\", size = 2) +\n  geom_text_repel(size = 3) +\n  labs(title = \"Niemetryczny MDS na danych o odległościach między miastami\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal()\n\n# 5) Faktyczne położenia miast (long/lat) – nazwy muszą pokrywać się z UScitiesD\ncity_coords &lt;- tribble(\n  ~City,           ~lon,      ~lat,\n  \"Atlanta\",       -84.39,     33.75,\n  \"Chicago\",       -87.63,     41.88,\n  \"Denver\",       -104.99,     39.74,\n  \"Houston\",       -95.37,     29.76,\n  \"LosAngeles\",   -118.24,     34.05,\n  \"Miami\",         -80.19,     25.77,\n  \"NewYork\",       -74.01,     40.71,\n  \"SanFrancisco\", -122.42,     37.77,\n  \"Seattle\",      -122.33,     47.61,\n  \"Washington\",    -77.04,     38.90\n)\n\n# 6) Zarys mapy USA\nusa_map &lt;- map_data(\"state\")\n\n# 7) Wykres p3: faktyczna mapa z punktami miast\np3 &lt;- ggplot() +\n  geom_polygon(data = usa_map,\n               aes(x = long, y = lat, group = group),\n               fill = \"grey95\", color = \"grey70\", linewidth = 0.3) +\n  geom_point(data = city_coords,\n             aes(x = lon, y = lat),\n             size = 2, color = \"black\") +\n  geom_text_repel(data = city_coords,\n                  aes(x = lon, y = lat, label = City),\n                  size = 3) +\n  labs(title = \"Faktyczne położenie miast na mapie USA\",\n       x = \"Długość geograficzna\", y = \"Szerokość geograficzna\") +\n  theme_minimal()\n\n# 8) Prezentacja obok siebie (patchwork)\np1/ p2 / p3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#ica",
    "href": "pca.html#ica",
    "title": "Metody redukcji wymiarowości",
    "section": "ICA",
    "text": "ICA\nPodstawowy model ICA (ang. Independent Component Analysis) zakłada, że wektor obserwacji \\(X \\in \\mathbb{R}^p\\) powstaje poprzez liniowe i natychmiastowe wymieszanie wektora ukrytych źródeł \\(s \\in \\mathbb{R}^m\\) o statystycznie niezależnych składowych \\[\nX = A s,\\qquad A \\in \\mathbb{R}^{p\\times m},\n\\] przy czym \\(m \\le \\min(p,n)\\) oraz macierz mieszająca \\(A\\) ma pełny rząd. Celem jest oszacowanie macierzy demiksującej \\(W \\in \\mathbb{R}^{m\\times p}\\) tak, aby \\(y = W X\\) aproksymować \\(s\\) składowymi możliwie niezależnymi w sensie probabilistycznym. Z istoty problemu rozwiązanie identyfikowalne jest jedynie do permutacji i skalowania - kolejność oraz skale (a więc i znaki) składowych nie są odzyskiwalne.\nWyprowadzenie algorytmów ICA rozpoczynamy od scentralizowania danych i ich whiteningu. Niech \\(\\Sigma_X = \\tfrac{1}{n}\\sum_i (X_i-\\bar X)(X_i-\\bar X)^\\top\\) oraz niech \\(V\\) oznacza macierz whitening taką, że \\(Z=V(X-\\bar X)\\) spełnia \\(\\operatorname{Cov}(Z)=I_p\\). W praktyce przyjmujemy \\(V=\\Lambda^{-1/2}U^\\top\\) z dekompozycji \\(\\Sigma_X=U\\Lambda U^\\top\\). W przestrzeni whitened model przyjmuje postać \\[\nZ = V A s \\equiv R\\, s,\n\\] gdzie \\(R\\) jest macierzą ortogonalną (dla przypadku \\(m=p\\)). Poszukujemy więc wektorów w jednostkowej normie, dla których skalarna projekcja \\(y=w^\\top Z\\) jest możliwie „nienormalna” (niesymetryczna lub ciężkoogonowa), co stanowi praktyczne kryterium niezależności.\n\nKod# Pakiety\nlibrary(fastICA)\nlibrary(patchwork)\n\nset.seed(44)\n\n# 1) Generowanie dwóch niezależnych źródeł (niegaussowskich)\nn &lt;- 3000\ns1 &lt;- rexp(n, rate = 1) - 1  # Jednostronnie cięższy ogon (Eksponencjalny przesunięty do zera średniej)\ns2 &lt;- runif(n, -2, 2)        # Równomierny (płaskie ogony)\nS  &lt;- cbind(s1, s2)\nS  &lt;- scale(S, center = TRUE, scale = FALSE) # zero-mean dla wygody\n\n# 2) Mieszanie źródeł macierzą A -&gt; obserwacje X\nA &lt;- matrix(c(1, 2,\n              2, 1), nrow = 2, byrow = TRUE)\nX &lt;- S %*% t(A)                        # model X = S A^T\nX &lt;- scale(X, center = TRUE, scale = FALSE)  # centrowanie (odjęcie średniej kolumn)\n\n# 3) Whitening (sferyzacja): Z = V X, gdzie V = Λ^{-1/2} U^T z dekompozycji Σ_X\nSigmaX &lt;- cov(X)\ne &lt;- eigen(SigmaX)\nU &lt;- e$vectors\nLambda &lt;- diag(e$values)\nV &lt;- solve(sqrt(Lambda)) %*% t(U)     # Λ^{-1/2} U^T\nZ &lt;- t(V %*% t(X))                    # Z = V X (w wierszach obserwacje)\n\n# Kontrola: kowariancja Z ~ I\nround(cov(Z), 3)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKod# 4) ICA (FastICA) na danych whitened (można też na X bo fastICA automatycznie wykonuje whitening)\nica &lt;- fastICA(Z, n.comp = 2, method = \"C\")  # odzyskane źródła Y i mieszanie A_ICA\nY &lt;- ica$S                                   # szacowane źródła niezależne (kolumny ~ komponenty)\nW &lt;- ica$K %*% ica$W                         # łączne \"demiksowanie\" względem X (tu pracowaliśmy na Z)\n# Uwaga: fastICA zwraca też K (whitening) i W (unmixing), składnia zależy od wejścia\n\n# 5) Przygotowanie danych do wykresów\nto_df &lt;- function(M, name){\n  as.data.frame(M) |&gt;\n    setNames(c(\"c1\",\"c2\")) |&gt;\n    mutate(stage = name)\n}\ndf_X &lt;- to_df(X, \"X: dane zmieszane\")\ndf_Z &lt;- to_df(Z, \"Z: po whitening\")\ndf_Y &lt;- to_df(Y, \"Y: po ICA (źródła)\")\n\ndf_all &lt;- bind_rows(df_X, df_Z, df_Y)\n\n# 6) Oś układu i wektory bazowe (do wizualizacji obrotów)\n#    W przestrzeni Z osie są już sferyczne (I), więc ICA to „tylko” obrót.\naxes_df &lt;- function(scale_len = 2){\n  data.frame(x = c(0,0), y = c(0,0),\n             xend = c(scale_len,0), yend = c(0,scale_len),\n             label = c(\"e1\",\"e2\"))\n}\naxesZ &lt;- axes_df()\n\n# 7) Wykresy: chmury punktów w 2D (X, Z, Y)\npX &lt;- ggplot(df_X, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8) +\n  coord_equal() +\n  labs(title = \"Przed whiteningiem (X)\",\n       x = \"X[,1]\", y = \"X[,2]\") +\n  theme_minimal()\n\npZ &lt;- ggplot(df_Z, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#2E86DE\") +\n  geom_segment(data = axesZ, aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.18,\"cm\")), linewidth = 0.8) +\n  geom_text(data = axesZ, aes(x = xend, y = yend, label = label),\n            nudge_x = 0.05, nudge_y = 0.05, size = 3) +\n  coord_equal() +\n  labs(title = \"Po whitening (Z): Cov ≈ I\",\n       x = \"Z[,1]\", y = \"Z[,2]\") +\n  theme_minimal()\n\npY &lt;- ggplot(df_Y, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#16A085\") +\n  coord_equal() +\n  labs(title = \"Po ICA (Y): odzyskane źródła niezależne\",\n       x = \"Y[,1]\", y = \"Y[,2]\") +\n  theme_minimal()\n\n(pX | pZ | pY)\n\n\n\n\n\n\nKod# 8) Dodatkowo: marginesowe histogramy pokazujące „nienormalność”\nhX &lt;- df_X |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"X\")\nhZ &lt;- df_Z |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Z\")\nhY &lt;- df_Y |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Y\")\n\nh_all &lt;- bind_rows(hX,hZ,hY) |&gt;\n  mutate(stage = factor(stage, levels = c(\"X\",\"Z\",\"Y\")))\n\nggplot(h_all, aes(val)) +\n  geom_histogram(bins = 60, fill = \"grey70\", color = \"white\") +\n  facet_grid(stage ~ col, scales = \"free_y\") +\n  labs(title = \"Marginalne rozkłady: przed whiteningiem, po whitening, po ICA\",\n       x = \"wartość\", y = \"liczność\") +\n  theme_minimal()\n\n\n\n\n\n\nKod# 9) Krótka kontrola: kowariancje i korelacje\ncat(\"\\nKowariancja X:\\n\"); print(round(cov(X),3))\n\n\nKowariancja X:\n\n\n      [,1]  [,2]\n[1,] 6.295 4.620\n[2,] 4.620 5.123\n\nKodcat(\"\\nKowariancja Z (powinna być bliska I):\\n\"); print(round(cov(Z),3))\n\n\nKowariancja Z (powinna być bliska I):\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKodcat(\"\\nKorelacje pomiędzy kolumnami Y (powinny być bliskie 0; niezależność jest mocniejsza niż brak korelacji):\\n\")\n\n\nKorelacje pomiędzy kolumnami Y (powinny być bliskie 0; niezależność jest mocniejsza niż brak korelacji):\n\nKodprint(round(cor(Y),3))\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nPodejście maksymalizujące nienormalność opiera się na kurtozie lub na przybliżonej negatywnej entropii (ang. negentropy). Dla \\(\\operatorname{Var}(y)=1\\) kurtoza \\(\\kappa(y)=\\mathbb{E}\\{y^4\\}-3\\) przyjmuje wartości 0 dla rozkładu normalnego i wartości odległe od zera dla rozkładów nienormalnych. Maksymalizacja \\(|\\kappa(w^\\top Z)|\\) prowadzi do składowych niezależnych. Stabilniejsze i bardziej ogólne kryterium stanowi negentropy \\(J(y)=H(y_{\\text{gauss}})-H(y)\\), gdzie \\(H\\) oznacza entropię. W praktyce stosuje się aproksymacje postaci \\[\nJ(y)\\approx \\Big(\\mathbb{E}\\,G(y)-\\mathbb{E}\\,G(v)\\Big)^2,\n\\] z dobraną nieliniowością \\(G\\) oraz \\(v\\sim \\mathcal N(0,1)\\). Maksymalizacja \\(J\\) przy ograniczeniu \\(\\|w\\|=1\\) zapewnia poszukiwanie najbardziej nienormalnych kierunków.\nZ kryteriów tych wynika algorytm FastICA jako iteracja stałego punktu. Dla jednego komponentu w przestrzeni whitened stosujemy aktualizację \\[\nw \\leftarrow \\mathbb{E}\\{Z\\,g(w^\\top Z)\\}-\\mathbb{E}\\{g’(w^\\top Z)\\}\\, w,\\qquad \\text{następnie } w\\leftarrow \\frac{w}{\\|w\\|},\n\\] gdzie \\(g=G’\\) jest score function (np. \\(g(u)=\\tanh(u)\\), \\(g(u)=u^3\\) lub \\(g(u)=u\\exp(-u^2/2)\\)). Dla wielu składowych stosujemy równoległe aktualizacje i ortogonalizację w kolejnych krokach, np. metodą rzutów Grama–Schmidta lub przez dekompozycję symetryczną \\(W\\leftarrow (WW^\\top)^{-1/2}W\\), co zachowuje wzajemną ortogonalność wektorów w przestrzeni whitened i zapobiega zbieżności do tej samej składowej.\nAlternatywne wyprowadzenie pochodzi z maksymalizacji funkcji wiarygodności (maximum likelihood). Zakładając niezależność źródeł z gęstościami \\(p_{s_i}\\) i (dla prostoty) brak szumu, otrzymujemy logarytm funkcji wiarygodności \\[\n\\mathcal L(W)=\\sum_{t=1}^n\\Bigg(\\sum_{i=1}^m \\log p_{s_i}\\big((W X_t)_i\\big)\\Bigg) + n\\log|\\det W|.\n\\] Jej gradient prowadzi do zasady Infomax, która brzmi: dobrać \\(W\\) tak, aby wyjścia miały jak największą sumę entropii (co przy zachowaniu \\(\\log|\\det W|\\) jest równoważne maksymalizacji wspólnej niezależności). W praktyce wybór rodziny \\(p_{s_i}\\) implikuje odpowiednie nieliniowości w regule uczenia, formalnie zbieżne z powyższymi kontrastami na negentropy.\nW obecności szumu addytywnego \\(X = A s + \\varepsilon\\) z \\(\\varepsilon\\sim \\mathcal N(0,\\sigma^2 I)\\) problem staje się trudniejszy. Stosuje się wówczas rozszerzone modele ICA z estymacją rzędu i składowej szumowej, warianty bayesowskie, lub metody wykorzystujące dodatkowe własności źródeł (np. niezależność czasową wyższych rzędów, jak w SOBI wykorzystującym autokowariancje).\nZałożenia identyfikowalności obejmują liniowość i natychmiastowość mieszania, niezależność składowych źródłowych, co najwyżej jedną składową o rozkładzie normalnym (inaczej problem staje się nierozwiązywalny z powodu nieodróżnialności kierunków gaussowskich), pełny rząd macierzy \\(A\\) oraz wystarczającą nienormalność źródeł, aby kontrasty informacyjne miały sens. Zwyczajowo zakłada się również stacjonarność w czasie, o ile wykorzystujemy momenty lub autokorelacje do estymacji.\nDobór liczby składowych w ICA nie opiera się na udziale wariancji, jak w PCA, ponieważ ICA nie porządkuje komponentów według wariancji. W praktyce najpierw wybiera się wymiar whiteningu \\(m\\) (efektywny rząd sygnału), a następnie ekstrahuje \\(m\\) składowych niezależnych. Kryteria wyboru \\(m\\) obejmują informacyjne miary rzędu macierzy kowariancji, takie jak MDL/BIC dopasowane do modelu składowej szumowej i niezerowych wartości własnych, testy istotności dla wartości własnych po whiteningu (warianty analizy równoległej, permutacyjne testy mierzące losowość), walidację na podstawie wiarygodności w modelu ML-ICA z różnymi \\(m\\) oraz kryteria stabilności. Kryteria stabilności polegają na wielokrotnym uruchomieniu algorytmu z różnymi inicjalizacjami i grupowaniu uzyskanych komponentów. Liczba dobrze replikujących się grup daje oszacowanie na \\(m\\). Dodatkowo stosuje się testy resztowej zależności między oszacowanymi źródłami (np. testy niezależności na bazie informacji wzajemnej). Jeśli po dodaniu kolejnej składowej informacja wzajemna między „źródłami” nie maleje, zwiększanie \\(m\\) nie przynosi korzyści. W zastosowaniach z szumem wybieramy \\(m\\) tak, by oddzielać podprzestrzeń sygnałową od szumowej, co praktycznie sprowadza się do analizy spektrum wartości własnych i modelowania ogona jako białego szumu.\nInterpretacja wyników ICA różnić się od PCA. Składowe ICA \\(y_i\\) stanowią oceny źródeł o maksymalnej niezależności, a wiersze \\(W\\) definiują filtry demiksujące, podczas gdy kolumny \\(A\\) (przyjmując \\(A\\approx W^{-1}\\)) reprezentują wzorce mieszania, czyli „mapy obciążenia” źródeł na czujniki/cechy. Skale i znaki składowych są arbitralne, co wymaga interpretować je względnie: znormalizować wariancję lub maksymalną wartość, a znak dobrać tak, by ułatwić opis dziedzinowy5. W przeciwieństwie do PCA, składowe ICA nie muszą być ortogonalne, a ich wariancje nie są uporządkowane6.\n5 Wyobraźmy sobie, że ICA rozdziela dwa źródła dźwięku — skrzypce i fortepian. Jeśli algorytm zwróci sygnał, który jest odwrócony w fazie (czyli pomnożony przez -1), to dźwięk fortepianu jest ten sam fizycznie, tylko wszystkie amplitudy mają odwrotny znak. Dlatego znak (i skala) nie mają znaczenia dla jakości separacji — są arbitralne.6 W sygnałach biologicznych, takich jak EEG, ICA może oddzielić artefakty ruchowe od sygnałów mózgowych. Artefakty te mogą być silnie nienormalne i niezależne od sygnałów mózgowych, co czyni ICA skuteczną metodą ich identyfikacji i usunięcia.\nPrzykład 5.2 (ICA na mieszance sygnałów)  \n\nKoddata(\"EuStockMarkets\")\nP &lt;- as.data.frame(EuStockMarkets)              # poziomy indeksów: DAX, SMI, CAC, FTSE\nR &lt;- as.data.frame(apply(P, 2, function(x) diff(log(x))))  # dzienne log-zwroty\ncolnames(R) &lt;- colnames(P)\n\n# ICA na dziennych zwrotach\nset.seed(123)\nica_res &lt;- fastICA(R, n.comp = 4, method = \"C\")\nS_est &lt;- as.data.frame(ica_res$S)               # odzyskane źród\ncolnames(S_est) &lt;- paste0(\"IC\", 1:4)\nA_est &lt;- ica_res$A                              # macierz mieszająca\nW_est &lt;- ica_res$K %*% ica_res$W                # macierz demiksująca\n\n\nMacierz A_est, czyli macierz mieszania, opisuje sposób, w jaki oryginalne zmienne obserwowalne — w tym przypadku cztery indeksy giełdowe: DAX, SMI, CAC i FTSE — powstają jako liniowe kombinacje ukrytych, niezależnych czynników. Każdy wiersz tej macierzy odpowiada jednemu indeksowi, a każda kolumna jednej składowej niezależnej. Wartości liczbowe oznaczają współczynniki liniowych kombinacji, czyli wpływ danej składowej na dany indeks. Wartość dodatnia wskazuje, że wzrost komponentu powoduje wzrost indeksu, wartość ujemna — że ruch komponentu przekłada się na spadek indeksu, a wartość bliska zeru oznacza brak istotnego związku.\n\nKodround(A_est, 3)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] -0.001 -0.001 -0.002 -0.006\n[2,]  0.003 -0.004  0.002  0.000\n[3,] -0.010 -0.008 -0.008 -0.005\n[4,]  0.000  0.001  0.008  0.001\n\n\n\nPierwszy komponent (IC1) najsilniej ładuje się na indeks CAC, a w mniejszym stopniu na SMI. Znak ujemny dla CAC i dodatni dla SMI sugeruje, że komponent ten uchwyca różnicę pomiędzy rynkami strefy euro a rynkiem szwajcarskim, czyli czynnik kontrastujący. W praktyce oznacza to, że wzrost aktywności na rynkach kontynentalnych wiązać się może z relatywnym osłabieniem rynku SMI lub odwrotnie.\nDrugi komponent (IC2) również oddziałuje na CAC i SMI w kierunku ujemnym, co może świadczyć o uchwyceniu wspólnego czynnika kontynentalnego o mniejszej amplitudzie. Dodatnie, choć niewielkie wartości dla FTSE wskazują, że komponent ten częściowo kontrastuje rynki kontynentalne z brytyjskim.\nTrzeci komponent (IC3) ma wyraźnie odmienną strukturę. Dla FTSE współczynnik jest dodatni i największy, natomiast dla pozostałych indeksów ujemny. Komponent ten rozdziela zatem rynek brytyjski od reszty Europy i można go interpretować jako czynnik geograficzny lub walutowy, związany z odmiennym otoczeniem gospodarczym Wielkiej Brytanii.\nCzwarty komponent (IC4) ma współczynniki bardzo małe, rzędu 10-3–10-2, co sugeruje, że jego wpływ na strukturę indeksów jest marginalny. Prawdopodobnie odpowiada on za szum lub krótkotrwałe, lokalne fluktuacje, które nie mają znaczenia ekonomicznego.\n\n\nKodround(W_est, 3)\n\n         [,1]     [,2]    [,3]    [,4]\n[1,]   43.737  112.245 -74.794 -85.258\n[2,]   34.702 -146.908 -47.294  -7.649\n[3,]   15.975   14.330   8.569 141.650\n[4,] -173.263   -5.797  10.661 -35.834\n\n\nMacierz W_est, czyli macierz demiksująca, zawiera współczynniki liniowych kombinacji oryginalnych zmiennych (indeksów giełdowych), które pozwalają uzyskać poszczególne niezależne komponenty. Każdy wiersz tej macierzy odpowiada jednemu komponentowi ICA (IC1–IC4), a każda kolumna — jednej zmiennej obserwowalnej (DAX, SMI, CAC, FTSE). Wartości w tej macierzy można zatem interpretować jako wagi, z jakimi poszczególne indeksy uczestniczą w tworzeniu danego odzyskanego źródła.\n\nPierwszy komponent (IC1) ma duże dodatnie wagi dla indeksów DAX i SMI oraz silnie ujemną wagę dla FTSE. Oznacza to, że IC1 odzwierciedla kontrast pomiędzy rynkami kontynentalnymi (Niemcy, Szwajcaria) a rynkiem brytyjskim. Wzrost wartości IC1 odpowiada sytuacji, w której indeksy kontynentalne zachowują się silniej niż FTSE — można więc interpretować ten czynnik jako różnicowy, typu „Europa kontynentalna kontra Wielka Brytania”.\nDrugi komponent (IC2) pokazuje odwrotny schemat: dodatni wpływ DAX, silnie ujemny SMI, a słaby wpływ pozostałych indeksów. Można go interpretować jako czynnik rozróżniający zachowanie rynku niemieckiego i szwajcarskiego, który w ICA często ujawnia się jako efekt odmiennych warunków walutowych i struktury gospodarczej.\nTrzeci komponent (IC3) ma wszystkie wagi ujemne, z wyjątkiem niewielkich dodatnich dla CAC i FTSE. Oznacza to, że IC3 reprezentuje wspólny kierunek zmian większości indeksów (ruch globalny), ale w konstrukcji demiksującej występuje ze znakiem ujemnym. W praktyce odpowiada to czynnikowi rynkowemu o charakterze ogólnym — globalnemu impulsowi, który oddziałuje w podobny sposób na większość rynków.\nCzwarty komponent (IC4) ma wysoką dodatnią wagę dla CAC oraz ujemne dla pozostałych indeksów, co sugeruje, że może on odzwierciedlać czynnik specyficzny dla rynku francuskiego — reakcje lokalne lub sektorowe, które nie są wspólne dla innych giełd..\n\nZnaki współczynników w ICA są arbitralne (zmiana wszystkich znaków w jednym wierszu nie zmienia modelu), dlatego przy interpretacji należy zwracać uwagę na względne zależności między indeksami, a nie na samą polaryzację znaków. Wartości bezwzględne wag pokazują natomiast, które indeksy mają największy udział w kształtowaniu danego czynnika.\n\nKodkurt &lt;- apply(S_est, 2, function(x) mean(x^4)-3)  # kurtozy odzyskanych źródeł\nprint(round(kurt, 3))                           # kurtozy (nienormalność)\n\n  IC1   IC2   IC3   IC4 \n5.601 1.985 8.206 2.274 \n\n\nWartości kurtozy stanowią miarę niegaussowskości rozkładu — czyli tego, jak bardzo dany sygnał odbiega od kształtu rozkładu normalnego. Wartości dodatnie oznaczają rozkłady o „cięższych ogonach” i bardziej spiczastym kształcie (tzw. leptokurtyczne), co jest typowe dla sygnałów rzadkich, zawierających wyraźne piki i okresy stabilności. W kontekście ICA wysoka kurtoza jest pożądana, ponieważ algorytm poszukuje właśnie takich komponentów — maksymalnie odmiennych od normalnych, a więc potencjalnie niezależnych źródeł.\n\n\nIC1 (5.601) ma bardzo wysoką kurtozę, co wskazuje na silną niegaussowskość. Komponent ten prawdopodobnie reprezentuje główny, „rzadki” czynnik ekonomiczny, który reaguje gwałtownie w momentach istotnych zmian rynkowych. Może to być globalny impuls rynkowy lub okresowe szoki finansowe.\n\nIC2 (1.985) ma umiarkowanie dodatnią kurtozę, sugerującą rozkład jedynie lekko leptokurtyczny. Oznacza to, że komponent jest bliższy rozkładowi normalnemu, a zatem mniej „niezależny” w sensie ICA. Może reprezentować łagodniejszy czynnik wspólny, np. codzienną zmienność lub trend regionalny.\n\nIC3 (8.206) ma najwyższą kurtozę spośród wszystkich komponentów. Jest to bardzo silny sygnał niegaussowski, typowy dla źródła zawierającego rzadkie, intensywne zdarzenia — w kontekście finansowym mogą to być momenty skokowych zmian cen lub kryzysów, wpływające selektywnie na część indeksów. Ten komponent można traktować jako najbardziej „czyste” źródło w sensie ICA.\n\nIC4 (2.274) wykazuje umiarkowaną kurtozę, zbliżoną do IC2. Można go interpretować jako dodatkowy, mniej wyraźny czynnik poboczny, który w pewnym stopniu odbiega od normalności, ale nie ma charakteru dominującego.\n\n\nKod# Sprawdzenie korelacji między oryginalnymi a odzyskanymi sygnałami\ncor_matrix &lt;- cor(R, S_est)\nprint(round(cor_matrix, 3))\n\n        IC1    IC2    IC3   IC4\nDAX  -0.076  0.287 -0.954 0.037\nSMI  -0.060 -0.475 -0.871 0.108\nCAC  -0.189  0.140 -0.686 0.689\nFTSE -0.788  0.001 -0.602 0.125\n\n\nMacierz korelacji między oryginalnymi indeksami giełdowymi a odzyskanymi komponentami niezależnymi (cor_matrix) pokazuje, jak silnie i w jakim kierunku (znak dodatni lub ujemny) każdy z indeksów jest powiązany z danym źródłem ICA. Wysokie wartości bezwzględne wskazują, że dany komponent w dużym stopniu tłumaczy zmienność danego indeksu, natomiast wartości bliskie zera oznaczają słaby związek.\n\nNajsilniejsze korelacje obserwuje się dla komponentu IC3, który ma wartości ujemne i bardzo wysokie w module: DAX (−0.954), SMI (−0.871) i CAC (−0.686). Oznacza to, że IC3 stanowi wspólny czynnik dominujący dla trzech kontynentalnych indeksów europejskich. Wszystkie trzy reagują w tym samym kierunku (ujemny znak jest konwencjonalny, jego odwrócenie nie zmienia interpretacji). Można zatem uznać, że IC3 reprezentuje globalny czynnik rynkowy, wspólny dla głównych giełd kontynentalnych, a jego wysoka kurtoza (8.206) wskazuje, że czynnik ten cechuje się silnymi, epizodycznymi wahaniami — typowymi dla okresów zawirowań finansowych.\nKomponent IC1 wykazuje wyraźną ujemną korelację z FTSE (−0.788), przy braku silnych zależności z pozostałymi indeksami. Oznacza to, że IC1 można interpretować jako czynnik specyficzny dla rynku brytyjskiego, niezależny od ruchów kontynentalnych. Wysoka wartość bezwzględna korelacji sugeruje, że ten komponent odpowiada za znaczną część zmienności FTSE, co dobrze współgra z interpretacją wcześniejszej macierzy mieszania — IC1 oddzielał Wielką Brytanię od reszty Europy.\nKomponent IC2 wykazuje umiarkowane korelacje o różnych znakach: dodatnią z DAX (0.287) i ujemną ze SMI (−0.475). Można go zatem interpretować jako czynnik różnicowy pomiędzy rynkami Niemiec i Szwajcarii. W praktyce może on odzwierciedla odmienną reakcję tych rynków na czynniki lokalne, np. różnice w strukturze sektorowej lub polityce monetarnej.\nKomponent IC4 ma umiarkowaną dodatnią korelację z CAC (0.689), a pozostałe indeksy reagują na niego słabo. Oznacza to, że IC4 może być czynnikiem częściowo specyficznym dla rynku francuskiego, prawdopodobnie o charakterze lokalnym lub szumowym.\n\nNa koniec wizualizacja ICA.\n\nKod# odzyskane komponenty \nS_est_long &lt;- S_est |&gt;\n  mutate(Time = 1:nrow(S_est)) |&gt;\n  pivot_longer(cols = starts_with(\"IC\"),\n                      names_to = \"Component\", values_to = \"Value\")\n\np1 &lt;- ggplot(S_est_long, aes(x = Time, y = Value, color = Component)) +\n  geom_line() +\n  facet_wrap(~ Component, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Odzyskane niezależne komponenty (ICA)\", x = \"Czas\", y = \"Wartość\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nR_long &lt;- R |&gt;\n  mutate(Time = 1:nrow(R)) |&gt;\n  pivot_longer(cols = -Time,    \n                      names_to = \"Index\", values_to = \"Return\")\n\np2 &lt;- ggplot(R_long, aes(x = Time, y = Return, color = Index)) +\n  geom_line() +\n  facet_wrap(~ Index, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Oryginalne dzienne log-zwroty indeksów\", x = \"Czas\", y = \"Log-zwrot\") +\n  scale_color_flat_d()+\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#mds-na-danych-o-odległościach-między-miastami",
    "href": "pca.html#mds-na-danych-o-odległościach-między-miastami",
    "title": "Metody redukcji wymiarowości",
    "section": "MDS na danych o odległościach między miastami",
    "text": "MDS na danych o odległościach między miastami\n\nKodlibrary(MASS)        # isoMDS, UScitiesD\nlibrary(maps)        # zarysy map\nlibrary(ggrepel)\n\n\n# Dane: odległości drogowe między miastami w USA (w milach)\ndata(\"UScitiesD\")  # obiekt klasy 'dist' z pakietu MASS\n\n# 1) Metryczny MDS\nmds_metric &lt;- cmdscale(UScitiesD, k = 2)\nmds_metric\n\n                    [,1]       [,2]\nAtlanta        -718.7594  142.99427\nChicago        -382.0558 -340.83962\nDenver          481.6023  -25.28504\nHouston        -161.4663  572.76991\nLosAngeles     1203.7380  390.10029\nMiami         -1133.5271  581.90731\nNewYork       -1072.2357 -519.02423\nSanFrancisco   1420.6033  112.58920\nSeattle        1341.7225 -579.73928\nWashington.DC  -979.6220 -335.47281\n\nKod# 2) Niemetryczny MDS\nmds_nonmetric &lt;- isoMDS(UScitiesD, k = 2)$points\n\ninitial  value 0.049975 \niter   5 value 0.049265\niter  10 value 0.048377\niter  15 value 0.047490\niter  20 value 0.046603\niter  25 value 0.045715\niter  30 value 0.044828\niter  35 value 0.043941\niter  40 value 0.043053\niter  45 value 0.042166\niter  50 value 0.041278\nfinal  value 0.041278 \nstopped after 50 iterations\n\nKodmds_nonmetric\n\n                    [,1]       [,2]\nAtlanta        -718.7595  142.99430\nChicago        -382.0558 -340.83969\nDenver          481.6024  -25.28505\nHouston        -161.1399  572.76696\nLosAngeles     1203.7842  389.77723\nMiami         -1133.8537  581.91049\nNewYork       -1072.2359 -519.02433\nSanFrancisco   1420.6036  112.58922\nSeattle        1341.6768 -579.41626\nWashington.DC  -979.6222 -335.47288\n\nKod# 3) Ramy danych do wykresu MDS\nmds_df &lt;- data.frame(\n  City = rownames(mds_metric),\n  Metric_X = mds_metric[,1],\n  Metric_Y = mds_metric[,2],\n  Nonmetric_X = mds_nonmetric[,1],\n  Nonmetric_Y = mds_nonmetric[,2]\n)\n\n# 4) Wykresy MDS (po symetrii względem obu osi dla lepszej czytelności)\np1 &lt;- ggplot(mds_df, aes(x = -Metric_X, y = -Metric_Y, label = City)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_text_repel(size = 3) +\n  labs(title = \"Metryczny MDS na danych o odległościach między miastami\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal()\n\np2 &lt;- ggplot(mds_df, aes(x = -Nonmetric_X, y = -Nonmetric_Y, label = City)) +\n  geom_point(color = \"red\", size = 2) +\n  geom_text_repel(size = 3) +\n  labs(title = \"Niemetryczny MDS na danych o odległościach między miastami\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal()\n\n# 5) Faktyczne położenia miast (long/lat) – nazwy muszą pokrywać się z UScitiesD\ncity_coords &lt;- tribble(\n  ~City,           ~lon,      ~lat,\n  \"Atlanta\",       -84.39,     33.75,\n  \"Chicago\",       -87.63,     41.88,\n  \"Denver\",       -104.99,     39.74,\n  \"Houston\",       -95.37,     29.76,\n  \"LosAngeles\",   -118.24,     34.05,\n  \"Miami\",         -80.19,     25.77,\n  \"NewYork\",       -74.01,     40.71,\n  \"SanFrancisco\", -122.42,     37.77,\n  \"Seattle\",      -122.33,     47.61,\n  \"Washington\",    -77.04,     38.90\n)\n\n# 6) Zarys mapy USA\nusa_map &lt;- map_data(\"state\")\n\n# 7) Wykres p3: faktyczna mapa z punktami miast\np3 &lt;- ggplot() +\n  geom_polygon(data = usa_map,\n               aes(x = long, y = lat, group = group),\n               fill = \"grey95\", color = \"grey70\", linewidth = 0.3) +\n  geom_point(data = city_coords,\n             aes(x = lon, y = lat),\n             size = 2, color = \"black\") +\n  geom_text_repel(data = city_coords,\n                  aes(x = lon, y = lat, label = City),\n                  size = 3) +\n  labs(title = \"Faktyczne położenie miast na mapie USA\",\n       x = \"Długość geograficzna\", y = \"Szerokość geograficzna\") +\n  theme_minimal()\n\n# 8) Prezentacja obok siebie (patchwork)\np1/ p2 / p3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#t-sne",
    "href": "pca.html#t-sne",
    "title": "Metody redukcji wymiarowości",
    "section": "t-SNE",
    "text": "t-SNE\nMetoda t-distributed Stochastic Neighbor Embedding (t-SNE) została opracowana przez Laurensa van der Maatena i Geoffreya Hintona w 2008 roku jako nieliniowa technika redukcji wymiarowości, której celem jest odwzorowanie lokalnej struktury danych wysokowymiarowych w przestrzeni o mniejszej liczbie wymiarów, zwykle dwuwymiarowej lub trójwymiarowej. W przeciwieństwie do metod liniowych, takich jak PCA, t-SNE nie dąży do maksymalizacji wariancji, lecz do zachowania sąsiedztw pomiędzy punktami – obserwacje, które w przestrzeni oryginalnej są blisko siebie, powinny również pozostawać blisko w przestrzeni odwzorowania.\nNiech dane wejściowe tworzą macierz \\(X = [x_1, x_2, \\dots, x_n]^\\top\\), gdzie każdy wektor \\(x_i \\in \\mathbb{R}^p\\) reprezentuje jedną obserwację w przestrzeni o wymiarze \\(p\\). Pierwszym krokiem jest przekształcenie danych wysokowymiarowych w macierz podobieństw, która opisuje, jak bardzo punkty są „bliskie” względem siebie. Dla każdego punktu \\(x_i\\) definiuje się rozkład warunkowy \\[\np_{j|i} = \\frac{\\exp\\!\\left(-\\frac{|x_i - x_j|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{|x_i - x_k|^2}{2\\sigma_i^2}\\right)}, \\quad p_{i|i} = 0,\n\\] gdzie parametr \\(\\sigma_i\\) (odpowiednik bandwidth) dobiera się tak, aby entropia rozkładu \\(P_i = (p_{j|i})_j = (p_{1|i}, p_{2|i}, \\dots, p_{n|i}).\\) odpowiadała zadanej perplexity, czyli efektywnej liczbie sąsiadów. Perplexity jest hiperparametrem kontrolującym zakres lokalności analizowanych relacji.\nNastępnie konstruuje się symetryczną macierz podobieństw \\[\np_{ij} = \\frac{p_{i|j} + p_{j|i}}{2n},\n\\] która reprezentuje prawdopodobieństwo, że punkty \\(x_i\\) i \\(x_j\\) są bliskimi sąsiadami w przestrzeni oryginalnej.\nKolejnym krokiem jest utworzenie analogicznego rozkładu w przestrzeni odwzorowania \\(Y = [y_1, y_2, \\dots, y_n]^\\top\\), gdzie \\(y_i \\in \\mathbb{R}^q\\) i zwykle \\(q = 2\\) lub 3. Dla tych punktów definiuje się rozkład podobieństw oparty na rozkładzie t-Studenta z jednym stopniem swobody \\[\nq_{ij} = \\frac{(1 + |y_i - y_j|^2)^{-1}}{\\sum_{k \\neq l} (1 + |y_k - y_l|^2)^{-1}}, \\quad q_{ii} = 0.\n\\] Rozkład t-Studenta ma grube ogony, co umożliwia bardziej realistyczne odwzorowanie relacji między punktami odległymi od siebie i redukuje problem crowding, czyli nadmiernego ściskania punktów w centrum przestrzeni odwzorowania.\nCelem t-SNE jest minimalizacja dywergencji Kullbacka–Leiblera między rozkładami \\(P\\) i \\(Q\\) \\[\nC = \\operatorname{KL}(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n\\] Optymalizacja tej funkcji, zwykle za pomocą spadku gradientowego, prowadzi do znalezienia takich współrzędnych \\(Y\\), które zachowują lokalne relacje między punktami w jak największym stopniu. Gradient funkcji celu względem współrzędnych \\(y_i\\) ma postać \\[\n\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij}) (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1},\n\\] a współczynnik 4 pełni rolę skalującą. W praktyce stosuje się dodatkowe techniki stabilizujące proces uczenia, takie jak momentum, etap early exaggeration zwiększający kontrast lokalnych podobieństw, oraz wcześniejszą redukcję wymiarowości metodą PCA w celu ograniczenia szumu.\nMetoda t-SNE nie zakłada liniowości ani rozkładu normalnego danych, lecz wymaga, aby dane były znormalizowane w przypadku różnych jednostek pomiarowych, ponieważ odległości euklidesowe są wrażliwe na skalę. Wskazane jest wcześniejsze zastosowanie PCA w celu usunięcia szumu i zmniejszenia złożoności obliczeniowej. Dane nie powinny zawierać dużej liczby wartości odstających ani duplikatów, które mogłyby zaburzyć lokalne struktury. Kluczowy hiperparametr perplexity powinien być dostosowany do liczby obserwacji — zbyt mała wartość prowadzi do przeuczenia lokalnego, a zbyt duża powoduje zatarcie drobnych struktur.\nWyniki t-SNE przedstawione w przestrzeni dwuwymiarowej lub trójwymiarowej nie mają interpretacji metrycznej. Oznacza to, że odległości między klastrami nie są bezpośrednio interpretowalne ilościowo. Interpretacja opiera się głównie na analizie sąsiedztwa: punkty znajdujące się blisko siebie w przestrzeni t-SNE odpowiadają obserwacjom podobnym w oryginalnych cechach, natomiast wyraźne skupiska punktów mogą wskazywać na istnienie klas lub podgrup. Oś pierwsza i druga nie mają znaczenia merytorycznego – są jedynie współrzędnymi w przestrzeni odwzorowania, które zachowuje lokalną strukturę, a nie globalną geometrię danych.\n\n\n\n\n\n\nWażne kroki przy stosowaniu t-SNE\n\n\n\n\nStandaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.\nWybór zakresu perplexity — zwykle testuje się kilka wartości (np. 5, 15, 30, 50) i ocenia stabilność struktur (czy klastry są rozdzielne, czy stabilne względem permutacji danych).\nUstawienie learning rate — zaczyna się od wartości domyślnej (200) i w razie potrzeby zwiększa do 500–1000, jeśli klastry są zbyt zwarte.\nUstalenie liczby iteracji — co najmniej 500; w przypadku dużych zbiorów można zwiększyć do 1000–2000, jeśli rozkład nadal się zmienia.\nPorównanie z PCA lub UMAP — warto sprawdzić, czy t-SNE nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykład 5.4 (t-SNE na danych iris)  \n\nKodlibrary(Rtsne)      # t-SNE\n\nset.seed(44)\n\n# Przygotowanie danych (jak wcześniej)\niris_data &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  as.matrix() %&gt;%\n  scale()\n\n# t-SNE\ntsne_result &lt;- Rtsne(\n  iris_data,\n  dims = 2, perplexity = 30, verbose = TRUE,\n  max_iter = 500, check_duplicates = FALSE\n)\n\nPerforming PCA\nRead the 150 x 4 data matrix successfully!\nUsing no_dims = 2, perplexity = 30.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.00 seconds (sparsity = 0.711156)!\nLearning embedding...\nIteration 50: error is 45.359701 (50 iterations in 0.01 seconds)\nIteration 100: error is 44.348491 (50 iterations in 0.01 seconds)\nIteration 150: error is 43.459291 (50 iterations in 0.01 seconds)\nIteration 200: error is 45.385876 (50 iterations in 0.01 seconds)\nIteration 250: error is 45.872962 (50 iterations in 0.01 seconds)\nIteration 300: error is 0.661337 (50 iterations in 0.01 seconds)\nIteration 350: error is 0.165504 (50 iterations in 0.01 seconds)\nIteration 400: error is 0.161554 (50 iterations in 0.01 seconds)\nIteration 450: error is 0.160806 (50 iterations in 0.01 seconds)\nIteration 500: error is 0.157935 (50 iterations in 0.01 seconds)\nFitting performed in 0.07 seconds.\n\nKodtsne_df &lt;- data.frame(\n  Dim1 = tsne_result$Y[,1],\n  Dim2 = tsne_result$Y[,2],\n  Species = iris$Species\n)\n\n# PCA na tych samych danych\npca_fit &lt;- prcomp(iris_data, center = FALSE, scale. = FALSE)\npca_df &lt;- data.frame(\n  PC1 = pca_fit$x[,1],\n  PC2 = pca_fit$x[,2],\n  Species = iris$Species\n)\n\n# Wykres t-SNE\np1 &lt;- ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"t-SNE\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Wykres PCA (pierwsze dwie składowe)\np2 &lt;- ggplot(pca_df, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#umap",
    "href": "pca.html#umap",
    "title": "Metody redukcji wymiarowości",
    "section": "UMAP",
    "text": "UMAP\nMetoda Uniform Manifold Approximation and Projection (UMAP) jest nieliniową techniką redukcji wymiarowości opracowaną przez McInnesa i Healy’ego w 2018 roku. Jej celem jest odwzorowanie danych z przestrzeni wysokowymiarowej w przestrzeń o mniejszej liczbie wymiarów przy zachowaniu struktury geometrycznej — zarówno lokalnej, jak i globalnej — poprzez modelowanie danych jako rozkładu na rozmaitości (manifold). W odróżnieniu od metody t-SNE, UMAP opiera się na teorii rozmaitości Riemanna oraz na pojęciach pochodzących z teorii zbiorów rozmytych i topologii algebraicznej.\nNiech dane wejściowe stanowią zbiór punktów \\[\nX = \\{x_1, x_2, \\dots, x_n\\}, \\quad x_i \\in \\mathbb{R}^p.\n\\] Zakłada się, że punkty te leżą na rozmaitości \\(\\mathcal{M} \\subset \\mathbb{R}^p\\) o niższym wymiarze rzeczywistym \\(d &lt; p\\), zanurzonej w przestrzeni obserwowalnej. Metoda UMAP tworzy dwie probabilistyczne reprezentacje tej rozmaitości: po pierwsze, graf sąsiedztwa w przestrzeni wysokowymiarowej (fuzzy simplicial set), który opisuje lokalne zależności między punktami, oraz po drugie, graf w przestrzeni niskowymiarowej, którego struktura ma jak najlepiej odwzorowywać pierwszy.\nW celu konstrukcji grafu w przestrzeni wejściowej dla każdego punktu \\(x_i\\) określa się odległości do jego \\(k\\)-najbliższych sąsiadów. Następnie wyznacza się dwa parametry lokalne \\[\n\\rho_i = \\min_{j: d(x_i,x_j) &gt; 0} d(x_i, x_j),\n\\] czyli najmniejszą dodatnią odległość (umożliwiającą niezerową gęstość), oraz \\(\\sigma_i &gt; 0,\\) skalę lokalną dobraną tak, aby spełniony był warunek normalizacji entropii \\[\n\\sum_{j} \\exp\\!\\left(-\\frac{\\max(0, d(x_i,x_j) - \\rho_i)}{\\sigma_i}\\right) = \\log_2(k).\n\\] Na tej podstawie definiuje się rozmyte prawdopodobieństwa sąsiedztwa \\[\np_{j|i} = \\exp\\!\\left(-\\frac{\\max(0, d(x_i, x_j) - \\rho_i)}{\\sigma_i}\\right).\n\\] Ponieważ macierz tych wartości nie jest symetryczna, łączy się oba kierunki zgodnie z zasadami teorii zbiorów rozmytych \\[\np_{ij} = p_{i|j} + p_{j|i} - p_{i|j}\\,p_{j|i}.\n\\] Tak powstały rozmyty graf sąsiedztwa zawiera wagi \\(p_{ij}\\), które odzwierciedlają siłę połączeń między punktami.\nNastępnie w przestrzeni wynikowej \\(Y = \\{y_1, y_2, \\dots, y_n\\} \\subset \\mathbb{R}^q\\), gdzie zwykle \\(q = 2\\) lub 3, definiuje się analogiczny rozmyty graf \\(q_{ij}\\), którego wagi opisuje funkcja jądra typu heavy-tailed \\[\nq_{ij} = \\frac{1}{1 + a\\,\\|y_i - y_j\\|^{2b}},\n\\] gdzie \\(a\\) i \\(b\\) są parametrami dopasowanymi empirycznie (standardowo \\(a \\approx 1.929,\\ b \\approx 0.7915\\)).\nZasadniczym celem UMAP jest znalezienie takiej konfiguracji punktów \\(Y\\), aby rozmyty graf \\(q_{ij}\\) jak najlepiej przybliżał graf \\(p_{ij}\\). Kryterium optymalizacji ma postać minimalizacji rozbieżności krzyżowej (ang. cross-entropy) między dwoma rozkładami sąsiedztwa \\[\nC = \\sum_{i &lt; j} \\left[ -p_{ij}\\log(q_{ij}) - (1 - p_{ij})\\log(1 - q_{ij}) \\right].\n\\] Minimalizacja tej funkcji jest realizowana metodami gradientowymi, zazwyczaj z wykorzystaniem stochastic gradient descent (SGD). W wyniku optymalizacji punkty \\(y_i\\) są przesuwane tak, aby utrzymać bliskie relacje w miejscach, gdzie \\(p_{ij}\\) jest duże i rozdzielać punkty, gdzie \\(p_{ij}\\) jest małe.\nZałożenia metody UMAP są stosunkowo niewielkie, lecz istotne. Zakłada się, że dane leżą na rozmaitości o niskim wymiarze, a więc można je opisać poprzez ciągłą strukturę geometryczną. Przyjmuje się również, że użyta miara odległości (zwykle euklidesowa) odzwierciedla faktyczne podobieństwo obserwacji oraz że rozkład punktów jest gładki, czyli w małych sąsiedztwach struktura jest dobrze przybliżana liniowo.\nInterpretacja wyników UMAP nie odnosi się do bezwzględnych wartości współrzędnych, lecz do relacji między punktami. Punkty położone blisko siebie w przestrzeni wynikowej są podobne w przestrzeni oryginalnej, a większe odległości odpowiadają mniejszemu podobieństwu. W przeciwieństwie do t-SNE metoda ta lepiej zachowuje nie tylko lokalne klastry, ale również częściowo strukturę globalną, co umożliwia analizę gradientów i ciągłych przejść między grupami obserwacji.\n\n\n\n\n\n\nWażne kroki przy stosowaniu UMAP\n\n\n\n\nStandaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.\nWybór liczby sąsiadów (n_neighbors) — kontroluje lokalność odwzorowania; mniejsze wartości (5–15) podkreślają lokalne struktury, większe (30–50) zachowują więcej globalnych relacji.\nUstawienie wymiaru wynikowego (n_components) — zwykle 2 lub 3, w zależności od potrzeb wizualizacji.\nWybór metryki odległości — domyślnie euklidesowa, ale można użyć innych (np. Manhattan, cosine) w zależności od charakteru danych.\nPorównanie z PCA lub t-SNE — warto sprawdzić, czy UMAP nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykład 5.5 (UMAP na danych iris)  \n\nKodlibrary(uwot)     # UMAP\n\nset.seed(44)\n\n# Przygotowanie danych: oddzielić etykiety klas i standaryzować cechy\nX &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  scale() %&gt;%\n  as.matrix()\n\ny &lt;- iris$Species\n\n# UMAP 2D: podstawowe parametry\n# n_neighbors = \"skala lokalności\", min_dist = \"zwartość klastrów\", metric = metryka odległości\nemb_umap &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist    = 0.1,\n  metric      = \"euclidean\",\n  n_components = 2,\n  verbose = TRUE\n)\n\n# Ramka wynikowa do wykresu\ndf_umap &lt;- data.frame(\n  UMAP1 = emb_umap[, 1],\n  UMAP2 = emb_umap[, 2],\n  Species = y\n)\n\n# Dla porównania: PCA 2D (opcjonalnie)\npca &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  PC1 = pca$x[, 1],\n  PC2 = pca$x[, 2],\n  Species = y\n)\n\n# Wykresy\np_umap &lt;- ggplot(df_umap, aes(x = UMAP1, y = UMAP2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"UMAP\",\n       x = \"UMAP1\", y = \"UMAP2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np_pca &lt;- ggplot(df_pca, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Wyświetlenie obok siebie\np_umap | p_pca\n\n\n\n\n\n\n\n\nPoniżej prezentuję zbiorcze porównanie wszystkich omówionych metod redukcji wymiarowości na tym samym zbiorze danych iris. Wykorzystuję PCA, ICA, MDS, t-SNE oraz UMAP, aby zobaczyć, jak różne techniki odwzorowują strukturę danych.\n\nKodset.seed(44)\n\n# Usuń duplikaty (t-SNE i MDS niemetryczny są na to wrażliwe)\niris_unique &lt;- iris[!duplicated(iris[, -5]), ]\nX &lt;- as.matrix(scale(iris_unique[, -5]))\ny &lt;- iris_unique$Species\n\n# PCA (2 pierwsze składowe)\npca_fit &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  Dim1 = pca_fit$x[, 1],\n  Dim2 = pca_fit$x[, 2],\n  Method = \"PCA\",\n  Species = y\n)\n\n# ICA (2 komponenty niezależne)\nica_fit &lt;- fastICA(X, n.comp = 2, method = \"C\")\ndf_ica &lt;- data.frame(\n  Dim1 = ica_fit$S[, 1],\n  Dim2 = ica_fit$S[, 2],\n  Method = \"ICA\",\n  Species = y\n)\n\n# MDS metryczny (klasyczny)\nmds_metric &lt;- cmdscale(dist(X), k = 2)\ndf_mds_metric &lt;- data.frame(\n  Dim1 = mds_metric[, 1],\n  Dim2 = mds_metric[, 2],\n  Method = \"MDS (metryczny)\",\n  Species = y\n)\n\n# MDS niemetryczny (isoMDS)\nmds_nonmetric &lt;- isoMDS(dist(X), k = 2)$points\n\ninitial  value 4.818373 \nfinal  value 4.818117 \nconverged\n\nKoddf_mds_nonmetric &lt;- data.frame(\n  Dim1 = mds_nonmetric[, 1],\n  Dim2 = mds_nonmetric[, 2],\n  Method = \"MDS (niemet.)\",\n  Species = y\n)\n\n# t-SNE\ntsne_fit &lt;- Rtsne(\n  X, dims = 2, perplexity = 30,\n  max_iter = 750, check_duplicates = FALSE, verbose = FALSE\n)\ndf_tsne &lt;- data.frame(\n  Dim1 = tsne_fit$Y[, 1],\n  Dim2 = tsne_fit$Y[, 2],\n  Method = \"t-SNE\",\n  Species = y\n)\n\n# UMAP\numap_emb &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist = 0.1,\n  metric = \"euclidean\",\n  n_components = 2,\n  verbose = FALSE\n)\ndf_umap &lt;- data.frame(\n  Dim1 = umap_emb[, 1],\n  Dim2 = umap_emb[, 2],\n  Method = \"UMAP\",\n  Species = y\n)\n\n# Połączenie wszystkich metod\ndf_all &lt;- bind_rows(\n  df_pca,\n  df_ica,\n  df_mds_metric,\n  df_mds_nonmetric,\n  df_tsne,\n  df_umap\n)\n\n# Wykres porównawczy\nggplot(df_all, aes(Dim1, Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  facet_wrap(~ Method, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Porównanie metod redukcji wymiarowości na zbiorze iris\",\n    x = \"Wymiar 1\", y = \"Wymiar 2\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nComon, Pierre. 1994. „Independent Component Analysis, A New Concept?” Signal Processing 36 (3): 287–314. https://doi.org/10.1016/0165-1684(94)90029-9.\n\n\nKruskal, J. B. 1964. „Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis”. Psychometrika 29 (1): 1–27. https://doi.org/10.1007/bf02289565.\n\n\nMaaten, Laurens van der, i Geoffrey Hinton. 2008. „Visualizing Data using t-SNE”. Journal of Machine Learning Research 9 (86): 2579–2605. http://jmlr.org/papers/v9/vandermaaten08a.html.\n\n\nPearson, Karl. 1901. „LIII. On Lines and Planes of Closest Fit to Systems of Points in Space”. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72. https://doi.org/10.1080/14786440109462720.\n\n\nTorgerson, Warren S. 1952. „Multidimensional Scaling: I. Theory and Method”. Psychometrika 17 (4): 401–19. https://doi.org/10.1007/bf02288916.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#pcapearson1901",
    "href": "pca.html#pcapearson1901",
    "title": "Metody redukcji wymiarowości",
    "section": "",
    "text": "Matematyczna definicja modelu\nPunktem wyjścia analizy głównych składowych jest problem odwzorowania wielowymiarowego zbioru danych w przestrzeni o mniejszej liczbie wymiarów przy możliwie minimalnej stracie informacji. W praktyce dąży się do kompresji i odszumiania sygnału, usuwania współliniowości, stabilizacji dalszych modeli (np. regresji), a także do wizualizacji struktur klasowych i gradientów zmienności. Przykładowo, dla dwóch silnie skorelowanych cech pierwsza składowa główna jest skierowana wzdłuż linii największego rozrzutu (blisko prostej \\(y \\approx x\\)), a redukcja do jednego wymiaru zachowuje większą część wariancji niż dowolna inna projekcja.\nMatematyczna definicja poprzez maksymalizację wariancji i dekompozycję spektralną polega na transformacji scentralizowanej macierzy danych \\(X \\in \\mathbb{R}^{n\\times p}\\) (każdą kolumnę odjąć o jej średnią). Niech \\(\\Sigma=\\frac{1}{n-1}X^\\top X\\) oznacza empiryczną macierz kowariancji. Pierwszą składową wyznaczamy jako kierunek \\(w\\in\\mathbb{R}^{p}\\) rozwiązujący zadanie maksymalizacji wariancji projekcji, czyli maksymalizacji \\(\\mathrm{Var}(Xw)=w^\\top\\Sigma w\\) przy ograniczeniu \\(\\|w\\|_{2}=1\\). Zastosowanie mnożników Lagrange’a prowadzi do warunku stacjonarności \\(\\Sigma w=\\lambda w\\), a więc \\(w\\) jest wektorem własnym \\(\\Sigma\\), zaś \\(\\lambda\\) jest odpowiadającą mu wartością własną. Wybieramy największą wartość własną \\(\\lambda_{1}\\) i jej wektor \\(w_{1}\\), wówczas wariancja pierwszych wyników projekcji \\(z_{1}=Xw_{1}\\) równa się \\(\\lambda_{1}\\). Kolejne składowe otrzymujemy analogicznie jako rozwiązania tego samego problemu z dodatkowymi ograniczeniami ortogonalności \\(w_{j}^\\top w_{k}=0\\) dla \\(k&lt;j\\), co ustawia kolejne wektory własne \\(\\Sigma\\) w porządku malejących wartości własnych \\(\\lambda_{1}\\ge \\lambda_{2}\\ge \\dots \\ge \\lambda_{p}\\). Wektor wyników projekcji \\(z_{j}\\) nazywamy w praktyce scores, a \\(w_{j}\\) — wektorem ładunków (loadings). Kumulatywny udział wariancji wyjaśnianej przez pierwsze \\(k\\) składowych wynosi wówczas \\(\\sum_{j=1}^{k}\\lambda_{j}\\big/\\sum_{j=1}^{p}\\lambda_{j}\\) i służy na często do doboru \\(k\\).\nRównoważne wyprowadzenie modelu przez rozkład na wartości osobliwe, czyli SVD (ang. Singular Value Decomposition), opiera się na faktoryzacji \\(X=UDV^\\top\\), gdzie \\(U\\in\\mathbb{R}^{n\\times r}\\) i \\(V\\in\\mathbb{R}^{p\\times r}\\) mają ortonormalne kolumny, \\(D=\\mathrm{diag}(d_{1},\\dots,d_{r})\\) zawiera uporządkowane wartości osobliwe \\(d_{1}\\ge \\dots \\ge d_{r}&gt;0\\), a \\(r=\\mathrm{rank}(X)\\). Wówczas kolumny \\(V\\) pokrywają się (co do znaku) z wektorami ładunków \\(w_{j}\\), zaś macierz wyników projekcji \\(T=XV\\) równa się \\(UD\\). Związek między oboma podejściami jest ścisły: \\(\\lambda_{j}=d_{j}^{2}/(n-1)\\), a więc wariancje składowych odwzorowuje się przez kwadraty wartości osobliwych przeskalowane czynnikiem \\(1/(n-1)\\). Projekcja do \\(k\\) wymiarów przyjmuje wówczas postać \\(X\\mapsto T_{k}=UD_{k}\\), a rekonstrukcja rzędu \\(k\\) ma postać \\[\nX_{k}=T_{k}V_{k}^\\top=U_{k}D_{k}V_{k}^\\top.\n\\] Z twierdzenia Eckarta–Younga–Mirsky’ego wynika, że \\(X_{k}\\) minimalizuje błąd Frobeniusa \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\) o rządzie co najwyżej \\(k\\), czyli PCA daje najlepszą aproksymację niskorangową w sensie średniokwadratowym (jest to tzw. obcięte SVD). Ta równoważność łączyć dwie intuicje: maksymalizacja przechwyconej wariancji i minimalizacja błędu rekonstrukcji.\n\nTwierdzenie 5.1 (Twierdzenie Eckarta–Younga–Mirsky) Niech \\(X\\in\\mathbb{R}^{n\\times p}\\) i niech \\(r=\\mathrm{rank}(X)\\). Dla \\(k&lt;r\\) niech \\(X_{k}=U_{k}D_{k}V_{k}^\\top\\) będzie obciętym rozkładem SVD rzędu \\(k\\). Wówczas \\(X_{k}\\) jest jedyną macierzą o \\(\\mathrm{rank}(X_{k})=k\\), która minimalizuje błąd Frobeniusa1 \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\in\\mathbb{R}^{n\\times p}\\) o \\(\\mathrm{rank}(Y)\\le k\\). Ponadto zachodzi równość \\(\\|X-X_{k}\\|_{F}^{2}=\\sum_{j=k+1}^{r}d_{j}^{2}\\).\n1 Błąd Frobeniusa \\(\\|A\\|_{F}\\) macierzy \\(A\\) definiujemy jako \\(\\|A\\|_{F}=\\sqrt{\\sum_{i,j}a_{ij}^{2}}=\\sqrt{\\mathrm{tr}(A^\\top A)}.\\)\nZadania optymalizacyjne wyraża się zarówno w wersji wektorowej, jak i macierzowej. Dla pierwszej składowej rozwiązujemy problem maksymalizacji \\(w^\\top\\Sigma\\) w przy \\(\\|w\\|_{2}=1\\), co prowadzi do największej wartości własnej. Dla \\(k\\) składowych poszukujemy macierzy \\(W\\in\\mathbb{R}^{p\\times k}\\) o kolumnach ortonormalnych, która maksymalizuje \\(\\mathrm{tr}(W^\\top\\Sigma W)\\), skąd wynika wybór \\(k\\) wektorów własnych \\(\\Sigma\\). Równoważnie, szukamy projekcji \\(P=WW^\\top\\) minimalizującej błąd rekonstrukcji \\(\\|X-XWW^\\top\\|_{F}^{2}\\). W notacji SVD rozwiązanie ma postać \\(W=V_{k}\\), a więc projekcja działa przez mnożenie przez \\(V_{k}V_{k}^\\top\\).\nGdy cechy mierzymy w różnych jednostkach i skalach, zaleca się stosować macierz korelacji zamiast kowariancji, co jest równoważne standaryzacji kolumn \\(X\\) do wariancji 1. Wiele implementacji (np. w R funkcja prcomp) wykorzystuje SVD na scentralizowanych i ewentualnie standaryzowanych danych, co zapewniać numeryczną stabilność, zwłaszcza gdy \\(p\\gg n\\). W sytuacji \\(p\\gg n\\) korzystniejsze bywa liczenie mniejszych rozkładów: albo dual PCA2 na macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\), albo bezpośrednio obciętego SVD. Dla danych zaburzonych wartościami odstającymi rozważa się wersje odporne, np. zastępuje się \\(\\Sigma\\) estymatorem odpornym (ang. Minimum Covariance Determinant, MCD)3 lub stosuje się robust PCA i dekompozycje oparte na normie jądra i normie \\(L_{1}\\)4.\n2 Wówczas wektory własne \\(\\tilde{w}_{j}\\) macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\) (która jest niższego wymiaru niż \\(X^\\top X\\) a co za tym idzie lepiej się zachowuje numerycznie) przekształca się w wektory własne \\(\\Sigma\\) przez \\(w_{j}=X^\\top \\tilde{w}_{j}/\\sqrt{(n-1)\\tilde{\\lambda}_{j}}\\), gdzie \\(\\tilde{\\lambda}_{j}\\) jest odpowiadającą wartością własną.3 Estymator MCD polega na znalezieniu podzbioru \\(h\\) obserwacji (zwykle \\(h \\approx 0.75 n\\)) o najmniejszym wyznaczniku macierzy kowariancji, a następnie obliczeniu średniej i kowariancji na tym podzbiorze. Jest odporny na wartości odstające, ponieważ ignoruje obserwacje, które znacznie zwiększają wyznacznik.4 Metoda ta zakłada, że macierz danych \\(X\\) ma postać \\(X=L+S+E\\), gdzie \\(L\\) jest macierzą niskorangową (sygnał), \\(S\\) jest macierzą rzadką (wartości odstające), a \\(E\\) jest szumem o małej wariancji. Celem jest odzyskanie \\(L\\) poprzez minimalizację funkcji celu \\(\\|L\\|_{*}+\\lambda\\|S\\|_{1}\\) przy ograniczeniu \\(X=L+S\\), gdzie \\(\\|L\\|_{*}\\) jest normą jądra (suma wartości osobliwych \\(L\\)), a \\(\\|S\\|_{1}\\) jest normą \\(L_{1}\\) macierzy \\(S\\).Podsumowując, PCA można sformułować trojako: jako maksymalizację wariancji projekcji przy ograniczeniach ortogonalności, jako dekompozycję spektralną macierzy kowariancji oraz jako obcięte SVD zapewniające najlepszą aproksymację niskorangową.\nZałożenia modelu\nZałożenia dotyczące danych wejściowych do analizy głównych składowych (PCA) są stosunkowo słabe, ale mają istotny wpływ na jakość wyników i interpretację. Można je podzielić na kilka grup:\n\nStruktura danych\n\nLiniowość – PCA zakłada, że główne wzorce zmienności w danych można uchwycić przez liniowe kombinacje zmiennych wejściowych. Jeśli zależności są silnie nieliniowe (np. dane leżą na zakrzywionej rozmaitości), PCA nie odwzoruje ich poprawnie – lepiej wtedy stosować kernel PCA albo metody sąsiedztwa (np. t-SNE, UMAP).\nWspółzależność zmiennych – metoda ma sens tylko wtedy, gdy między cechami istnieją korelacje. Jeśli wszystkie zmienne są niezależne, PCA nie zredukuje wymiarów i każda składowa odpowiadać będzie jednej zmiennej.\n\n\nJednostki i skale pomiarowe\n\nPCA jest wrażliwa na skalę zmiennych, ponieważ opiera się na wariancji.\nJeśli cechy mierzone są w różnych jednostkach (np. temperatura w °C i masa w kg), należy je standaryzować (np. do średniej 0 i wariancji 1).\nGdy wszystkie cechy są w tej samej skali, można pracować na macierzy kowariancji; w przeciwnym razie lepiej korzystać z macierzy korelacji.\n\n\nRozkład danych\n\nNormalność wielowymiarowa nie jest wymagana, ale jeżeli dane mają rozkład wielowymiarowo normalny, to składowe główne są niezależne (nie tylko nieskorelowane), co upraszcza interpretację. Naruszenie założenia o normalności nie sprawia, że PCA nie działa, lecz niezależność składowych nie jest zagwarantowana.\nBrak wartości odstających – PCA jest bardzo wrażliwa na outliery, które mogą wpłynąć na kierunki głównych składowych, bo opiera się na kowariancji. Dlatego dane powinny być oczyszczone lub należy stosować wersje metody odporne (robust PCA).\n\n\nLiczebność próby\n\nAby oszacować macierz kowariancji, liczba obserwacji \\(n\\) powinna być odpowiednio duża względem liczby zmiennych \\(p\\).\nGdy \\(p \\gg n\\), klasyczna PCA bywa niestabilna i stosuje się wtedy dual PCA albo obcięte SVD.\n\n\nBraki danych - PCA wymaga pełnej macierzy danych (bez braków). W przypadku braków stosuje się najczęściej imputację (np. metodą średnich czy metody oparte na modelach).\nInterpretacja graficzna i praktyczna\n\nKod# Pakiety\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(scales)\n\nset.seed(44)\n\n# 1) Dane 2D o eliptycznym rozkładzie (silna współzmienność)\nn  &lt;- 300\nmu &lt;- c(0, 0)\nsd1 &lt;- 2\nsd2 &lt;- 1\nrho &lt;- 0.8\nSigma &lt;- matrix(c(sd1^2, rho*sd1*sd2,\n                  rho*sd1*sd2, sd2^2), nrow = 2, byrow = TRUE)\n\nX &lt;- MASS::mvrnorm(n, mu = mu, Sigma = Sigma) %&gt;%\n  as_tibble(.name_repair = ~c(\"x1\",\"x2\"))\n\n# 2) PCA na danych scentralizowanych (bez standaryzacji)\npca &lt;- prcomp(X, center = TRUE, scale. = FALSE)\n\n# Wartości własne i wektory (ładunki)\nlambda &lt;- pca$sdev^2\nV &lt;- pca$rotation    # kolumny: PC1, PC2\ncenter &lt;- colMeans(X)\n\n# 3) Punkty końcowe wektorów PC1 i PC2 (skalować długością ~ odchylenie wzdłuż składowej)\n# Skala wektora: k * sd wzdłuż danej składowej (tu k = 2 dla czytelności)\nk &lt;- 2\npc1_end &lt;- center + k * pca$sdev[1] * V[,1]\npc2_end &lt;- center + k * pca$sdev[2] * V[,2]\n\n# 4) Ramy wykresu i linie osi oryginalnego układu\nxr &lt;- range(X$x1); yr &lt;- range(X$x2)\n\n# 5) Dane pomocnicze do geometrii\narrows_df &lt;- tribble(\n  ~x,          ~y,          ~xend,        ~yend,     ~label,\n  center[1],   center[2],   pc1_end[1],   pc1_end[2], \"PC1\",\n  center[1],   center[2],   pc2_end[1],   pc2_end[2], \"PC2\"\n)\n\n# Opisy udziału wariancji\nexpl &lt;- percent(lambda / sum(lambda), accuracy = 0.1)\n\n# 6) Wykres\nggplot(X, aes(x = x1, y = x2)) +\n  # chmura punktów\n  geom_point(alpha = 0.5, size = 1.6) +\n  # elipsa rozrzutu (1 odchylenie standardowe ~ poziom 0.68)\n  stat_ellipse(type = \"norm\", level = 0.68, linewidth = 0.8) +\n  # oryginalne osie układu współrzędnych (przez (0,0))\n  geom_hline(yintercept = 0, linetype = 3, linewidth = 0.5) +\n  geom_vline(xintercept = 0, linetype = 3, linewidth = 0.5) +\n  # wektory składowych głównych (wychodzące ze środka danych)\n  geom_segment(data = arrows_df,\n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")),\n               linewidth = 1) +\n  # etykiety PC z udziałem wariancji\n  geom_text(data = arrows_df %&gt;%\n              mutate(txt = ifelse(label==\"PC1\",\n                                  paste0(\"PC1 (\", expl[1], \")\"),\n                                  paste0(\"PC2 (\", expl[2], \")\"))),\n            aes(x = xend, y = yend, label = txt),\n            nudge_x = 0.05, nudge_y = 0.05, hjust = 0, vjust = 0,\n            size = 3.5) +\n  # punkt środka\n  geom_point(aes(x = center[1], y = center[2]), color = \"black\", size = 2) +\n  coord_fixed() +\n  labs(x = \"x1 (oś oryginalna)\",\n       y = \"x2 (oś oryginalna)\",\n       title = \"Oryginalne osie, dane oraz dwie składowe główne (2D)\",\n       subtitle = paste0(\"Udział wariancji: PC1 = \", expl[1], \", PC2 = \", expl[2])) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"plain\"),\n        plot.subtitle = element_text(face = \"plain\"))\n\n\n\n\n\n\n\nDla dwóch wymiarów elipsa rozrzutu danych ma osie ustawione dokładnie wzdłuż \\(w_{1}\\) i \\(w_{2}\\), a ich długości proporcjonalne do \\(\\sqrt{\\lambda_{1}}\\) i \\(\\sqrt{\\lambda_{2}}\\). Transformacja do przestrzeni składowych odpowiada obrotowi układu współrzędnych tak, by oś \\(X_{1}'\\) leżała w kierunku największego rozrzutu, a \\(X_{2}'\\) — w kierunku pozostałej zmienności. Projekcja do \\(k&lt;p\\) wymiarów działa jak rzut ortogonalny na podprzestrzeń rozpiętą przez pierwsze \\(k\\) osi i „spłaszczenie” w pominiętych kierunkach, co minimalizuje błąd rekonstrukcji w sensie średniokwadratowym. Wykresy scores prezentują obiekty w przestrzeni składowych głównych i często ujawniają skupiska lub obserwacje odstające. Wektory loadings są przedstawiane na tzw. kole korelacji, gdzie końce strzałek leżą na okręgu jednostkowym, a ich długości i kąty odzwierciedlają korelacje zmiennych oryginalnych ze składowymi. Zmienne wskazujące podobne kierunki tworzą grupy, co pomaga rozumieć współzmienność. Wykres biplot łączy obie perspektywy: punkty obiektów i kierunki zmiennych w tej samej płaszczyźnie, dzięki czemu można podejrzeć jednocześnie relacje między obiektami i kontrybucje cech. Dodatkowo wykres udziału wariancji, czyli scree plot, porządkuje \\(\\lambda_{j}\\) i pomagać wyznaczyć \\(k\\) przez identyfikację „łokcia” krzywej lub przez osiągnięcie założonego poziomu wariancji kumulatywnej.\nŁadunek \\(w_{jk}\\) to współczynnik liniowej kombinacji \\(j\\)-tej składowej dla \\(k\\)-tej zmiennej; jego znak i wartość bezwzględna informują o kierunku i sile związku. Korelację zmiennej z \\(j\\)-tą składową szacujemy jako cosinus kąta między wektorem zmiennej a osią składowej na kole korelacji; duże wartości sugeruję dużą kontrybucję tej cechy do składowej. Rekonstrukcja obiektu \\(i\\)-tego z \\(k\\) składowych ma postać \\(\\hat{x}_{i}=\\sum_{j=1}^{k} t_{ij} \\, w_{j}^\\top\\), co pozwala na analizę błędów rekonstrukcji i odszumianie przez odcięcie składowych o małych \\(d_{j}\\). W regresji, gdy predyktory są współliniowe, stosuje się regresję na składowych głównych albo regresję grzbietową w przestrzeni scores, co poprawia stronę obliczeniową i zmniejszać wariancję estymatorów.\nKryteria doboru liczby składowych głównych\nDobór liczby składowych głównych (\\(k\\)) jest jednym z kluczowych etapów analizy PCA, ponieważ decyduje o tym, ile informacji (wariancji) zostanie zachowane przy redukcji wymiarowości. Zbyt mała liczba składowych prowadzi do utraty istotnych informacji, a zbyt duża – do utrzymania szumu i nadmiarowej redundancji. W praktyce stosuje się zestaw kryteriów ilościowych i jakościowych, które można podzielić na kilka grup.\nKryteria oparte na wariancji wyjaśnianej\nNajbardziej klasyczne podejście polega na analizie udziału wariancji przechwyconej przez pierwsze \\(k\\) składowych. Dla każdej składowej liczy się wartość własną \\(\\lambda_j\\) macierzy kowariancji, a udział wariancji wyjaśnianej przez pierwsze \\(k\\) składowych to \\[\n\\eta(k) = \\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n\\] Stosowane reguły:\n\nReguła progu wariancji - wybiera się najmniejsze \\(k\\), dla którego \\(\\eta(k)\\) przekracza ustalony próg, np. 80%, 90% lub 95% (w literaturze nie ma jednego progu). Gdy dane silnie skorelowane – wystarczą 2–3 składowe, a gdy dane są bardziej złożone – potrzeba więcej (5–10 i więcej).\nWykres osypiska (scree plot) – wykres wartości własnych \\(\\lambda_j\\) uporządkowanych malejąco. Wybiera się punkt, w którym tempo spadku gwałtownie maleje („łokieć krzywej”).\nWskaźnik udziału marginalnego - \\(\\Delta \\eta_j = \\eta(j) - \\eta(j-1)\\). Gdy przyrost staje się znikomy, dalsze składowe nie wnoszą istotnej informacji.\nKryteria algebraiczne\nKryterium wartości własnej (Kaisera–Guttmana) oparte jest na macierzy korelacji, które mówi, że zachowuje się tylko te składowe, których wartości własne \\(\\lambda_j &gt; 1\\). Oznacza to, że dana składowa wyjaśnia więcej wariancji niż pojedyncza standaryzowana zmienna. Reguła ta jest prosta, ale często zbyt konserwatywna (tendencja do wyboru zbyt wielu składowych).\nKryteria statystyczne i walidacyjne\n\nAnaliza równoległa (Parallel Analysis) - polega na porównaniu wartości własnych uzyskanych z danych rzeczywistych z wartościami własnymi uzyskanymi z wielu symulowanych zestawów danych o tych samych wymiarach, ale z losowym szumem. Zachowuje się tylko te składowe, których wartości własne przekraczają średnią (lub kwantyl) z rozkładu symulowanego. Ta metoda ogranicza ryzyko wyboru składowych wynikających z przypadku.\nWalidacja krzyżowa (Cross-Validation) - gdy PCA wykorzystuje się w kontekście modelowania predykcyjnego (np. PCA regression). Wybiera się takie \\(k\\), które minimalizuje błąd predykcji (np. RMSE) obliczany metodą walidacji krzyżowej.\n\nBartlett’s Test of Sphericity sprawdza, czy korelacje są wystarczająco silne, by PCA miała sens.\n\nBroken Stick Model porównuje udział wariancji każdej składowej z oczekiwaną wartością przy losowym rozkładzie wariancji – zachowuje się tylko te składowe, które przekraczają tę wartość (\\(E_k=\\frac{1}{p}\\sum_{j=k}^{p}\\frac{1}{j}\\)).\nKryteria interpretacyjne i dziedzinowe\nCzasami najważniejszy jest nie wynik numeryczny, lecz użyteczność interpretacyjna:\n\nWybiera się tyle składowych, ile da się sensownie zinterpretować (np. odpowiadających znanym procesom fizycznym, ekonomicznym, biologicznym).\nW analizie wizualnej (np. w eksploracji danych) często wybiera się 2 lub 3 pierwsze składowe, które umożliwiają wykresy 2D lub 3D.\n\n\n\n\n\n\n\n\n\nKryterium\nOpis\nZalety\nOgraniczenia\n\n\n\nUdział wariancji (np. ≥90%)\nZachowaj tyle składowych, by wyjaśnić określony procent całkowitej wariancji\nProste i intuicyjne\nWybór progu bywa arbitralny\n\n\nWykres osypiska (scree plot)\nWybór punktu „kolana” na krzywej wartości własnych\nWizualnie czytelne\nSubiektywne, zależy od interpretacji obserwatora\n\n\nWartość własna &gt; 1 (Kaiser–Guttman)\nZachowaj składowe, których wartości własne przekraczają 1 (dla macierzy korelacji)\nŁatwe obliczeniowo\nCzęsto zbyt liberalne – wybiera zbyt wiele składowych\n\n\nAnaliza równoległa (Parallel Analysis)\nPorównanie wartości własnych z rozkładem uzyskanym z danych losowych\nStatystycznie uzasadnione, ogranicza wybór przypadkowych komponentów\nWymaga symulacji lub dedykowanego oprogramowania\n\n\nWalidacja krzyżowa (Cross-Validation)\nWybór liczby składowych minimalizującej błąd predykcji (np. RMSE)\nNajlepsza w kontekście modeli predykcyjnych\nKosztowna obliczeniowo, wymaga podziału danych\n\n\nModel Broken Stick\nPorównanie udziału wariancji składowych z oczekiwanym rozkładem losowym\nUzasadnione teoretycznie, ogranicza przeuczenie\nMniej intuicyjne, rzadziej używane\n\n\nKryterium interpretacyjne\nWybór liczby składowych możliwych do sensownej interpretacji\nPraktyczne i kontekstowe\nSubiektywne i zależne od wiedzy dziedzinowej\n\n\n\n\nPrzykład 5.1 (PCA na danych irysów)  \n\nKodlibrary(factoextra)\nlibrary(easystats)\nlibrary(gt)\n\npca_iris &lt;- prcomp(iris[,-5], center = TRUE, scale. = TRUE) \n# albo\npca &lt;- principal_components(iris, n = 4, rotate = \"none\") # domyślnie standaryzuje zmienne\n\n# Wykres osypiska\nfviz_eig(pca_iris, addlabels = TRUE)\n\n\n\n\n\n\n\nJak widać z powyższego wykresu osypiska pierwsza składowa wyjaśnia około 73% całkowitej wariancji, a druga 23%. Jeśli chcieć opierać wybór liczby składowych głównych na kryteriach (również takich, które nie były prezentowane powyżej), to można użyć funkcji n_components() pakietu parameters w ekosystemie easystats.\n\nKodk &lt;- n_components(iris[,-5])\nas.data.frame(k)\n\n   n_Factors              Method       Family\n1          0          Scree (R2)     Scree_SE\n2          1             Bentler      Bentler\n3          1 Optimal coordinates        Scree\n4          1 Acceleration factor        Scree\n5          1   Parallel analysis        Scree\n6          1    Kaiser criterion        Scree\n7          1       Velicer's MAP Velicers_MAP\n8          2          Scree (SE)     Scree_SE\n9          2    VSS complexity 1          VSS\n10         2    VSS complexity 2          VSS\n11         3            Bartlett      Barlett\n12         3            Anderson      Barlett\n13         3              Lawley      Barlett\n\nKodplot(k)\n\n\n\n\n\n\n\nChoć większość kryteriów wskazuje na 1 składową, to na potrzeby przykładu wykorzystamy dwie składowe. Wyjaśniają one blisko 96% całkowitej wariancji (patrz poniżej). Możemy teraz przejrzeć wyniki PCA, czyli macierz ładunków (wektorów własnych) i macierz wyników projekcji (scores).\n\nKod# Udział wariancji\nsummary(pca_iris)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\nKod# Ładunki (wektory własne)\npca_iris$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\nKod# scores\nhead(pca_iris$x)\n\n           PC1        PC2         PC3          PC4\n[1,] -2.257141 -0.4784238  0.12727962  0.024087508\n[2,] -2.074013  0.6718827  0.23382552  0.102662845\n[3,] -2.356335  0.3407664 -0.04405390  0.028282305\n[4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n[5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n[6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\nPierwsza składowa główna (PC1) jest kombinacją liniową wszystkich czterech zmiennych, przy czym trzy z nich — Sepal.Length, Petal.Length oraz Petal.Width — mają dodatnie ładunki, natomiast Sepal.Width ma ładunek ujemny. Oznacza to, że składowa ta rośnie, gdy długość działki kielicha oraz długość i szerokość płatków są duże, a maleje, gdy szerokość działki jest duża. Można zatem interpretować PC1 jako wymiar opisujący ogólny rozmiar kwiatu: kwiaty o większych płatkach i węższych działkach uzyskują wyższe wartości tej składowej. W zbiorze iris PC1 bardzo dobrze rozdziela gatunki – setosa charakteryzuje się niskimi wartościami tej składowej (krótkie płatki, szerokie działki), natomiast versicolor i virginica mają wartości wysokie, co odpowiada większym rozmiarom kwiatów.\nDruga składowa główna (PC2) ma zupełnie inną strukturę ładunków. Zdominowana jest przez bardzo silny ujemny współczynnik dla Sepal.Width oraz mniejszy, również ujemny, dla Sepal.Length. Wpływ płatków na tę składową jest niewielki. PC2 odzwierciedla zatem zmienność w obrębie kształtu działki kielicha, a zwłaszcza jej proporcji długości do szerokości. Kwiaty o węższych działkach mają wyższe wartości PC2, natomiast te o szerszych – niższe.\nInterpretując wspólnie obie składowe, można stwierdzić, że PC1 opisuje rozmiar kwiatu, natomiast PC2 – proporcje i kształt działki. W przestrzeni PC1–PC2 dane tworzą układ, w którym setosa jest wyraźnie oddzielona od pozostałych gatunków poprzez niskie wartości PC1 i wysokie PC2, a versicolor i virginica różnią się między sobą głównie wzdłuż drugiej osi. W rezultacie te dwie składowe pozwalają na niemal pełne odwzorowanie i wizualne rozdzielenie gatunków, przy czym PC1 odpowiada za wymiar wielkościowy, a PC2 – za wymiar kształtowy. Na potrzeby wizualizacji możemy narysować wykres biplot łączący obiekty i zmienne w przestrzeni dwóch pierwszych składowych.\n\nKodfviz_pca_biplot(pca_iris, repel = TRUE,\n                col.var = \"blue\", # kolor zmiennych\n                col.ind = iris$Species) + # kolor obiektów wg gatunku\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#icacomon1994",
    "href": "pca.html#icacomon1994",
    "title": "Metody redukcji wymiarowości",
    "section": "ICA(Comon 1994)\n",
    "text": "ICA(Comon 1994)\n\nPodstawowy model ICA (ang. Independent Component Analysis) zakłada, że wektor obserwacji \\(X \\in \\mathbb{R}^p\\) powstaje poprzez liniowe i natychmiastowe wymieszanie wektora ukrytych źródeł \\(s \\in \\mathbb{R}^m\\) o statystycznie niezależnych składowych \\[\nX = A s,\\qquad A \\in \\mathbb{R}^{p\\times m},\n\\] przy czym \\(m \\le \\min(p,n)\\) oraz macierz mieszająca \\(A\\) ma pełny rząd. Celem jest oszacowanie macierzy demiksującej \\(W \\in \\mathbb{R}^{m\\times p}\\) tak, aby \\(y = W X\\) aproksymować \\(s\\) składowymi możliwie niezależnymi w sensie probabilistycznym. Z istoty problemu rozwiązanie identyfikowalne jest jedynie do permutacji i skalowania - kolejność oraz skale (a więc i znaki) składowych nie są odzyskiwalne.\nWyprowadzenie algorytmów ICA rozpoczynamy od scentralizowania danych i ich whiteningu. Niech \\(\\Sigma_X = \\tfrac{1}{n}\\sum_i (X_i-\\bar X)(X_i-\\bar X)^\\top\\) oraz niech \\(V\\) oznacza macierz whitening taką, że \\(Z=V(X-\\bar X)\\) spełnia \\(\\operatorname{Cov}(Z)=I_p\\). W praktyce przyjmujemy \\(V=\\Lambda^{-1/2}U^\\top\\) z dekompozycji \\(\\Sigma_X=U\\Lambda U^\\top\\). W przestrzeni whitened model przyjmuje postać \\[\nZ = V A s \\equiv R\\, s,\n\\] gdzie \\(R\\) jest macierzą ortogonalną (dla przypadku \\(m=p\\)). Poszukujemy więc wektorów w jednostkowej normie, dla których skalarna projekcja \\(y=w^\\top Z\\) jest możliwie „nienormalna” (niesymetryczna lub ciężkoogonowa), co stanowi praktyczne kryterium niezależności.\n\nKod# Pakiety\nlibrary(fastICA)\nlibrary(patchwork)\n\nset.seed(44)\n\n# 1) Generowanie dwóch niezależnych źródeł (niegaussowskich)\nn &lt;- 3000\ns1 &lt;- rexp(n, rate = 1) - 1  # Jednostronnie cięższy ogon (Eksponencjalny przesunięty do zera średniej)\ns2 &lt;- runif(n, -2, 2)        # Równomierny (płaskie ogony)\nS  &lt;- cbind(s1, s2)\nS  &lt;- scale(S, center = TRUE, scale = FALSE) # zero-mean dla wygody\n\n# 2) Mieszanie źródeł macierzą A -&gt; obserwacje X\nA &lt;- matrix(c(1, 2,\n              2, 1), nrow = 2, byrow = TRUE)\nX &lt;- S %*% t(A)                        # model X = S A^T\nX &lt;- scale(X, center = TRUE, scale = FALSE)  # centrowanie (odjęcie średniej kolumn)\n\n# 3) Whitening (sferyzacja): Z = V X, gdzie V = Λ^{-1/2} U^T z dekompozycji Σ_X\nSigmaX &lt;- cov(X)\ne &lt;- eigen(SigmaX)\nU &lt;- e$vectors\nLambda &lt;- diag(e$values)\nV &lt;- solve(sqrt(Lambda)) %*% t(U)     # Λ^{-1/2} U^T\nZ &lt;- t(V %*% t(X))                    # Z = V X (w wierszach obserwacje)\n\n# Kontrola: kowariancja Z ~ I\nround(cov(Z), 3)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKod# 4) ICA (FastICA) na danych whitened (można też na X bo fastICA automatycznie wykonuje whitening)\nica &lt;- fastICA(Z, n.comp = 2, method = \"C\")  # odzyskane źródła Y i mieszanie A_ICA\nY &lt;- ica$S                                   # szacowane źródła niezależne (kolumny ~ komponenty)\nW &lt;- ica$K %*% ica$W                         # łączne \"demiksowanie\" względem X (tu pracowaliśmy na Z)\n# Uwaga: fastICA zwraca też K (whitening) i W (unmixing), składnia zależy od wejścia\n\n# 5) Przygotowanie danych do wykresów\nto_df &lt;- function(M, name){\n  as.data.frame(M) |&gt;\n    setNames(c(\"c1\",\"c2\")) |&gt;\n    mutate(stage = name)\n}\ndf_X &lt;- to_df(X, \"X: dane zmieszane\")\ndf_Z &lt;- to_df(Z, \"Z: po whitening\")\ndf_Y &lt;- to_df(Y, \"Y: po ICA (źródła)\")\n\ndf_all &lt;- bind_rows(df_X, df_Z, df_Y)\n\n# 6) Oś układu i wektory bazowe (do wizualizacji obrotów)\n#    W przestrzeni Z osie są już sferyczne (I), więc ICA to „tylko” obrót.\naxes_df &lt;- function(scale_len = 2){\n  data.frame(x = c(0,0), y = c(0,0),\n             xend = c(scale_len,0), yend = c(0,scale_len),\n             label = c(\"e1\",\"e2\"))\n}\naxesZ &lt;- axes_df()\n\n# 7) Wykresy: chmury punktów w 2D (X, Z, Y)\npX &lt;- ggplot(df_X, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8) +\n  coord_equal() +\n  labs(title = \"Przed whiteningiem (X)\",\n       x = \"X[,1]\", y = \"X[,2]\") +\n  theme_minimal()\n\npZ &lt;- ggplot(df_Z, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#2E86DE\") +\n  geom_segment(data = axesZ, aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.18,\"cm\")), linewidth = 0.8) +\n  geom_text(data = axesZ, aes(x = xend, y = yend, label = label),\n            nudge_x = 0.05, nudge_y = 0.05, size = 3) +\n  coord_equal() +\n  labs(title = \"Po whitening (Z): Cov ≈ I\",\n       x = \"Z[,1]\", y = \"Z[,2]\") +\n  theme_minimal()\n\npY &lt;- ggplot(df_Y, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#16A085\") +\n  coord_equal() +\n  labs(title = \"Po ICA (Y): odzyskane źródła niezależne\",\n       x = \"Y[,1]\", y = \"Y[,2]\") +\n  theme_minimal()\n\n(pX | pZ | pY)\n\n\n\n\n\n\nKod# 8) Dodatkowo: marginesowe histogramy pokazujące „nienormalność”\nhX &lt;- df_X |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"X\")\nhZ &lt;- df_Z |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Z\")\nhY &lt;- df_Y |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Y\")\n\nh_all &lt;- bind_rows(hX,hZ,hY) |&gt;\n  mutate(stage = factor(stage, levels = c(\"X\",\"Z\",\"Y\")))\n\nggplot(h_all, aes(val)) +\n  geom_histogram(bins = 60, fill = \"grey70\", color = \"white\") +\n  facet_grid(stage ~ col, scales = \"free_y\") +\n  labs(title = \"Marginalne rozkłady: przed whiteningiem, po whitening, po ICA\",\n       x = \"wartość\", y = \"liczność\") +\n  theme_minimal()\n\n\n\n\n\n\nKod# 9) Krótka kontrola: kowariancje i korelacje\ncat(\"\\nKowariancja X:\\n\"); print(round(cov(X),3))\n\n\nKowariancja X:\n\n\n      [,1]  [,2]\n[1,] 6.295 4.620\n[2,] 4.620 5.123\n\nKodcat(\"\\nKowariancja Z (powinna być bliska I):\\n\"); print(round(cov(Z),3))\n\n\nKowariancja Z (powinna być bliska I):\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKodcat(\"\\nKorelacje pomiędzy kolumnami Y (powinny być bliskie 0; niezależność jest mocniejsza niż brak korelacji):\\n\")\n\n\nKorelacje pomiędzy kolumnami Y (powinny być bliskie 0; niezależność jest mocniejsza niż brak korelacji):\n\nKodprint(round(cor(Y),3))\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nPodejście maksymalizujące nienormalność opiera się na kurtozie lub na przybliżonej negatywnej entropii (ang. negentropy). Dla \\(\\operatorname{Var}(y)=1\\) kurtoza \\(\\kappa(y)=\\mathbb{E}\\{y^4\\}-3\\) przyjmuje wartości 0 dla rozkładu normalnego i wartości odległe od zera dla rozkładów nienormalnych. Maksymalizacja \\(|\\kappa(w^\\top Z)|\\) prowadzi do składowych niezależnych. Stabilniejsze i bardziej ogólne kryterium stanowi negentropy \\(J(y)=H(y_{\\text{gauss}})-H(y)\\), gdzie \\(H\\) oznacza entropię. W praktyce stosuje się aproksymacje postaci \\[\nJ(y)\\approx \\Big(\\mathbb{E}\\,G(y)-\\mathbb{E}\\,G(v)\\Big)^2,\n\\] z dobraną nieliniowością \\(G\\) oraz \\(v\\sim \\mathcal N(0,1)\\). Maksymalizacja \\(J\\) przy ograniczeniu \\(\\|w\\|=1\\) zapewnia poszukiwanie najbardziej nienormalnych kierunków.\nZ kryteriów tych wynika algorytm FastICA jako iteracja stałego punktu. Dla jednego komponentu w przestrzeni whitened stosujemy aktualizację \\[\nw \\leftarrow \\mathbb{E}\\{Z\\,g(w^\\top Z)\\}-\\mathbb{E}\\{g’(w^\\top Z)\\}\\, w,\\qquad \\text{następnie } w\\leftarrow \\frac{w}{\\|w\\|},\n\\] gdzie \\(g=G’\\) jest score function (np. \\(g(u)=\\tanh(u)\\), \\(g(u)=u^3\\) lub \\(g(u)=u\\exp(-u^2/2)\\)). Dla wielu składowych stosujemy równoległe aktualizacje i ortogonalizację w kolejnych krokach, np. metodą rzutów Grama–Schmidta lub przez dekompozycję symetryczną \\(W\\leftarrow (WW^\\top)^{-1/2}W\\), co zachowuje wzajemną ortogonalność wektorów w przestrzeni whitened i zapobiega zbieżności do tej samej składowej.\nAlternatywne wyprowadzenie pochodzi z maksymalizacji funkcji wiarygodności (maximum likelihood). Zakładając niezależność źródeł z gęstościami \\(p_{s_i}\\) i (dla prostoty) brak szumu, otrzymujemy logarytm funkcji wiarygodności \\[\n\\mathcal L(W)=\\sum_{t=1}^n\\Bigg(\\sum_{i=1}^m \\log p_{s_i}\\big((W X_t)_i\\big)\\Bigg) + n\\log|\\det W|.\n\\] Jej gradient prowadzi do zasady Infomax, która brzmi: dobrać \\(W\\) tak, aby wyjścia miały jak największą sumę entropii (co przy zachowaniu \\(\\log|\\det W|\\) jest równoważne maksymalizacji wspólnej niezależności). W praktyce wybór rodziny \\(p_{s_i}\\) implikuje odpowiednie nieliniowości w regule uczenia, formalnie zbieżne z powyższymi kontrastami na negentropy.\nW obecności szumu addytywnego \\(X = A s + \\varepsilon\\) z \\(\\varepsilon\\sim \\mathcal N(0,\\sigma^2 I)\\) problem staje się trudniejszy. Stosuje się wówczas rozszerzone modele ICA z estymacją rzędu i składowej szumowej, warianty bayesowskie, lub metody wykorzystujące dodatkowe własności źródeł (np. niezależność czasową wyższych rzędów, jak w SOBI wykorzystującym autokowariancje).\nZałożenia identyfikowalności obejmują liniowość i natychmiastowość mieszania, niezależność składowych źródłowych, co najwyżej jedną składową o rozkładzie normalnym (inaczej problem staje się nierozwiązywalny z powodu nieodróżnialności kierunków gaussowskich), pełny rząd macierzy \\(A\\) oraz wystarczającą nienormalność źródeł, aby kontrasty informacyjne miały sens. Zwyczajowo zakłada się również stacjonarność w czasie, o ile wykorzystujemy momenty lub autokorelacje do estymacji.\nDobór liczby składowych w ICA nie opiera się na udziale wariancji, jak w PCA, ponieważ ICA nie porządkuje komponentów według wariancji. W praktyce najpierw wybiera się wymiar whiteningu \\(m\\) (efektywny rząd sygnału), a następnie ekstrahuje \\(m\\) składowych niezależnych. Kryteria wyboru \\(m\\) obejmują informacyjne miary rzędu macierzy kowariancji, takie jak MDL/BIC dopasowane do modelu składowej szumowej i niezerowych wartości własnych, testy istotności dla wartości własnych po whiteningu (warianty analizy równoległej, permutacyjne testy mierzące losowość), walidację na podstawie wiarygodności w modelu ML-ICA z różnymi \\(m\\) oraz kryteria stabilności. Kryteria stabilności polegają na wielokrotnym uruchomieniu algorytmu z różnymi inicjalizacjami i grupowaniu uzyskanych komponentów. Liczba dobrze replikujących się grup daje oszacowanie na \\(m\\). Dodatkowo stosuje się testy resztowej zależności między oszacowanymi źródłami (np. testy niezależności na bazie informacji wzajemnej). Jeśli po dodaniu kolejnej składowej informacja wzajemna między „źródłami” nie maleje, zwiększanie \\(m\\) nie przynosi korzyści. W zastosowaniach z szumem wybieramy \\(m\\) tak, by oddzielać podprzestrzeń sygnałową od szumowej, co praktycznie sprowadza się do analizy spektrum wartości własnych i modelowania ogona jako białego szumu.\nInterpretacja wyników ICA różnić się od PCA. Składowe ICA \\(y_i\\) stanowią oceny źródeł o maksymalnej niezależności, a wiersze \\(W\\) definiują filtry demiksujące, podczas gdy kolumny \\(A\\) (przyjmując \\(A\\approx W^{-1}\\)) reprezentują wzorce mieszania, czyli „mapy obciążenia” źródeł na czujniki/cechy. Skale i znaki składowych są arbitralne, co wymaga interpretować je względnie: znormalizować wariancję lub maksymalną wartość, a znak dobrać tak, by ułatwić opis dziedzinowy5. W przeciwieństwie do PCA, składowe ICA nie muszą być ortogonalne, a ich wariancje nie są uporządkowane6.\n5 Wyobraźmy sobie, że ICA rozdziela dwa źródła dźwięku — skrzypce i fortepian. Jeśli algorytm zwróci sygnał, który jest odwrócony w fazie (czyli pomnożony przez -1), to dźwięk fortepianu jest ten sam fizycznie, tylko wszystkie amplitudy mają odwrotny znak. Dlatego znak (i skala) nie mają znaczenia dla jakości separacji — są arbitralne.6 W sygnałach biologicznych, takich jak EEG, ICA może oddzielić artefakty ruchowe od sygnałów mózgowych. Artefakty te mogą być silnie nienormalne i niezależne od sygnałów mózgowych, co czyni ICA skuteczną metodą ich identyfikacji i usunięcia.\nPrzykład 5.2 (ICA na mieszance sygnałów)  \n\nKoddata(\"EuStockMarkets\")\nP &lt;- as.data.frame(EuStockMarkets)              # poziomy indeksów: DAX, SMI, CAC, FTSE\nR &lt;- as.data.frame(apply(P, 2, function(x) diff(log(x))))  # dzienne log-zwroty\ncolnames(R) &lt;- colnames(P)\n\n# ICA na dziennych zwrotach\nset.seed(123)\nica_res &lt;- fastICA(R, n.comp = 4, method = \"C\")\nS_est &lt;- as.data.frame(ica_res$S)               # odzyskane źród\ncolnames(S_est) &lt;- paste0(\"IC\", 1:4)\nA_est &lt;- ica_res$A                              # macierz mieszająca\nW_est &lt;- ica_res$K %*% ica_res$W                # macierz demiksująca\n\n\nMacierz A_est, czyli macierz mieszania, opisuje sposób, w jaki oryginalne zmienne obserwowalne — w tym przypadku cztery indeksy giełdowe: DAX, SMI, CAC i FTSE — powstają jako liniowe kombinacje ukrytych, niezależnych czynników. Każdy wiersz tej macierzy odpowiada jednemu indeksowi, a każda kolumna jednej składowej niezależnej. Wartości liczbowe oznaczają współczynniki liniowych kombinacji, czyli wpływ danej składowej na dany indeks. Wartość dodatnia wskazuje, że wzrost komponentu powoduje wzrost indeksu, wartość ujemna — że ruch komponentu przekłada się na spadek indeksu, a wartość bliska zeru oznacza brak istotnego związku.\n\nKodround(A_est, 3)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] -0.001 -0.001 -0.002 -0.006\n[2,]  0.003 -0.004  0.002  0.000\n[3,] -0.010 -0.008 -0.008 -0.005\n[4,]  0.000  0.001  0.008  0.001\n\n\n\nPierwszy komponent (IC1) najsilniej ładuje się na indeks CAC, a w mniejszym stopniu na SMI. Znak ujemny dla CAC i dodatni dla SMI sugeruje, że komponent ten uchwyca różnicę pomiędzy rynkami strefy euro a rynkiem szwajcarskim, czyli czynnik kontrastujący. W praktyce oznacza to, że wzrost aktywności na rynkach kontynentalnych wiązać się może z relatywnym osłabieniem rynku SMI lub odwrotnie.\nDrugi komponent (IC2) również oddziałuje na CAC i SMI w kierunku ujemnym, co może świadczyć o uchwyceniu wspólnego czynnika kontynentalnego o mniejszej amplitudzie. Dodatnie, choć niewielkie wartości dla FTSE wskazują, że komponent ten częściowo kontrastuje rynki kontynentalne z brytyjskim.\nTrzeci komponent (IC3) ma wyraźnie odmienną strukturę. Dla FTSE współczynnik jest dodatni i największy, natomiast dla pozostałych indeksów ujemny. Komponent ten rozdziela zatem rynek brytyjski od reszty Europy i można go interpretować jako czynnik geograficzny lub walutowy, związany z odmiennym otoczeniem gospodarczym Wielkiej Brytanii.\nCzwarty komponent (IC4) ma współczynniki bardzo małe, rzędu 10-3–10-2, co sugeruje, że jego wpływ na strukturę indeksów jest marginalny. Prawdopodobnie odpowiada on za szum lub krótkotrwałe, lokalne fluktuacje, które nie mają znaczenia ekonomicznego.\n\n\nKodround(W_est, 3)\n\n         [,1]     [,2]    [,3]    [,4]\n[1,]   43.737  112.245 -74.794 -85.258\n[2,]   34.702 -146.908 -47.294  -7.649\n[3,]   15.975   14.330   8.569 141.650\n[4,] -173.263   -5.797  10.661 -35.834\n\n\nMacierz W_est, czyli macierz demiksująca, zawiera współczynniki liniowych kombinacji oryginalnych zmiennych (indeksów giełdowych), które pozwalają uzyskać poszczególne niezależne komponenty. Każdy wiersz tej macierzy odpowiada jednemu komponentowi ICA (IC1–IC4), a każda kolumna — jednej zmiennej obserwowalnej (DAX, SMI, CAC, FTSE). Wartości w tej macierzy można zatem interpretować jako wagi, z jakimi poszczególne indeksy uczestniczą w tworzeniu danego odzyskanego źródła.\n\nPierwszy komponent (IC1) ma duże dodatnie wagi dla indeksów DAX i SMI oraz silnie ujemną wagę dla FTSE. Oznacza to, że IC1 odzwierciedla kontrast pomiędzy rynkami kontynentalnymi (Niemcy, Szwajcaria) a rynkiem brytyjskim. Wzrost wartości IC1 odpowiada sytuacji, w której indeksy kontynentalne zachowują się silniej niż FTSE — można więc interpretować ten czynnik jako różnicowy, typu „Europa kontynentalna kontra Wielka Brytania”.\nDrugi komponent (IC2) pokazuje odwrotny schemat: dodatni wpływ DAX, silnie ujemny SMI, a słaby wpływ pozostałych indeksów. Można go interpretować jako czynnik rozróżniający zachowanie rynku niemieckiego i szwajcarskiego, który w ICA często ujawnia się jako efekt odmiennych warunków walutowych i struktury gospodarczej.\nTrzeci komponent (IC3) ma wszystkie wagi ujemne, z wyjątkiem niewielkich dodatnich dla CAC i FTSE. Oznacza to, że IC3 reprezentuje wspólny kierunek zmian większości indeksów (ruch globalny), ale w konstrukcji demiksującej występuje ze znakiem ujemnym. W praktyce odpowiada to czynnikowi rynkowemu o charakterze ogólnym — globalnemu impulsowi, który oddziałuje w podobny sposób na większość rynków.\nCzwarty komponent (IC4) ma wysoką dodatnią wagę dla CAC oraz ujemne dla pozostałych indeksów, co sugeruje, że może on odzwierciedlać czynnik specyficzny dla rynku francuskiego — reakcje lokalne lub sektorowe, które nie są wspólne dla innych giełd..\n\nZnaki współczynników w ICA są arbitralne (zmiana wszystkich znaków w jednym wierszu nie zmienia modelu), dlatego przy interpretacji należy zwracać uwagę na względne zależności między indeksami, a nie na samą polaryzację znaków. Wartości bezwzględne wag pokazują natomiast, które indeksy mają największy udział w kształtowaniu danego czynnika.\n\nKodkurt &lt;- apply(S_est, 2, function(x) mean(x^4) - 3)  # kurtozy odzyskanych źródeł\nprint(round(kurt, 3))                           # kurtozy (nienormalność)\n\n  IC1   IC2   IC3   IC4 \n5.601 1.985 8.206 2.274 \n\n\nWartości kurtozy stanowią miarę niegaussowskości rozkładu — czyli tego, jak bardzo dany sygnał odbiega od kształtu rozkładu normalnego. Wartości dodatnie oznaczają rozkłady o „cięższych ogonach” i bardziej spiczastym kształcie (tzw. leptokurtyczne), co jest typowe dla sygnałów rzadkich, zawierających wyraźne piki i okresy stabilności. W kontekście ICA wysoka kurtoza jest pożądana, ponieważ algorytm poszukuje właśnie takich komponentów — maksymalnie odmiennych od normalnych, a więc potencjalnie niezależnych źródeł.\n\n\nIC1 (5.601) ma bardzo wysoką kurtozę, co wskazuje na silną niegaussowskość. Komponent ten prawdopodobnie reprezentuje główny, „rzadki” czynnik ekonomiczny, który reaguje gwałtownie w momentach istotnych zmian rynkowych. Może to być globalny impuls rynkowy lub okresowe szoki finansowe.\n\nIC2 (1.985) ma umiarkowanie dodatnią kurtozę, sugerującą rozkład jedynie lekko leptokurtyczny. Oznacza to, że komponent jest bliższy rozkładowi normalnemu, a zatem mniej „niezależny” w sensie ICA. Może reprezentować łagodniejszy czynnik wspólny, np. codzienną zmienność lub trend regionalny.\n\nIC3 (8.206) ma najwyższą kurtozę spośród wszystkich komponentów. Jest to bardzo silny sygnał niegaussowski, typowy dla źródła zawierającego rzadkie, intensywne zdarzenia — w kontekście finansowym mogą to być momenty skokowych zmian cen lub kryzysów, wpływające selektywnie na część indeksów. Ten komponent można traktować jako najbardziej „czyste” źródło w sensie ICA.\n\nIC4 (2.274) wykazuje umiarkowaną kurtozę, zbliżoną do IC2. Można go interpretować jako dodatkowy, mniej wyraźny czynnik poboczny, który w pewnym stopniu odbiega od normalności, ale nie ma charakteru dominującego.\n\n\nKod# Sprawdzenie korelacji między oryginalnymi a odzyskanymi sygnałami\ncor_matrix &lt;- cor(R, S_est)\nprint(round(cor_matrix, 3))\n\n        IC1    IC2    IC3   IC4\nDAX  -0.076  0.287 -0.954 0.037\nSMI  -0.060 -0.475 -0.871 0.108\nCAC  -0.189  0.140 -0.686 0.689\nFTSE -0.788  0.001 -0.602 0.125\n\n\nMacierz korelacji między oryginalnymi indeksami giełdowymi a odzyskanymi komponentami niezależnymi (cor_matrix) pokazuje, jak silnie i w jakim kierunku (znak dodatni lub ujemny) każdy z indeksów jest powiązany z danym źródłem ICA. Wysokie wartości bezwzględne wskazują, że dany komponent w dużym stopniu tłumaczy zmienność danego indeksu, natomiast wartości bliskie zera oznaczają słaby związek.\n\nNajsilniejsze korelacje obserwuje się dla komponentu IC3, który ma wartości ujemne i bardzo wysokie w module: DAX (−0.954), SMI (−0.871) i CAC (−0.686). Oznacza to, że IC3 stanowi wspólny czynnik dominujący dla trzech kontynentalnych indeksów europejskich. Wszystkie trzy reagują w tym samym kierunku (ujemny znak jest konwencjonalny, jego odwrócenie nie zmienia interpretacji). Można zatem uznać, że IC3 reprezentuje globalny czynnik rynkowy, wspólny dla głównych giełd kontynentalnych, a jego wysoka kurtoza (8.206) wskazuje, że czynnik ten cechuje się silnymi, epizodycznymi wahaniami — typowymi dla okresów zawirowań finansowych.\nKomponent IC1 wykazuje wyraźną ujemną korelację z FTSE (−0.788), przy braku silnych zależności z pozostałymi indeksami. Oznacza to, że IC1 można interpretować jako czynnik specyficzny dla rynku brytyjskiego, niezależny od ruchów kontynentalnych. Wysoka wartość bezwzględna korelacji sugeruje, że ten komponent odpowiada za znaczną część zmienności FTSE, co dobrze współgra z interpretacją wcześniejszej macierzy mieszania — IC1 oddzielał Wielką Brytanię od reszty Europy.\nKomponent IC2 wykazuje umiarkowane korelacje o różnych znakach: dodatnią z DAX (0.287) i ujemną ze SMI (−0.475). Można go zatem interpretować jako czynnik różnicowy pomiędzy rynkami Niemiec i Szwajcarii. W praktyce może on odzwierciedla odmienną reakcję tych rynków na czynniki lokalne, np. różnice w strukturze sektorowej lub polityce monetarnej.\nKomponent IC4 ma umiarkowaną dodatnią korelację z CAC (0.689), a pozostałe indeksy reagują na niego słabo. Oznacza to, że IC4 może być czynnikiem częściowo specyficznym dla rynku francuskiego, prawdopodobnie o charakterze lokalnym lub szumowym.\n\nNa koniec wizualizacja ICA.\n\nKod# odzyskane komponenty \nS_est_long &lt;- S_est |&gt;\n  mutate(Time = 1:nrow(S_est)) |&gt;\n  pivot_longer(cols = starts_with(\"IC\"),\n                      names_to = \"Component\", values_to = \"Value\")\n\np1 &lt;- ggplot(S_est_long, aes(x = Time, y = Value, color = Component)) +\n  geom_line() +\n  facet_wrap(~ Component, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Odzyskane niezależne komponenty (ICA)\", x = \"Czas\", y = \"Wartość\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nR_long &lt;- R |&gt;\n  mutate(Time = 1:nrow(R)) |&gt;\n  pivot_longer(cols = -Time,    \n                      names_to = \"Index\", values_to = \"Return\")\n\np2 &lt;- ggplot(R_long, aes(x = Time, y = Return, color = Index)) +\n  geom_line() +\n  facet_wrap(~ Index, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Oryginalne dzienne log-zwroty indeksów\", x = \"Czas\", y = \"Log-zwrot\") +\n  scale_color_flat_d() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#t-snevandermaaten08a",
    "href": "pca.html#t-snevandermaaten08a",
    "title": "Metody redukcji wymiarowości",
    "section": "t-SNE(Maaten i Hinton 2008)\n",
    "text": "t-SNE(Maaten i Hinton 2008)\n\nMetoda t-distributed Stochastic Neighbor Embedding (t-SNE) została opracowana przez Laurensa van der Maatena i Geoffreya Hintona w 2008 roku jako nieliniowa technika redukcji wymiarowości, której celem jest odwzorowanie lokalnej struktury danych wysokowymiarowych w przestrzeni o mniejszej liczbie wymiarów, zwykle dwuwymiarowej lub trójwymiarowej. W przeciwieństwie do metod liniowych, takich jak PCA, t-SNE nie dąży do maksymalizacji wariancji, lecz do zachowania sąsiedztw pomiędzy punktami – obserwacje, które w przestrzeni oryginalnej są blisko siebie, powinny również pozostawać blisko w przestrzeni odwzorowania.\nNiech dane wejściowe tworzą macierz \\(X = [x_1, x_2, \\dots, x_n]^\\top\\), gdzie każdy wektor \\(x_i \\in \\mathbb{R}^p\\) reprezentuje jedną obserwację w przestrzeni o wymiarze \\(p\\). Pierwszym krokiem jest przekształcenie danych wysokowymiarowych w macierz podobieństw, która opisuje, jak bardzo punkty są „bliskie” względem siebie. Dla każdego punktu \\(x_i\\) definiuje się rozkład warunkowy \\[\np_{j|i} = \\frac{\\exp\\!\\left(-\\frac{|x_i - x_j|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{|x_i - x_k|^2}{2\\sigma_i^2}\\right)}, \\quad p_{i|i} = 0,\n\\] gdzie parametr \\(\\sigma_i\\) (odpowiednik bandwidth) dobiera się tak, aby entropia rozkładu \\(P_i = (p_{j|i})_j = (p_{1|i}, p_{2|i}, \\dots, p_{n|i}).\\) odpowiadała zadanej perplexity, czyli efektywnej liczbie sąsiadów. Perplexity jest hiperparametrem kontrolującym zakres lokalności analizowanych relacji.\nNastępnie konstruuje się symetryczną macierz podobieństw \\[\np_{ij} = \\frac{p_{i|j} + p_{j|i}}{2n},\n\\] która reprezentuje prawdopodobieństwo, że punkty \\(x_i\\) i \\(x_j\\) są bliskimi sąsiadami w przestrzeni oryginalnej.\nKolejnym krokiem jest utworzenie analogicznego rozkładu w przestrzeni odwzorowania \\(Y = [y_1, y_2, \\dots, y_n]^\\top\\), gdzie \\(y_i \\in \\mathbb{R}^q\\) i zwykle \\(q = 2\\) lub 3. Dla tych punktów definiuje się rozkład podobieństw oparty na rozkładzie t-Studenta z jednym stopniem swobody \\[\nq_{ij} = \\frac{(1 + |y_i - y_j|^2)^{-1}}{\\sum_{k \\neq l} (1 + |y_k - y_l|^2)^{-1}}, \\quad q_{ii} = 0.\n\\] Rozkład t-Studenta ma grube ogony, co umożliwia bardziej realistyczne odwzorowanie relacji między punktami odległymi od siebie i redukuje problem crowding, czyli nadmiernego ściskania punktów w centrum przestrzeni odwzorowania.\nCelem t-SNE jest minimalizacja dywergencji Kullbacka–Leiblera między rozkładami \\(P\\) i \\(Q\\) \\[\nC = \\operatorname{KL}(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n\\] Optymalizacja tej funkcji, zwykle za pomocą spadku gradientowego, prowadzi do znalezienia takich współrzędnych \\(Y\\), które zachowują lokalne relacje między punktami w jak największym stopniu. Gradient funkcji celu względem współrzędnych \\(y_i\\) ma postać \\[\n\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij}) (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1},\n\\] a współczynnik 4 pełni rolę skalującą. W praktyce stosuje się dodatkowe techniki stabilizujące proces uczenia, takie jak momentum, etap early exaggeration zwiększający kontrast lokalnych podobieństw, oraz wcześniejszą redukcję wymiarowości metodą PCA w celu ograniczenia szumu.\nMetoda t-SNE nie zakłada liniowości ani rozkładu normalnego danych, lecz wymaga, aby dane były znormalizowane w przypadku różnych jednostek pomiarowych, ponieważ odległości euklidesowe są wrażliwe na skalę. Wskazane jest wcześniejsze zastosowanie PCA w celu usunięcia szumu i zmniejszenia złożoności obliczeniowej. Dane nie powinny zawierać dużej liczby wartości odstających ani duplikatów, które mogłyby zaburzyć lokalne struktury. Kluczowy hiperparametr perplexity powinien być dostosowany do liczby obserwacji — zbyt mała wartość prowadzi do przeuczenia lokalnego, a zbyt duża powoduje zatarcie drobnych struktur.\nWyniki t-SNE przedstawione w przestrzeni dwuwymiarowej lub trójwymiarowej nie mają interpretacji metrycznej. Oznacza to, że odległości między klastrami nie są bezpośrednio interpretowalne ilościowo. Interpretacja opiera się głównie na analizie sąsiedztwa: punkty znajdujące się blisko siebie w przestrzeni t-SNE odpowiadają obserwacjom podobnym w oryginalnych cechach, natomiast wyraźne skupiska punktów mogą wskazywać na istnienie klas lub podgrup. Oś pierwsza i druga nie mają znaczenia merytorycznego – są jedynie współrzędnymi w przestrzeni odwzorowania, które zachowuje lokalną strukturę, a nie globalną geometrię danych.\n\n\n\n\n\n\nWażne kroki przy stosowaniu t-SNE\n\n\n\n\nStandaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.\nWybór zakresu perplexity — zwykle testuje się kilka wartości (np. 5, 15, 30, 50) i ocenia stabilność struktur (czy klastry są rozdzielne, czy stabilne względem permutacji danych).\nUstawienie learning rate — zaczyna się od wartości domyślnej (200) i w razie potrzeby zwiększa do 500–1000, jeśli klastry są zbyt zwarte.\nUstalenie liczby iteracji — co najmniej 500; w przypadku dużych zbiorów można zwiększyć do 1000–2000, jeśli rozkład nadal się zmienia.\nPorównanie z PCA lub UMAP — warto sprawdzić, czy t-SNE nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykład 5.4 (t-SNE na danych iris)  \n\nKodlibrary(Rtsne)      # t-SNE\n\nset.seed(44)\n\n# Przygotowanie danych (jak wcześniej)\niris_data &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  as.matrix() %&gt;%\n  scale()\n\n# t-SNE\ntsne_result &lt;- Rtsne(\n  iris_data,\n  dims = 2, perplexity = 30, verbose = TRUE,\n  max_iter = 500, check_duplicates = FALSE\n)\n\nPerforming PCA\nRead the 150 x 4 data matrix successfully!\nUsing no_dims = 2, perplexity = 30.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.00 seconds (sparsity = 0.711156)!\nLearning embedding...\nIteration 50: error is 45.359701 (50 iterations in 0.01 seconds)\nIteration 100: error is 44.348491 (50 iterations in 0.01 seconds)\nIteration 150: error is 43.459291 (50 iterations in 0.01 seconds)\nIteration 200: error is 45.385876 (50 iterations in 0.01 seconds)\nIteration 250: error is 45.872962 (50 iterations in 0.01 seconds)\nIteration 300: error is 0.661337 (50 iterations in 0.01 seconds)\nIteration 350: error is 0.165504 (50 iterations in 0.01 seconds)\nIteration 400: error is 0.161554 (50 iterations in 0.01 seconds)\nIteration 450: error is 0.160806 (50 iterations in 0.01 seconds)\nIteration 500: error is 0.157935 (50 iterations in 0.01 seconds)\nFitting performed in 0.07 seconds.\n\nKodtsne_df &lt;- data.frame(\n  Dim1 = tsne_result$Y[,1],\n  Dim2 = tsne_result$Y[,2],\n  Species = iris$Species\n)\n\n# PCA na tych samych danych\npca_fit &lt;- prcomp(iris_data, center = FALSE, scale. = FALSE)\npca_df &lt;- data.frame(\n  PC1 = pca_fit$x[,1],\n  PC2 = pca_fit$x[,2],\n  Species = iris$Species\n)\n\n# Wykres t-SNE\np1 &lt;- ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"t-SNE\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Wykres PCA (pierwsze dwie składowe)\np2 &lt;- ggplot(pca_df, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#umapkonopka2018",
    "href": "pca.html#umapkonopka2018",
    "title": "Metody redukcji wymiarowości",
    "section": "UMAP(Konopka 2018)\n",
    "text": "UMAP(Konopka 2018)\n\nMetoda Uniform Manifold Approximation and Projection (UMAP) jest nieliniową techniką redukcji wymiarowości opracowaną przez McInnesa i Healy’ego w 2018 roku. Jej celem jest odwzorowanie danych z przestrzeni wysokowymiarowej w przestrzeń o mniejszej liczbie wymiarów przy zachowaniu struktury geometrycznej — zarówno lokalnej, jak i globalnej — poprzez modelowanie danych jako rozkładu na rozmaitości (manifold). W odróżnieniu od metody t-SNE, UMAP opiera się na teorii rozmaitości Riemanna oraz na pojęciach pochodzących z teorii zbiorów rozmytych i topologii algebraicznej.\nNiech dane wejściowe stanowią zbiór punktów \\[\nX = \\{x_1, x_2, \\dots, x_n\\}, \\quad x_i \\in \\mathbb{R}^p.\n\\] Zakłada się, że punkty te leżą na rozmaitości \\(\\mathcal{M} \\subset \\mathbb{R}^p\\) o niższym wymiarze rzeczywistym \\(d &lt; p\\), zanurzonej w przestrzeni obserwowalnej. Metoda UMAP tworzy dwie probabilistyczne reprezentacje tej rozmaitości: po pierwsze, graf sąsiedztwa w przestrzeni wysokowymiarowej (fuzzy simplicial set), który opisuje lokalne zależności między punktami, oraz po drugie, graf w przestrzeni niskowymiarowej, którego struktura ma jak najlepiej odwzorowywać pierwszy.\nW celu konstrukcji grafu w przestrzeni wejściowej dla każdego punktu \\(x_i\\) określa się odległości do jego \\(k\\)-najbliższych sąsiadów. Następnie wyznacza się dwa parametry lokalne \\[\n\\rho_i = \\min_{j: d(x_i,x_j) &gt; 0} d(x_i, x_j),\n\\] czyli najmniejszą dodatnią odległość (umożliwiającą niezerową gęstość), oraz \\(\\sigma_i &gt; 0,\\) skalę lokalną dobraną tak, aby spełniony był warunek normalizacji entropii \\[\n\\sum_{j} \\exp\\!\\left(-\\frac{\\max(0, d(x_i,x_j) - \\rho_i)}{\\sigma_i}\\right) = \\log_2(k).\n\\] Na tej podstawie definiuje się rozmyte prawdopodobieństwa sąsiedztwa \\[\np_{j|i} = \\exp\\!\\left(-\\frac{\\max(0, d(x_i, x_j) - \\rho_i)}{\\sigma_i}\\right).\n\\] Ponieważ macierz tych wartości nie jest symetryczna, łączy się oba kierunki zgodnie z zasadami teorii zbiorów rozmytych \\[\np_{ij} = p_{i|j} + p_{j|i} - p_{i|j}\\,p_{j|i}.\n\\] Tak powstały rozmyty graf sąsiedztwa zawiera wagi \\(p_{ij}\\), które odzwierciedlają siłę połączeń między punktami.\nNastępnie w przestrzeni wynikowej \\(Y = \\{y_1, y_2, \\dots, y_n\\} \\subset \\mathbb{R}^q\\), gdzie zwykle \\(q = 2\\) lub 3, definiuje się analogiczny rozmyty graf \\(q_{ij}\\), którego wagi opisuje funkcja jądra typu heavy-tailed \\[\nq_{ij} = \\frac{1}{1 + a\\,\\|y_i - y_j\\|^{2b}},\n\\] gdzie \\(a\\) i \\(b\\) są parametrami dopasowanymi empirycznie (standardowo \\(a \\approx 1.929,\\ b \\approx 0.7915\\)).\nZasadniczym celem UMAP jest znalezienie takiej konfiguracji punktów \\(Y\\), aby rozmyty graf \\(q_{ij}\\) jak najlepiej przybliżał graf \\(p_{ij}\\). Kryterium optymalizacji ma postać minimalizacji rozbieżności krzyżowej (ang. cross-entropy) między dwoma rozkładami sąsiedztwa \\[\nC = \\sum_{i &lt; j} \\left[ -p_{ij}\\log(q_{ij}) - (1 - p_{ij})\\log(1 - q_{ij}) \\right].\n\\] Minimalizacja tej funkcji jest realizowana metodami gradientowymi, zazwyczaj z wykorzystaniem stochastic gradient descent (SGD). W wyniku optymalizacji punkty \\(y_i\\) są przesuwane tak, aby utrzymać bliskie relacje w miejscach, gdzie \\(p_{ij}\\) jest duże i rozdzielać punkty, gdzie \\(p_{ij}\\) jest małe.\nZałożenia metody UMAP są stosunkowo niewielkie, lecz istotne. Zakłada się, że dane leżą na rozmaitości o niskim wymiarze, a więc można je opisać poprzez ciągłą strukturę geometryczną. Przyjmuje się również, że użyta miara odległości (zwykle euklidesowa) odzwierciedla faktyczne podobieństwo obserwacji oraz że rozkład punktów jest gładki, czyli w małych sąsiedztwach struktura jest dobrze przybliżana liniowo.\nInterpretacja wyników UMAP nie odnosi się do bezwzględnych wartości współrzędnych, lecz do relacji między punktami. Punkty położone blisko siebie w przestrzeni wynikowej są podobne w przestrzeni oryginalnej, a większe odległości odpowiadają mniejszemu podobieństwu. W przeciwieństwie do t-SNE metoda ta lepiej zachowuje nie tylko lokalne klastry, ale również częściowo strukturę globalną, co umożliwia analizę gradientów i ciągłych przejść między grupami obserwacji.\n\n\n\n\n\n\nWażne kroki przy stosowaniu UMAP\n\n\n\n\nStandaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.\nWybór liczby sąsiadów (n_neighbors) — kontroluje lokalność odwzorowania; mniejsze wartości (5–15) podkreślają lokalne struktury, większe (30–50) zachowują więcej globalnych relacji.\nUstawienie wymiaru wynikowego (n_components) — zwykle 2 lub 3, w zależności od potrzeb wizualizacji.\nWybór metryki odległości — domyślnie euklidesowa, ale można użyć innych (np. Manhattan, cosine) w zależności od charakteru danych.\nPorównanie z PCA lub t-SNE — warto sprawdzić, czy UMAP nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykład 5.5 (UMAP na danych iris)  \n\nKodlibrary(uwot)     # UMAP\n\nset.seed(44)\n\n# Przygotowanie danych: oddzielić etykiety klas i standaryzować cechy\nX &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  scale() %&gt;%\n  as.matrix()\n\ny &lt;- iris$Species\n\n# UMAP 2D: podstawowe parametry\n# n_neighbors = \"skala lokalności\", min_dist = \"zwartość klastrów\", metric = metryka odległości\nemb_umap &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist    = 0.1,\n  metric      = \"euclidean\",\n  n_components = 2,\n  verbose = TRUE\n)\n\n# Ramka wynikowa do wykresu\ndf_umap &lt;- data.frame(\n  UMAP1 = emb_umap[, 1],\n  UMAP2 = emb_umap[, 2],\n  Species = y\n)\n\n# Dla porównania: PCA 2D (opcjonalnie)\npca &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  PC1 = pca$x[, 1],\n  PC2 = pca$x[, 2],\n  Species = y\n)\n\n# Wykresy\np_umap &lt;- ggplot(df_umap, aes(x = UMAP1, y = UMAP2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"UMAP\",\n       x = \"UMAP1\", y = \"UMAP2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np_pca &lt;- ggplot(df_pca, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Wyświetlenie obok siebie\np_umap | p_pca\n\n\n\n\n\n\n\n\nPoniżej prezentuję zbiorcze porównanie wszystkich omówionych metod redukcji wymiarowości na tym samym zbiorze danych iris. Wykorzystuję PCA, ICA, MDS, t-SNE oraz UMAP, aby zobaczyć, jak różne techniki odwzorowują strukturę danych.\n\nKodset.seed(44)\n\n# Usuń duplikaty (t-SNE i MDS niemetryczny są na to wrażliwe)\niris_unique &lt;- iris[!duplicated(iris[, -5]), ]\nX &lt;- as.matrix(scale(iris_unique[, -5]))\ny &lt;- iris_unique$Species\n\n# PCA (2 pierwsze składowe)\npca_fit &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  Dim1 = pca_fit$x[, 1],\n  Dim2 = pca_fit$x[, 2],\n  Method = \"PCA\",\n  Species = y\n)\n\n# ICA (2 komponenty niezależne)\nica_fit &lt;- fastICA(X, n.comp = 2, method = \"C\")\ndf_ica &lt;- data.frame(\n  Dim1 = ica_fit$S[, 1],\n  Dim2 = ica_fit$S[, 2],\n  Method = \"ICA\",\n  Species = y\n)\n\n# MDS metryczny (klasyczny)\nmds_metric &lt;- cmdscale(dist(X), k = 2)\ndf_mds_metric &lt;- data.frame(\n  Dim1 = mds_metric[, 1],\n  Dim2 = mds_metric[, 2],\n  Method = \"MDS (metryczny)\",\n  Species = y\n)\n\n# MDS niemetryczny (isoMDS)\nmds_nonmetric &lt;- isoMDS(dist(X), k = 2)$points\n\ninitial  value 4.818373 \nfinal  value 4.818117 \nconverged\n\nKoddf_mds_nonmetric &lt;- data.frame(\n  Dim1 = mds_nonmetric[, 1],\n  Dim2 = mds_nonmetric[, 2],\n  Method = \"MDS (niemet.)\",\n  Species = y\n)\n\n# t-SNE\ntsne_fit &lt;- Rtsne(\n  X, dims = 2, perplexity = 30,\n  max_iter = 750, check_duplicates = FALSE, verbose = FALSE\n)\ndf_tsne &lt;- data.frame(\n  Dim1 = tsne_fit$Y[, 1],\n  Dim2 = tsne_fit$Y[, 2],\n  Method = \"t-SNE\",\n  Species = y\n)\n\n# UMAP\numap_emb &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist = 0.1,\n  metric = \"euclidean\",\n  n_components = 2,\n  verbose = FALSE\n)\ndf_umap &lt;- data.frame(\n  Dim1 = umap_emb[, 1],\n  Dim2 = umap_emb[, 2],\n  Method = \"UMAP\",\n  Species = y\n)\n\n# Połączenie wszystkich metod\ndf_all &lt;- bind_rows(\n  df_pca,\n  df_ica,\n  df_mds_metric,\n  df_mds_nonmetric,\n  df_tsne,\n  df_umap\n)\n\n# Wykres porównawczy\nggplot(df_all, aes(Dim1, Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  facet_wrap(~ Method, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Porównanie metod redukcji wymiarowości na zbiorze iris\",\n    x = \"Wymiar 1\", y = \"Wymiar 2\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nComon, Pierre. 1994. „Independent Component Analysis, A New Concept?” Signal Processing 36 (3): 287–314. https://doi.org/10.1016/0165-1684(94)90029-9.\n\n\nKonopka, Tomasz. 2018. „umap: Uniform Manifold Approximation and Projection”. The R Foundation. https://doi.org/10.32614/cran.package.umap.\n\n\nKruskal, J. B. 1964. „Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis”. Psychometrika 29 (1): 1–27. https://doi.org/10.1007/bf02289565.\n\n\nMaaten, Laurens van der, i Geoffrey Hinton. 2008. „Visualizing Data using t-SNE”. Journal of Machine Learning Research 9 (86): 2579–2605. http://jmlr.org/papers/v9/vandermaaten08a.html.\n\n\nPearson, Karl. 1901. „LIII. On Lines and Planes of Closest Fit to Systems of Points in Space”. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72. https://doi.org/10.1080/14786440109462720.\n\n\nTorgerson, Warren S. 1952. „Multidimensional Scaling: I. Theory and Method”. Psychometrika 17 (4): 401–19. https://doi.org/10.1007/bf02288916.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#pca-pearson1901",
    "href": "pca.html#pca-pearson1901",
    "title": "Metody redukcji wymiarowości",
    "section": "",
    "text": "Matematyczna definicja modelu\nPunktem wyjścia analizy głównych składowych jest problem odwzorowania wielowymiarowego zbioru danych w przestrzeni o mniejszej liczbie wymiarów przy możliwie minimalnej stracie informacji. W praktyce dąży się do kompresji i odszumiania sygnału, usuwania współliniowości, stabilizacji dalszych modeli (np. regresji), a także do wizualizacji struktur klasowych i gradientów zmienności. Przykładowo, dla dwóch silnie skorelowanych cech pierwsza składowa główna jest skierowana wzdłuż linii największego rozrzutu (blisko prostej \\(y \\approx x\\)), a redukcja do jednego wymiaru zachowuje większą część wariancji niż dowolna inna projekcja.\nMatematyczna definicja poprzez maksymalizację wariancji i dekompozycję spektralną polega na transformacji scentralizowanej macierzy danych \\(X \\in \\mathbb{R}^{n\\times p}\\) (każdą kolumnę odjąć o jej średnią). Niech \\(\\Sigma=\\frac{1}{n-1}X^\\top X\\) oznacza empiryczną macierz kowariancji. Pierwszą składową wyznaczamy jako kierunek \\(w\\in\\mathbb{R}^{p}\\) rozwiązujący zadanie maksymalizacji wariancji projekcji, czyli maksymalizacji \\(\\mathrm{Var}(Xw)=w^\\top\\Sigma w\\) przy ograniczeniu \\(\\|w\\|_{2}=1\\). Zastosowanie mnożników Lagrange’a prowadzi do warunku stacjonarności \\(\\Sigma w=\\lambda w\\), a więc \\(w\\) jest wektorem własnym \\(\\Sigma\\), zaś \\(\\lambda\\) jest odpowiadającą mu wartością własną. Wybieramy największą wartość własną \\(\\lambda_{1}\\) i jej wektor \\(w_{1}\\), wówczas wariancja pierwszych wyników projekcji \\(z_{1}=Xw_{1}\\) równa się \\(\\lambda_{1}\\). Kolejne składowe otrzymujemy analogicznie jako rozwiązania tego samego problemu z dodatkowymi ograniczeniami ortogonalności \\(w_{j}^\\top w_{k}=0\\) dla \\(k&lt;j\\), co ustawia kolejne wektory własne \\(\\Sigma\\) w porządku malejących wartości własnych \\(\\lambda_{1}\\ge \\lambda_{2}\\ge \\dots \\ge \\lambda_{p}\\). Wektor wyników projekcji \\(z_{j}\\) nazywamy w praktyce scores, a \\(w_{j}\\) — wektorem ładunków (loadings). Kumulatywny udział wariancji wyjaśnianej przez pierwsze \\(k\\) składowych wynosi wówczas \\(\\sum_{j=1}^{k}\\lambda_{j}\\big/\\sum_{j=1}^{p}\\lambda_{j}\\) i służy na często do doboru \\(k\\).\nRównoważne wyprowadzenie modelu przez rozkład na wartości osobliwe, czyli SVD (ang. Singular Value Decomposition), opiera się na faktoryzacji \\(X=UDV^\\top\\), gdzie \\(U\\in\\mathbb{R}^{n\\times r}\\) i \\(V\\in\\mathbb{R}^{p\\times r}\\) mają ortonormalne kolumny, \\(D=\\mathrm{diag}(d_{1},\\dots,d_{r})\\) zawiera uporządkowane wartości osobliwe \\(d_{1}\\ge \\dots \\ge d_{r}&gt;0\\), a \\(r=\\mathrm{rank}(X)\\). Wówczas kolumny \\(V\\) pokrywają się (co do znaku) z wektorami ładunków \\(w_{j}\\), zaś macierz wyników projekcji \\(T=XV\\) równa się \\(UD\\). Związek między oboma podejściami jest ścisły: \\(\\lambda_{j}=d_{j}^{2}/(n-1)\\), a więc wariancje składowych odwzorowuje się przez kwadraty wartości osobliwych przeskalowane czynnikiem \\(1/(n-1)\\). Projekcja do \\(k\\) wymiarów przyjmuje wówczas postać \\(X\\mapsto T_{k}=UD_{k}\\), a rekonstrukcja rzędu \\(k\\) ma postać \\[\nX_{k}=T_{k}V_{k}^\\top=U_{k}D_{k}V_{k}^\\top.\n\\] Z twierdzenia Eckarta–Younga–Mirsky’ego wynika, że \\(X_{k}\\) minimalizuje błąd Frobeniusa \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\) o rządzie co najwyżej \\(k\\), czyli PCA daje najlepszą aproksymację niskorangową w sensie średniokwadratowym (jest to tzw. obcięte SVD). Ta równoważność łączyć dwie intuicje: maksymalizacja przechwyconej wariancji i minimalizacja błędu rekonstrukcji.\n\nTwierdzenie 5.1 (Twierdzenie Eckarta–Younga–Mirsky) Niech \\(X\\in\\mathbb{R}^{n\\times p}\\) i niech \\(r=\\mathrm{rank}(X)\\). Dla \\(k&lt;r\\) niech \\(X_{k}=U_{k}D_{k}V_{k}^\\top\\) będzie obciętym rozkładem SVD rzędu \\(k\\). Wówczas \\(X_{k}\\) jest jedyną macierzą o \\(\\mathrm{rank}(X_{k})=k\\), która minimalizuje błąd Frobeniusa1 \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\in\\mathbb{R}^{n\\times p}\\) o \\(\\mathrm{rank}(Y)\\le k\\). Ponadto zachodzi równość \\(\\|X-X_{k}\\|_{F}^{2}=\\sum_{j=k+1}^{r}d_{j}^{2}\\).\n1 Błąd Frobeniusa \\(\\|A\\|_{F}\\) macierzy \\(A\\) definiujemy jako \\(\\|A\\|_{F}=\\sqrt{\\sum_{i,j}a_{ij}^{2}}=\\sqrt{\\mathrm{tr}(A^\\top A)}.\\)\nZadania optymalizacyjne wyraża się zarówno w wersji wektorowej, jak i macierzowej. Dla pierwszej składowej rozwiązujemy problem maksymalizacji \\(w^\\top\\Sigma\\) w przy \\(\\|w\\|_{2}=1\\), co prowadzi do największej wartości własnej. Dla \\(k\\) składowych poszukujemy macierzy \\(W\\in\\mathbb{R}^{p\\times k}\\) o kolumnach ortonormalnych, która maksymalizuje \\(\\mathrm{tr}(W^\\top\\Sigma W)\\), skąd wynika wybór \\(k\\) wektorów własnych \\(\\Sigma\\). Równoważnie, szukamy projekcji \\(P=WW^\\top\\) minimalizującej błąd rekonstrukcji \\(\\|X-XWW^\\top\\|_{F}^{2}\\). W notacji SVD rozwiązanie ma postać \\(W=V_{k}\\), a więc projekcja działa przez mnożenie przez \\(V_{k}V_{k}^\\top\\).\nGdy cechy mierzymy w różnych jednostkach i skalach, zaleca się stosować macierz korelacji zamiast kowariancji, co jest równoważne standaryzacji kolumn \\(X\\) do wariancji 1. Wiele implementacji (np. w R funkcja prcomp) wykorzystuje SVD na scentralizowanych i ewentualnie standaryzowanych danych, co zapewniać numeryczną stabilność, zwłaszcza gdy \\(p\\gg n\\). W sytuacji \\(p\\gg n\\) korzystniejsze bywa liczenie mniejszych rozkładów: albo dual PCA2 na macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\), albo bezpośrednio obciętego SVD. Dla danych zaburzonych wartościami odstającymi rozważa się wersje odporne, np. zastępuje się \\(\\Sigma\\) estymatorem odpornym (ang. Minimum Covariance Determinant, MCD)3 lub stosuje się robust PCA i dekompozycje oparte na normie jądra i normie \\(L_{1}\\)4.\n2 Wówczas wektory własne \\(\\tilde{w}_{j}\\) macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\) (która jest niższego wymiaru niż \\(X^\\top X\\) a co za tym idzie lepiej się zachowuje numerycznie) przekształca się w wektory własne \\(\\Sigma\\) przez \\(w_{j}=X^\\top \\tilde{w}_{j}/\\sqrt{(n-1)\\tilde{\\lambda}_{j}}\\), gdzie \\(\\tilde{\\lambda}_{j}\\) jest odpowiadającą wartością własną.3 Estymator MCD polega na znalezieniu podzbioru \\(h\\) obserwacji (zwykle \\(h \\approx 0.75 n\\)) o najmniejszym wyznaczniku macierzy kowariancji, a następnie obliczeniu średniej i kowariancji na tym podzbiorze. Jest odporny na wartości odstające, ponieważ ignoruje obserwacje, które znacznie zwiększają wyznacznik.4 Metoda ta zakłada, że macierz danych \\(X\\) ma postać \\(X=L+S+E\\), gdzie \\(L\\) jest macierzą niskorangową (sygnał), \\(S\\) jest macierzą rzadką (wartości odstające), a \\(E\\) jest szumem o małej wariancji. Celem jest odzyskanie \\(L\\) poprzez minimalizację funkcji celu \\(\\|L\\|_{*}+\\lambda\\|S\\|_{1}\\) przy ograniczeniu \\(X=L+S\\), gdzie \\(\\|L\\|_{*}\\) jest normą jądra (suma wartości osobliwych \\(L\\)), a \\(\\|S\\|_{1}\\) jest normą \\(L_{1}\\) macierzy \\(S\\).Podsumowując, PCA można sformułować trojako: jako maksymalizację wariancji projekcji przy ograniczeniach ortogonalności, jako dekompozycję spektralną macierzy kowariancji oraz jako obcięte SVD zapewniające najlepszą aproksymację niskorangową.\nZałożenia modelu\nZałożenia dotyczące danych wejściowych do analizy głównych składowych (PCA) są stosunkowo słabe, ale mają istotny wpływ na jakość wyników i interpretację. Można je podzielić na kilka grup:\n\nStruktura danych\n\nLiniowość – PCA zakłada, że główne wzorce zmienności w danych można uchwycić przez liniowe kombinacje zmiennych wejściowych. Jeśli zależności są silnie nieliniowe (np. dane leżą na zakrzywionej rozmaitości), PCA nie odwzoruje ich poprawnie – lepiej wtedy stosować kernel PCA albo metody sąsiedztwa (np. t-SNE, UMAP).\nWspółzależność zmiennych – metoda ma sens tylko wtedy, gdy między cechami istnieją korelacje. Jeśli wszystkie zmienne są niezależne, PCA nie zredukuje wymiarów i każda składowa odpowiadać będzie jednej zmiennej.\n\n\nJednostki i skale pomiarowe\n\nPCA jest wrażliwa na skalę zmiennych, ponieważ opiera się na wariancji.\nJeśli cechy mierzone są w różnych jednostkach (np. temperatura w °C i masa w kg), należy je standaryzować (np. do średniej 0 i wariancji 1).\nGdy wszystkie cechy są w tej samej skali, można pracować na macierzy kowariancji; w przeciwnym razie lepiej korzystać z macierzy korelacji.\n\n\nRozkład danych\n\nNormalność wielowymiarowa nie jest wymagana, ale jeżeli dane mają rozkład wielowymiarowo normalny, to składowe główne są niezależne (nie tylko nieskorelowane), co upraszcza interpretację. Naruszenie założenia o normalności nie sprawia, że PCA nie działa, lecz niezależność składowych nie jest zagwarantowana.\nBrak wartości odstających – PCA jest bardzo wrażliwa na outliery, które mogą wpłynąć na kierunki głównych składowych, bo opiera się na kowariancji. Dlatego dane powinny być oczyszczone lub należy stosować wersje metody odporne (robust PCA).\n\n\nLiczebność próby\n\nAby oszacować macierz kowariancji, liczba obserwacji \\(n\\) powinna być odpowiednio duża względem liczby zmiennych \\(p\\).\nGdy \\(p \\gg n\\), klasyczna PCA bywa niestabilna i stosuje się wtedy dual PCA albo obcięte SVD.\n\n\nBraki danych - PCA wymaga pełnej macierzy danych (bez braków). W przypadku braków stosuje się najczęściej imputację (np. metodą średnich czy metody oparte na modelach).\nInterpretacja graficzna i praktyczna\n\nKod# Pakiety\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(scales)\n\nset.seed(44)\n\n# 1) Dane 2D o eliptycznym rozkładzie (silna współzmienność)\nn  &lt;- 300\nmu &lt;- c(0, 0)\nsd1 &lt;- 2\nsd2 &lt;- 1\nrho &lt;- 0.8\nSigma &lt;- matrix(c(sd1^2, rho*sd1*sd2,\n                  rho*sd1*sd2, sd2^2), nrow = 2, byrow = TRUE)\n\nX &lt;- MASS::mvrnorm(n, mu = mu, Sigma = Sigma) %&gt;%\n  as_tibble(.name_repair = ~c(\"x1\",\"x2\"))\n\n# 2) PCA na danych scentralizowanych (bez standaryzacji)\npca &lt;- prcomp(X, center = TRUE, scale. = FALSE)\n\n# Wartości własne i wektory (ładunki)\nlambda &lt;- pca$sdev^2\nV &lt;- pca$rotation    # kolumny: PC1, PC2\ncenter &lt;- colMeans(X)\n\n# 3) Punkty końcowe wektorów PC1 i PC2 (skalować długością ~ odchylenie wzdłuż składowej)\n# Skala wektora: k * sd wzdłuż danej składowej (tu k = 2 dla czytelności)\nk &lt;- 2\npc1_end &lt;- center + k * pca$sdev[1] * V[,1]\npc2_end &lt;- center + k * pca$sdev[2] * V[,2]\n\n# 4) Ramy wykresu i linie osi oryginalnego układu\nxr &lt;- range(X$x1); yr &lt;- range(X$x2)\n\n# 5) Dane pomocnicze do geometrii\narrows_df &lt;- tribble(\n  ~x,          ~y,          ~xend,        ~yend,     ~label,\n  center[1],   center[2],   pc1_end[1],   pc1_end[2], \"PC1\",\n  center[1],   center[2],   pc2_end[1],   pc2_end[2], \"PC2\"\n)\n\n# Opisy udziału wariancji\nexpl &lt;- percent(lambda / sum(lambda), accuracy = 0.1)\n\n# 6) Wykres\nggplot(X, aes(x = x1, y = x2)) +\n  # chmura punktów\n  geom_point(alpha = 0.5, size = 1.6) +\n  # elipsa rozrzutu (1 odchylenie standardowe ~ poziom 0.68)\n  stat_ellipse(type = \"norm\", level = 0.68, linewidth = 0.8) +\n  # oryginalne osie układu współrzędnych (przez (0,0))\n  geom_hline(yintercept = 0, linetype = 3, linewidth = 0.5) +\n  geom_vline(xintercept = 0, linetype = 3, linewidth = 0.5) +\n  # wektory składowych głównych (wychodzące ze środka danych)\n  geom_segment(data = arrows_df,\n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")),\n               linewidth = 1) +\n  # etykiety PC z udziałem wariancji\n  geom_text(data = arrows_df %&gt;%\n              mutate(txt = ifelse(label==\"PC1\",\n                                  paste0(\"PC1 (\", expl[1], \")\"),\n                                  paste0(\"PC2 (\", expl[2], \")\"))),\n            aes(x = xend, y = yend, label = txt),\n            nudge_x = 0.05, nudge_y = 0.05, hjust = 0, vjust = 0,\n            size = 3.5) +\n  # punkt środka\n  geom_point(aes(x = center[1], y = center[2]), color = \"black\", size = 2) +\n  coord_fixed() +\n  labs(x = \"x1 (oś oryginalna)\",\n       y = \"x2 (oś oryginalna)\",\n       title = \"Oryginalne osie, dane oraz dwie składowe główne (2D)\",\n       subtitle = paste0(\"Udział wariancji: PC1 = \", expl[1], \", PC2 = \", expl[2])) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"plain\"),\n        plot.subtitle = element_text(face = \"plain\"))\n\n\n\n\n\n\n\nDla dwóch wymiarów elipsa rozrzutu danych ma osie ustawione dokładnie wzdłuż \\(w_{1}\\) i \\(w_{2}\\), a ich długości proporcjonalne do \\(\\sqrt{\\lambda_{1}}\\) i \\(\\sqrt{\\lambda_{2}}\\). Transformacja do przestrzeni składowych odpowiada obrotowi układu współrzędnych tak, by oś \\(X_{1}'\\) leżała w kierunku największego rozrzutu, a \\(X_{2}'\\) — w kierunku pozostałej zmienności. Projekcja do \\(k&lt;p\\) wymiarów działa jak rzut ortogonalny na podprzestrzeń rozpiętą przez pierwsze \\(k\\) osi i „spłaszczenie” w pominiętych kierunkach, co minimalizuje błąd rekonstrukcji w sensie średniokwadratowym. Wykresy scores prezentują obiekty w przestrzeni składowych głównych i często ujawniają skupiska lub obserwacje odstające. Wektory loadings są przedstawiane na tzw. kole korelacji, gdzie końce strzałek leżą na okręgu jednostkowym, a ich długości i kąty odzwierciedlają korelacje zmiennych oryginalnych ze składowymi. Zmienne wskazujące podobne kierunki tworzą grupy, co pomaga rozumieć współzmienność. Wykres biplot łączy obie perspektywy: punkty obiektów i kierunki zmiennych w tej samej płaszczyźnie, dzięki czemu można podejrzeć jednocześnie relacje między obiektami i kontrybucje cech. Dodatkowo wykres udziału wariancji, czyli scree plot, porządkuje \\(\\lambda_{j}\\) i pomagać wyznaczyć \\(k\\) przez identyfikację „łokcia” krzywej lub przez osiągnięcie założonego poziomu wariancji kumulatywnej.\nŁadunek \\(w_{jk}\\) to współczynnik liniowej kombinacji \\(j\\)-tej składowej dla \\(k\\)-tej zmiennej; jego znak i wartość bezwzględna informują o kierunku i sile związku. Korelację zmiennej z \\(j\\)-tą składową szacujemy jako cosinus kąta między wektorem zmiennej a osią składowej na kole korelacji; duże wartości sugeruję dużą kontrybucję tej cechy do składowej. Rekonstrukcja obiektu \\(i\\)-tego z \\(k\\) składowych ma postać \\(\\hat{x}_{i}=\\sum_{j=1}^{k} t_{ij} \\, w_{j}^\\top\\), co pozwala na analizę błędów rekonstrukcji i odszumianie przez odcięcie składowych o małych \\(d_{j}\\). W regresji, gdy predyktory są współliniowe, stosuje się regresję na składowych głównych albo regresję grzbietową w przestrzeni scores, co poprawia stronę obliczeniową i zmniejszać wariancję estymatorów.\nKryteria doboru liczby składowych głównych\nDobór liczby składowych głównych (\\(k\\)) jest jednym z kluczowych etapów analizy PCA, ponieważ decyduje o tym, ile informacji (wariancji) zostanie zachowane przy redukcji wymiarowości. Zbyt mała liczba składowych prowadzi do utraty istotnych informacji, a zbyt duża – do utrzymania szumu i nadmiarowej redundancji. W praktyce stosuje się zestaw kryteriów ilościowych i jakościowych, które można podzielić na kilka grup.\nKryteria oparte na wariancji wyjaśnianej\nNajbardziej klasyczne podejście polega na analizie udziału wariancji przechwyconej przez pierwsze \\(k\\) składowych. Dla każdej składowej liczy się wartość własną \\(\\lambda_j\\) macierzy kowariancji, a udział wariancji wyjaśnianej przez pierwsze \\(k\\) składowych to \\[\n\\eta(k) = \\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n\\] Stosowane reguły:\n\nReguła progu wariancji - wybiera się najmniejsze \\(k\\), dla którego \\(\\eta(k)\\) przekracza ustalony próg, np. 80%, 90% lub 95% (w literaturze nie ma jednego progu). Gdy dane silnie skorelowane – wystarczą 2–3 składowe, a gdy dane są bardziej złożone – potrzeba więcej (5–10 i więcej).\nWykres osypiska (scree plot) – wykres wartości własnych \\(\\lambda_j\\) uporządkowanych malejąco. Wybiera się punkt, w którym tempo spadku gwałtownie maleje („łokieć krzywej”).\nWskaźnik udziału marginalnego - \\(\\Delta \\eta_j = \\eta(j) - \\eta(j-1)\\). Gdy przyrost staje się znikomy, dalsze składowe nie wnoszą istotnej informacji.\nKryteria algebraiczne\nKryterium wartości własnej (Kaisera–Guttmana) oparte jest na macierzy korelacji, które mówi, że zachowuje się tylko te składowe, których wartości własne \\(\\lambda_j &gt; 1\\). Oznacza to, że dana składowa wyjaśnia więcej wariancji niż pojedyncza standaryzowana zmienna. Reguła ta jest prosta, ale często zbyt konserwatywna (tendencja do wyboru zbyt wielu składowych).\nKryteria statystyczne i walidacyjne\n\nAnaliza równoległa (Parallel Analysis) - polega na porównaniu wartości własnych uzyskanych z danych rzeczywistych z wartościami własnymi uzyskanymi z wielu symulowanych zestawów danych o tych samych wymiarach, ale z losowym szumem. Zachowuje się tylko te składowe, których wartości własne przekraczają średnią (lub kwantyl) z rozkładu symulowanego. Ta metoda ogranicza ryzyko wyboru składowych wynikających z przypadku.\nWalidacja krzyżowa (Cross-Validation) - gdy PCA wykorzystuje się w kontekście modelowania predykcyjnego (np. PCA regression). Wybiera się takie \\(k\\), które minimalizuje błąd predykcji (np. RMSE) obliczany metodą walidacji krzyżowej.\n\nBartlett’s Test of Sphericity sprawdza, czy korelacje są wystarczająco silne, by PCA miała sens.\n\nBroken Stick Model porównuje udział wariancji każdej składowej z oczekiwaną wartością przy losowym rozkładzie wariancji – zachowuje się tylko te składowe, które przekraczają tę wartość (\\(E_k=\\frac{1}{p}\\sum_{j=k}^{p}\\frac{1}{j}\\)).\nKryteria interpretacyjne i dziedzinowe\nCzasami najważniejszy jest nie wynik numeryczny, lecz użyteczność interpretacyjna:\n\nWybiera się tyle składowych, ile da się sensownie zinterpretować (np. odpowiadających znanym procesom fizycznym, ekonomicznym, biologicznym).\nW analizie wizualnej (np. w eksploracji danych) często wybiera się 2 lub 3 pierwsze składowe, które umożliwiają wykresy 2D lub 3D.\n\n\n\n\n\n\n\n\n\nKryterium\nOpis\nZalety\nOgraniczenia\n\n\n\nUdział wariancji (np. ≥90%)\nZachowaj tyle składowych, by wyjaśnić określony procent całkowitej wariancji\nProste i intuicyjne\nWybór progu bywa arbitralny\n\n\nWykres osypiska (scree plot)\nWybór punktu „kolana” na krzywej wartości własnych\nWizualnie czytelne\nSubiektywne, zależy od interpretacji obserwatora\n\n\nWartość własna &gt; 1 (Kaiser–Guttman)\nZachowaj składowe, których wartości własne przekraczają 1 (dla macierzy korelacji)\nŁatwe obliczeniowo\nCzęsto zbyt liberalne – wybiera zbyt wiele składowych\n\n\nAnaliza równoległa (Parallel Analysis)\nPorównanie wartości własnych z rozkładem uzyskanym z danych losowych\nStatystycznie uzasadnione, ogranicza wybór przypadkowych komponentów\nWymaga symulacji lub dedykowanego oprogramowania\n\n\nWalidacja krzyżowa (Cross-Validation)\nWybór liczby składowych minimalizującej błąd predykcji (np. RMSE)\nNajlepsza w kontekście modeli predykcyjnych\nKosztowna obliczeniowo, wymaga podziału danych\n\n\nModel Broken Stick\nPorównanie udziału wariancji składowych z oczekiwanym rozkładem losowym\nUzasadnione teoretycznie, ogranicza przeuczenie\nMniej intuicyjne, rzadziej używane\n\n\nKryterium interpretacyjne\nWybór liczby składowych możliwych do sensownej interpretacji\nPraktyczne i kontekstowe\nSubiektywne i zależne od wiedzy dziedzinowej\n\n\n\n\nPrzykład 5.1 (PCA na danych irysów)  \n\nKodlibrary(factoextra)\nlibrary(easystats)\nlibrary(gt)\n\npca_iris &lt;- prcomp(iris[,-5], center = TRUE, scale. = TRUE) \n# albo\npca &lt;- principal_components(iris, n = 4, rotate = \"none\") # domyślnie standaryzuje zmienne\n\n# Wykres osypiska\nfviz_eig(pca_iris, addlabels = TRUE)\n\n\n\n\n\n\n\nJak widać z powyższego wykresu osypiska pierwsza składowa wyjaśnia około 73% całkowitej wariancji, a druga 23%. Jeśli chcieć opierać wybór liczby składowych głównych na kryteriach (również takich, które nie były prezentowane powyżej), to można użyć funkcji n_components() pakietu parameters w ekosystemie easystats.\n\nKodk &lt;- n_components(iris[,-5])\nas.data.frame(k)\n\n   n_Factors              Method       Family\n1          0          Scree (R2)     Scree_SE\n2          1             Bentler      Bentler\n3          1 Optimal coordinates        Scree\n4          1 Acceleration factor        Scree\n5          1   Parallel analysis        Scree\n6          1    Kaiser criterion        Scree\n7          1       Velicer's MAP Velicers_MAP\n8          2          Scree (SE)     Scree_SE\n9          2    VSS complexity 1          VSS\n10         2    VSS complexity 2          VSS\n11         3            Bartlett      Barlett\n12         3            Anderson      Barlett\n13         3              Lawley      Barlett\n\nKodplot(k)\n\n\n\n\n\n\n\nChoć większość kryteriów wskazuje na 1 składową, to na potrzeby przykładu wykorzystamy dwie składowe. Wyjaśniają one blisko 96% całkowitej wariancji (patrz poniżej). Możemy teraz przejrzeć wyniki PCA, czyli macierz ładunków (wektorów własnych) i macierz wyników projekcji (scores).\n\nKod# Udział wariancji\nsummary(pca_iris)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\nKod# Ładunki (wektory własne)\npca_iris$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\nKod# scores\nhead(pca_iris$x)\n\n           PC1        PC2         PC3          PC4\n[1,] -2.257141 -0.4784238  0.12727962  0.024087508\n[2,] -2.074013  0.6718827  0.23382552  0.102662845\n[3,] -2.356335  0.3407664 -0.04405390  0.028282305\n[4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n[5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n[6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\nPierwsza składowa główna (PC1) jest kombinacją liniową wszystkich czterech zmiennych, przy czym trzy z nich — Sepal.Length, Petal.Length oraz Petal.Width — mają dodatnie ładunki, natomiast Sepal.Width ma ładunek ujemny. Oznacza to, że składowa ta rośnie, gdy długość działki kielicha oraz długość i szerokość płatków są duże, a maleje, gdy szerokość działki jest duża. Można zatem interpretować PC1 jako wymiar opisujący ogólny rozmiar kwiatu: kwiaty o większych płatkach i węższych działkach uzyskują wyższe wartości tej składowej. W zbiorze iris PC1 bardzo dobrze rozdziela gatunki – setosa charakteryzuje się niskimi wartościami tej składowej (krótkie płatki, szerokie działki), natomiast versicolor i virginica mają wartości wysokie, co odpowiada większym rozmiarom kwiatów.\nDruga składowa główna (PC2) ma zupełnie inną strukturę ładunków. Zdominowana jest przez bardzo silny ujemny współczynnik dla Sepal.Width oraz mniejszy, również ujemny, dla Sepal.Length. Wpływ płatków na tę składową jest niewielki. PC2 odzwierciedla zatem zmienność w obrębie kształtu działki kielicha, a zwłaszcza jej proporcji długości do szerokości. Kwiaty o węższych działkach mają wyższe wartości PC2, natomiast te o szerszych – niższe.\nInterpretując wspólnie obie składowe, można stwierdzić, że PC1 opisuje rozmiar kwiatu, natomiast PC2 – proporcje i kształt działki. W przestrzeni PC1–PC2 dane tworzą układ, w którym setosa jest wyraźnie oddzielona od pozostałych gatunków poprzez niskie wartości PC1 i wysokie PC2, a versicolor i virginica różnią się między sobą głównie wzdłuż drugiej osi. W rezultacie te dwie składowe pozwalają na niemal pełne odwzorowanie i wizualne rozdzielenie gatunków, przy czym PC1 odpowiada za wymiar wielkościowy, a PC2 – za wymiar kształtowy. Na potrzeby wizualizacji możemy narysować wykres biplot łączący obiekty i zmienne w przestrzeni dwóch pierwszych składowych.\n\nKodfviz_pca_biplot(pca_iris, repel = TRUE,\n                col.var = \"blue\", # kolor zmiennych\n                col.ind = iris$Species) + # kolor obiektów wg gatunku\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#ica-comon1994",
    "href": "pca.html#ica-comon1994",
    "title": "Metody redukcji wymiarowości",
    "section": "ICA (Comon 1994)\n",
    "text": "ICA (Comon 1994)\n\nPodstawowy model ICA (ang. Independent Component Analysis) zakłada, że wektor obserwacji \\(X \\in \\mathbb{R}^p\\) powstaje poprzez liniowe i natychmiastowe wymieszanie wektora ukrytych źródeł \\(s \\in \\mathbb{R}^m\\) o statystycznie niezależnych składowych \\[\nX = A s,\\qquad A \\in \\mathbb{R}^{p\\times m},\n\\] przy czym \\(m \\le \\min(p,n)\\) oraz macierz mieszająca \\(A\\) ma pełny rząd. Celem jest oszacowanie macierzy demiksującej \\(W \\in \\mathbb{R}^{m\\times p}\\) tak, aby \\(y = W X\\) aproksymować \\(s\\) składowymi możliwie niezależnymi w sensie probabilistycznym. Z istoty problemu rozwiązanie identyfikowalne jest jedynie do permutacji i skalowania - kolejność oraz skale (a więc i znaki) składowych nie są odzyskiwalne.\nWyprowadzenie algorytmów ICA rozpoczynamy od scentralizowania danych i ich whiteningu. Niech \\(\\Sigma_X = \\tfrac{1}{n}\\sum_i (X_i-\\bar X)(X_i-\\bar X)^\\top\\) oraz niech \\(V\\) oznacza macierz whitening taką, że \\(Z=V(X-\\bar X)\\) spełnia \\(\\operatorname{Cov}(Z)=I_p\\). W praktyce przyjmujemy \\(V=\\Lambda^{-1/2}U^\\top\\) z dekompozycji \\(\\Sigma_X=U\\Lambda U^\\top\\). W przestrzeni whitened model przyjmuje postać \\[\nZ = V A s \\equiv R\\, s,\n\\] gdzie \\(R\\) jest macierzą ortogonalną (dla przypadku \\(m=p\\)). Poszukujemy więc wektorów w jednostkowej normie, dla których skalarna projekcja \\(y=w^\\top Z\\) jest możliwie „nienormalna” (niesymetryczna lub ciężkoogonowa), co stanowi praktyczne kryterium niezależności.\n\nKod# Pakiety\nlibrary(fastICA)\nlibrary(patchwork)\n\nset.seed(44)\n\n# 1) Generowanie dwóch niezależnych źródeł (niegaussowskich)\nn &lt;- 3000\ns1 &lt;- rexp(n, rate = 1) - 1  # Jednostronnie cięższy ogon (Eksponencjalny przesunięty do zera średniej)\ns2 &lt;- runif(n, -2, 2)        # Równomierny (płaskie ogony)\nS  &lt;- cbind(s1, s2)\nS  &lt;- scale(S, center = TRUE, scale = FALSE) # zero-mean dla wygody\n\n# 2) Mieszanie źródeł macierzą A -&gt; obserwacje X\nA &lt;- matrix(c(1, 2,\n              2, 1), nrow = 2, byrow = TRUE)\nX &lt;- S %*% t(A)                        # model X = S A^T\nX &lt;- scale(X, center = TRUE, scale = FALSE)  # centrowanie (odjęcie średniej kolumn)\n\n# 3) Whitening (sferyzacja): Z = V X, gdzie V = Λ^{-1/2} U^T z dekompozycji Σ_X\nSigmaX &lt;- cov(X)\ne &lt;- eigen(SigmaX)\nU &lt;- e$vectors\nLambda &lt;- diag(e$values)\nV &lt;- solve(sqrt(Lambda)) %*% t(U)     # Λ^{-1/2} U^T\nZ &lt;- t(V %*% t(X))                    # Z = V X (w wierszach obserwacje)\n\n# Kontrola: kowariancja Z ~ I\nround(cov(Z), 3)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKod# 4) ICA (FastICA) na danych whitened (można też na X bo fastICA automatycznie wykonuje whitening)\nica &lt;- fastICA(Z, n.comp = 2, method = \"C\")  # odzyskane źródła Y i mieszanie A_ICA\nY &lt;- ica$S                                   # szacowane źródła niezależne (kolumny ~ komponenty)\nW &lt;- ica$K %*% ica$W                         # łączne \"demiksowanie\" względem X (tu pracowaliśmy na Z)\n# Uwaga: fastICA zwraca też K (whitening) i W (unmixing), składnia zależy od wejścia\n\n# 5) Przygotowanie danych do wykresów\nto_df &lt;- function(M, name){\n  as.data.frame(M) |&gt;\n    setNames(c(\"c1\",\"c2\")) |&gt;\n    mutate(stage = name)\n}\ndf_X &lt;- to_df(X, \"X: dane zmieszane\")\ndf_Z &lt;- to_df(Z, \"Z: po whitening\")\ndf_Y &lt;- to_df(Y, \"Y: po ICA (źródła)\")\n\ndf_all &lt;- bind_rows(df_X, df_Z, df_Y)\n\n# 6) Oś układu i wektory bazowe (do wizualizacji obrotów)\n#    W przestrzeni Z osie są już sferyczne (I), więc ICA to „tylko” obrót.\naxes_df &lt;- function(scale_len = 2){\n  data.frame(x = c(0,0), y = c(0,0),\n             xend = c(scale_len,0), yend = c(0,scale_len),\n             label = c(\"e1\",\"e2\"))\n}\naxesZ &lt;- axes_df()\n\n# 7) Wykresy: chmury punktów w 2D (X, Z, Y)\npX &lt;- ggplot(df_X, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8) +\n  coord_equal() +\n  labs(title = \"Przed whiteningiem (X)\",\n       x = \"X[,1]\", y = \"X[,2]\") +\n  theme_minimal()\n\npZ &lt;- ggplot(df_Z, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#2E86DE\") +\n  geom_segment(data = axesZ, aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.18,\"cm\")), linewidth = 0.8) +\n  geom_text(data = axesZ, aes(x = xend, y = yend, label = label),\n            nudge_x = 0.05, nudge_y = 0.05, size = 3) +\n  coord_equal() +\n  labs(title = \"Po whitening (Z): Cov ≈ I\",\n       x = \"Z[,1]\", y = \"Z[,2]\") +\n  theme_minimal()\n\npY &lt;- ggplot(df_Y, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#16A085\") +\n  coord_equal() +\n  labs(title = \"Po ICA (Y): odzyskane źródła niezależne\",\n       x = \"Y[,1]\", y = \"Y[,2]\") +\n  theme_minimal()\n\n(pX | pZ | pY)\n\n\n\n\n\n\nKod# 8) Dodatkowo: marginesowe histogramy pokazujące „nienormalność”\nhX &lt;- df_X |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"X\")\nhZ &lt;- df_Z |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Z\")\nhY &lt;- df_Y |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Y\")\n\nh_all &lt;- bind_rows(hX,hZ,hY) |&gt;\n  mutate(stage = factor(stage, levels = c(\"X\",\"Z\",\"Y\")))\n\nggplot(h_all, aes(val)) +\n  geom_histogram(bins = 60, fill = \"grey70\", color = \"white\") +\n  facet_grid(stage ~ col, scales = \"free_y\") +\n  labs(title = \"Marginalne rozkłady: przed whiteningiem, po whitening, po ICA\",\n       x = \"wartość\", y = \"liczność\") +\n  theme_minimal()\n\n\n\n\n\n\nKod# 9) Krótka kontrola: kowariancje i korelacje\ncat(\"\\nKowariancja X:\\n\"); print(round(cov(X),3))\n\n\nKowariancja X:\n\n\n      [,1]  [,2]\n[1,] 6.295 4.620\n[2,] 4.620 5.123\n\nKodcat(\"\\nKowariancja Z (powinna być bliska I):\\n\"); print(round(cov(Z),3))\n\n\nKowariancja Z (powinna być bliska I):\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKodcat(\"\\nKorelacje pomiędzy kolumnami Y (powinny być bliskie 0; niezależność jest mocniejsza niż brak korelacji):\\n\")\n\n\nKorelacje pomiędzy kolumnami Y (powinny być bliskie 0; niezależność jest mocniejsza niż brak korelacji):\n\nKodprint(round(cor(Y),3))\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nPodejście maksymalizujące nienormalność opiera się na kurtozie lub na przybliżonej negatywnej entropii (ang. negentropy). Dla \\(\\operatorname{Var}(y)=1\\) kurtoza \\(\\kappa(y)=\\mathbb{E}\\{y^4\\}-3\\) przyjmuje wartości 0 dla rozkładu normalnego i wartości odległe od zera dla rozkładów nienormalnych. Maksymalizacja \\(|\\kappa(w^\\top Z)|\\) prowadzi do składowych niezależnych. Stabilniejsze i bardziej ogólne kryterium stanowi negentropy \\(J(y)=H(y_{\\text{gauss}})-H(y)\\), gdzie \\(H\\) oznacza entropię. W praktyce stosuje się aproksymacje postaci \\[\nJ(y)\\approx \\Big(\\mathbb{E}\\,G(y)-\\mathbb{E}\\,G(v)\\Big)^2,\n\\] z dobraną nieliniowością \\(G\\) oraz \\(v\\sim \\mathcal N(0,1)\\). Maksymalizacja \\(J\\) przy ograniczeniu \\(\\|w\\|=1\\) zapewnia poszukiwanie najbardziej nienormalnych kierunków.\nZ kryteriów tych wynika algorytm FastICA jako iteracja stałego punktu. Dla jednego komponentu w przestrzeni whitened stosujemy aktualizację \\[\nw \\leftarrow \\mathbb{E}\\{Z\\,g(w^\\top Z)\\}-\\mathbb{E}\\{g’(w^\\top Z)\\}\\, w,\\qquad \\text{następnie } w\\leftarrow \\frac{w}{\\|w\\|},\n\\] gdzie \\(g=G’\\) jest score function (np. \\(g(u)=\\tanh(u)\\), \\(g(u)=u^3\\) lub \\(g(u)=u\\exp(-u^2/2)\\)). Dla wielu składowych stosujemy równoległe aktualizacje i ortogonalizację w kolejnych krokach, np. metodą rzutów Grama–Schmidta lub przez dekompozycję symetryczną \\(W\\leftarrow (WW^\\top)^{-1/2}W\\), co zachowuje wzajemną ortogonalność wektorów w przestrzeni whitened i zapobiega zbieżności do tej samej składowej.\nAlternatywne wyprowadzenie pochodzi z maksymalizacji funkcji wiarygodności (maximum likelihood). Zakładając niezależność źródeł z gęstościami \\(p_{s_i}\\) i (dla prostoty) brak szumu, otrzymujemy logarytm funkcji wiarygodności \\[\n\\mathcal L(W)=\\sum_{t=1}^n\\Bigg(\\sum_{i=1}^m \\log p_{s_i}\\big((W X_t)_i\\big)\\Bigg) + n\\log|\\det W|.\n\\] Jej gradient prowadzi do zasady Infomax, która brzmi: dobrać \\(W\\) tak, aby wyjścia miały jak największą sumę entropii (co przy zachowaniu \\(\\log|\\det W|\\) jest równoważne maksymalizacji wspólnej niezależności). W praktyce wybór rodziny \\(p_{s_i}\\) implikuje odpowiednie nieliniowości w regule uczenia, formalnie zbieżne z powyższymi kontrastami na negentropy.\nW obecności szumu addytywnego \\(X = A s + \\varepsilon\\) z \\(\\varepsilon\\sim \\mathcal N(0,\\sigma^2 I)\\) problem staje się trudniejszy. Stosuje się wówczas rozszerzone modele ICA z estymacją rzędu i składowej szumowej, warianty bayesowskie, lub metody wykorzystujące dodatkowe własności źródeł (np. niezależność czasową wyższych rzędów, jak w SOBI wykorzystującym autokowariancje).\nZałożenia identyfikowalności obejmują liniowość i natychmiastowość mieszania, niezależność składowych źródłowych, co najwyżej jedną składową o rozkładzie normalnym (inaczej problem staje się nierozwiązywalny z powodu nieodróżnialności kierunków gaussowskich), pełny rząd macierzy \\(A\\) oraz wystarczającą nienormalność źródeł, aby kontrasty informacyjne miały sens. Zwyczajowo zakłada się również stacjonarność w czasie, o ile wykorzystujemy momenty lub autokorelacje do estymacji.\nDobór liczby składowych w ICA nie opiera się na udziale wariancji, jak w PCA, ponieważ ICA nie porządkuje komponentów według wariancji. W praktyce najpierw wybiera się wymiar whiteningu \\(m\\) (efektywny rząd sygnału), a następnie ekstrahuje \\(m\\) składowych niezależnych. Kryteria wyboru \\(m\\) obejmują informacyjne miary rzędu macierzy kowariancji, takie jak MDL/BIC dopasowane do modelu składowej szumowej i niezerowych wartości własnych, testy istotności dla wartości własnych po whiteningu (warianty analizy równoległej, permutacyjne testy mierzące losowość), walidację na podstawie wiarygodności w modelu ML-ICA z różnymi \\(m\\) oraz kryteria stabilności. Kryteria stabilności polegają na wielokrotnym uruchomieniu algorytmu z różnymi inicjalizacjami i grupowaniu uzyskanych komponentów. Liczba dobrze replikujących się grup daje oszacowanie na \\(m\\). Dodatkowo stosuje się testy resztowej zależności między oszacowanymi źródłami (np. testy niezależności na bazie informacji wzajemnej). Jeśli po dodaniu kolejnej składowej informacja wzajemna między „źródłami” nie maleje, zwiększanie \\(m\\) nie przynosi korzyści. W zastosowaniach z szumem wybieramy \\(m\\) tak, by oddzielać podprzestrzeń sygnałową od szumowej, co praktycznie sprowadza się do analizy spektrum wartości własnych i modelowania ogona jako białego szumu.\nInterpretacja wyników ICA różnić się od PCA. Składowe ICA \\(y_i\\) stanowią oceny źródeł o maksymalnej niezależności, a wiersze \\(W\\) definiują filtry demiksujące, podczas gdy kolumny \\(A\\) (przyjmując \\(A\\approx W^{-1}\\)) reprezentują wzorce mieszania, czyli „mapy obciążenia” źródeł na czujniki/cechy. Skale i znaki składowych są arbitralne, co wymaga interpretować je względnie: znormalizować wariancję lub maksymalną wartość, a znak dobrać tak, by ułatwić opis dziedzinowy5. W przeciwieństwie do PCA, składowe ICA nie muszą być ortogonalne, a ich wariancje nie są uporządkowane6.\n5 Wyobraźmy sobie, że ICA rozdziela dwa źródła dźwięku — skrzypce i fortepian. Jeśli algorytm zwróci sygnał, który jest odwrócony w fazie (czyli pomnożony przez -1), to dźwięk fortepianu jest ten sam fizycznie, tylko wszystkie amplitudy mają odwrotny znak. Dlatego znak (i skala) nie mają znaczenia dla jakości separacji — są arbitralne.6 W sygnałach biologicznych, takich jak EEG, ICA może oddzielić artefakty ruchowe od sygnałów mózgowych. Artefakty te mogą być silnie nienormalne i niezależne od sygnałów mózgowych, co czyni ICA skuteczną metodą ich identyfikacji i usunięcia.\nPrzykład 5.2 (ICA na mieszance sygnałów)  \n\nKoddata(\"EuStockMarkets\")\nP &lt;- as.data.frame(EuStockMarkets)              # poziomy indeksów: DAX, SMI, CAC, FTSE\nR &lt;- as.data.frame(apply(P, 2, function(x) diff(log(x))))  # dzienne log-zwroty\ncolnames(R) &lt;- colnames(P)\n\n# ICA na dziennych zwrotach\nset.seed(123)\nica_res &lt;- fastICA(R, n.comp = 4, method = \"C\")\nS_est &lt;- as.data.frame(ica_res$S)               # odzyskane źród\ncolnames(S_est) &lt;- paste0(\"IC\", 1:4)\nA_est &lt;- ica_res$A                              # macierz mieszająca\nW_est &lt;- ica_res$K %*% ica_res$W                # macierz demiksująca\n\n\nMacierz A_est, czyli macierz mieszania, opisuje sposób, w jaki oryginalne zmienne obserwowalne — w tym przypadku cztery indeksy giełdowe: DAX, SMI, CAC i FTSE — powstają jako liniowe kombinacje ukrytych, niezależnych czynników. Każdy wiersz tej macierzy odpowiada jednemu indeksowi, a każda kolumna jednej składowej niezależnej. Wartości liczbowe oznaczają współczynniki liniowych kombinacji, czyli wpływ danej składowej na dany indeks. Wartość dodatnia wskazuje, że wzrost komponentu powoduje wzrost indeksu, wartość ujemna — że ruch komponentu przekłada się na spadek indeksu, a wartość bliska zeru oznacza brak istotnego związku.\n\nKodround(A_est, 3)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] -0.001 -0.001 -0.002 -0.006\n[2,]  0.003 -0.004  0.002  0.000\n[3,] -0.010 -0.008 -0.008 -0.005\n[4,]  0.000  0.001  0.008  0.001\n\n\n\nPierwszy komponent (IC1) najsilniej ładuje się na indeks CAC, a w mniejszym stopniu na SMI. Znak ujemny dla CAC i dodatni dla SMI sugeruje, że komponent ten uchwyca różnicę pomiędzy rynkami strefy euro a rynkiem szwajcarskim, czyli czynnik kontrastujący. W praktyce oznacza to, że wzrost aktywności na rynkach kontynentalnych wiązać się może z relatywnym osłabieniem rynku SMI lub odwrotnie.\nDrugi komponent (IC2) również oddziałuje na CAC i SMI w kierunku ujemnym, co może świadczyć o uchwyceniu wspólnego czynnika kontynentalnego o mniejszej amplitudzie. Dodatnie, choć niewielkie wartości dla FTSE wskazują, że komponent ten częściowo kontrastuje rynki kontynentalne z brytyjskim.\nTrzeci komponent (IC3) ma wyraźnie odmienną strukturę. Dla FTSE współczynnik jest dodatni i największy, natomiast dla pozostałych indeksów ujemny. Komponent ten rozdziela zatem rynek brytyjski od reszty Europy i można go interpretować jako czynnik geograficzny lub walutowy, związany z odmiennym otoczeniem gospodarczym Wielkiej Brytanii.\nCzwarty komponent (IC4) ma współczynniki bardzo małe, rzędu 10-3–10-2, co sugeruje, że jego wpływ na strukturę indeksów jest marginalny. Prawdopodobnie odpowiada on za szum lub krótkotrwałe, lokalne fluktuacje, które nie mają znaczenia ekonomicznego.\n\n\nKodround(W_est, 3)\n\n         [,1]     [,2]    [,3]    [,4]\n[1,]   43.737  112.245 -74.794 -85.258\n[2,]   34.702 -146.908 -47.294  -7.649\n[3,]   15.975   14.330   8.569 141.650\n[4,] -173.263   -5.797  10.661 -35.834\n\n\nMacierz W_est, czyli macierz demiksująca, zawiera współczynniki liniowych kombinacji oryginalnych zmiennych (indeksów giełdowych), które pozwalają uzyskać poszczególne niezależne komponenty. Każdy wiersz tej macierzy odpowiada jednemu komponentowi ICA (IC1–IC4), a każda kolumna — jednej zmiennej obserwowalnej (DAX, SMI, CAC, FTSE). Wartości w tej macierzy można zatem interpretować jako wagi, z jakimi poszczególne indeksy uczestniczą w tworzeniu danego odzyskanego źródła.\n\nPierwszy komponent (IC1) ma duże dodatnie wagi dla indeksów DAX i SMI oraz silnie ujemną wagę dla FTSE. Oznacza to, że IC1 odzwierciedla kontrast pomiędzy rynkami kontynentalnymi (Niemcy, Szwajcaria) a rynkiem brytyjskim. Wzrost wartości IC1 odpowiada sytuacji, w której indeksy kontynentalne zachowują się silniej niż FTSE — można więc interpretować ten czynnik jako różnicowy, typu „Europa kontynentalna kontra Wielka Brytania”.\nDrugi komponent (IC2) pokazuje odwrotny schemat: dodatni wpływ DAX, silnie ujemny SMI, a słaby wpływ pozostałych indeksów. Można go interpretować jako czynnik rozróżniający zachowanie rynku niemieckiego i szwajcarskiego, który w ICA często ujawnia się jako efekt odmiennych warunków walutowych i struktury gospodarczej.\nTrzeci komponent (IC3) ma wszystkie wagi ujemne, z wyjątkiem niewielkich dodatnich dla CAC i FTSE. Oznacza to, że IC3 reprezentuje wspólny kierunek zmian większości indeksów (ruch globalny), ale w konstrukcji demiksującej występuje ze znakiem ujemnym. W praktyce odpowiada to czynnikowi rynkowemu o charakterze ogólnym — globalnemu impulsowi, który oddziałuje w podobny sposób na większość rynków.\nCzwarty komponent (IC4) ma wysoką dodatnią wagę dla CAC oraz ujemne dla pozostałych indeksów, co sugeruje, że może on odzwierciedlać czynnik specyficzny dla rynku francuskiego — reakcje lokalne lub sektorowe, które nie są wspólne dla innych giełd..\n\nZnaki współczynników w ICA są arbitralne (zmiana wszystkich znaków w jednym wierszu nie zmienia modelu), dlatego przy interpretacji należy zwracać uwagę na względne zależności między indeksami, a nie na samą polaryzację znaków. Wartości bezwzględne wag pokazują natomiast, które indeksy mają największy udział w kształtowaniu danego czynnika.\n\nKodkurt &lt;- apply(S_est, 2, function(x) mean(x^4) - 3)  # kurtozy odzyskanych źródeł\nprint(round(kurt, 3))                           # kurtozy (nienormalność)\n\n  IC1   IC2   IC3   IC4 \n5.601 1.985 8.206 2.274 \n\n\nWartości kurtozy stanowią miarę niegaussowskości rozkładu — czyli tego, jak bardzo dany sygnał odbiega od kształtu rozkładu normalnego. Wartości dodatnie oznaczają rozkłady o „cięższych ogonach” i bardziej spiczastym kształcie (tzw. leptokurtyczne), co jest typowe dla sygnałów rzadkich, zawierających wyraźne piki i okresy stabilności. W kontekście ICA wysoka kurtoza jest pożądana, ponieważ algorytm poszukuje właśnie takich komponentów — maksymalnie odmiennych od normalnych, a więc potencjalnie niezależnych źródeł.\n\n\nIC1 (5.601) ma bardzo wysoką kurtozę, co wskazuje na silną niegaussowskość. Komponent ten prawdopodobnie reprezentuje główny, „rzadki” czynnik ekonomiczny, który reaguje gwałtownie w momentach istotnych zmian rynkowych. Może to być globalny impuls rynkowy lub okresowe szoki finansowe.\n\nIC2 (1.985) ma umiarkowanie dodatnią kurtozę, sugerującą rozkład jedynie lekko leptokurtyczny. Oznacza to, że komponent jest bliższy rozkładowi normalnemu, a zatem mniej „niezależny” w sensie ICA. Może reprezentować łagodniejszy czynnik wspólny, np. codzienną zmienność lub trend regionalny.\n\nIC3 (8.206) ma najwyższą kurtozę spośród wszystkich komponentów. Jest to bardzo silny sygnał niegaussowski, typowy dla źródła zawierającego rzadkie, intensywne zdarzenia — w kontekście finansowym mogą to być momenty skokowych zmian cen lub kryzysów, wpływające selektywnie na część indeksów. Ten komponent można traktować jako najbardziej „czyste” źródło w sensie ICA.\n\nIC4 (2.274) wykazuje umiarkowaną kurtozę, zbliżoną do IC2. Można go interpretować jako dodatkowy, mniej wyraźny czynnik poboczny, który w pewnym stopniu odbiega od normalności, ale nie ma charakteru dominującego.\n\n\nKod# Sprawdzenie korelacji między oryginalnymi a odzyskanymi sygnałami\ncor_matrix &lt;- cor(R, S_est)\nprint(round(cor_matrix, 3))\n\n        IC1    IC2    IC3   IC4\nDAX  -0.076  0.287 -0.954 0.037\nSMI  -0.060 -0.475 -0.871 0.108\nCAC  -0.189  0.140 -0.686 0.689\nFTSE -0.788  0.001 -0.602 0.125\n\n\nMacierz korelacji między oryginalnymi indeksami giełdowymi a odzyskanymi komponentami niezależnymi (cor_matrix) pokazuje, jak silnie i w jakim kierunku (znak dodatni lub ujemny) każdy z indeksów jest powiązany z danym źródłem ICA. Wysokie wartości bezwzględne wskazują, że dany komponent w dużym stopniu tłumaczy zmienność danego indeksu, natomiast wartości bliskie zera oznaczają słaby związek.\n\nNajsilniejsze korelacje obserwuje się dla komponentu IC3, który ma wartości ujemne i bardzo wysokie w module: DAX (−0.954), SMI (−0.871) i CAC (−0.686). Oznacza to, że IC3 stanowi wspólny czynnik dominujący dla trzech kontynentalnych indeksów europejskich. Wszystkie trzy reagują w tym samym kierunku (ujemny znak jest konwencjonalny, jego odwrócenie nie zmienia interpretacji). Można zatem uznać, że IC3 reprezentuje globalny czynnik rynkowy, wspólny dla głównych giełd kontynentalnych, a jego wysoka kurtoza (8.206) wskazuje, że czynnik ten cechuje się silnymi, epizodycznymi wahaniami — typowymi dla okresów zawirowań finansowych.\nKomponent IC1 wykazuje wyraźną ujemną korelację z FTSE (−0.788), przy braku silnych zależności z pozostałymi indeksami. Oznacza to, że IC1 można interpretować jako czynnik specyficzny dla rynku brytyjskiego, niezależny od ruchów kontynentalnych. Wysoka wartość bezwzględna korelacji sugeruje, że ten komponent odpowiada za znaczną część zmienności FTSE, co dobrze współgra z interpretacją wcześniejszej macierzy mieszania — IC1 oddzielał Wielką Brytanię od reszty Europy.\nKomponent IC2 wykazuje umiarkowane korelacje o różnych znakach: dodatnią z DAX (0.287) i ujemną ze SMI (−0.475). Można go zatem interpretować jako czynnik różnicowy pomiędzy rynkami Niemiec i Szwajcarii. W praktyce może on odzwierciedla odmienną reakcję tych rynków na czynniki lokalne, np. różnice w strukturze sektorowej lub polityce monetarnej.\nKomponent IC4 ma umiarkowaną dodatnią korelację z CAC (0.689), a pozostałe indeksy reagują na niego słabo. Oznacza to, że IC4 może być czynnikiem częściowo specyficznym dla rynku francuskiego, prawdopodobnie o charakterze lokalnym lub szumowym.\n\nNa koniec wizualizacja ICA.\n\nKod# odzyskane komponenty \nS_est_long &lt;- S_est |&gt;\n  mutate(Time = 1:nrow(S_est)) |&gt;\n  pivot_longer(cols = starts_with(\"IC\"),\n                      names_to = \"Component\", values_to = \"Value\")\n\np1 &lt;- ggplot(S_est_long, aes(x = Time, y = Value, color = Component)) +\n  geom_line() +\n  facet_wrap(~ Component, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Odzyskane niezależne komponenty (ICA)\", x = \"Czas\", y = \"Wartość\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nR_long &lt;- R |&gt;\n  mutate(Time = 1:nrow(R)) |&gt;\n  pivot_longer(cols = -Time,    \n                      names_to = \"Index\", values_to = \"Return\")\n\np2 &lt;- ggplot(R_long, aes(x = Time, y = Return, color = Index)) +\n  geom_line() +\n  facet_wrap(~ Index, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Oryginalne dzienne log-zwroty indeksów\", x = \"Czas\", y = \"Log-zwrot\") +\n  scale_color_flat_d() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#t-sne-vandermaaten08a",
    "href": "pca.html#t-sne-vandermaaten08a",
    "title": "Metody redukcji wymiarowości",
    "section": "t-SNE (Maaten i Hinton 2008)\n",
    "text": "t-SNE (Maaten i Hinton 2008)\n\nMetoda t-distributed Stochastic Neighbor Embedding (t-SNE) została opracowana przez Laurensa van der Maatena i Geoffreya Hintona w 2008 roku jako nieliniowa technika redukcji wymiarowości, której celem jest odwzorowanie lokalnej struktury danych wysokowymiarowych w przestrzeni o mniejszej liczbie wymiarów, zwykle dwuwymiarowej lub trójwymiarowej. W przeciwieństwie do metod liniowych, takich jak PCA, t-SNE nie dąży do maksymalizacji wariancji, lecz do zachowania sąsiedztw pomiędzy punktami – obserwacje, które w przestrzeni oryginalnej są blisko siebie, powinny również pozostawać blisko w przestrzeni odwzorowania.\nNiech dane wejściowe tworzą macierz \\(X = [x_1, x_2, \\dots, x_n]^\\top\\), gdzie każdy wektor \\(x_i \\in \\mathbb{R}^p\\) reprezentuje jedną obserwację w przestrzeni o wymiarze \\(p\\). Pierwszym krokiem jest przekształcenie danych wysokowymiarowych w macierz podobieństw, która opisuje, jak bardzo punkty są „bliskie” względem siebie. Dla każdego punktu \\(x_i\\) definiuje się rozkład warunkowy \\[\np_{j|i} = \\frac{\\exp\\!\\left(-\\frac{|x_i - x_j|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{|x_i - x_k|^2}{2\\sigma_i^2}\\right)}, \\quad p_{i|i} = 0,\n\\] gdzie parametr \\(\\sigma_i\\) (odpowiednik bandwidth) dobiera się tak, aby entropia rozkładu \\(P_i = (p_{j|i})_j = (p_{1|i}, p_{2|i}, \\dots, p_{n|i}).\\) odpowiadała zadanej perplexity, czyli efektywnej liczbie sąsiadów. Perplexity jest hiperparametrem kontrolującym zakres lokalności analizowanych relacji.\nNastępnie konstruuje się symetryczną macierz podobieństw \\[\np_{ij} = \\frac{p_{i|j} + p_{j|i}}{2n},\n\\] która reprezentuje prawdopodobieństwo, że punkty \\(x_i\\) i \\(x_j\\) są bliskimi sąsiadami w przestrzeni oryginalnej.\nKolejnym krokiem jest utworzenie analogicznego rozkładu w przestrzeni odwzorowania \\(Y = [y_1, y_2, \\dots, y_n]^\\top\\), gdzie \\(y_i \\in \\mathbb{R}^q\\) i zwykle \\(q = 2\\) lub 3. Dla tych punktów definiuje się rozkład podobieństw oparty na rozkładzie t-Studenta z jednym stopniem swobody \\[\nq_{ij} = \\frac{(1 + |y_i - y_j|^2)^{-1}}{\\sum_{k \\neq l} (1 + |y_k - y_l|^2)^{-1}}, \\quad q_{ii} = 0.\n\\] Rozkład t-Studenta ma grube ogony, co umożliwia bardziej realistyczne odwzorowanie relacji między punktami odległymi od siebie i redukuje problem crowding, czyli nadmiernego ściskania punktów w centrum przestrzeni odwzorowania.\nCelem t-SNE jest minimalizacja dywergencji Kullbacka–Leiblera między rozkładami \\(P\\) i \\(Q\\) \\[\nC = \\operatorname{KL}(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n\\] Optymalizacja tej funkcji, zwykle za pomocą spadku gradientowego, prowadzi do znalezienia takich współrzędnych \\(Y\\), które zachowują lokalne relacje między punktami w jak największym stopniu. Gradient funkcji celu względem współrzędnych \\(y_i\\) ma postać \\[\n\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij}) (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1},\n\\] a współczynnik 4 pełni rolę skalującą. W praktyce stosuje się dodatkowe techniki stabilizujące proces uczenia, takie jak momentum, etap early exaggeration zwiększający kontrast lokalnych podobieństw, oraz wcześniejszą redukcję wymiarowości metodą PCA w celu ograniczenia szumu.\nMetoda t-SNE nie zakłada liniowości ani rozkładu normalnego danych, lecz wymaga, aby dane były znormalizowane w przypadku różnych jednostek pomiarowych, ponieważ odległości euklidesowe są wrażliwe na skalę. Wskazane jest wcześniejsze zastosowanie PCA w celu usunięcia szumu i zmniejszenia złożoności obliczeniowej. Dane nie powinny zawierać dużej liczby wartości odstających ani duplikatów, które mogłyby zaburzyć lokalne struktury. Kluczowy hiperparametr perplexity powinien być dostosowany do liczby obserwacji — zbyt mała wartość prowadzi do przeuczenia lokalnego, a zbyt duża powoduje zatarcie drobnych struktur.\nWyniki t-SNE przedstawione w przestrzeni dwuwymiarowej lub trójwymiarowej nie mają interpretacji metrycznej. Oznacza to, że odległości między klastrami nie są bezpośrednio interpretowalne ilościowo. Interpretacja opiera się głównie na analizie sąsiedztwa: punkty znajdujące się blisko siebie w przestrzeni t-SNE odpowiadają obserwacjom podobnym w oryginalnych cechach, natomiast wyraźne skupiska punktów mogą wskazywać na istnienie klas lub podgrup. Oś pierwsza i druga nie mają znaczenia merytorycznego – są jedynie współrzędnymi w przestrzeni odwzorowania, które zachowuje lokalną strukturę, a nie globalną geometrię danych.\n\n\n\n\n\n\nWażne kroki przy stosowaniu t-SNE\n\n\n\n\nStandaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.\nWybór zakresu perplexity — zwykle testuje się kilka wartości (np. 5, 15, 30, 50) i ocenia stabilność struktur (czy klastry są rozdzielne, czy stabilne względem permutacji danych).\nUstawienie learning rate — zaczyna się od wartości domyślnej (200) i w razie potrzeby zwiększa do 500–1000, jeśli klastry są zbyt zwarte.\nUstalenie liczby iteracji — co najmniej 500; w przypadku dużych zbiorów można zwiększyć do 1000–2000, jeśli rozkład nadal się zmienia.\nPorównanie z PCA lub UMAP — warto sprawdzić, czy t-SNE nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykład 5.4 (t-SNE na danych iris)  \n\nKodlibrary(Rtsne)      # t-SNE\n\nset.seed(44)\n\n# Przygotowanie danych (jak wcześniej)\niris_data &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  as.matrix() %&gt;%\n  scale()\n\n# t-SNE\ntsne_result &lt;- Rtsne(\n  iris_data,\n  dims = 2, perplexity = 30, verbose = TRUE,\n  max_iter = 500, check_duplicates = FALSE\n)\n\nPerforming PCA\nRead the 150 x 4 data matrix successfully!\nUsing no_dims = 2, perplexity = 30.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.00 seconds (sparsity = 0.711156)!\nLearning embedding...\nIteration 50: error is 45.359701 (50 iterations in 0.01 seconds)\nIteration 100: error is 44.348491 (50 iterations in 0.01 seconds)\nIteration 150: error is 43.459291 (50 iterations in 0.01 seconds)\nIteration 200: error is 45.385876 (50 iterations in 0.01 seconds)\nIteration 250: error is 45.872962 (50 iterations in 0.01 seconds)\nIteration 300: error is 0.661337 (50 iterations in 0.01 seconds)\nIteration 350: error is 0.165504 (50 iterations in 0.01 seconds)\nIteration 400: error is 0.161554 (50 iterations in 0.01 seconds)\nIteration 450: error is 0.160806 (50 iterations in 0.01 seconds)\nIteration 500: error is 0.157935 (50 iterations in 0.01 seconds)\nFitting performed in 0.07 seconds.\n\nKodtsne_df &lt;- data.frame(\n  Dim1 = tsne_result$Y[,1],\n  Dim2 = tsne_result$Y[,2],\n  Species = iris$Species\n)\n\n# PCA na tych samych danych\npca_fit &lt;- prcomp(iris_data, center = FALSE, scale. = FALSE)\npca_df &lt;- data.frame(\n  PC1 = pca_fit$x[,1],\n  PC2 = pca_fit$x[,2],\n  Species = iris$Species\n)\n\n# Wykres t-SNE\np1 &lt;- ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"t-SNE\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Wykres PCA (pierwsze dwie składowe)\np2 &lt;- ggplot(pca_df, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "pca.html#umap-konopka2018",
    "href": "pca.html#umap-konopka2018",
    "title": "Metody redukcji wymiarowości",
    "section": "UMAP (Konopka 2018)\n",
    "text": "UMAP (Konopka 2018)\n\nMetoda Uniform Manifold Approximation and Projection (UMAP) jest nieliniową techniką redukcji wymiarowości opracowaną przez McInnesa i Healy’ego w 2018 roku. Jej celem jest odwzorowanie danych z przestrzeni wysokowymiarowej w przestrzeń o mniejszej liczbie wymiarów przy zachowaniu struktury geometrycznej — zarówno lokalnej, jak i globalnej — poprzez modelowanie danych jako rozkładu na rozmaitości (manifold). W odróżnieniu od metody t-SNE, UMAP opiera się na teorii rozmaitości Riemanna oraz na pojęciach pochodzących z teorii zbiorów rozmytych i topologii algebraicznej.\nNiech dane wejściowe stanowią zbiór punktów \\[\nX = \\{x_1, x_2, \\dots, x_n\\}, \\quad x_i \\in \\mathbb{R}^p.\n\\] Zakłada się, że punkty te leżą na rozmaitości \\(\\mathcal{M} \\subset \\mathbb{R}^p\\) o niższym wymiarze rzeczywistym \\(d &lt; p\\), zanurzonej w przestrzeni obserwowalnej. Metoda UMAP tworzy dwie probabilistyczne reprezentacje tej rozmaitości: po pierwsze, graf sąsiedztwa w przestrzeni wysokowymiarowej (fuzzy simplicial set), który opisuje lokalne zależności między punktami, oraz po drugie, graf w przestrzeni niskowymiarowej, którego struktura ma jak najlepiej odwzorowywać pierwszy.\nW celu konstrukcji grafu w przestrzeni wejściowej dla każdego punktu \\(x_i\\) określa się odległości do jego \\(k\\)-najbliższych sąsiadów. Następnie wyznacza się dwa parametry lokalne \\[\n\\rho_i = \\min_{j: d(x_i,x_j) &gt; 0} d(x_i, x_j),\n\\] czyli najmniejszą dodatnią odległość (umożliwiającą niezerową gęstość), oraz \\(\\sigma_i &gt; 0,\\) skalę lokalną dobraną tak, aby spełniony był warunek normalizacji entropii \\[\n\\sum_{j} \\exp\\!\\left(-\\frac{\\max(0, d(x_i,x_j) - \\rho_i)}{\\sigma_i}\\right) = \\log_2(k).\n\\] Na tej podstawie definiuje się rozmyte prawdopodobieństwa sąsiedztwa \\[\np_{j|i} = \\exp\\!\\left(-\\frac{\\max(0, d(x_i, x_j) - \\rho_i)}{\\sigma_i}\\right).\n\\] Ponieważ macierz tych wartości nie jest symetryczna, łączy się oba kierunki zgodnie z zasadami teorii zbiorów rozmytych \\[\np_{ij} = p_{i|j} + p_{j|i} - p_{i|j}\\,p_{j|i}.\n\\] Tak powstały rozmyty graf sąsiedztwa zawiera wagi \\(p_{ij}\\), które odzwierciedlają siłę połączeń między punktami.\nNastępnie w przestrzeni wynikowej \\(Y = \\{y_1, y_2, \\dots, y_n\\} \\subset \\mathbb{R}^q\\), gdzie zwykle \\(q = 2\\) lub 3, definiuje się analogiczny rozmyty graf \\(q_{ij}\\), którego wagi opisuje funkcja jądra typu heavy-tailed \\[\nq_{ij} = \\frac{1}{1 + a\\,\\|y_i - y_j\\|^{2b}},\n\\] gdzie \\(a\\) i \\(b\\) są parametrami dopasowanymi empirycznie (standardowo \\(a \\approx 1.929,\\ b \\approx 0.7915\\)).\nZasadniczym celem UMAP jest znalezienie takiej konfiguracji punktów \\(Y\\), aby rozmyty graf \\(q_{ij}\\) jak najlepiej przybliżał graf \\(p_{ij}\\). Kryterium optymalizacji ma postać minimalizacji rozbieżności krzyżowej (ang. cross-entropy) między dwoma rozkładami sąsiedztwa \\[\nC = \\sum_{i &lt; j} \\left[ -p_{ij}\\log(q_{ij}) - (1 - p_{ij})\\log(1 - q_{ij}) \\right].\n\\] Minimalizacja tej funkcji jest realizowana metodami gradientowymi, zazwyczaj z wykorzystaniem stochastic gradient descent (SGD). W wyniku optymalizacji punkty \\(y_i\\) są przesuwane tak, aby utrzymać bliskie relacje w miejscach, gdzie \\(p_{ij}\\) jest duże i rozdzielać punkty, gdzie \\(p_{ij}\\) jest małe.\nZałożenia metody UMAP są stosunkowo niewielkie, lecz istotne. Zakłada się, że dane leżą na rozmaitości o niskim wymiarze, a więc można je opisać poprzez ciągłą strukturę geometryczną. Przyjmuje się również, że użyta miara odległości (zwykle euklidesowa) odzwierciedla faktyczne podobieństwo obserwacji oraz że rozkład punktów jest gładki, czyli w małych sąsiedztwach struktura jest dobrze przybliżana liniowo.\nInterpretacja wyników UMAP nie odnosi się do bezwzględnych wartości współrzędnych, lecz do relacji między punktami. Punkty położone blisko siebie w przestrzeni wynikowej są podobne w przestrzeni oryginalnej, a większe odległości odpowiadają mniejszemu podobieństwu. W przeciwieństwie do t-SNE metoda ta lepiej zachowuje nie tylko lokalne klastry, ale również częściowo strukturę globalną, co umożliwia analizę gradientów i ciągłych przejść między grupami obserwacji.\n\n\n\n\n\n\nWażne kroki przy stosowaniu UMAP\n\n\n\n\nStandaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.\nWybór liczby sąsiadów (n_neighbors) — kontroluje lokalność odwzorowania; mniejsze wartości (5–15) podkreślają lokalne struktury, większe (30–50) zachowują więcej globalnych relacji.\nUstawienie wymiaru wynikowego (n_components) — zwykle 2 lub 3, w zależności od potrzeb wizualizacji.\nWybór metryki odległości — domyślnie euklidesowa, ale można użyć innych (np. Manhattan, cosine) w zależności od charakteru danych.\nPorównanie z PCA lub t-SNE — warto sprawdzić, czy UMAP nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykład 5.5 (UMAP na danych iris)  \n\nKodlibrary(uwot)     # UMAP\n\nset.seed(44)\n\n# Przygotowanie danych: oddzielić etykiety klas i standaryzować cechy\nX &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  scale() %&gt;%\n  as.matrix()\n\ny &lt;- iris$Species\n\n# UMAP 2D: podstawowe parametry\n# n_neighbors = \"skala lokalności\", min_dist = \"zwartość klastrów\", metric = metryka odległości\nemb_umap &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist    = 0.1,\n  metric      = \"euclidean\",\n  n_components = 2,\n  verbose = TRUE\n)\n\n# Ramka wynikowa do wykresu\ndf_umap &lt;- data.frame(\n  UMAP1 = emb_umap[, 1],\n  UMAP2 = emb_umap[, 2],\n  Species = y\n)\n\n# Dla porównania: PCA 2D (opcjonalnie)\npca &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  PC1 = pca$x[, 1],\n  PC2 = pca$x[, 2],\n  Species = y\n)\n\n# Wykresy\np_umap &lt;- ggplot(df_umap, aes(x = UMAP1, y = UMAP2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"UMAP\",\n       x = \"UMAP1\", y = \"UMAP2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np_pca &lt;- ggplot(df_pca, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Wyświetlenie obok siebie\np_umap | p_pca\n\n\n\n\n\n\n\n\nPoniżej prezentuję zbiorcze porównanie wszystkich omówionych metod redukcji wymiarowości na tym samym zbiorze danych iris. Wykorzystuję PCA, ICA, MDS, t-SNE oraz UMAP, aby zobaczyć, jak różne techniki odwzorowują strukturę danych.\n\nKodset.seed(44)\n\n# Usuń duplikaty (t-SNE i MDS niemetryczny są na to wrażliwe)\niris_unique &lt;- iris[!duplicated(iris[, -5]), ]\nX &lt;- as.matrix(scale(iris_unique[, -5]))\ny &lt;- iris_unique$Species\n\n# PCA (2 pierwsze składowe)\npca_fit &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  Dim1 = pca_fit$x[, 1],\n  Dim2 = pca_fit$x[, 2],\n  Method = \"PCA\",\n  Species = y\n)\n\n# ICA (2 komponenty niezależne)\nica_fit &lt;- fastICA(X, n.comp = 2, method = \"C\")\ndf_ica &lt;- data.frame(\n  Dim1 = ica_fit$S[, 1],\n  Dim2 = ica_fit$S[, 2],\n  Method = \"ICA\",\n  Species = y\n)\n\n# MDS metryczny (klasyczny)\nmds_metric &lt;- cmdscale(dist(X), k = 2)\ndf_mds_metric &lt;- data.frame(\n  Dim1 = mds_metric[, 1],\n  Dim2 = mds_metric[, 2],\n  Method = \"MDS (metryczny)\",\n  Species = y\n)\n\n# MDS niemetryczny (isoMDS)\nmds_nonmetric &lt;- isoMDS(dist(X), k = 2)$points\n\ninitial  value 4.818373 \nfinal  value 4.818117 \nconverged\n\nKoddf_mds_nonmetric &lt;- data.frame(\n  Dim1 = mds_nonmetric[, 1],\n  Dim2 = mds_nonmetric[, 2],\n  Method = \"MDS (niemet.)\",\n  Species = y\n)\n\n# t-SNE\ntsne_fit &lt;- Rtsne(\n  X, dims = 2, perplexity = 30,\n  max_iter = 750, check_duplicates = FALSE, verbose = FALSE\n)\ndf_tsne &lt;- data.frame(\n  Dim1 = tsne_fit$Y[, 1],\n  Dim2 = tsne_fit$Y[, 2],\n  Method = \"t-SNE\",\n  Species = y\n)\n\n# UMAP\numap_emb &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist = 0.1,\n  metric = \"euclidean\",\n  n_components = 2,\n  verbose = FALSE\n)\ndf_umap &lt;- data.frame(\n  Dim1 = umap_emb[, 1],\n  Dim2 = umap_emb[, 2],\n  Method = \"UMAP\",\n  Species = y\n)\n\n# Połączenie wszystkich metod\ndf_all &lt;- bind_rows(\n  df_pca,\n  df_ica,\n  df_mds_metric,\n  df_mds_nonmetric,\n  df_tsne,\n  df_umap\n)\n\n# Wykres porównawczy\nggplot(df_all, aes(Dim1, Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  facet_wrap(~ Method, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Porównanie metod redukcji wymiarowości na zbiorze iris\",\n    x = \"Wymiar 1\", y = \"Wymiar 2\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nComon, Pierre. 1994. „Independent Component Analysis, A New Concept?” Signal Processing 36 (3): 287–314. https://doi.org/10.1016/0165-1684(94)90029-9.\n\n\nKonopka, Tomasz. 2018. „umap: Uniform Manifold Approximation and Projection”. The R Foundation. https://doi.org/10.32614/cran.package.umap.\n\n\nKruskal, J. B. 1964. „Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis”. Psychometrika 29 (1): 1–27. https://doi.org/10.1007/bf02289565.\n\n\nMaaten, Laurens van der, i Geoffrey Hinton. 2008. „Visualizing Data using t-SNE”. Journal of Machine Learning Research 9 (86): 2579–2605. http://jmlr.org/papers/v9/vandermaaten08a.html.\n\n\nPearson, Karl. 1901. „LIII. On Lines and Planes of Closest Fit to Systems of Points in Space”. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72. https://doi.org/10.1080/14786440109462720.\n\n\nTorgerson, Warren S. 1952. „Multidimensional Scaling: I. Theory and Method”. Psychometrika 17 (4): 401–19. https://doi.org/10.1007/bf02288916.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Metody redukcji wymiarowości</span>"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "Analiza skupień",
    "section": "",
    "text": "Rys historyczny\nAnaliza skupień, znana również jako cluster analysis, ma swoje korzenie w połowie XX wieku, choć jej podstawy koncepcyjne pojawiły się znacznie wcześniej w statystyce i biologii systematycznej. Jej rozwój przebiegał równolegle w kilku dziedzinach, w tym w psychologii, biologii, socjologii i informatyce, a z czasem stała się jednym z fundamentalnych narzędzi eksploracyjnej analizy danych. Pierwsze idee grupowania obiektów o podobnych cechach można odnaleźć już w XVIII i XIX wieku w klasyfikacji biologicznej. Carl Linneusz wprowadził system binominalny oparty na cechach morfologicznych organizmów, co stanowiło wczesny przykład klasyfikacji hierarchicznej. Współczesne podejście matematyczne do analizy skupień zaczęło się jednak kształtować dopiero w XX wieku wraz z rozwojem metod statystycznych i koncepcji odległości w przestrzeni wielowymiarowej. Za właściwy początek analizy skupień w sensie statystycznym uznaje się lata 30. i 40. XX wieku. W 1939 roku Tryon wprowadził pojęcie analizy grupowej (cluster analysis) w psychologii, stosując ją do klasyfikacji zmiennych i jednostek na podstawie macierzy podobieństw. W latach 50. i 60. intensywny rozwój metod klasyfikacji hierarchicznej był związany z rozwojem biologii numerycznej (numerical taxonomy), głównie dzięki pracom Sokal’a i Sneath’a, którzy w latach 60. zaproponowali formalne podstawy taksonomii numerycznej opartej na macierzach podobieństw między organizmami. Lata 60. i 70. XX wieku przyniosły znaczący rozwój metod niehierarchicznych, w tym przede wszystkim metody k-means, zaproponowanej przez MacQueena w 1967 roku. Algorytm ten stał się jednym z najczęściej stosowanych narzędzi w analizie skupień dzięki swojej prostocie, interpretowalności i efektywności obliczeniowej. W tym samym okresie rozwijano również metody oparte na gęstości (np. późniejszy DBSCAN), metody probabilistyczne (modele mieszanek Gaussa) oraz techniki optymalizacyjne pozwalające na automatyczne wyznaczanie liczby skupień. W latach 80. i 90. wraz z rozwojem informatyki oraz eksploracji danych (data mining), analiza skupień zaczęła być szeroko stosowana w zastosowaniach praktycznych – od segmentacji rynku, przez rozpoznawanie obrazów, po bioinformatykę. Pojawiły się również metody adaptacyjne i oparte na uczeniu nienadzorowanym, w tym sieci neuronowe typu self-organizing maps (SOM) opracowane przez Kohonena. W XXI wieku analiza skupień stała się kluczowym elementem nauki o danych (data science). Współczesne metody integrują klasyczne podejścia statystyczne z algorytmami uczenia maszynowego. Opracowano wiele nowych technik, takich jak metody oparte na gęstości (DBSCAN, OPTICS), metody spektralne wykorzystujące wartości własne macierzy podobieństwa, czy algorytmy głębokiego grupowania (deep clustering) bazujące na sieciach neuronowych. Równocześnie rozwinięto teoretyczne podstawy walidacji skupień, takie jak współczynniki silhouette, indeks Calinskiego-Harabasza czy Davies-Bouldin, umożliwiające obiektywną ocenę jakości grupowania.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analiza skupień</span>"
    ]
  },
  {
    "objectID": "cluster.html#podział-metod-analizy-skupień",
    "href": "cluster.html#podział-metod-analizy-skupień",
    "title": "Analiza skupień",
    "section": "Podział metod analizy skupień",
    "text": "Podział metod analizy skupień\nMetody analizy skupień można klasyfikować według różnych kryteriów, takich jak sposób tworzenia skupień, założenia o strukturze danych, rodzaj miary podobieństwa czy sposób reprezentacji wyników. Najczęściej przyjmuje się podział taksonomiczny oparty na sposobie grupowania obiektów, który pozwala wyróżnić cztery główne klasy metod: hierarchiczne, niehierarchiczne, oparte na gęstości i oparte na modelach probabilistycznych.\nPierwszą i jedną z najstarszych kategorii są metody hierarchiczne. Ich istotą jest budowa dendrogramu odzwierciedlającego stopniowe łączenie (lub rozdzielanie) obiektów w skupienia. Wyróżnia się dwa podejścia: aglomeracyjne, które rozpoczynają od traktowania każdego obiektu jako odrębnego skupienia i następnie łączą je zgodnie z określoną miarą odległości (np. metoda pojedynczego, pełnego lub średniego wiązania), oraz dzielące, które zaczynają od jednego skupienia zawierającego wszystkie obiekty i w kolejnych krokach dokonują jego podziału. Metody hierarchiczne mają tę zaletę, że nie wymagają wcześniejszego określenia liczby skupień, lecz ich wadą jest wysoka złożoność obliczeniowa i wrażliwość na szumy.\nDrugą grupę stanowią metody niehierarchiczne, wśród których najbardziej znane są algorytmy typu k-means oraz k-medoids. Ich celem jest bezpośrednie przypisanie każdego obiektu do jednego z ustalonej liczby skupień na podstawie minimalizacji sumy kwadratów odległości wewnątrzgrupowych. Metoda k-means jest szybka i skuteczna przy danych o wyraźnie kulistych skupieniach, natomiast k-medoids (np. algorytm PAM) jest bardziej odporna na wartości odstające. Do tej kategorii należą również algorytmy optymalizacyjne, takie jak k-means++ czy mini-batch k-means, dostosowane do dużych zbiorów danych.\nTrzecią kategorię tworzą metody oparte na gęstości, w których skupienia definiuje się jako obszary przestrzeni danych o wysokim zagęszczeniu punktów oddzielone obszarami o niskiej gęstości. Klasycznym przykładem jest algorytm DBSCAN, który wykrywa skupienia dowolnego kształtu i pozwala automatycznie identyfikować punkty szumu. Udoskonaloną wersją tej metody jest OPTICS, umożliwiająca hierarchiczne przedstawienie struktur gęstościowych. Metody tego typu są szczególnie użyteczne przy analizie danych przestrzennych oraz w sytuacjach, gdy skupienia nie mają regularnego kształtu.\nCzwartą grupą są metody oparte na modelach probabilistycznych. Zakładają one, że dane pochodzą z mieszaniny rozkładów (najczęściej wielowymiarowych normalnych), a zadaniem algorytmu jest estymacja parametrów tych rozkładów oraz przypisanie obiektów do skupień na podstawie maksymalnego prawdopodobieństwa. Do tej kategorii należą modele mieszanek Gaussa (GMM) estymowane metodą EM (Expectation–Maximization), które umożliwiają probabilistyczne przypisanie obiektów do wielu skupień z różnym stopniem przynależności.\nPoza głównymi czterema klasami wyróżnia się również metody hybrydowe i współczesne podejścia uczenia nienadzorowanego. Przykładem są metody spektralne, które wykorzystują analizę wartości własnych macierzy podobieństwa, oraz metody głębokiego grupowania (deep clustering), integrujące sieci neuronowe autoenkoderowe z klasycznymi procedurami klastrowania1.\n1 Ten rodzaj klastrowania nie będzie przedmiotem tego rozdziału ponieważ wykracza poza klasyczne podejście statystyczne i wymaga wiedzy na temat sieci neuronowych, która pojawia się na późniejszych semestrach.\n\nPodział metod grupowania",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analiza skupień</span>"
    ]
  },
  {
    "objectID": "cluster.html#metody-hierarchiczne",
    "href": "cluster.html#metody-hierarchiczne",
    "title": "Analiza skupień",
    "section": "Metody hierarchiczne",
    "text": "Metody hierarchiczne\nMetody hierarchiczne w analizie skupień opierają się na iteracyjnym łączeniu lub dzieleniu obiektów w sposób odzwierciedlający ich podobieństwo, prowadząc do utworzenia struktury drzewiastej (dendrogramu). Struktura ta ukazuje hierarchiczne relacje między obiektami – od indywidualnych elementów aż po jedną nadrzędną grupę lub odwrotnie. Wyróżnia się dwa główne podejścia: metody aglomeracyjne oraz deglomeracyjne (dzielące).\nMetody aglomeracyjne\nW podejściu aglomeracyjnym proces rozpoczyna się od traktowania każdego obiektu jako odrębnego skupienia jednoelementowego. Następnie w kolejnych krokach łączy się dwa najbardziej podobne skupienia, aż do momentu uzyskania jednego skupienia zawierającego wszystkie obiekty. Proces ten można formalnie zapisać następująco\n\nNiech zbiór danych składa się z \\(n\\) obiektów \\[\nX = \\{x_1, x_2, \\ldots, x_n\\},\n\\] gdzie każdy obiekt \\(x_i \\in \\mathbb{R}^p.\\)\n\nPoczątkowo każdy obiekt stanowi odrębne skupienie \\[\nC_i^{(0)} = \\{x_i\\} \\quad \\text{dla}\\quad i = 1, \\ldots, n.\n\\]\n\nDefiniuje się macierz odległości \\(D = [d(x_i, x_j)]\\), gdzie funkcja \\(d(\\cdot, \\cdot)\\) określa miarę odległości (np. euklidesową, Mahalanobisa, Manhattan).\nNa każdym kroku \\(t\\) wyszukuje się dwa skupienia \\(C_p^{(t)}\\) i \\(C_q^{(t)}\\), które są najbliższe względem przyjętej miary odległości między skupieniami \\(D(C_p, C_q)\\). Następnie łączy się je w jedno nowe skupienie \\[\nC_{pq}^{(t+1)} = C_p^{(t)} \\cup C_q^{(t)}.\n\\]\n\nOdległości między nowo utworzonym skupieniem a pozostałymi aktualizuje się zgodnie z przyjętą regułą wiązania (linkage criterion). Niech \\(D(C_i,C_j)\\) oznacza odległość klaster–klaster, \\(d(x,y)\\) bazową odległość punkt–punkt, \\(|C_i|=n_i\\) liczność klastra \\(C_i\\), \\(\\bar x_i\\) centroid \\(C_i\\). Reguła aglomeracji w postaci rekurencji Lance’a–Williamsa przyjmuje wówczas postać \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\alpha_i D(C_i,C_k)+\\alpha_j D(C_j,C_k)+\\beta D(C_i,C_j)+\\gamma\\,\\big|D(C_i,C_k)-D(C_j,C_k)\\big|,\n\\] z współczynnikami (\\(\\alpha_i,\\alpha_j,\\beta,\\gamma\\)) zależnymi od wybranego sposobu łączenia. Dla metod centroidowych i Warda inicjalizujemy macierz odległości kwadratami odległości euklidesowych i interpretujemy wyniki jako wartości kwadratowe. Możemy wówczas wyróżnić następujące metody aglomeracji:\n\nMetoda pojedynczego wiązania (single linkage) \\[\nD(C_i,C_j)=\\min_{x\\in C_i,\\,y\\in C_j} d(x,y).\n\\] Zbiory łączymy regułą \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\min\\!\\big(D(C_i,C_k),\\,D(C_j,C_k)\\big),\n\\] co odpowiada \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=0,\\ \\gamma=-\\tfrac12\\) przy inicjalizacji \\(D(\\{x\\},\\{y\\})=d(x,y)\\).\nMetoda pełnego wiązania (complete linkage) \\[\nD(C_i,C_j)=\\max_{x\\in C_i,\\,y\\in C_j} d(x,y).\n\\] Zbiory łączymy regułą \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\max\\!\\big(D(C_i,C_k),\\,D(C_j,C_k)\\big),\n\\] czyli \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=0,\\ \\gamma=+\\tfrac12\\), z inicjalizacją \\(d(x,y)\\).\nMetoda średniego wiązania (average linkage, UPGMA - Unweighted Pair Group Method using Arithmetic Averages) \\[\nD(C_i,C_j)=\\frac{1}{n_i n_j}\\sum_{x\\in C_i}\\sum_{y\\in C_j} d(x,y).\n\\] Zbiory łączymy regułą \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\frac{n_i\\,D(C_i,C_k)+n_j\\,D(C_j,C_k)}{n_i+n_j},\n\\] co daje \\(\\alpha_i=\\tfrac{n_i}{n_i+n_j},\\ \\alpha_j=\\tfrac{n_j}{n_i+n_j},\\ \\beta=\\gamma=0\\).\nMetoda ważonego średniego wiązania (weighted average linkage, WPGMA - Weighted Pair Group Method using Arithmetic Averages, McQuitty) - zbiory łączymy regułą \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\tfrac12\\big(D(C_i,C_k)+D(C_j,C_k)\\big),\n\\] tj. \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=\\gamma=0\\), przy inicjalizacji \\(d(x,y)\\).\nMetoda centroidów (centroid linkage, UPGMC - Unweighted Pair Group Method using Centroids)2 - definicja przez centroidy (wymaga kwadratów odległości euklidesowych) \\[\nD(C_i,C_j)=\\|\\bar x_i-\\bar x_j\\|^2,\\qquad \\bar x_i=\\frac{1}{n_i}\\sum_{x\\in C_i}x.\n\\] Zbiory łączymy regułą \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\frac{n_i}{n_i+n_j}D(C_i,C_k)+\\frac{n_j}{n_i+n_j}D(C_j,C_k)-\\frac{n_i n_j}{(n_i+n_j)^2}D(C_i,C_j),\n\\] co odpowiada \\(\\alpha_i=\\tfrac{n_i}{n_i+n_j},\\ \\alpha_j=\\tfrac{n_j}{n_i+n_j},\\ \\beta=-\\tfrac{n_i n_j}{(n_i+n_j)^2},\\ \\gamma=0\\), inicjalizujemy \\(D(\\{x\\},\\{y\\})=\\|x-y\\|^2\\).\nMetoda mediany (median linkage, WPGMC - Weighted Pair Group Method using Centroids) - centra klastrów aktualizujemy przez punkt środkowy median \\(m_{i\\cup j}=\\tfrac12(m_i+m_j).\\) Zbiory łączymy regułą \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\tfrac12\\big(D(C_i,C_k)+D(C_j,C_k)\\big)-\\tfrac14\\,D(C_i,C_j),\n\\] czyli \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=-\\tfrac14,\\ \\gamma=0\\), z inicjalizacją \\(D(\\{x\\},\\{y\\})=\\|x-y\\|^2\\).\nMetoda Warda (Ward’s linkage) \\[\nD(C_i,C_j)=\\frac{2\\,n_i n_j}{n_i+n_j}\\,\\|\\bar x_i-\\bar x_j\\|^2,\n\\] Zbiory łączymy regułą \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\frac{n_i+n_k}{n_i+n_j+n_k}D(C_i,C_k)+\\frac{n_j+n_k}{n_i+n_j+n_k}D(C_j,C_k)-\\frac{n_k}{n_i+n_j+n_k}D(C_i,C_j),\n\\] przy inicjalizacji \\(D(\\{x\\},\\{y\\})=\\|x-y\\|^2\\).\n\n\nProces powtarza się do momentu, gdy wszystkie obiekty znajdą się w jednym skupieniu, tworząc hierarchiczny układ połączeń.\n\n2 W metodach centroidowych, medianowej i Warda podkreśla się konieczność pracy na kwadratach odległości euklidesowych.Zaletą metod aglomeracyjnych jest to, że nie wymagają a priori określenia liczby skupień. Wadą jest natomiast ich nieodwracalność – raz połączone skupienia nie mogą zostać rozdzielone, a wynik końcowy jest wrażliwy na wybór miary odległości i kryterium wiązania.\n\nPrzykład 6.1  \n\nKodlibrary(factoextra)\nlibrary(ggpubr)\n\n# 1. Przygotowanie danych: standaryzacja czterech cech numerycznych\nX &lt;- scale(iris[, 1:4])\n\n# 2. Macierz odległości euklidesowych\nd &lt;- dist(X, method = \"euclidean\")\n\n# 3. Budowa dendrogramów dla różnych metod łączenia\nhc_single   &lt;- hclust(d, method = \"single\")\nhc_complete &lt;- hclust(d, method = \"complete\")\nhc_average  &lt;- hclust(d, method = \"average\")     # UPGMA\nhc_ward     &lt;- hclust(d, method = \"ward.D2\")     # Ward (zalecany wariant .D2)\n\n# 4. Wizualizacja dendrogramów z linią cięcia na K=3\np_d_single   &lt;- fviz_dend(hc_single,   k = 3, cex = 0.6, main = \"single linkage (K=3)\")\np_d_complete &lt;- fviz_dend(hc_complete, k = 3, cex = 0.6, main = \"complete linkage (K=3)\")\np_d_average  &lt;- fviz_dend(hc_average,  k = 3, cex = 0.6, main = \"average linkage (K=3)\")\np_d_ward     &lt;- fviz_dend(hc_ward,     k = 3, cex = 0.6, main = \"Ward.D2 (K=3)\")\n\nggarrange(p_d_single, p_d_complete, p_d_average, p_d_ward, ncol = 2, nrow = 2)\n\n\n\n\n\n\nKod# 5. Uzyskanie etykiet klastrów dla K=3 i rzutowanie grup w przestrzeń PCA\ncl_single   &lt;- cutree(hc_single,   k = 3)\ncl_complete &lt;- cutree(hc_complete, k = 3)\ncl_average  &lt;- cutree(hc_average,  k = 3)\ncl_ward     &lt;- cutree(hc_ward,     k = 3)\n\np_c_single   &lt;- fviz_cluster(list(data = X, cluster = cl_single),\n                             geom = \"point\", ellipse.type = \"norm\",\n                             main = \"single linkage\")\np_c_complete &lt;- fviz_cluster(list(data = X, cluster = cl_complete),\n                             geom = \"point\", ellipse.type = \"norm\",\n                             main = \"complete linkage\")\np_c_average  &lt;- fviz_cluster(list(data = X, cluster = cl_average),\n                             geom = \"point\", ellipse.type = \"norm\",\n                             main = \"average linkage\")\np_c_ward     &lt;- fviz_cluster(list(data = X, cluster = cl_ward),\n                             geom = \"point\", ellipse.type = \"norm\",\n                             main = \"Ward.D2\")\n\nggarrange(p_c_single, p_c_complete, p_c_average, p_c_ward, ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\nMetody deglomeracyjne\nMetody deglomeracyjne (dzielące) stanowią odwrotność podejścia aglomeracyjnego. Zaczyna się od jednego skupienia zawierającego wszystkie obiekty, które następnie są iteracyjnie dzielone na mniejsze podzbiory, aż do osiągnięcia oczekiwanej liczby skupień lub spełnienia kryterium zatrzymania.\nFormalnie proces można przedstawić w postaci\n\nPoczątkowo przyjmuje się jedno skupienie \\[C^{(0)} = X.\\]\n\nW każdym kroku wybiera się skupienie \\(C_i^{(t)}\\), które zostanie podzielone. Wybór ten może wynikać z maksymalnej wariancji wewnątrzgrupowej, liczby elementów lub innych kryteriów jakości skupień.\nDokonuje się podziału wybranego skupienia na dwa mniejsze, minimalizując błąd wewnątrzgrupowegy lub maksymalizując różnice międzygrupowe. Najczęściej stosuje się algorytm analogiczny do bisecting k-means \\[\nC_i^{(t)} \\rightarrow \\{C_{i1}^{(t+1)}, C_{i2}^{(t+1)}\\},\n\\] przy czym podział realizuje się poprzez iteracyjne zastosowanie k-means z \\(k = 2\\).\nProces dzielenia jest powtarzany do momentu uzyskania żądanej liczby skupień lub gdy dalszy podział nie prowadzi do istotnej poprawy jakości.\n\nMetody deglomeracyjne są mniej popularne z powodu wyższego kosztu obliczeniowego i konieczności przyjęcia dodatkowych kryteriów decyzyjnych dotyczących wyboru skupienia do podziału. Jednak w dużych zbiorach danych mogą być efektywniejsze niż aglomeracyjne, szczególnie gdy implementuje się je z wykorzystaniem metod heurystycznych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analiza skupień</span>"
    ]
  },
  {
    "objectID": "cluster.html#metody-niehierarchiczne",
    "href": "cluster.html#metody-niehierarchiczne",
    "title": "Analiza skupień",
    "section": "Metody niehierarchiczne",
    "text": "Metody niehierarchiczne\nMetody niehierarchiczne w analizie skupień koncentrują się na bezpośrednim przypisaniu obiektów do określonej liczby skupień bez tworzenia struktury hierarchicznej. Najbardziej znane i szeroko stosowane są algorytmy typu k-means oraz k-medoids. Podstawą matematyczną metod niehierarchicznych jest minimalizacja pewnej funkcji celu, najczęściej sumy kwadratów odchyleń punktów od środków grup, zwanych centroidami. W przeciwieństwie do podejścia hierarchicznego, proces ten ma charakter iteracyjny i wymaga wcześniejszego określenia liczby klastrów. W efekcie powstaje partycja przestrzeni danych, w której każdy obiekt zostaje przypisany do jednego lub kilku klastrów w zależności od przyjętej koncepcji przynależności (podział płaski3). W tym kontekście wyróżnia się dwa główne typy podziałów: podział twardy i podział rozmyty.\n3 podział płaski oznacza, że każdy obiekt należy do dokładnie jednej grupy i nie istnieje hierarchia między grupamiPodział twardy\nPodział twardy opiera się na jednoznacznym przypisaniu każdego obiektu do dokładnie jednego klastra. W ujęciu matematycznym przyjmuje się, że dla zbioru obserwacji \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\) oraz ustalonej liczby klastrów \\(K\\), istnieje macierz przynależności \\(U = [u_{ik}]\\), w której każdy element przyjmuje wartość 0 lub 1. Wartość \\(u_{ik} = 1\\) oznacza, że obiekt \\(x_i\\) należy do klastra \\(k\\), natomiast \\(u_{ik} = 0\\) – że do niego nie należy.\nPierwszym warunkiem podziału twardego jest to, że każdy obiekt musi należeć dokładnie do jednej grupy. Oznacza to, że dla każdego obiektu suma przynależności po wszystkich klastrach równa się jeden \\[\n\\sum_{k=1}^{K} u_{ik} = 1, \\quad \\forall i \\in \\{1, 2, \\ldots, n\\}.\n\\] Z kolei każdy klaster powinien zawierać przynajmniej jeden element, co można zapisać jako \\[\n1 \\leq \\sum_{i=1}^{n} u_{ik}, \\quad \\forall k \\in \\{1, 2, \\ldots, K\\}.\n\\] Wynika z tego, że nie dopuszcza się powstawania pustych grup. Kolejnym warunkiem jest binarność przypisań, czyli \\[\nu_{ik} \\in \\{0, 1\\}, \\quad \\forall i, k.\n\\] Przynależność obiektu do klastra jest zatem całkowita i nie dopuszcza stanów pośrednich. Wreszcie, formalnym celem podziału twardego jest minimalizacja funkcji błędu, która określa sumę kwadratów odchyleń poszczególnych obiektów od centroidów klastrów, do których zostały przypisane \\[\nJ = \\sum_{k=1}^{K} \\sum_{i=1}^{n} u_{ik} \\, \\|x_i - \\mu_k\\|^2,\n\\] gdzie \\(\\mu_k\\) oznacza środek klastra \\(k\\), a \\(\\|\\cdot\\|\\) jest najczęściej normą euklidesową. Każdy obiekt powinien zostać przypisany do tego klastra, którego centroid jest najbliższy, czyli \\[\nu_{ik} =\n\\begin{cases}\n1, & \\text{jeśli } k = \\arg \\min_{j} \\|x_i - \\mu_j\\|, \\\\\n0, & \\text{w przeciwnym razie.}\n\\end{cases}\n\\]\nPodział rozmyty\nPodział rozmyty (ang. fuzzy clustering) stanowi uogólnienie klasycznego, twardego podejścia do grupowania, w którym dopuszcza się możliwość częściowej przynależności obiektu do więcej niż jednego klastra. Zamiast przypisywać każdy element jednoznacznie do jednej grupy, wprowadza się pojęcie stopnia przynależności, który przyjmuje wartości z przedziału \\([0,1]\\). W ten sposób odzwierciedla się niepewność lub płynność granic między grupami, co czyni tę metodę bardziej elastyczną i lepiej dostosowaną do danych o niejednoznacznej strukturze.\nMatematycznie, dla zbioru obserwacji \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\) oraz ustalonej liczby klastrów \\(K\\), definiuje się macierz przynależności \\(U = [u_{ik}]\\), gdzie każdy element \\(u_{ik}\\) oznacza stopień, w jakim obiekt \\(x_i\\) należy do klastra \\(k\\). W odróżnieniu od podziału twardego, tutaj \\(u_{ik} \\in [0,1]\\), a nie tylko \\(\\{0,1\\}\\). Zachowany zostaje jednak warunek, że suma stopni przynależności danego obiektu do wszystkich klastrów musi być równa jeden \\[\n\\sum_{k=1}^{K} u_{ik} = 1, \\quad \\forall i \\in \\{1, 2, \\ldots, n\\}.\n\\] Warunek ten oznacza, że przynależności mają charakter względny – im silniejszy związek obiektu z jednym klastrem, tym słabszy z innymi.\nPodział rozmyty opiera się na minimalizacji rozmytej funkcji celu, znanej z algorytmu Fuzzy c-means \\[\nJ_m = \\sum_{k=1}^{K} \\sum_{i=1}^{n} (u_{ik})^m \\, \\|x_i - \\mu_k\\|^2,\n\\] gdzie \\(\\mu_k\\) oznacza centroid klastra \\(k\\), a parametr \\(m &gt; 1\\) kontroluje poziom rozmycia. Im większa wartość \\(m\\), tym bardziej rozmyty staje się podział, ponieważ różnice pomiędzy wartościami przynależności poszczególnych obiektów do klastrów ulegają spłaszczeniu. W praktyce najczęściej przyjmuje się \\(m = 2\\). Optymalizacja funkcji celu prowadzi do następujących warunków aktualizacji. Stopnie przynależności obliczane są według wzoru \\[\nu_{ik} = \\frac{1}{\\sum_{j=1}^{K} \\left( \\frac{\\|x_i - \\mu_k\\|}{\\|x_i - \\mu_j\\|} \\right)^{\\frac{2}{m-1}}},\n\\] natomiast nowe położenie centroidów wyznacza się jako ważoną średnią punktów, gdzie wagi stanowią stopnie przynależności podniesione do potęgi \\(m\\) \\[\n\\mu_k = \\frac{\\sum_{i=1}^{n} (u_{ik})^m x_i}{\\sum_{i=1}^{n} (u_{ik})^m}.\n\\] Proces ten przebiega iteracyjnie – w każdej iteracji obliczane są nowe wartości \\(u_{ik}\\) i \\(\\mu_k\\), aż do osiągnięcia zbieżności funkcji celu \\(J_m\\).\nMetoda k-średnich (k-means)\nAlgorytm k-means wynika z problemu minimalizacji sumy kwadratów odchyleń punktów od reprezentantów grup w metryce euklidesowej. Niech dany będzie zbiór obserwacji \\(X=\\{x_1,\\dots,x_n\\}\\subset\\mathbb{R}^p\\) oraz liczba klastrów \\(K\\). Celem jest znalezienie partycji danych i wektorów \\(\\mu_1,\\dots,\\mu_K\\in\\mathbb{R}^p\\) minimalizujących funkcję celu \\[\nJ(U,\\mu)=\\sum_{k=1}^K\\sum_{i=1}^n u_{ik}\\,\\|x_i-\\mu_k\\|^2,\n\\] gdzie \\(U=[u_{ik}]\\) jest macierzą przypisań spełniającą warunki podziału twardego \\(u_{ik}\\in\\{0,1\\}\\), \\(\\sum_{k=1}^K u_{ik}=1\\) dla każdego \\(i\\), a \\(\\sum_{i=1}^n u_{ik}\\ge 1\\) dla każdego \\(k\\).\nWyprowadzenie algorytmu polega na zastosowaniu naprzemiennej minimalizacji względem \\(U\\) i \\(\\mu\\), ponieważ jednoczesna minimalizacja jest problemem kombinatorycznym trudnym obliczeniowo. Rozważmy najpierw minimalizację względem centroidów przy ustalonych przypisaniach. Dla danego \\(k\\) rozważmy funkcję \\[\nJ_k(\\mu_k)=\\sum_{i=1}^n u_{ik}\\,\\|x_i-\\mu_k\\|^2.\n\\] Jest to funkcja kwadratowa ściśle wypukła w \\(\\mu_k\\). Obliczamy gradient \\[\n\\nabla_{\\mu_k}J_k(\\mu_k)=2\\sum_{i=1}^n u_{ik}\\,(\\mu_k-x_i)=2\\left(\\Big(\\sum_{i}u_{ik}\\Big)\\mu_k-\\sum_{i}u_{ik}x_i\\right).\n\\] Warunek \\(\\nabla_{\\mu_k}J_k(\\mu_k)=0\\) daje \\[\n\\mu_k^\\star=\\frac{\\sum_{i=1}^n u_{ik}x_i}{\\sum_{i=1}^n u_{ik}},\n\\] czyli optymalny centroid jest średnią arytmetyczną punktów przypisanych do klastra. Wypukłość zapewnia, że jest to minimum globalne względem \\(\\mu_k\\). Zatem przy ustalonych \\(U\\) krok aktualizacji centroidów ma postać średniej ważonej ze wskaźnikami \\(u_{ik}\\).\nNastępnie dokonujemy minimalizacji względem przypisań przy ustalonych centroidach. Dla każdego obiektu \\(x_i\\) problem redukuje się do \\[\n\\min_{u_{i1},\\dots,u_{iK}} \\sum_{k=1}^K u_{ik}\\,\\|x_i-\\mu_k\\|^2\\quad \\text{przy}\\quad u_{ik}\\in\\{0,1\\},\\ \\sum_k u_{ik}=1.\n\\] Ponieważ wyrażenie jest liniowe w \\(u_{ik}\\), optimum osiąga się, wybierając \\(u_{ik}=1\\) dla indeksu \\(k\\) minimalizującego odległość euklidesową \\[\nu_{ik}=\\mathbf{1}_\\left\\{k=\\operatorname{argmin}_{j\\in\\{1,\\dots,K\\}}\\|x_i-\\mu_j\\|^2\\right\\}.\n\\] Wynika stąd reguła „przypisz do najbliższego centroidu”, co geometrycznie odpowiada podziałowi przestrzeni na komórki Woronoja wyznaczone przez \\(\\{\\mu_k\\}\\).\n\nZłożenie obu kroków prowadzi do procedury znanej jako Lloyd’s algorithm:\n\nStartujemy od wstępnych centroidów \\(\\mu^{(0)}\\).\nNaprzemiennie wykonujemy przypisanie do najbliższego centroidu.\nPrzeprowadzamy aktualizację centroidów jako średnich.\nWykonujemy kroki 2-3 aż do osiągnięcia zbieżności (punkty nie zmieniają swoich skupień).\n\nKażdy z kroków 2-3 nie zwiększa funkcji celu, bo przy ustalonych centroidach wybór najbliższego centroidu minimalizuje składnik \\(\\|x_i-\\mu_k\\|^2\\) dla każdego \\(i\\), więc \\(J\\) maleje lub pozostaje stała, a przy ustalonych przypisaniach do klastrów wybór średniej minimalizuje sumę kwadratów, więc \\(J\\) również maleje lub pozostaje stała. Ponieważ istnieje skończona liczba możliwych partycji i \\(J\\ge 0\\), monotonicznie niemalejąca sekwencja wartości funkcji celu musi zatrzymać się w skończonej liczbie kroków na punkcie stacjonarnym, czyli minimum lokalnym problemu z ograniczeniami twardych przypisań.\nWarto zauważyć, że równoważnie można interpretować cel jako minimalizację wariancji wewnątrzklastrowej. W klasycznej dekompozycji SST \\[\n\\underbrace{\\sum_{i=1}^n \\|x_i-\\bar{x}\\|^2}_{\\text{SST}}=\\underbrace{\\sum_{k=1}^K\\sum_{i=1}^n u_{ik}\\|x_i-\\mu_k\\|^2}_{\\text{WSS}}+\\underbrace{\\sum_{k=1}^K n_k\\|\\mu_k-\\bar{x}\\|^2}_{\\text{BSS}},\n\\] gdzie \\(\\bar{x}\\) jest średnią globalną, a \\(n_k=\\sum_i u_{ik}\\). Minimalizacja WSS (ang. within-cluster sum of squares) przy zadanym \\(K\\) jest równoważna maksymalizacji BSS, czyli maksymalizacji separacji centroidów względem średniej globalnej, co formalnie uzasadnia intuicję „maksymalizować jednorodność wewnątrz klastrów i różnice między klastrami”.\n\n\n\n\n\n\nk-means++\n\n\n\nZastosowanie inicjalizacji k-means++ polega na losowaniu początków z uprzywilejowaniem punktów odległych od już wybranych centroidów, co w sensie teoretycznym daje gwarancje aproksymacyjne rzędu \\(O(\\log K)\\) względem optimum oczekiwanego, a w praktyce istotnie poprawia jakość minimum lokalnego.\n\n\n\n\n\n\n\n\nWarunki stacjonarności otrzymanego rozwiązania\n\n\n\nPara \\((U^\\star,\\mu^\\star)\\) jest punktem stałym algorytmu wtedy i tylko wtedy, gdy spełnia jednocześnie dwa warunki: po pierwsze \\(\\mu_k^\\star\\) są średnimi swoich klastrów, po drugie przypisania \\(U^\\star\\) są zgodne z najbliższymi centroidami \\(\\mu^\\star.\\) Takie rozwiązanie spełnia warunki optymalności pierwszego rzędu względem naprzemiennych bloków zmiennych i stanowi minimum lokalne funkcji \\(J\\) na zbiorze dopuszczalnych rozwiązań wyznaczonych z ograniczeniami twardych przypisań.\n\n\nMetoda k-medoidów (k-medoids)\nMetoda k-medoid (ang. k-medoids) stanowi bliski odpowiednik klasycznej metody k-means, lecz wprowadza zasadniczą zmianę w sposobie definiowania reprezentanta klastra. Zamiast centroidu obliczanego jako średnia arytmetyczna wszystkich punktów w danym klastrze, metoda k-medoid wykorzystuje medoid, czyli rzeczywisty punkt ze zbioru danych, który minimalizuje sumę odległości do pozostałych elementów tego samego klastra. Dzięki temu metoda ta jest bardziej odporna na obserwacje odstające oraz umożliwia zastosowanie dowolnej miary odległości, nie tylko euklidesowej.\nNiech dany będzie zbiór obserwacji \\(X = \\{x_1, x_2, \\ldots, x_n\\} \\subset \\mathbb{R}^p\\) oraz liczba klastrów \\(K\\). Celem jest podział zbioru \\(X\\) na \\(K\\) grup w taki sposób, aby suma odległości pomiędzy punktami a reprezentantami ich klastrów była minimalna. Funkcję celu można zapisać jako \\[\nJ(M, U) = \\sum_{k=1}^{K} \\sum_{i=1}^{n} u_{ik} \\, d(x_i, m_k),\n\\] gdzie \\(M = \\{m_1, m_2, \\ldots, m_K\\} \\subset X\\) to zbiór medoidów, \\(U = [u_{ik}]\\) to macierz przypisań punktów do klastrów, a \\(d(x_i, m_k)\\) oznacza wybraną miarę odległości. Dla każdego obiektu zachodzi warunek \\(u_{ik} \\in \\{0,1\\}\\) oraz \\(\\sum_{k=1}^{K} u_{ik} = 1\\), co oznacza, że każdy punkt należy dokładnie do jednego klastra. Medoid klastra definiuje się jako punkt \\(m_k \\in X\\), który minimalizuje sumę odległości do wszystkich pozostałych punktów tego klastra \\[\nm_k = \\operatorname{argmin}_{x_j \\in X_k} \\sum_{x_i \\in X_k} d(x_i, x_j),\n\\] gdzie \\(X_k = \\{x_i : u_{ik} = 1\\}\\).\nW praktyce metoda realizowana jest iteracyjnie, analogicznie do k-means, ale z innym sposobem aktualizacji reprezentantów. Najbardziej znanym algorytmem implementującym tę ideę jest PAM (Partitioning Around Medoids). Procedura ta obejmuje następujące kroki. Po pierwsze, inicjalizuje się losowo \\(K\\) punktów jako początkowe medoidy. Następnie każdy obiekt przypisywany jest do najbliższego medoidu, zgodnie z regułą \\[\nu_{ik} = \\mathbf{1}_\\left\\{\\,k = \\operatorname{argmin}_{j} d(x_i, m_j)\\right\\}.\n\\] W ten sposób powstaje podział przestrzeni na obszary przypominające komórki Woronoja. W kolejnym kroku, dla każdego klastra wybiera się nowy medoid, czyli punkt, który minimalizuje sumę odległości do pozostałych punktów w tym klastrze. Algorytm powtarza naprzemienne kroki przypisania i aktualizacji aż do momentu, gdy zestaw medoidów przestaje się zmieniać lub wartość funkcji celu stabilizuje się.\nMetoda k-medoid jest blisko spokrewniona z metodą k-means, która minimalizuje sumę kwadratów odległości euklidesowych \\[\nJ_{\\text{k-means}} = \\sum_{k=1}^{K}\\sum_{i=1}^{n} u_{ik}\\,\\|x_i - \\mu_k\\|^2,\n\\] gdzie \\(\\mu_k\\) oznacza centroid klastra. W k-medoid zamiast średniej stosuje się rzeczywisty punkt danych, a w funkcji celu pojawia się bezpośrednia odległość, nie jej kwadrat. W konsekwencji metoda k-means jest szybsza, lecz wrażliwa na wartości odstające i ograniczona do przestrzeni euklidesowych, natomiast k-medoid jest bardziej odporna i umożliwia pracę z dowolnymi macierzami odległości, także nieliczbowymi.\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nWarto odróżnić metodę k-medoid od metody k-median. W k-medoid reprezentantem klastra jest rzeczywisty punkt ze zbioru danych, natomiast w k-median mediana klastra może znajdować się w dowolnym miejscu przestrzeni. Funkcja celu w k-median minimalizuje sumę odległości w sensie L1 (Manhattan) \\[\nJ_{\\text{k-median}} = \\sum_{k=1}^{K}\\sum_{i=1}^{n} u_{ik} \\, \\|x_i - m_k\\|_1.\n\\] Zatem k-median stanowi ciągły odpowiednik metody k-medoid, podobnie jak k-means jest wersją ciągłą dla odległości euklidesowych w kwadracie.\n\n\n\n\n\n\n\n\n\n\nCechy\nk-means\nk-median\nk-medoid\n\n\n\nReprezentant\nśrednia arytmetyczna (punkt w ℝᵖ)\nmediana geometryczna (punkt w ℝᵖ)\nrzeczywisty punkt danych\n\n\nMiara odległości\nkwadrat euklidesowej\nManhattan (L1)\ndowolna miara\n\n\nOdporność na odstające\nniska\nśrednia\nwysoka\n\n\nTyp zmiennych\nciągłe\nciągłe\ndowolne (także kategoryczne)\n\n\nCLARA i CLARANS\nAlgorytmy CLARA i CLARANS stanowią rozwinięcia metody k-medoid, opracowane w celu rozwiązania problemu wysokiej złożoności obliczeniowej klasycznego algorytmu PAM. Oba podejścia zachowują tę samą ideę — minimalizację sumy odległości punktów do reprezentantów (medoidów) — lecz różnią się strategią poszukiwania najlepszego zbioru medoidów w dużych zbiorach danych.\nAlgorytm CLARA (Clustering LARge Applications)\nAlgorytm CLARA został zaproponowany przez Kaufmana i Rousseeuwa (1990) jako metoda przybliżona dla k-medoids, umożliwiająca efektywne działanie przy dużej liczbie obserwacji. Kluczową ideą CLARA jest ograniczenie pełnych obliczeń do próbek danych, zamiast całego zbioru.\nProcedura przebiega w kilku etapach:\n\nLosowanie próbki - z całego zbioru danych \\(X\\) losuje się podzbiór \\(S \\subset X\\) o umiarkowanej liczności (np. 5–10% wszystkich obserwacji).\nZastosowanie algorytmu PAM - na wylosowanej próbce \\(S\\) przeprowadza się pełną procedurę PAM w celu wyznaczenia \\(K\\) medoidów \\(M_S = \\{m_1, \\ldots, m_K\\}\\).\nOcena jakości podziału - uzyskane medoidy testuje się na całym zbiorze danych, obliczając wartość funkcji kosztu \\[\nJ(M_S) = \\sum_{i=1}^{n} \\min_{m_k \\in M_S} d(x_i, m_k),\n\\] czyli sumę odległości każdego punktu do najbliższego medoidu.\nPowtórzenia i wybór najlepszego rozwiązania - proces losowania próbki i przeprowadzania PAM powtarza się kilka razy (np. 5–10), a końcowy wynik wybiera się na podstawie minimalnej wartości funkcji celu \\(J(M_S)\\).\n\nZaletą CLARA jest znaczne obniżenie kosztów obliczeniowych w porównaniu z PAM, którego złożoność wynosi \\(O(k(n-k)^2)\\). W CLARA złożoność zależy od rozmiaru próbki, a nie całego zbioru, co umożliwia stosowanie metody na dużych danych. Wadą jest jednak możliwość utraty jakości rozwiązania, jeśli próbka nie jest reprezentatywna — w szczególności, jeśli pomija mniejsze skupienia obecne w zbiorze danych.\nAlgorytm CLARANS (Clustering Large Applications based on RANdomized Search)\nAlgorytm CLARANS, opracowany przez Ng i Hana (1994), stanowi dalsze rozwinięcie CLARA i PAM, oparte na losowym przeszukiwaniu przestrzeni możliwych zbiorów medoidów. Jego działanie inspirowane jest technikami heurystycznymi, takimi jak local search lub simulated annealing. CLARANS traktuje przestrzeń wszystkich możliwych zestawów medoidów jako graf, w którym każdy wierzchołek odpowiada pewnemu zestawowi \\(K\\) medoidów, a krawędzie łączą wierzchołki różniące się jednym medoidem. Poszukiwanie najlepszego rozwiązania odbywa się przez losowe przechodzenie po tym grafie, przy czym zmiany medoidów dokonuje się tylko wtedy, gdy poprawiają funkcję celu.\nSchemat działania można opisać następująco:\n\nInicjalizacja - losowo wybrać zestaw \\(K\\) medoidów \\(M\\).\nLosowa eksploracja sąsiedztwa - spośród wszystkich możliwych „zamian” jednego medoidu \\(m \\in M\\) na punkt niebędący medoidem \\(x \\in X \\setminus M\\), losowo wybrać kilka par kandydatów (tzw. neighbors).\nOcena sąsiadów - dla każdego kandydata obliczyć zmianę funkcji celu \\[\n\\Delta J = J(M’) - J(M),\n\\] gdzie \\(M’\\) to nowy zestaw medoidów po zamianie.\nKrok optymalizacyjny - jeśli znajdzie się sąsiad z mniejszą wartością funkcji kosztu, przyjąć go jako nowy zestaw medoidów \\(M \\leftarrow M’\\).\nKontynuacja - powtarzać losowe przeszukiwanie do osiągnięcia lokalnego minimum (brak poprawiających się sąsiadów) lub do wyczerpania limitu iteracji.\nPowtórzenia - dla zwiększenia szansy znalezienia rozwiązania globalnego, procedurę powtarza się kilka razy z różnymi początkowymi zestawami medoidów.\n\nCLARANS jest więc algorytmem probabilistycznym, który w każdym kroku dokonuje losowej eksploracji przestrzeni możliwych rozwiązań. W przeciwieństwie do CLARA nie ogranicza się do jednej próbki danych, lecz do ograniczonej liczby losowo sprawdzanych sąsiadów, co pozwala zachować kompromis między dokładnością a szybkością.\n\nPrzykład 6.2  \n\nKodlibrary(cluster)\n\nset.seed(44)\nX &lt;- scale(iris[, 1:4])\nd &lt;- dist(X)\n\n# 1. k-means\nkmeans_res &lt;- kmeans(X, centers = 3, nstart = 25)\np_kmeans &lt;- fviz_cluster(kmeans_res, data = X, geom = \"point\", ellipse.type =\"norm\", main = \"k-means\")\n# 2. k-medoids (PAM)\npam_res &lt;- pam(X, k = 3)\np_pam &lt;- fviz_cluster(pam_res, geom = \"point\", ellipse.type =\"norm\", main = \"PAM (k-medoids)\")\n# 3. CLARA\nclara_res &lt;- clara(X, k = 3, samples = 5, pamLike= TRUE)\np_clara &lt;- fviz_cluster(clara_res, geom = \"point\", ellipse.type =\n                               \"norm\", main = \"CLARA\")\n# 4. CLARANS\nlibrary(fastkmedoids)\nclarans_res &lt;- fastclarans(d, k = 3, n = nrow(X))\np_clarans &lt;- fviz_cluster(list(data = X, cluster = clara_res$clustering), geom = \"point\", ellipse.type =\n                                 \"norm\", main = \"CLARANS\")\nggarrange(p_kmeans, p_pam, p_clara, p_clarans, ncol = 2, nrow = 2)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analiza skupień</span>"
    ]
  },
  {
    "objectID": "cluster.html#metoda-k-medoidów-k-medoids",
    "href": "cluster.html#metoda-k-medoidów-k-medoids",
    "title": "Analiza skupień",
    "section": "Metoda k-medoidów (k-medoids)",
    "text": "Metoda k-medoidów (k-medoids)\nMetoda k-medoid (ang. k-medoids) stanowi bliski odpowiednik klasycznej metody k-means, lecz wprowadza zasadniczą zmianę w sposobie definiowania reprezentanta klastra. Zamiast centroidu obliczanego jako średnia arytmetyczna wszystkich punktów w danym klastrze, metoda k-medoid wykorzystuje medoid, czyli rzeczywisty punkt ze zbioru danych, który minimalizuje sumę odległości do pozostałych elementów tego samego klastra. Dzięki temu metoda ta jest bardziej odporna na obserwacje odstające oraz umożliwia zastosowanie dowolnej miary odległości, nie tylko euklidesowej.\nNiech dany będzie zbiór obserwacji \\(X = \\{x_1, x_2, \\ldots, x_n\\} \\subset \\mathbb{R}^p\\) oraz liczba klastrów \\(K\\). Celem jest podział zbioru \\(X\\) na \\(K\\) grup w taki sposób, aby suma odległości pomiędzy punktami a reprezentantami ich klastrów była minimalna. Funkcję celu można zapisać jako \\[\nJ(M, U) = \\sum_{k=1}^{K} \\sum_{i=1}^{n} u_{ik} \\, d(x_i, m_k),\n\\] gdzie \\(M = \\{m_1, m_2, \\ldots, m_K\\} \\subset X\\) to zbiór medoidów, \\(U = [u_{ik}]\\) to macierz przypisań punktów do klastrów, a \\(d(x_i, m_k)\\) oznacza wybraną miarę odległości. Dla każdego obiektu zachodzi warunek \\(u_{ik} \\in \\{0,1\\}\\) oraz \\(\\sum_{k=1}^{K} u_{ik} = 1\\), co oznacza, że każdy punkt należy dokładnie do jednego klastra. Medoid klastra definiuje się jako punkt \\(m_k \\in X\\), który minimalizuje sumę odległości do wszystkich pozostałych punktów tego klastra \\[\nm_k = \\operatorname{argmin}_{x_j \\in X_k} \\sum_{x_i \\in X_k} d(x_i, x_j),\n\\] gdzie \\(X_k = \\{x_i : u_{ik} = 1\\}\\).\nW praktyce metoda realizowana jest iteracyjnie, analogicznie do k-means, ale z innym sposobem aktualizacji reprezentantów. Najbardziej znanym algorytmem implementującym tę ideę jest PAM (Partitioning Around Medoids). Procedura ta obejmuje następujące kroki. Po pierwsze, inicjalizuje się losowo \\(K\\) punktów jako początkowe medoidy. Następnie każdy obiekt przypisywany jest do najbliższego medoidu, zgodnie z regułą \\[\nu_{ik} = \\mathbf{1}_\\left\\{\\,k = \\operatorname{argmin}_{j} d(x_i, m_j)\\right\\}.\n\\] W ten sposób powstaje podział przestrzeni na obszary przypominające komórki Woronoja. W kolejnym kroku, dla każdego klastra wybiera się nowy medoid, czyli punkt, który minimalizuje sumę odległości do pozostałych punktów w tym klastrze. Algorytm powtarza naprzemienne kroki przypisania i aktualizacji aż do momentu, gdy zestaw medoidów przestaje się zmieniać lub wartość funkcji celu stabilizuje się.\nMetoda k-medoid jest blisko spokrewniona z metodą k-means, która minimalizuje sumę kwadratów odległości euklidesowych \\[\nJ_{\\text{k-means}} = \\sum_{k=1}^{K}\\sum_{i=1}^{n} u_{ik}\\,\\|x_i - \\mu_k\\|^2,\n\\] gdzie \\(\\mu_k\\) oznacza centroid klastra. W k-medoid zamiast średniej stosuje się rzeczywisty punkt danych, a w funkcji celu pojawia się bezpośrednia odległość, nie jej kwadrat. W konsekwencji metoda k-means jest szybsza, lecz wrażliwa na wartości odstające i ograniczona do przestrzeni euklidesowych, natomiast k-medoid jest bardziej odporna i umożliwia pracę z dowolnymi macierzami odległości, także nieliczbowymi.\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nWarto odróżnić metodę k-medoid od metody k-median. W k-medoid reprezentantem klastra jest rzeczywisty punkt ze zbioru danych, natomiast w k-median mediana klastra może znajdować się w dowolnym miejscu przestrzeni. Funkcja celu w k-median minimalizuje sumę odległości w sensie L1 (Manhattan) \\[\nJ_{\\text{k-median}} = \\sum_{k=1}^{K}\\sum_{i=1}^{n} u_{ik} \\, \\|x_i - m_k\\|_1.\n\\] Zatem k-median stanowi ciągły odpowiednik metody k-medoid, podobnie jak k-means jest wersją ciągłą dla odległości euklidesowych w kwadracie.\n\n\n\n\n\n\n\n\n\n\nCechy\nk-means\nk-median\nk-medoid\n\n\n\nReprezentant\nśrednia arytmetyczna (punkt w ℝᵖ)\nmediana geometryczna (punkt w ℝᵖ)\nrzeczywisty punkt danych\n\n\nMiara odległości\nkwadrat euklidesowej\nManhattan (L1)\ndowolna miara\n\n\nOdporność na odstające\nniska\nśrednia\nwysoka\n\n\nTyp zmiennych\nciągłe\nciągłe\ndowolne (także kategoryczne)\n\n\n\nCLARA i CLARANS\nAlgorytmy CLARA i CLARANS stanowią rozwinięcia metody k-medoid, opracowane w celu rozwiązania problemu wysokiej złożoności obliczeniowej klasycznego algorytmu PAM. Oba podejścia zachowują tę samą ideę — minimalizację sumy odległości punktów do reprezentantów (medoidów) — lecz różnią się strategią poszukiwania najlepszego zbioru medoidów w dużych zbiorach danych.\nAlgorytm CLARA (Clustering LARge Applications)\nAlgorytm CLARA został zaproponowany przez Kaufmana i Rousseeuwa (1990) jako metoda przybliżona dla k-medoids, umożliwiająca efektywne działanie przy dużej liczbie obserwacji. Kluczową ideą CLARA jest ograniczenie pełnych obliczeń do próbek danych, zamiast całego zbioru.\nProcedura przebiega w kilku etapach:\n\nLosowanie próbki - z całego zbioru danych \\(X\\) losuje się podzbiór \\(S \\subset X\\) o umiarkowanej liczności (np. 5–10% wszystkich obserwacji).\nZastosowanie algorytmu PAM - na wylosowanej próbce \\(S\\) przeprowadza się pełną procedurę PAM w celu wyznaczenia \\(K\\) medoidów \\(M_S = \\{m_1, \\ldots, m_K\\}\\).\nOcena jakości podziału - uzyskane medoidy testuje się na całym zbiorze danych, obliczając wartość funkcji kosztu \\[\nJ(M_S) = \\sum_{i=1}^{n} \\min_{m_k \\in M_S} d(x_i, m_k),\n\\] czyli sumę odległości każdego punktu do najbliższego medoidu.\nPowtórzenia i wybór najlepszego rozwiązania - proces losowania próbki i przeprowadzania PAM powtarza się kilka razy (np. 5–10), a końcowy wynik wybiera się na podstawie minimalnej wartości funkcji celu \\(J(M_S)\\).\n\nZaletą CLARA jest znaczne obniżenie kosztów obliczeniowych w porównaniu z PAM, którego złożoność wynosi \\(O(k(n-k)^2)\\). W CLARA złożoność zależy od rozmiaru próbki, a nie całego zbioru, co umożliwia stosowanie metody na dużych danych. Wadą jest jednak możliwość utraty jakości rozwiązania, jeśli próbka nie jest reprezentatywna — w szczególności, jeśli pomija mniejsze skupienia obecne w zbiorze danych.\nAlgorytm CLARANS (Clustering Large Applications based on RANdomized Search)\nAlgorytm CLARANS, opracowany przez Ng i Hana (1994), stanowi dalsze rozwinięcie CLARA i PAM, oparte na losowym przeszukiwaniu przestrzeni możliwych zbiorów medoidów. Jego działanie inspirowane jest technikami heurystycznymi, takimi jak local search lub simulated annealing. CLARANS traktuje przestrzeń wszystkich możliwych zestawów medoidów jako graf, w którym każdy wierzchołek odpowiada pewnemu zestawowi \\(K\\) medoidów, a krawędzie łączą wierzchołki różniące się jednym medoidem. Poszukiwanie najlepszego rozwiązania odbywa się przez losowe przechodzenie po tym grafie, przy czym zmiany medoidów dokonuje się tylko wtedy, gdy poprawiają funkcję celu.\nSchemat działania można opisać następująco:\n\nInicjalizacja - losowo wybrać zestaw \\(K\\) medoidów \\(M\\).\nLosowa eksploracja sąsiedztwa - spośród wszystkich możliwych „zamian” jednego medoidu \\(m \\in M\\) na punkt niebędący medoidem \\(x \\in X \\setminus M\\), losowo wybrać kilka par kandydatów (tzw. neighbors).\nOcena sąsiadów - dla każdego kandydata obliczyć zmianę funkcji celu \\[\n\\Delta J = J(M’) - J(M),\n\\] gdzie \\(M’\\) to nowy zestaw medoidów po zamianie.\nKrok optymalizacyjny - jeśli znajdzie się sąsiad z mniejszą wartością funkcji kosztu, przyjąć go jako nowy zestaw medoidów \\(M \\leftarrow M’\\).\nKontynuacja - powtarzać losowe przeszukiwanie do osiągnięcia lokalnego minimum (brak poprawiających się sąsiadów) lub do wyczerpania limitu iteracji.\nPowtórzenia - dla zwiększenia szansy znalezienia rozwiązania globalnego, procedurę powtarza się kilka razy z różnymi początkowymi zestawami medoidów.\n\nCLARANS jest więc algorytmem probabilistycznym, który w każdym kroku dokonuje losowej eksploracji przestrzeni możliwych rozwiązań. W przeciwieństwie do CLARA nie ogranicza się do jednej próbki danych, lecz do ograniczonej liczby losowo sprawdzanych sąsiadów, co pozwala zachować kompromis między dokładnością a szybkością.\n\nPrzykład 6.2  \n\nKodlibrary(cluster)\n\nset.seed(44)\nX &lt;- scale(iris[, 1:4])\nd &lt;- dist(X)\n\n# 1. k-means\nkmeans_res &lt;- kmeans(X, centers = 3, nstart = 25)\np_kmeans &lt;- fviz_cluster(kmeans_res, data = X, geom = \"point\", ellipse.type =\"norm\", main = \"k-means\")\n# 2. k-medoids (PAM)\npam_res &lt;- pam(X, k = 3)\np_pam &lt;- fviz_cluster(pam_res, geom = \"point\", ellipse.type =\"norm\", main = \"PAM (k-medoids)\")\n# 3. CLARA\nclara_res &lt;- clara(X, k = 3, samples = 5, pamLike= TRUE)\np_clara &lt;- fviz_cluster(clara_res, geom = \"point\", ellipse.type =\n                               \"norm\", main = \"CLARA\")\n# 4. CLARANS\nlibrary(fastkmedoids)\nclarans_res &lt;- fastclarans(d, k = 3, n = nrow(X))\np_clarans &lt;- fviz_cluster(list(data = X, cluster = clara_res$clustering), geom = \"point\", ellipse.type =\n                                 \"norm\", main = \"CLARANS\")\nggarrange(p_kmeans, p_pam, p_clara, p_clarans, ncol = 2, nrow = 2)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analiza skupień</span>"
    ]
  }
]