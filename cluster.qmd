---
output: html_document
number-sections: false
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

# Analiza skupień

## Rys historyczny

Analiza skupień, znana również jako *cluster analysis*, ma swoje korzenie w
połowie XX wieku, choć jej podstawy koncepcyjne pojawiły się znacznie wcześniej
w statystyce i biologii systematycznej. Jej rozwój przebiegał równolegle w
kilku dziedzinach, w tym w psychologii, biologii, socjologii i informatyce, a z
czasem stała się jednym z fundamentalnych narzędzi eksploracyjnej analizy
danych. Pierwsze idee grupowania obiektów o podobnych cechach można odnaleźć
już w XVIII i XIX wieku w klasyfikacji biologicznej. Carl Linneusz wprowadził
system binominalny oparty na cechach morfologicznych organizmów, co stanowiło
wczesny przykład klasyfikacji hierarchicznej. Współczesne podejście
matematyczne do analizy skupień zaczęło się jednak kształtować dopiero w XX
wieku wraz z rozwojem metod statystycznych i koncepcji odległości w przestrzeni
wielowymiarowej. Za właściwy początek analizy skupień w sensie statystycznym
uznaje się lata 30. i 40. XX wieku. W 1939 roku Tryon wprowadził pojęcie
analizy grupowej (*cluster analysis*) w psychologii, stosując ją do
klasyfikacji zmiennych i jednostek na podstawie macierzy podobieństw. W latach
50. i 60. intensywny rozwój metod klasyfikacji hierarchicznej był związany z
rozwojem biologii numerycznej (*numerical taxonomy*), głównie dzięki pracom
Sokal’a i Sneath’a, którzy w latach 60. zaproponowali formalne podstawy
taksonomii numerycznej opartej na macierzach podobieństw między organizmami.
Lata 60. i 70. XX wieku przyniosły znaczący rozwój metod niehierarchicznych, w
tym przede wszystkim metody *k-means*, zaproponowanej przez MacQueena w 1967
roku. Algorytm ten stał się jednym z najczęściej stosowanych narzędzi w
analizie skupień dzięki swojej prostocie, interpretowalności i efektywności
obliczeniowej. W tym samym okresie rozwijano również metody oparte na gęstości
(np. późniejszy DBSCAN), metody probabilistyczne (modele mieszanek Gaussa) oraz
techniki optymalizacyjne pozwalające na automatyczne wyznaczanie liczby
skupień. W latach 80. i 90. wraz z rozwojem informatyki oraz eksploracji danych
(data mining), analiza skupień zaczęła być szeroko stosowana w zastosowaniach
praktycznych – od segmentacji rynku, przez rozpoznawanie obrazów, po
bioinformatykę. Pojawiły się również metody adaptacyjne i oparte na uczeniu
nienadzorowanym, w tym sieci neuronowe typu *self-organizing maps* (SOM)
opracowane przez Kohonena. W XXI wieku analiza skupień stała się kluczowym
elementem nauki o danych (*data science*). Współczesne metody integrują
klasyczne podejścia statystyczne z algorytmami uczenia maszynowego. Opracowano
wiele nowych technik, takich jak metody oparte na gęstości (DBSCAN, OPTICS),
metody spektralne wykorzystujące wartości własne macierzy podobieństwa, czy
algorytmy głębokiego grupowania (*deep clustering*) bazujące na sieciach
neuronowych. Równocześnie rozwinięto teoretyczne podstawy walidacji skupień,
takie jak współczynniki *silhouette*, indeks Calinskiego-Harabasza czy
Davies-Bouldin, umożliwiające obiektywną ocenę jakości grupowania.

## Podział metod analizy skupień

Metody analizy skupień można klasyfikować według różnych kryteriów, takich jak
sposób tworzenia skupień, założenia o strukturze danych, rodzaj miary
podobieństwa czy sposób reprezentacji wyników. Najczęściej przyjmuje się
podział taksonomiczny oparty na sposobie grupowania obiektów, który pozwala
wyróżnić cztery główne klasy metod: hierarchiczne, niehierarchiczne, oparte na
gęstości i oparte na modelach probabilistycznych.

Pierwszą i jedną z najstarszych kategorii są metody hierarchiczne. Ich istotą
jest budowa dendrogramu odzwierciedlającego stopniowe łączenie (lub
rozdzielanie) obiektów w skupienia. Wyróżnia się dwa podejścia: aglomeracyjne,
które rozpoczynają od traktowania każdego obiektu jako odrębnego skupienia i
następnie łączą je zgodnie z określoną miarą odległości (np. metoda
pojedynczego, pełnego lub średniego wiązania), oraz dzielące, które zaczynają
od jednego skupienia zawierającego wszystkie obiekty i w kolejnych krokach
dokonują jego podziału. Metody hierarchiczne mają tę zaletę, że nie wymagają
wcześniejszego określenia liczby skupień, lecz ich wadą jest wysoka złożoność
obliczeniowa i wrażliwość na szumy.

Drugą grupę stanowią metody niehierarchiczne, wśród których najbardziej znane
są algorytmy typu *k-means* oraz *k-medoids*. Ich celem jest bezpośrednie
przypisanie każdego obiektu do jednego z ustalonej liczby skupień na podstawie
minimalizacji sumy kwadratów odległości wewnątrzgrupowych. Metoda *k-means*
jest szybka i skuteczna przy danych o wyraźnie kulistych skupieniach, natomiast
*k-medoids* (np. algorytm PAM) jest bardziej odporna na wartości odstające. Do
tej kategorii należą również algorytmy optymalizacyjne, takie jak *k-means++*
czy *mini-batch k-means*, dostosowane do dużych zbiorów danych.

Trzecią kategorię tworzą metody oparte na gęstości, w których skupienia
definiuje się jako obszary przestrzeni danych o wysokim zagęszczeniu punktów
oddzielone obszarami o niskiej gęstości. Klasycznym przykładem jest algorytm
DBSCAN, który wykrywa skupienia dowolnego kształtu i pozwala automatycznie
identyfikować punkty szumu. Udoskonaloną wersją tej metody jest OPTICS,
umożliwiająca hierarchiczne przedstawienie struktur gęstościowych. Metody tego
typu są szczególnie użyteczne przy analizie danych przestrzennych oraz w
sytuacjach, gdy skupienia nie mają regularnego kształtu.

Czwartą grupą są metody oparte na modelach probabilistycznych. Zakładają one,
że dane pochodzą z mieszaniny rozkładów (najczęściej wielowymiarowych
normalnych), a zadaniem algorytmu jest estymacja parametrów tych rozkładów oraz
przypisanie obiektów do skupień na podstawie maksymalnego prawdopodobieństwa.
Do tej kategorii należą modele mieszanek Gaussa (GMM) estymowane metodą EM
(*Expectation–Maximization*), które umożliwiają probabilistyczne przypisanie
obiektów do wielu skupień z różnym stopniem przynależności.

Poza głównymi czterema klasami wyróżnia się również metody hybrydowe i
współczesne podejścia uczenia nienadzorowanego. Przykładem są metody
spektralne, które wykorzystują analizę wartości własnych macierzy podobieństwa,
oraz metody głębokiego grupowania (*deep clustering*), integrujące sieci
neuronowe autoenkoderowe z klasycznymi procedurami klastrowania[^cluster-1].

![Podział metod grupowania](images/clust1.png)

[^cluster-1]: Ten rodzaj klastrowania nie będzie przedmiotem tego rozdziału
    ponieważ wykracza poza klasyczne podejście statystyczne i wymaga wiedzy na
    temat sieci neuronowych, która pojawia się na późniejszych semestrach.


## Metody hierarchiczne

Metody hierarchiczne w analizie skupień opierają się na iteracyjnym łączeniu
lub dzieleniu obiektów w sposób odzwierciedlający ich podobieństwo, prowadząc
do utworzenia struktury drzewiastej (dendrogramu). Struktura ta ukazuje
hierarchiczne relacje między obiektami – od indywidualnych elementów aż po
jedną nadrzędną grupę lub odwrotnie. Wyróżnia się dwa główne podejścia: metody
aglomeracyjne oraz deglomeracyjne (dzielące).

### Metody aglomeracyjne

W podejściu aglomeracyjnym proces rozpoczyna się od traktowania każdego obiektu
jako odrębnego skupienia jednoelementowego. Następnie w kolejnych krokach łączy
się dwa najbardziej podobne skupienia, aż do momentu uzyskania jednego
skupienia zawierającego wszystkie obiekty. Proces ten można formalnie zapisać
następująco

1.  Niech zbiór danych składa się z $n$ obiektów $$
    X = \{x_1, x_2, \ldots, x_n\},
    $$ gdzie każdy obiekt $x_i \in \mathbb{R}^p.$
2.  Początkowo każdy obiekt stanowi odrębne skupienie $$
    C_i^{(0)} = \{x_i\} \quad \text{dla}\quad i = 1, \ldots, n.
    $$
3.  Definiuje się macierz odległości $D = [d(x_i, x_j)]$, gdzie funkcja
    $d(\cdot, \cdot)$ określa miarę odległości (np. euklidesową, Mahalanobisa,
    Manhattan).
4.  Na każdym kroku $t$ wyszukuje się dwa skupienia $C_p^{(t)}$ i $C_q^{(t)}$,
    które są najbliższe względem przyjętej miary odległości między skupieniami
    $D(C_p, C_q)$. Następnie łączy się je w jedno nowe skupienie $$
    C_{pq}^{(t+1)} = C_p^{(t)} \cup C_q^{(t)}.
    $$
5.  Odległości między nowo utworzonym skupieniem a pozostałymi aktualizuje się
    zgodnie z przyjętą regułą wiązania (*linkage criterion*). Niech
    $D(C_i,C_j)$ oznacza odległość klaster–klaster, $d(x,y)$ bazową odległość
    punkt–punkt, $|C_i|=n_i$ liczność klastra $C_i$, $\bar x_i$ centroid $C_i$.
    Reguła aglomeracji w postaci rekurencji Lance’a–Williamsa przyjmuje wówczas
    postać $$
    D\big((C_i\!\cup\!C_j),C_k\big)=\alpha_i D(C_i,C_k)+\alpha_j D(C_j,C_k)+\beta D(C_i,C_j)+\gamma\,\big|D(C_i,C_k)-D(C_j,C_k)\big|,
    $$ z współczynnikami ($\alpha_i,\alpha_j,\beta,\gamma$) zależnymi od
    wybranego sposobu łączenia. Dla metod centroidowych i Warda inicjalizujemy
    macierz odległości kwadratami odległości euklidesowych i interpretujemy
    wyniki jako wartości kwadratowe. Możemy wówczas wyróżnić następujące metody
    aglomeracji:
    -   Metoda pojedynczego wiązania (*single linkage*) $$
        D(C_i,C_j)=\min_{x\in C_i,\,y\in C_j} d(x,y).
        $$ Zbiory łączymy regułą $$
        D\big((C_i\!\cup\!C_j),C_k\big)=\min\!\big(D(C_i,C_k),\,D(C_j,C_k)\big),
        $$ co odpowiada
        $\alpha_i=\alpha_j=\tfrac12,\ \beta=0,\ \gamma=-\tfrac12$ przy
        inicjalizacji $D(\{x\},\{y\})=d(x,y)$.
    -   Metoda pełnego wiązania (*complete linkage*) $$
        D(C_i,C_j)=\max_{x\in C_i,\,y\in C_j} d(x,y).
        $$ Zbiory łączymy regułą $$
        D\big((C_i\!\cup\!C_j),C_k\big)=\max\!\big(D(C_i,C_k),\,D(C_j,C_k)\big),
        $$ czyli $\alpha_i=\alpha_j=\tfrac12,\ \beta=0,\ \gamma=+\tfrac12$, z
        inicjalizacją $d(x,y)$.
    -   Metoda średniego wiązania (*average linkage*, UPGMA - *Unweighted Pair
        Group Method using Arithmetic Averages*) $$
        D(C_i,C_j)=\frac{1}{n_i n_j}\sum_{x\in C_i}\sum_{y\in C_j} d(x,y).
        $$ Zbiory łączymy regułą $$
        D\big((C_i\!\cup\!C_j),C_k\big)=\frac{n_i\,D(C_i,C_k)+n_j\,D(C_j,C_k)}{n_i+n_j},
        $$ co daje
        $\alpha_i=\tfrac{n_i}{n_i+n_j},\ \alpha_j=\tfrac{n_j}{n_i+n_j},\ \beta=\gamma=0$.
    -   Metoda ważonego średniego wiązania (*weighted average linkage*, WPGMA -
        *Weighted Pair Group Method using Arithmetic Averages*, McQuitty) -
        zbiory łączymy regułą $$
        D\big((C_i\!\cup\!C_j),C_k\big)=\tfrac12\big(D(C_i,C_k)+D(C_j,C_k)\big),
        $$ tj. $\alpha_i=\alpha_j=\tfrac12,\ \beta=\gamma=0$, przy
        inicjalizacji $d(x,y)$.
    -   Metoda centroidów (*centroid linkage*, UPGMC - *Unweighted Pair Group
        Method using Centroids*)[^cluster-2] - definicja przez centroidy
        (wymaga kwadratów odległości euklidesowych) $$
        D(C_i,C_j)=\|\bar x_i-\bar x_j\|^2,\qquad \bar x_i=\frac{1}{n_i}\sum_{x\in C_i}x.
        $$ Zbiory łączymy regułą $$
        D\big((C_i\!\cup\!C_j),C_k\big)=\frac{n_i}{n_i+n_j}D(C_i,C_k)+\frac{n_j}{n_i+n_j}D(C_j,C_k)-\frac{n_i n_j}{(n_i+n_j)^2}D(C_i,C_j),
        $$ co odpowiada
        $\alpha_i=\tfrac{n_i}{n_i+n_j},\ \alpha_j=\tfrac{n_j}{n_i+n_j},\ \beta=-\tfrac{n_i n_j}{(n_i+n_j)^2},\ \gamma=0$,
        inicjalizujemy $D(\{x\},\{y\})=\|x-y\|^2$.
    -   Metoda mediany (*median linkage*, WPGMC - *Weighted Pair Group Method
        using Centroids*) - centra klastrów aktualizujemy przez punkt środkowy
        median $m_{i\cup j}=\tfrac12(m_i+m_j).$ Zbiory łączymy regułą $$
        D\big((C_i\!\cup\!C_j),C_k\big)=\tfrac12\big(D(C_i,C_k)+D(C_j,C_k)\big)-\tfrac14\,D(C_i,C_j),
        $$ czyli $\alpha_i=\alpha_j=\tfrac12,\ \beta=-\tfrac14,\ \gamma=0$, z
        inicjalizacją $D(\{x\},\{y\})=\|x-y\|^2$.
    -   Metoda Warda (*Ward’s linkage*) $$
        D(C_i,C_j)=\frac{2\,n_i n_j}{n_i+n_j}\,\|\bar x_i-\bar x_j\|^2,
        $$ Zbiory łączymy regułą $$
        D\big((C_i\!\cup\!C_j),C_k\big)=\frac{n_i+n_k}{n_i+n_j+n_k}D(C_i,C_k)+\frac{n_j+n_k}{n_i+n_j+n_k}D(C_j,C_k)-\frac{n_k}{n_i+n_j+n_k}D(C_i,C_j),
        $$ przy inicjalizacji $D(\{x\},\{y\})=\|x-y\|^2$.
6.  Proces powtarza się do momentu, gdy wszystkie obiekty znajdą się w jednym
    skupieniu, tworząc hierarchiczny układ połączeń.

Zaletą metod aglomeracyjnych jest to, że nie wymagają a priori określenia
liczby skupień. Wadą jest natomiast ich nieodwracalność – raz połączone
skupienia nie mogą zostać rozdzielone, a wynik końcowy jest wrażliwy na wybór
miary odległości i kryterium wiązania.

[^cluster-2]: W metodach centroidowych, medianowej i Warda podkreśla się
    konieczność pracy na kwadratach odległości euklidesowych.
    
:::{#exm-1}

```{r}
library(factoextra)
library(ggpubr)

# 1. Przygotowanie danych: standaryzacja czterech cech numerycznych
X <- scale(iris[, 1:4])

# 2. Macierz odległości euklidesowych
d <- dist(X, method = "euclidean")

# 3. Budowa dendrogramów dla różnych metod łączenia
hc_single   <- hclust(d, method = "single")
hc_complete <- hclust(d, method = "complete")
hc_average  <- hclust(d, method = "average")     # UPGMA
hc_ward     <- hclust(d, method = "ward.D2")     # Ward (zalecany wariant .D2)

# 4. Wizualizacja dendrogramów z linią cięcia na K=3
p_d_single   <- fviz_dend(hc_single,   k = 3, cex = 0.6, main = "single linkage (K=3)")
p_d_complete <- fviz_dend(hc_complete, k = 3, cex = 0.6, main = "complete linkage (K=3)")
p_d_average  <- fviz_dend(hc_average,  k = 3, cex = 0.6, main = "average linkage (K=3)")
p_d_ward     <- fviz_dend(hc_ward,     k = 3, cex = 0.6, main = "Ward.D2 (K=3)")

ggarrange(p_d_single, p_d_complete, p_d_average, p_d_ward, ncol = 2, nrow = 2)

# 5. Uzyskanie etykiet klastrów dla K=3 i rzutowanie grup w przestrzeń PCA
cl_single   <- cutree(hc_single,   k = 3)
cl_complete <- cutree(hc_complete, k = 3)
cl_average  <- cutree(hc_average,  k = 3)
cl_ward     <- cutree(hc_ward,     k = 3)

p_c_single   <- fviz_cluster(list(data = X, cluster = cl_single),
                             geom = "point", ellipse.type = "norm",
                             main = "single linkage")
p_c_complete <- fviz_cluster(list(data = X, cluster = cl_complete),
                             geom = "point", ellipse.type = "norm",
                             main = "complete linkage")
p_c_average  <- fviz_cluster(list(data = X, cluster = cl_average),
                             geom = "point", ellipse.type = "norm",
                             main = "average linkage")
p_c_ward     <- fviz_cluster(list(data = X, cluster = cl_ward),
                             geom = "point", ellipse.type = "norm",
                             main = "Ward.D2")

ggarrange(p_c_single, p_c_complete, p_c_average, p_c_ward, ncol = 2, nrow = 2)
```

:::

### Metody deglomeracyjne

Metody deglomeracyjne (dzielące) stanowią odwrotność podejścia aglomeracyjnego.
Zaczyna się od jednego skupienia zawierającego wszystkie obiekty, które
następnie są iteracyjnie dzielone na mniejsze podzbiory, aż do osiągnięcia
oczekiwanej liczby skupień lub spełnienia kryterium zatrzymania.

Formalnie proces można przedstawić w postaci

1.  Początkowo przyjmuje się jedno skupienie $$C^{(0)} = X.$$
2.  W każdym kroku wybiera się skupienie $C_i^{(t)}$, które zostanie
    podzielone. Wybór ten może wynikać z maksymalnej wariancji
    wewnątrzgrupowej, liczby elementów lub innych kryteriów jakości skupień.
3.  Dokonuje się podziału wybranego skupienia na dwa mniejsze, minimalizując
    błąd wewnątrzgrupowegy lub maksymalizując różnice międzygrupowe.
    Najczęściej stosuje się algorytm analogiczny do *bisecting k-means* 
    $$
    C_i^{(t)} \rightarrow \{C_{i1}^{(t+1)}, C_{i2}^{(t+1)}\},
    $$ przy czym podział realizuje się poprzez iteracyjne zastosowanie k-means
    z $k = 2$.
4.  Proces dzielenia jest powtarzany do momentu uzyskania żądanej liczby
    skupień lub gdy dalszy podział nie prowadzi do istotnej poprawy jakości.

Metody deglomeracyjne są mniej popularne z powodu wyższego kosztu
obliczeniowego i konieczności przyjęcia dodatkowych kryteriów decyzyjnych
dotyczących wyboru skupienia do podziału. Jednak w dużych zbiorach danych mogą
być efektywniejsze niż aglomeracyjne, szczególnie gdy implementuje się je z
wykorzystaniem metod heurystycznych.

## Metody niehierarchiczne

Metody niehierarchiczne w analizie skupień koncentrują się na bezpośrednim
przypisaniu obiektów do określonej liczby skupień bez tworzenia struktury
hierarchicznej. Najbardziej znane i szeroko stosowane są algorytmy typu
*k-means* oraz *k-medoids*. Podstawą matematyczną metod niehierarchicznych jest minimalizacja pewnej funkcji celu, najczęściej sumy kwadratów odchyleń punktów od środków grup, zwanych centroidami. W przeciwieństwie do podejścia hierarchicznego, proces ten ma charakter iteracyjny i wymaga wcześniejszego określenia liczby klastrów. W efekcie powstaje partycja przestrzeni danych, w której każdy obiekt zostaje przypisany do jednego lub kilku klastrów w zależności od przyjętej koncepcji przynależności (podział płaski[^clus-3]). W tym kontekście wyróżnia się dwa główne typy podziałów: podział twardy i podział rozmyty.

[^clus-3]: podział płaski oznacza, że każdy obiekt należy do dokładnie jednej grupy i nie
    istnieje hierarchia między grupami

### Podział twardy

Podział twardy opiera się na jednoznacznym przypisaniu każdego obiektu do dokładnie jednego klastra. W ujęciu matematycznym przyjmuje się, że dla zbioru obserwacji $X = \{x_1, x_2, \ldots, x_n\}$ oraz ustalonej liczby klastrów $K$, istnieje macierz przynależności $U = [u_{ik}]$, w której każdy element przyjmuje wartość 0 lub 1. Wartość $u_{ik} = 1$ oznacza, że obiekt $x_i$ należy do klastra $k$, natomiast $u_{ik} = 0$ – że do niego nie należy.

Pierwszym warunkiem podziału twardego jest to, że każdy obiekt musi należeć dokładnie do jednej grupy. Oznacza to, że dla każdego obiektu suma przynależności po wszystkich klastrach równa się jeden
$$
\sum_{k=1}^{K} u_{ik} = 1, \quad \forall i \in \{1, 2, \ldots, n\}.
$$
Z kolei każdy klaster powinien zawierać przynajmniej jeden element, co można zapisać jako
$$
1 \leq \sum_{i=1}^{n} u_{ik}, \quad \forall k \in \{1, 2, \ldots, K\}.
$$
Wynika z tego, że nie dopuszcza się powstawania pustych grup. Kolejnym warunkiem jest binarność przypisań, czyli
$$
u_{ik} \in \{0, 1\}, \quad \forall i, k.
$$
Przynależność obiektu do klastra jest zatem całkowita i nie dopuszcza stanów pośrednich. Wreszcie, formalnym celem podziału twardego jest minimalizacja funkcji błędu, która określa sumę kwadratów odchyleń poszczególnych obiektów od centroidów klastrów, do których zostały przypisane
$$
J = \sum_{k=1}^{K} \sum_{i=1}^{n} u_{ik} \, \|x_i - \mu_k\|^2,
$$
gdzie $\mu_k$ oznacza środek klastra $k$, a $\|\cdot\|$ jest najczęściej normą euklidesową. Każdy obiekt powinien zostać przypisany do tego klastra, którego centroid jest najbliższy, czyli
$$
u_{ik} =
\begin{cases}
1, & \text{jeśli } k = \arg \min_{j} \|x_i - \mu_j\|, \\
0, & \text{w przeciwnym razie.}
\end{cases}
$$

### Podział rozmyty

Podział rozmyty (ang. *fuzzy clustering*) stanowi uogólnienie klasycznego, twardego podejścia do grupowania, w którym dopuszcza się możliwość częściowej przynależności obiektu do więcej niż jednego klastra. Zamiast przypisywać każdy element jednoznacznie do jednej grupy, wprowadza się pojęcie stopnia przynależności, który przyjmuje wartości z przedziału $[0,1]$. W ten sposób odzwierciedla się niepewność lub płynność granic między grupami, co czyni tę metodę bardziej elastyczną i lepiej dostosowaną do danych o niejednoznacznej strukturze.

Matematycznie, dla zbioru obserwacji $X = \{x_1, x_2, \ldots, x_n\}$ oraz ustalonej liczby klastrów $K$, definiuje się macierz przynależności $U = [u_{ik}]$, gdzie każdy element $u_{ik}$ oznacza stopień, w jakim obiekt $x_i$ należy do klastra $k$. W odróżnieniu od podziału twardego, tutaj $u_{ik} \in [0,1]$, a nie tylko $\{0,1\}$. Zachowany zostaje jednak warunek, że suma stopni przynależności danego obiektu do wszystkich klastrów musi być równa jeden
$$
\sum_{k=1}^{K} u_{ik} = 1, \quad \forall i \in \{1, 2, \ldots, n\}.
$$
Warunek ten oznacza, że przynależności mają charakter względny – im silniejszy związek obiektu z jednym klastrem, tym słabszy z innymi.

Podział rozmyty opiera się na minimalizacji rozmytej funkcji celu, znanej z algorytmu *Fuzzy c-means*
$$
J_m = \sum_{k=1}^{K} \sum_{i=1}^{n} (u_{ik})^m \, \|x_i - \mu_k\|^2,
$$
gdzie $\mu_k$ oznacza centroid klastra $k$, a parametr $m > 1$ kontroluje poziom rozmycia. Im większa wartość $m$, tym bardziej rozmyty staje się podział, ponieważ różnice pomiędzy wartościami przynależności poszczególnych obiektów do klastrów ulegają spłaszczeniu. W praktyce najczęściej przyjmuje się $m = 2$. Optymalizacja funkcji celu prowadzi do następujących warunków aktualizacji. Stopnie przynależności obliczane są według wzoru
$$
u_{ik} = \frac{1}{\sum_{j=1}^{K} \left( \frac{\|x_i - \mu_k\|}{\|x_i - \mu_j\|} \right)^{\frac{2}{m-1}}},
$$
natomiast nowe położenie centroidów wyznacza się jako ważoną średnią punktów, gdzie wagi stanowią stopnie przynależności podniesione do potęgi $m$
$$
\mu_k = \frac{\sum_{i=1}^{n} (u_{ik})^m x_i}{\sum_{i=1}^{n} (u_{ik})^m}.
$$
Proces ten przebiega iteracyjnie – w każdej iteracji obliczane są nowe wartości $u_{ik}$ i $\mu_k$, aż do osiągnięcia zbieżności funkcji celu $J_m$.

### Metoda k-średnich (*k-means*)

Algorytm *k-means* wynika z problemu minimalizacji sumy kwadratów odchyleń punktów od reprezentantów grup w metryce euklidesowej. Niech dany będzie zbiór obserwacji $X=\{x_1,\dots,x_n\}\subset\mathbb{R}^p$ oraz liczba klastrów $K$. Celem jest znalezienie partycji danych i wektorów $\mu_1,\dots,\mu_K\in\mathbb{R}^p$ minimalizujących funkcję celu
$$
J(U,\mu)=\sum_{k=1}^K\sum_{i=1}^n u_{ik}\,\|x_i-\mu_k\|^2,
$$
gdzie $U=[u_{ik}]$ jest macierzą przypisań spełniającą warunki podziału twardego $u_{ik}\in\{0,1\}$, $\sum_{k=1}^K u_{ik}=1$ dla każdego $i$, a $\sum_{i=1}^n u_{ik}\ge 1$ dla każdego $k$.

Wyprowadzenie algorytmu polega na zastosowaniu naprzemiennej minimalizacji względem $U$ i $\mu$, ponieważ jednoczesna minimalizacja jest problemem kombinatorycznym trudnym obliczeniowo. Rozważmy najpierw minimalizację względem centroidów przy ustalonych przypisaniach. Dla danego $k$ rozważmy funkcję
$$
J_k(\mu_k)=\sum_{i=1}^n u_{ik}\,\|x_i-\mu_k\|^2.
$$
Jest to funkcja kwadratowa ściśle wypukła w $\mu_k$. Obliczamy gradient
$$
\nabla_{\mu_k}J_k(\mu_k)=2\sum_{i=1}^n u_{ik}\,(\mu_k-x_i)=2\left(\Big(\sum_{i}u_{ik}\Big)\mu_k-\sum_{i}u_{ik}x_i\right).
$$
Warunek $\nabla_{\mu_k}J_k(\mu_k)=0$ daje
$$
\mu_k^\star=\frac{\sum_{i=1}^n u_{ik}x_i}{\sum_{i=1}^n u_{ik}},
$$
czyli optymalny centroid jest średnią arytmetyczną punktów przypisanych do klastra. Wypukłość zapewnia, że jest to minimum globalne względem $\mu_k$. Zatem przy ustalonych $U$ krok aktualizacji centroidów ma postać średniej ważonej ze wskaźnikami $u_{ik}$.

Następnie dokonujemy minimalizacji względem przypisań przy ustalonych centroidach. Dla każdego obiektu $x_i$ problem redukuje się do
$$
\min_{u_{i1},\dots,u_{iK}} \sum_{k=1}^K u_{ik}\,\|x_i-\mu_k\|^2\quad \text{przy}\quad u_{ik}\in\{0,1\},\ \sum_k u_{ik}=1.
$$
Ponieważ wyrażenie jest liniowe w $u_{ik}$, optimum osiąga się, wybierając $u_{ik}=1$ dla indeksu $k$ minimalizującego odległość euklidesową
$$
u_{ik}=\mathbf{1}_\left\{k=\operatorname{argmin}_{j\in\{1,\dots,K\}}\|x_i-\mu_j\|^2\right\}.
$$
Wynika stąd reguła „przypisz do najbliższego centroidu”, co geometrycznie odpowiada podziałowi przestrzeni na komórki Woronoja wyznaczone przez $\{\mu_k\}$.

![](images/kmeans_animation.gif)

Złożenie obu kroków prowadzi do procedury znanej jako *Lloyd’s algorithm*: 

1. Startujemy od wstępnych centroidów $\mu^{(0)}$.
2. Naprzemiennie wykonujemy przypisanie do najbliższego centroidu.
3. Przeprowadzamy aktualizację centroidów jako średnich.
4. Wykonujemy kroki 2-3 aż do osiągnięcia zbieżności (punkty nie zmieniają swoich skupień). 

Każdy z kroków 2-3 nie zwiększa funkcji celu, bo przy ustalonych centroidach wybór najbliższego centroidu minimalizuje składnik $\|x_i-\mu_k\|^2$ dla każdego $i$, więc $J$ maleje lub pozostaje stała, a przy ustalonych przypisaniach do klastrów wybór średniej minimalizuje sumę kwadratów, więc $J$ również maleje lub pozostaje stała. Ponieważ istnieje skończona liczba możliwych partycji i $J\ge 0$, monotonicznie niemalejąca sekwencja wartości funkcji celu musi zatrzymać się w skończonej liczbie kroków na punkcie stacjonarnym, czyli minimum lokalnym problemu z ograniczeniami twardych przypisań.

Warto zauważyć, że równoważnie można interpretować cel jako minimalizację wariancji wewnątrzklastrowej. W klasycznej dekompozycji SST
$$
\underbrace{\sum_{i=1}^n \|x_i-\bar{x}\|^2}_{\text{SST}}=\underbrace{\sum_{k=1}^K\sum_{i=1}^n u_{ik}\|x_i-\mu_k\|^2}_{\text{WSS}}+\underbrace{\sum_{k=1}^K n_k\|\mu_k-\bar{x}\|^2}_{\text{BSS}},
$$
gdzie $\bar{x}$ jest średnią globalną, a $n_k=\sum_i u_{ik}$. Minimalizacja WSS (ang. *within-cluster sum of squares*) przy zadanym $K$ jest równoważna maksymalizacji BSS, czyli maksymalizacji separacji centroidów względem średniej globalnej, co formalnie uzasadnia intuicję „maksymalizować jednorodność wewnątrz klastrów i różnice między klastrami”.

::: callout-note
## *k-means++*

Zastosowanie inicjalizacji *k-means++* polega na losowaniu początków z uprzywilejowaniem punktów odległych od już wybranych centroidów, co w sensie teoretycznym daje gwarancje aproksymacyjne rzędu $O(\log K)$ względem optimum oczekiwanego, a w praktyce istotnie poprawia jakość minimum lokalnego.
:::

::: callout-warning

## Warunki stacjonarności otrzymanego rozwiązania

Para $(U^\star,\mu^\star)$ jest punktem stałym algorytmu wtedy i tylko wtedy, gdy spełnia jednocześnie dwa warunki: po pierwsze $\mu_k^\star$ są średnimi swoich klastrów, po drugie przypisania $U^\star$ są zgodne z najbliższymi centroidami $\mu^\star.$ Takie rozwiązanie spełnia warunki optymalności pierwszego rzędu względem naprzemiennych bloków zmiennych i stanowi minimum lokalne funkcji $J$ na zbiorze dopuszczalnych rozwiązań wyznaczonych z ograniczeniami twardych przypisań.
:::

### Metoda k-medoidów (*k-medoids*)

Metoda k-medoid (ang. *k-medoids*) stanowi bliski odpowiednik klasycznej metody *k-means*, lecz wprowadza zasadniczą zmianę w sposobie definiowania reprezentanta klastra. Zamiast centroidu obliczanego jako średnia arytmetyczna wszystkich punktów w danym klastrze, metoda k-medoid wykorzystuje medoid, czyli rzeczywisty punkt ze zbioru danych, który minimalizuje sumę odległości do pozostałych elementów tego samego klastra. Dzięki temu metoda ta jest bardziej odporna na obserwacje odstające oraz umożliwia zastosowanie dowolnej miary odległości, nie tylko euklidesowej.

Niech dany będzie zbiór obserwacji $X = \{x_1, x_2, \ldots, x_n\} \subset \mathbb{R}^p$ oraz liczba klastrów $K$. Celem jest podział zbioru $X$ na $K$ grup w taki sposób, aby suma odległości pomiędzy punktami a reprezentantami ich klastrów była minimalna. Funkcję celu można zapisać jako
$$
J(M, U) = \sum_{k=1}^{K} \sum_{i=1}^{n} u_{ik} \, d(x_i, m_k),
$$
gdzie $M = \{m_1, m_2, \ldots, m_K\} \subset X$ to zbiór medoidów, $U = [u_{ik}]$ to macierz przypisań punktów do klastrów, a $d(x_i, m_k)$ oznacza wybraną miarę odległości. Dla każdego obiektu zachodzi warunek $u_{ik} \in \{0,1\}$ oraz $\sum_{k=1}^{K} u_{ik} = 1$, co oznacza, że każdy punkt należy dokładnie do jednego klastra. Medoid klastra definiuje się jako punkt $m_k \in X$, który minimalizuje sumę odległości do wszystkich pozostałych punktów tego klastra
$$
m_k = \operatorname{argmin}_{x_j \in X_k} \sum_{x_i \in X_k} d(x_i, x_j),
$$
gdzie $X_k = \{x_i : u_{ik} = 1\}$.

W praktyce metoda realizowana jest iteracyjnie, analogicznie do *k-means*, ale z innym sposobem aktualizacji reprezentantów. Najbardziej znanym algorytmem implementującym tę ideę jest PAM (*Partitioning Around Medoids*). Procedura ta obejmuje następujące kroki. Po pierwsze, inicjalizuje się losowo $K$ punktów jako początkowe medoidy. Następnie każdy obiekt przypisywany jest do najbliższego medoidu, zgodnie z regułą
$$
u_{ik} = \mathbf{1}_\left\{\,k = \operatorname{argmin}_{j} d(x_i, m_j)\right\}.
$$
W ten sposób powstaje podział przestrzeni na obszary przypominające komórki Woronoja. W kolejnym kroku, dla każdego klastra wybiera się nowy medoid, czyli punkt, który minimalizuje sumę odległości do pozostałych punktów w tym klastrze. Algorytm powtarza naprzemienne kroki przypisania i aktualizacji aż do momentu, gdy zestaw medoidów przestaje się zmieniać lub wartość funkcji celu stabilizuje się.

Metoda k-medoid jest blisko spokrewniona z metodą k-means, która minimalizuje sumę kwadratów odległości euklidesowych
$$
J_{\text{k-means}} = \sum_{k=1}^{K}\sum_{i=1}^{n} u_{ik}\,\|x_i - \mu_k\|^2,
$$
gdzie $\mu_k$ oznacza centroid klastra. W k-medoid zamiast średniej stosuje się rzeczywisty punkt danych, a w funkcji celu pojawia się bezpośrednia odległość, nie jej kwadrat. W konsekwencji metoda k-means jest szybsza, lecz wrażliwa na wartości odstające i ograniczona do przestrzeni euklidesowych, natomiast k-medoid jest bardziej odporna i umożliwia pracę z dowolnymi macierzami odległości, także nieliczbowymi.

::: callout-warning

Warto odróżnić metodę k-medoid od metody k-median. W k-medoid reprezentantem klastra jest rzeczywisty punkt ze zbioru danych, natomiast w k-median mediana klastra może znajdować się w dowolnym miejscu przestrzeni. Funkcja celu w k-median minimalizuje sumę odległości w sensie L1 (Manhattan)
$$
J_{\text{k-median}} = \sum_{k=1}^{K}\sum_{i=1}^{n} u_{ik} \, \|x_i - m_k\|_1.
$$
Zatem k-median stanowi ciągły odpowiednik metody k-medoid, podobnie jak k-means jest wersją ciągłą dla odległości euklidesowych w kwadracie.
:::

| Cechy                    | k-means                                   | k-median                                   | k-medoid                         |
|:--------------------------|:------------------------------------------|:-------------------------------------------|:----------------------------------|
| Reprezentant              | średnia arytmetyczna (punkt w ℝᵖ)         | mediana geometryczna (punkt w ℝᵖ)          | rzeczywisty punkt danych          |
| Miara odległości          | kwadrat euklidesowej                      | Manhattan (L1)                             | dowolna miara                     |
| Odporność na odstające    | niska                                     | średnia                                    | wysoka                            |
| Typ zmiennych             | ciągłe                       | ciągłe                                     | dowolne (także kategoryczne)      |

### CLARA i CLARANS

Algorytmy CLARA i CLARANS stanowią rozwinięcia metody k-medoid, opracowane w celu rozwiązania problemu wysokiej złożoności obliczeniowej klasycznego algorytmu PAM. Oba podejścia zachowują tę samą ideę — minimalizację sumy odległości punktów do reprezentantów (medoidów) — lecz różnią się strategią poszukiwania najlepszego zbioru medoidów w dużych zbiorach danych.

#### Algorytm CLARA (*Clustering LARge Applications*)

Algorytm CLARA został zaproponowany przez Kaufmana i Rousseeuwa (1990) jako metoda przybliżona dla k-medoids, umożliwiająca efektywne działanie przy dużej liczbie obserwacji. Kluczową ideą CLARA jest ograniczenie pełnych obliczeń do próbek danych, zamiast całego zbioru.

Procedura przebiega w kilku etapach:

1. Losowanie próbki - z całego zbioru danych $X$ losuje się podzbiór $S \subset X$ o umiarkowanej liczności (np. 5–10% wszystkich obserwacji).
2. Zastosowanie algorytmu PAM - na wylosowanej próbce $S$ przeprowadza się pełną procedurę PAM w celu wyznaczenia $K$ medoidów $M_S = \{m_1, \ldots, m_K\}$.
3. Ocena jakości podziału - uzyskane medoidy testuje się na całym zbiorze danych, obliczając wartość funkcji kosztu
$$
J(M_S) = \sum_{i=1}^{n} \min_{m_k \in M_S} d(x_i, m_k),
$$
czyli sumę odległości każdego punktu do najbliższego medoidu.
4. Powtórzenia i wybór najlepszego rozwiązania - proces losowania próbki i przeprowadzania PAM powtarza się kilka razy (np. 5–10), a końcowy wynik wybiera się na podstawie minimalnej wartości funkcji celu $J(M_S)$.

Zaletą CLARA jest znaczne obniżenie kosztów obliczeniowych w porównaniu z PAM, którego złożoność wynosi $O(k(n-k)^2)$. W CLARA złożoność zależy od rozmiaru próbki, a nie całego zbioru, co umożliwia stosowanie metody na dużych danych. Wadą jest jednak możliwość utraty jakości rozwiązania, jeśli próbka nie jest reprezentatywna — w szczególności, jeśli pomija mniejsze skupienia obecne w zbiorze danych.

#### Algorytm CLARANS (*Clustering Large Applications based on RANdomized Search*)

Algorytm CLARANS, opracowany przez Ng i Hana (1994), stanowi dalsze rozwinięcie CLARA i PAM, oparte na losowym przeszukiwaniu przestrzeni możliwych zbiorów medoidów. Jego działanie inspirowane jest technikami heurystycznymi, takimi jak *local search* lub *simulated annealing*. 
CLARANS traktuje przestrzeń wszystkich możliwych zestawów medoidów jako graf, w którym każdy wierzchołek odpowiada pewnemu zestawowi $K$ medoidów, a krawędzie łączą wierzchołki różniące się jednym medoidem. Poszukiwanie najlepszego rozwiązania odbywa się przez losowe przechodzenie po tym grafie, przy czym zmiany medoidów dokonuje się tylko wtedy, gdy poprawiają funkcję celu.

Schemat działania można opisać następująco:

1. Inicjalizacja - losowo wybrać zestaw $K$ medoidów $M$.
2. Losowa eksploracja sąsiedztwa - spośród wszystkich możliwych „zamian” jednego medoidu $m \in M$ na punkt niebędący medoidem $x \in X \setminus M$, losowo wybrać kilka par kandydatów (tzw. *neighbors*).
3. Ocena sąsiadów - dla każdego kandydata obliczyć zmianę funkcji celu
$$
\Delta J = J(M’) - J(M),
$$
gdzie $M’$ to nowy zestaw medoidów po zamianie.
4. Krok optymalizacyjny - jeśli znajdzie się sąsiad z mniejszą wartością funkcji kosztu, przyjąć go jako nowy zestaw medoidów $M \leftarrow M’$.
5. Kontynuacja - powtarzać losowe przeszukiwanie do osiągnięcia lokalnego minimum (brak poprawiających się sąsiadów) lub do wyczerpania limitu iteracji.
6. Powtórzenia - dla zwiększenia szansy znalezienia rozwiązania globalnego, procedurę powtarza się kilka razy z różnymi początkowymi zestawami medoidów.

CLARANS jest więc algorytmem probabilistycznym, który w każdym kroku dokonuje losowej eksploracji przestrzeni możliwych rozwiązań. W przeciwieństwie do CLARA nie ogranicza się do jednej próbki danych, lecz do ograniczonej liczby losowo sprawdzanych sąsiadów, co pozwala zachować kompromis między dokładnością a szybkością.

:::{#exm-2}

```{r}
library(cluster)

set.seed(44)
X <- scale(iris[, 1:4])
d <- dist(X)

# 1. k-means
kmeans_res <- kmeans(X, centers = 3, nstart = 25)
p_kmeans <- fviz_cluster(kmeans_res, data = X, geom = "point", ellipse.type ="norm", main = "k-means")
# 2. k-medoids (PAM)
pam_res <- pam(X, k = 3)
p_pam <- fviz_cluster(pam_res, geom = "point", ellipse.type ="norm", main = "PAM (k-medoids)")
# 3. CLARA
clara_res <- clara(X, k = 3, samples = 5, pamLike= TRUE)
p_clara <- fviz_cluster(clara_res, geom = "point", ellipse.type =
                               "norm", main = "CLARA")
# 4. CLARANS
library(fastkmedoids)
clarans_res <- fastclarans(d, k = 3, n = nrow(X))
p_clarans <- fviz_cluster(list(data = X, cluster = clara_res$clustering), geom = "point", ellipse.type =
                                 "norm", main = "CLARANS")
ggarrange(p_kmeans, p_pam, p_clara, p_clarans, ncol = 2, nrow = 2)
```

:::