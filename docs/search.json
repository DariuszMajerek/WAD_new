[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wielowymiarowa analiza danych",
    "section": "",
    "text": "WstÄ™p\nWielowymiarowa analiza danych stanowi jeden z filarÃ³w wspÃ³Å‚czesnej statystyki i eksploracji danych, oferujÄ…c metody pozwalajÄ…ce zrozumieÄ‡ strukturÄ™ i zaleÅ¼noÅ›ci w zbiorach danych, w ktÃ³rych kaÅ¼da obserwacja jest opisana wieloma zmiennymi jednoczeÅ›nie. W dobie powszechnego dostÄ™pu do danych oraz rosnÄ…cego zapotrzebowania na ich zaawansowanÄ… analizÄ™, umiejÄ™tnoÅ›Ä‡ stosowania metod wielowymiarowych staje siÄ™ nieodzowna zarÃ³wno w badaniach naukowych, jak i w analizie danych stosowanej w przemyÅ›le, finansach, biologii, medycynie czy naukach spoÅ‚ecznych.\nNiniejsza ksiÄ…Å¼ka zostaÅ‚a opracowana z myÅ›lÄ… o dwÃ³ch kierunkach ksztaÅ‚cenia akademickiego: matematyce oraz inÅ¼ynierii i analizie danych. Jej celem jest zapewnienie solidnych podstaw teoretycznych oraz praktycznych umiejÄ™tnoÅ›ci niezbÄ™dnych do stosowania metod wielowymiarowych w rzeczywistych problemach badawczych i aplikacyjnych. Zakres tematyczny ksiÄ…Å¼ki zostaÅ‚ dobrany tak, aby uwzglÄ™dniaÄ‡ zarÃ³wno klasyczne metody statystyczne, jak i techniki wykorzystywane we wspsÃ³Å‚czesnej analizie danych.\nW pierwszej czÄ™Å›ci ksiÄ…Å¼ki omÃ³wione zostanÄ… testy wielowymiarowe, ktÃ³re stanowiÄ… rozszerzenie klasycznych metod statystycznych na przypadki, w ktÃ³rych kaÅ¼da obserwacja opisana jest wieloma zmiennymi. SzczegÃ³lna uwaga zostanie poÅ›wiÄ™cona testowi Hotellinga TÂ², bÄ™dÄ…cemu odpowiednikiem testu t dla wielu zmiennych, oraz analizie wariancji dla wielu zmiennych (MANOVA), pozwalajÄ…cej na badanie rÃ³Å¼nic miÄ™dzy grupami z uwzglÄ™dnieniem wspÃ³Å‚zaleÅ¼noÅ›ci zmiennych. Celem tej czÄ™Å›ci bÄ™dzie zrozumienie podstaw inferencji w przestrzeni wielowymiarowej i interpretacji wynikÃ³w testÃ³w z uwzglÄ™dnieniem macierzy kowariancji.\nNastÄ™pnie przedstawiona zostanie analiza kanoniczna, ktÃ³ra sÅ‚uÅ¼y do badania zaleÅ¼noÅ›ci pomiÄ™dzy dwoma zestawami zmiennych. Czytelnik pozna konstrukcjÄ™ zmiennych kanonicznych, sposoby ich interpretacji oraz znaczenie wag i korelacji kanonicznych. Analiza ta ma kluczowe znaczenie wszÄ™dzie tam, gdzie celem jest znalezienie skorelowanych struktur w dwÃ³ch grupach cech, np. w badaniach biologicznych, spoÅ‚ecznych lub psychometrycznych.\nKolejna czÄ™Å›Ä‡ ksiÄ…Å¼ki bÄ™dzie poÅ›wiÄ™cona analizie czynnikowej (FA), ktÃ³ra umoÅ¼liwia modelowanie wspÃ³Å‚zmiennoÅ›ci zestawu zmiennych za pomocÄ… mniejszej liczby zmiennych ukrytych, zwanych czynnikami. Przedstawione zostanÄ… metody estymacji, kryteria wyboru liczby czynnikÃ³w oraz techniki rotacji, ktÃ³re sÅ‚uÅ¼Ä… lepszej interpretacji wynikÃ³w. Analiza czynnikowa jest czÄ™sto stosowana w badaniach ankietowych i psychometrycznych, ale znajduje rÃ³wnieÅ¼ zastosowanie w analizie danych ekonomicznych i marketingowych.\nW dalszej kolejnoÅ›ci wprowadzony zostanie model Å›cieÅ¼kowy oraz jego uogÃ³lnienie w postaci modeli rÃ³wnaÅ„ strukturalnych (SEM). Modele te pozwalajÄ… na modelowanie zarÃ³wno obserwowalnych, jak i ukrytych zmiennych oraz relacji przyczynowych pomiÄ™dzy nimi. Czytelnik pozna strukturÄ™ modelu Å›cieÅ¼kowego, pojÄ™cie identyfikowalnoÅ›ci, miary dopasowania oraz techniki estymacji parametrÃ³w. Modele SEM sÄ… obecnie szeroko stosowane w naukach spoÅ‚ecznych, biologii, psychologii i ekonomii.\nNastÄ™pnie omÃ³wione zostanÄ… metody redukcji wymiarowoÅ›ci, ktÃ³rych celem jest uproszczenie reprezentacji danych bez utraty istotnej informacji. KluczowÄ… technikÄ… bÄ™dzie analiza skÅ‚adowych gÅ‚Ã³wnych (PCA), ktÃ³ra pozwala na znalezienie nowych osi zmiennoÅ›ci w danych. Kolejno zaprezentowana zostanie analiza niezaleÅ¼nych skÅ‚adowych (ICA), ktÃ³ra poszukuje skÅ‚adnikÃ³w statystycznie niezaleÅ¼nych, co jest szczegÃ³lnie uÅ¼yteczne w analizie sygnaÅ‚Ã³w. Obie metody znajdÄ… zastosowanie zarÃ³wno w przygotowaniu danych, jak i w ich eksploracji.\nKolejna czÄ™Å›Ä‡ ksiÄ…Å¼ki poÅ›wiÄ™cona bÄ™dzie metodom skalowania wielowymiarowego (Multidimensional Scaling, MDS), ktÃ³re umoÅ¼liwiajÄ… odwzorowanie relacji odlegÅ‚oÅ›ciowych pomiÄ™dzy obiektami w przestrzeni o mniejszym wymiarze. Wariant metric zakÅ‚ada zachowanie rzeczywistych wartoÅ›ci odlegÅ‚oÅ›ci, natomiast non-metric koncentruje siÄ™ na porzÄ…dku dystansÃ³w. Metody te pozwalajÄ… uzyskaÄ‡ intuicyjne wizualizacje struktur danych, szczegÃ³lnie przydatne w psychologii, socjologii czy analizie rynku.\nW uzupeÅ‚nieniu do klasycznych technik przedstawione zostanÄ… nieliniowe metody redukcji wymiarowoÅ›ci, takie jak t-distributed Stochastic Neighbor Embedding (t-SNE) oraz Uniform Manifold Approximation and Projection (UMAP). Obie techniki pozwalajÄ… na odwzorowanie skomplikowanych struktur danych w przestrzeniach dwu- lub trÃ³jwymiarowych, zachowujÄ…c lokalne sÄ…siedztwa. ChoÄ‡ sÄ… to metody przede wszystkim eksploracyjne i wizualizacyjne, ich wartoÅ›Ä‡ w analizie duÅ¼ych zbiorÃ³w danych jest trudna do przecenienia.\nNastÄ™pnie przedstawiona zostanie analiza skupieÅ„, ktÃ³rej celem jest odkrywanie naturalnych grup w zbiorze danych. OmÃ³wione zostanÄ… zarÃ³wno metody hierarchiczne, jak i niehierarchiczne, w tym popularna metoda k-Å›rednich. Poruszona zostanie problematyka doboru liczby skupieÅ„ oraz oceny stabilnoÅ›ci i jakoÅ›ci otrzymanych rozwiÄ…zaÅ„. Analiza skupieÅ„ znajduje zastosowanie w segmentacji rynku, biologii molekularnej, diagnostyce medycznej i wielu innych dziedzinach.\nKolejna czÄ™Å›Ä‡ ksiÄ…Å¼ki poÅ›wiÄ™cona bÄ™dzie analizie korespondencji, stosowanej do eksploracji zwiÄ…zkÃ³w pomiÄ™dzy zmiennymi jakoÅ›ciowymi przedstawionymi w postaci tablicy kontyngencji. Przedstawiona zostanie zarÃ³wno analiza korespondencji prosta (dla dwÃ³ch zmiennych), jak i zÅ‚oÅ¼ona (dla wiÄ™cej niÅ¼ dwÃ³ch). OmÃ³wione zostanÄ… interpretacja map percepcyjnych, odwzorowanie profili oraz zwiÄ…zki z metodami takimi jak PCA czy MDS.\nOstatni rozdziaÅ‚ poÅ›wiÄ™cony bÄ™dzie analizie log-liniowej, ktÃ³ra umoÅ¼liwia modelowanie czÄ™stoÅ›ci w tablicach wielodzielczych na podstawie interakcji pomiÄ™dzy zmiennymi kategorycznymi. ZostanÄ… zaprezentowane modele peÅ‚ne i uproszczone, zasady testowania zÅ‚oÅ¼onoÅ›ci modeli oraz interpretacji parametrÃ³w. Analiza log-liniowa jest szczegÃ³lnie przydatna przy badaniu wielowymiarowych zaleÅ¼noÅ›ci miÄ™dzy zmiennymi kategorycznymi w badaniach spoÅ‚ecznych, medycznych oraz w analizie zachowaÅ„ konsumenckich.\nWszystkie metody zostanÄ… zilustrowane przykÅ‚adami praktycznymi, realizowanymi w jÄ™zyku R. Pozwoli to Czytelnikowi nie tylko zrozumieÄ‡ teoretyczne podstawy omawianych technik, ale takÅ¼e nabyÄ‡ umiejÄ™tnoÅ›Ä‡ ich stosowania w praktyce analitycznej.",
    "crumbs": [
      "WstÄ™p"
    ]
  },
  {
    "objectID": "multi_tests.html",
    "href": "multi_tests.html",
    "title": "Testy wielowymiarowe",
    "section": "",
    "text": "Test Hotellinga dla znanej macierzy kowariancji\nW tradycyjnej analizie statystycznej czÄ™sto koncentrujemy siÄ™ na porÃ³wnywaniu grup ze wzglÄ™du na jednÄ… zmiennÄ… â€“ np. porÃ³wnujemy Å›redni wzrost kobiet i mÄ™Å¼czyzn, wykorzystujÄ…c test t-Studenta. Jednak w rzeczywistoÅ›ci badawczej rzadko interesuje nas tylko jedna cecha. PrzykÅ‚adowo, porÃ³wnujÄ…c grupy pacjentÃ³w, moÅ¼emy jednoczeÅ›nie rozwaÅ¼aÄ‡ poziom ciÅ›nienia, cholesterolu i BMI. Albo, analizujÄ…c dane socjologiczne, chcemy porÃ³wnaÄ‡ grupy pod wzglÄ™dem dochodÃ³w, wyksztaÅ‚cenia i poziomu zadowolenia z Å¼ycia.\nUÅ¼ycie wielu testÃ³w jednowymiarowych wydaje siÄ™ kuszÄ…ce â€“ testujemy kaÅ¼dÄ… zmiennÄ… osobno. Jednak prowadzi to do trzech istotnych problemÃ³w:\n\\[\n\\mathbb{P}(\\text{co najmniej jeden bÅ‚Ä…d I rodzaju}) = 1 - (1 - \\alpha)^p = 1 - 0.95^{10} \\approx 0.40\n\\]\nPowyÅ¼sze problemy uzasadniajÄ… potrzebÄ™ stosowania testÃ³w wielowymiarowych â€“ uwzglÄ™dniajÄ…cych strukturÄ™ wspÃ³Å‚zmiennoÅ›ci miÄ™dzy cechami oraz pozwalajÄ…cych na testowanie hipotez dotyczÄ…cych caÅ‚ych wektorÃ³w Å›rednich.\nRozwaÅ¼my prÃ³bkÄ™ \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\ldots, \\boldsymbol{y}_n \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), gdzie \\(\\boldsymbol{\\Sigma}\\) jest znana. Oznacza to, Å¼e mamy do czynienia z ciÄ…giem niezaleÅ¼nych losowych wektorÃ³w \\(\\boldsymbol{y}_i\\), z ktÃ³rych kaÅ¼dy ma ten sam wielowymiarowy rozkÅ‚ad normalny o wymiarze \\(p\\), Å›redniej \\(\\boldsymbol{\\mu}\\) i macierzy kowariancji \\(\\boldsymbol{\\Sigma}\\). KaÅ¼dy wektor \\(\\boldsymbol{y}_i\\) moÅ¼na interpretowaÄ‡ jako punkt w przestrzeni \\(\\mathbb{R}^p\\), opisujÄ…cy \\(p\\) cech (zmiennych) dla jednej obserwacji. Formalnie jest to wektor kolumnowy postaci \\(\\boldsymbol{y}_i = [y_{i1}, y_{i2}, \\ldots, y_{ip}]^\\top\\), gdzie indeks \\(i\\) numeruje jednostki (np. osoby, obiekty pomiaru), a indeksy \\(j = 1, \\ldots, p\\) odpowiadajÄ… poszczegÃ³lnym zmiennym. Wektor ten traktowany jest jako zmienna losowa, poniewaÅ¼ jego wartoÅ›ci sÄ… wynikiem losowego procesu generujÄ…cego dane. RozkÅ‚ad \\(\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) jest wielowymiarowÄ… wersjÄ… rozkÅ‚adu normalnego. Opisuje on sytuacjÄ™, w ktÃ³rej kaÅ¼da kombinacja liniowa zmiennych losowych w \\(\\boldsymbol{y}_i\\) rÃ³wnieÅ¼ ma rozkÅ‚ad normalny, a funkcja gÄ™stoÅ›ci prawdopodobieÅ„stwa ma postaÄ‡ zaleÅ¼nÄ… od wartoÅ›ci wektora Å›rednich oraz struktury kowariancji. Jest to fundamentalne zaÅ‚oÅ¼enie w klasycznej analizie statystycznej, pozwalajÄ…ce na stosowanie wielu narzÄ™dzi statystycznych.\nParametr \\(\\boldsymbol{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_p]^\\top\\) to wektor wartoÅ›ci oczekiwanych kaÅ¼dej z analizowanych zmiennych. Oznacza on przeciÄ™tny poziom zmiennej \\(y_{ij}\\) w populacji dla kaÅ¼dej cechy \\(j\\). Jest to parametr istotny z punktu widzenia testowania hipotez, poniewaÅ¼ wiele testÃ³w statystycznych dotyczy wÅ‚aÅ›nie rÃ³wnoÅ›ci lub rÃ³Å¼nic wektorÃ³w Å›rednich miÄ™dzy grupami.\nZ kolei macierz \\(\\boldsymbol{\\Sigma}\\) to dodatnio okreÅ›lona, symetryczna macierz kowariancji o wymiarach \\(p \\times p\\). Jej elementy \\(\\sigma_{jj}\\) opisujÄ… wariancje poszczegÃ³lnych zmiennych, natomiast elementy poza gÅ‚Ã³wnÄ… przekÄ…tnÄ… \\(\\sigma_{jk}\\) (dla \\(j \\ne k\\)) opisujÄ… kowariancje, czyli wspÃ³Å‚zmiennoÅ›Ä‡ pomiÄ™dzy zmiennymi \\(y_{ij}\\) i \\(y_{ik}\\). W analizie wielowymiarowej uwzglÄ™dnienie tych zaleÅ¼noÅ›ci miÄ™dzy cechami jest kluczowe, poniewaÅ¼ pozwala lepiej zrozumieÄ‡ strukturÄ™ danych i dokonywaÄ‡ bardziej trafnych wnioskÃ³w statystycznych.\nO prÃ³bie \\(\\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n\\) zakÅ‚adamy, Å¼e wszystkie obserwacje sÄ… niezaleÅ¼ne oraz pochodzÄ… z tego samego rozkÅ‚adu. Oznacza to, Å¼e mamy do czynienia z prÃ³bÄ… losowÄ…, niezaleÅ¼nÄ… o identycznych rozkÅ‚adach (i.i.d.), co jest podstawowym zaÅ‚oÅ¼eniem wielu testÃ³w i metod estymacji. CaÅ‚Ä… prÃ³bkÄ™ moÅ¼na przedstawiÄ‡ jako macierz danych o wymiarach \\(n \\times p\\), w ktÃ³rej wiersze odpowiadajÄ… jednostkom, a kolumny cechom.\nW przypadku, gdy macierz kowariancji \\(\\boldsymbol{\\Sigma}\\) jest znana, moÅ¼emy zastosowaÄ‡ uproszczone wersje testÃ³w statystycznych, takie jak klasyczny test Hotellinga \\(T^2\\) dla jednej prÃ³by. Jest to jednak sytuacja czysto teoretyczna, poniewaÅ¼ w praktyce \\(\\boldsymbol{\\Sigma}\\) musi byÄ‡ zazwyczaj estymowana na podstawie danych. Pomimo tego, przypadek znanej macierzy jest uÅ¼yteczny do budowania intuicji, zrozumienia rÃ³l poszczegÃ³lnych parametrÃ³w i wyprowadzania wÅ‚asnoÅ›ci statystyk testowych.\nChcemy przetestowaÄ‡ hipotezÄ™:\n\\[\nH_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu} \\neq \\boldsymbol{\\mu}_0\n\\]\nStatystyka testowa opiera siÄ™ na uogÃ³lnionej odlegÅ‚oÅ›ci Mahalanobisa:\n\\[\nT^2 = n (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Sigma}^{-1} (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)\n\\]\nPod warunkiem speÅ‚nienia \\(H_0\\), statystyka \\(T^2 \\sim \\chi^2_p\\). Zatem moÅ¼emy porÃ³wnaÄ‡ wartoÅ›Ä‡ \\(T^2\\) z odpowiednim kwantylem rozkÅ‚adu chi-kwadrat.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#zaÅ‚oÅ¼enia",
    "href": "multi_tests.html#zaÅ‚oÅ¼enia",
    "title": "Testy wielowymiarowe",
    "section": "ZaÅ‚oÅ¼enia",
    "text": "ZaÅ‚oÅ¼enia\n\nPrÃ³by sÄ… niezaleÅ¼ne;\nObserwacje w kaÅ¼dej grupie pochodzÄ… z rozkÅ‚adu wielowymiarowego normalnego;\n\nMacierze kowariancji sÄ… rÃ³wne \\(\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}\\). Jest to kluczowe zaÅ‚oÅ¼enie umoÅ¼liwiajÄ…ce zbudowanie wspÃ³lnego estymatora kowariancji i zastosowanie rozkÅ‚adu \\(T^2\\) Hotellinga. ChoÄ‡ moÅ¼e byÄ‡ ono naruszone w praktyce, to dla duÅ¼ych prÃ³b test zachowuje swoje wÅ‚aÅ›ciwoÅ›ci asymptotyczne (Rencher 1998).\n\nWektory Å›rednich z prÃ³by wyraÅ¼amy jako:\n\\[\n\\bar{\\boldsymbol{y}}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} \\boldsymbol{y}_{1,i}, \\quad\n\\bar{\\boldsymbol{y}}_2 = \\frac{1}{n_2} \\sum_{i=1}^{n_2} \\boldsymbol{y}_{2,i}\n\\]\nNieobciÄ…Å¼onym estymatorem macierzy kowariancji (\\(\\boldsymbol{\\Sigma}\\)) jest tzw. poÅ‚Ä…czony estymator kowariancji:\n\\[\n\\mathbf{S} =\n\\frac{(n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2}{n_1 + n_2 - 2}\n\\] gdzie \\[\n\\mathbf{S}_1 = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top\n\\]\n\\[\n\\mathbf{S}_2 = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top\n\\]\nZatem:\n\\[\n\\mathbf{S} = \\frac{\\mathbf{W}_1 + \\mathbf{W}_2}{n_1 + n_2 - 2}\n\\] gdzie: \\[\n\\mathbf{W}_1 = \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top = (n_1 - 1)\\mathbf{S}_1\n\\]\n\\[\n\\mathbf{W}_2 = \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top = (n_2 - 1)\\mathbf{S}_2\n\\] Statystyka testowa Hotellinga wÃ³wczas ma postaÄ‡:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)\n\\]\nA gdy \\(H_0\\) jest prawdziwa, to po przeksztaÅ‚ceniu: \\[\nF = \\frac{(n_1 + n_2 - p - 1)}{p(n_1 + n_2 - 2)} T^2 \\sim F_{p, n_1 + n_2 - p - 1}\n\\]\nAlternatywnie, moÅ¼na zapisaÄ‡, Å¼e: \\[\nT^2 \\sim \\frac{p(n_1 + n_2 - 2)}{n_1 + n_2 - p - 1} F_{p, n_1 + n_2 - p - 1}\n\\]\nHipotezÄ™ zerowÄ… \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) odrzucamy na poziomie istotnoÅ›ci \\(\\alpha\\), jeÅ›li: \\[\nT^2 &gt; T^2_{\\alpha, p, n_1 + n_2 - 2}\n\\] lub rÃ³wnowaÅ¼nie: \\[\nF &gt; F_{\\alpha, p, n_1 + n_2 - p - 1}\n\\]\nAby test byÅ‚ moÅ¼liwy do przeprowadzenia, konieczne jest, aby \\(n_1 + n_2 - 2 &gt; p\\), czyli liczba stopni swobody w estymacji wspÃ³lnej kowariancji byÅ‚a wiÄ™ksza niÅ¼ wymiar przestrzeni cech.\n\n\n\n\n\n\nAdnotacja\n\n\n\nW praktyce istotne jest, aby przed zastosowaniem testu \\(T^2\\) Hotellinga dla dwÃ³ch prÃ³b zweryfikowaÄ‡ zaÅ‚oÅ¼enie o rÃ³wnoÅ›ci macierzy kowariancji â€” np. za pomocÄ… testu Boxa. Test M Boxa (ang. Boxâ€™s M test) sÅ‚uÅ¼y do statystycznej weryfikacji hipotezy rÃ³wnoÅ›ci macierzy kowariancji w wielu grupach.\nZaÅ‚Ã³Å¼my, Å¼e mamy \\(G\\) niezaleÅ¼nych prÃ³b z wielowymiarowego rozkÅ‚adu normalnego:\n\\[\n\\boldsymbol{y}_{g} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_g, \\boldsymbol{\\Sigma}_g), \\quad g = 1, \\ldots, G\n\\] Testujemy hipotezÄ™: \\[\nH_0: \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\ldots = \\boldsymbol{\\Sigma}_G = \\boldsymbol{\\Sigma}\n\\] przeciwko alternatywie: \\[\nH_1: \\exists\\, g, h: \\boldsymbol{\\Sigma}_g \\ne \\boldsymbol{\\Sigma}_h\n\\] Niech\n\n\n\\(\\mathbf{S}_g\\) â€“ macierz kowariancji w grupie \\(g\\),\n\n\\(n_g\\) â€“ liczba obserwacji w grupie \\(g\\),\n\n\\(\\mathbf{S}_p\\) â€“ poÅ‚Ä…czony estymator macierzy kowariancji:\n\n\\[\n\\mathbf{S}_p = \\frac{1}{N - G} \\sum_{g=1}^G (n_g - 1)\\mathbf{S}_g\n\\] gdzie \\(N = \\sum_{g=1}^G n_g\\) â€“ Å‚Ä…czna liczba obserwacji. WÃ³wczas, statystyka testowa Boxa ma postaÄ‡: \\[\nM = (N - G) \\cdot \\ln|\\mathbf{S}_p| - \\sum_{g=1}^G (n_g - 1) \\cdot \\ln|\\mathbf{S}_g|\n\\] Poprawka na skoÅ„czonÄ… prÃ³bkÄ™ prowadzi do statystyki: \\[\nC = \\left(1 - c\\right) \\cdot M\n\\] gdzie: \\[\nc = \\frac{1}{3(p + 1)(G - 1)} \\left[ \\sum_{g=1}^G \\frac{1}{n_g - 1} - \\frac{1}{N - G} \\right]\n\\] Statystyka \\(C\\) jest asymptotycznie zbierzna do rozkÅ‚adu \\(\\chi^2\\left(\\frac{p}{2}(p + 1)(G - 1)\\right)\\) liczbÄ… stopni swobody.\nHipotezÄ™ \\(H_0\\) o rÃ³wnoÅ›ci macierzy kowariancji odrzuca siÄ™, jeÅ›li: \\[\nC &gt; \\chi^2_{1 - \\alpha, df}\n\\] lub gdy \\(p\\) testu jest mniejsza od poziomu istotnoÅ›ci \\(\\alpha\\).\nUwagi praktyczne\n\nTest M Boxa jest wraÅ¼liwy na odchylenia od normalnoÅ›ci â€“ jeÅ›li dane nie sÄ… zbliÅ¼one do normalnych, test moÅ¼e dawaÄ‡ mylÄ…ce wyniki.\nW duÅ¼ych prÃ³bach nawet drobne rÃ³Å¼nice miÄ™dzy macierzami kowariancji mogÄ… prowadziÄ‡ do odrzucenia \\(H_0\\), choÄ‡ nie majÄ… istotnego wpÅ‚ywu praktycznego.\nW maÅ‚ych prÃ³bach test moÅ¼e byÄ‡ niestabilny â€“ zaleca siÄ™ ostroÅ¼noÅ›Ä‡ przy interpretacji.\n\n\n\n\nPrzykÅ‚ad 4.1 (PorÃ³wnanie dwÃ³ch grup za pomocÄ… testu \\(T^2\\) Hotellinga) Â \n\nKodlibrary(MASS)\n# Parametry symulacji\nset.seed(44)\np &lt;- 2          # liczba zmiennych\nn1 &lt;- 30        # liczba obserwacji w grupie 1\nn2 &lt;- 35        # liczba obserwacji w grupie 2\n\n# Parametry rozkÅ‚adu\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1, 1)\nSigma &lt;- matrix(c(1, 0.5,\n                  0.5, 1), nrow = 2)\n\n# Generowanie danych\nY1 &lt;- mvrnorm(n1, mu = mu1, Sigma = Sigma)\nY2 &lt;- mvrnorm(n2, mu = mu2, Sigma = Sigma)\n\n# Åšrednie z prÃ³by\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymatory kowariancji\nS1 &lt;- cov(Y1)\nS2 &lt;- cov(Y2)\n\n# WspÃ³lna kowariancja (poÅ‚Ä…czona)\nSp &lt;- ((n1 - 1)*S1 + (n2 - 1)*S2) / (n1 + n2 - 2)\n\n# Statystyka testowa Hotellinga T^2\ndiff_mean &lt;- y1_bar - y2_bar\nT2 &lt;- (n1 * n2) / (n1 + n2) * t(diff_mean) %*% solve(Sp) %*% diff_mean\nT2 &lt;- as.numeric(T2)\n\n# PrzeksztaÅ‚cenie do F\ndf1 &lt;- p\ndf2 &lt;- n1 + n2 - p - 1\nF_stat &lt;- (df2 / (df1 * (n1 + n2 - 2))) * T2\n\n# WartoÅ›Ä‡ krytyczna\nalpha &lt;- 0.05\nF_crit &lt;- qf(1 - alpha, df1, df2)\n\n# p-wartoÅ›Ä‡\np_val &lt;- 1 - pf(F_stat, df1, df2)\n\n# Wynik testu\nsprintf(\"Statystyka TÂ² = %.3f\", T2)\n\n[1] \"Statystyka TÂ² = 17.340\"\n\nKodsprintf(\"Statystyka F = %.3f\", F_stat)\n\n[1] \"Statystyka F = 8.532\"\n\nKodsprintf(\"WartoÅ›Ä‡ krytyczna F = %.3f\", F_crit)\n\n[1] \"WartoÅ›Ä‡ krytyczna F = 3.145\"\n\nKodsprintf(\"p-value = %e\", p_val)\n\n[1] \"p-value = 5.330310e-04\"\n\nKod# Wizualizacja \ndf1 &lt;- as.data.frame(Y1) %&gt;%\n  mutate(grupa = \"Grupa 1\")\n\ndf2 &lt;- as.data.frame(Y2) %&gt;%\n  mutate(grupa = \"Grupa 2\")\n\ndf_all &lt;- bind_rows(df1, df2)\ncolnames(df_all)[1:2] &lt;- c(\"X1\", \"X2\")\n\n# Ramka danych ze Å›rednimi\nmeans &lt;- data.frame(\n  X1 = c(y1_bar[1], y2_bar[1]),\n  X2 = c(y1_bar[2], y2_bar[2]),\n  grupa = c(\"Grupa 1\", \"Grupa 2\")\n)\n\n# Wykres\nggplot(df_all, aes(x = X1, y = X2, color = grupa, shape = grupa)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_point(data = means, aes(x = X1, y = X2),\n             shape = c(1, 2), size = 5, stroke = 1.2, show.legend = FALSE) +\n  scale_shape_manual(values = c(16, 17)) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  coord_equal() +\n  theme_minimal() +\n  labs(title = \"PorÃ³wnanie dwÃ³ch grup\",\n       x = \"X1\", y = \"X2\", color = \"Grupa\", shape = \"Grupa\")\n\n\n\n\n\n\n\n\nKod# Alternatywnie, uÅ¼ycie gotowej funkcji z pakietu ICSNP\nlibrary(ICSNP)\nHotellingsT2(rbind(Y1, Y2)~factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Hotelling's two sample T2-test\n\ndata:  rbind(Y1, Y2) by factor(c(rep(1, n1), rep(2, n2)))\nT.2 = 8.5321, df1 = 2, df2 = 62, p-value = 0.000533\nalternative hypothesis: true location difference is not equal to c(0,0)\n\n\nW analizowanym przykÅ‚adzie zdefiniowaliÅ›my macierze kowariancji identycznie ale w rzeczywistoÅ›ci naleÅ¼aÅ‚oby testowaÄ‡ hipotezÄ™ o rÃ³wnoÅ›ci macierzy kowariancji. Tylko dla celÃ³w Ä‡wiczeniowych pokaÅ¼Ä™ jak to zrobiÄ‡.\n\nKod# Test Boxa na rÃ³wnoÅ›Ä‡ macierzy kowariancji\nlibrary(biotools)\nboxM(rbind(Y1, Y2), factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  rbind(Y1, Y2)\nChi-Sq (approx.) = 2.1261, df = 3, p-value = 0.5466\n\nKod# lub z wykorzystaniem pakietu rstatix\nlibrary(rstatix)\nbox_m(df_all[,-3], df_all[,3])\n\n# A tibble: 1 Ã— 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      2.13   0.547         3 Box's M-test for Homogeneity of Covariance Matricâ€¦\n\n\n\n\n\n\n\n\n\nWskazÃ³wka\n\n\n\nW sytuacji, gdy zaÅ‚oÅ¼enie o rÃ³wnoÅ›ci macierzy kowariancji jest naruszone, moÅ¼na stosowaÄ‡ alternatywne metody, takie jak:\n\nTesty permutacyjne (Hotelling::hotelling.test(Y~Group, perm = TRUE, B = 5000)).\nUogÃ³lniony test Hotellinga - test Jamesa (ang. Jamesâ€™s second-order test) lub czasami nazywany rÃ³wnieÅ¼ testem Welch-type Hotelling test (Hotelling::hotelling.test(Y~Group, var.equal = FALSE)).\nW przypadku danych charakteryzujÄ…cych siÄ™ duÅ¼Ä… liczbÄ… zmiennych w stosunku do liczby obserwacji, moÅ¼na rozwaÅ¼yÄ‡ uÅ¼ycie estymatora Jamesa-Steina do stabilizaji macierzy kowariancji (Hotelling::hotelling.test(Y~Group, shrinkage = TRUE)).\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nOdrzucenie hipotezy zerowej \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) w teÅ›cie Hotellinga \\(T^2\\) oznacza, Å¼e mamy statystycznie istotny dowÃ³d na to, iÅ¼ rozkÅ‚ady Å›rednich wektorÃ³w dwÃ³ch populacji wielowymiarowych rÃ³Å¼niÄ… siÄ™, biorÄ…c pod uwagÄ™ wspÃ³Å‚zmiennoÅ›Ä‡ miÄ™dzy zmiennymi. W kontekÅ›cie zastosowaÅ„ praktycznych, oznacza to, Å¼e przynajmniej jedna zmienna (lub kombinacja zmiennych) odrÃ³Å¼nia grupy, nawet jeÅ›li nie da siÄ™ tego wykazaÄ‡ za pomocÄ… testÃ³w jednowymiarowych.\nW sytuacji, gdy mamy dane wielowymiarowe \\(\\boldsymbol{y}_{gi} \\in \\mathbb{R}^p\\) z dwÃ³ch niezaleÅ¼nych grup (\\(g = 1,2\\)), testujemy:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu}_1 \\neq \\boldsymbol{\\mu}_2.\n\\]\nOdrzucenie \\(H_0\\) sugeruje, Å¼e istnieje rÃ³Å¼nica pomiÄ™dzy Å›rednimi wektorami, ale nie musi oznaczaÄ‡, Å¼e ktÃ³rakolwiek ze Å›rednich poszczegÃ³lnych zmiennych \\(\\mu_{1j}, \\mu_{2j}\\) rÃ³Å¼ni siÄ™ istotnie â€” zwÅ‚aszcza jeÅ›li uwzglÄ™dnimy korelacje miÄ™dzy zmiennymi.\nTesty jednowymiarowe ignorujÄ… te wspÃ³Å‚zaleÅ¼noÅ›ci, dlatego mogÄ… nie wykazaÄ‡ istotnych rÃ³Å¼nic, mimo Å¼e ogÃ³lny profil wielowymiarowy siÄ™ rÃ³Å¼ni. Innymi sÅ‚owy, moÅ¼e nie istnieÄ‡ Å¼adna istotna rÃ³Å¼nica w poszczegÃ³lnych zmiennych, ale pewna kombinacja liniowa tych zmiennych pozwala na rozrÃ³Å¼nienie grup.\nRozwaÅ¼my kombinacjÄ™ liniowÄ…:\n\\[\nz = \\boldsymbol{a}^\\top \\boldsymbol{y},\n\\]\ngdzie \\(\\boldsymbol{a} \\in \\mathbb{R}^p\\) to niezerowy wektor wag. WÃ³wczas \\(z\\) jest jednowymiarowÄ… zmiennÄ… losowÄ… bÄ™dÄ…cÄ… projekcjÄ… obserwacji \\(\\boldsymbol{y}\\) na kierunek \\(\\boldsymbol{a}\\).\nJeÅ›li hipoteza \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) jest faÅ‚szywa, to istnieje taki wektor \\(\\boldsymbol{a}\\), dla ktÃ³rego:\n\\[\nH_0: \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_1 = \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_2\n\\]\nzostanie odrzucona w teÅ›cie jednowymiarowym. Dla takiej kombinacji liniowej moÅ¼emy zdefiniowaÄ‡ statystykÄ™ \\(t\\)-Studenta:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\bar{z}_1 - \\bar{z}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) s_z^2}},\n\\]\ngdzie:\n\n\n\\(\\bar{z}_g = \\boldsymbol{a}^\\top \\bar{\\boldsymbol{y}}_g\\) â€“ Å›rednia z projekcji grupy \\(g\\),\n\n\\(s_z^2\\) â€“ nieobciÄ…Å¼ony estymator wariancji \\(z\\), czyli:\n\n\\[\ns_z^2 = \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a},\n\\]\na \\(\\mathbf{S}\\) to wspÃ³lna macierz kowariancji.\nStÄ…d peÅ‚na postaÄ‡ statystyki:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\boldsymbol{a}^\\top (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a}}}.\n\\]\nPoniewaÅ¼ statystyka \\(t(\\boldsymbol{a})\\) moÅ¼e byÄ‡ zarÃ³wno dodatnia, jak i ujemna, stosuje siÄ™ czÄ™sto jej kwadrat jako miarÄ™ istotnoÅ›ci:\n\\[\nT^2 = t^2(\\boldsymbol{a}).\n\\]\nStatystyka Hotellinga \\(T^2\\) przyjmuje postaÄ‡:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nJest to forma uogÃ³lnionej odlegÅ‚oÅ›ci Mahalanobisa miÄ™dzy Å›rednimi wektorami. MoÅ¼na pokazaÄ‡, Å¼e istnieje taki wektor \\(\\boldsymbol{a}\\), ktÃ³ry maksymalizuje rÃ³Å¼nicÄ™ \\(t(\\boldsymbol{a})\\) â€” to tzw. funkcja dyskryminacyjna:\n\\[\n\\boldsymbol{a} = \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nDla tej wartoÅ›ci wektora \\(\\boldsymbol{a}\\), statystyka \\(T^2\\) przyjmuje najwiÄ™kszÄ… wartoÅ›Ä‡ i jest najbardziej czuÅ‚a na rÃ³Å¼nice miÄ™dzy grupami.\nOdrzucenie \\(H_0\\) oznacza wiÄ™c, Å¼e w przestrzeni \\(\\mathbb{R}^p\\) istnieje kierunek \\(\\boldsymbol{a}\\), dla ktÃ³rego grupy majÄ… rÃ³Å¼ne Å›rednie projekcje. To otwiera drogÄ™ do:\n\nkonstrukcji funkcji dyskryminacyjnych (jak w analizie dyskryminacyjnej),\nidentyfikacji zmiennych lub ich kombinacji odpowiedzialnych za rÃ³Å¼nicÄ™,\ndalszych analiz jednowymiarowych dla projekcji \\(z = \\boldsymbol{a}^\\top \\boldsymbol{y}\\).\n\n\nKodlibrary(gridExtra)  # dla strzaÅ‚ki jako warstwy\n\nset.seed(42)\n\n# Parametry\nn1 &lt;- n2 &lt;- 100\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1.5, 0.5)\nSigma &lt;- matrix(c(1, 0.8, 0.8, 1), ncol = 2)\n\n# Dane\nY1 &lt;- mvrnorm(n1, mu1, Sigma)\nY2 &lt;- mvrnorm(n2, mu2, Sigma)\n\n# Åšrednie\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymacja wspÃ³lnej kowariancji\nS_pooled &lt;- ((n1 - 1) * cov(Y1) + (n2 - 1) * cov(Y2)) / (n1 + n2 - 2)\n\n# Kierunek dyskryminacyjny a\ndiff &lt;- y1_bar - y2_bar\na &lt;- solve(S_pooled, diff)\na_norm &lt;- a / sqrt(sum(a^2))  # normalizacja\n\n# Punkt startowy strzaÅ‚ki (Å›rodek miÄ™dzy Å›rednimi)\norigin &lt;- (y1_bar + y2_bar) / 2\nscale &lt;- 3  # dÅ‚ugoÅ›Ä‡ strzaÅ‚ki\narrow_end &lt;- origin + scale * a_norm\n\n# Ramka danych do wykresu\ndf &lt;- rbind(\n  data.frame(X1 = Y1[,1], X2 = Y1[,2], Grupa = \"Grupa 1\"),\n  data.frame(X1 = Y2[,1], X2 = Y2[,2], Grupa = \"Grupa 2\")\n)\n\n# Wektory Å›rednich i strzaÅ‚ka\nmeans_df &lt;- data.frame(rbind(y1_bar, y2_bar))\ncolnames(means_df) &lt;- c(\"X1\", \"X2\")\nmeans_df$Grupa &lt;- c(\"Grupa 1\", \"Grupa 2\")  # te same etykiety co w danych punktÃ³w\n\narrow_df &lt;- data.frame(\n  x = origin[1],\n  y = origin[2],\n  xend = arrow_end[1],\n  yend = arrow_end[2]\n)\n\n# Wykres\nggplot(df, aes(x = X1, y = X2, color = Grupa)) +\n  geom_point(alpha = 0.5) +\n  geom_point(data = means_df, aes(x = X1, y = X2),\n             size = 4, shape = 16, show.legend = FALSE) +  # Å›rednie tym samym kolorem; bez legendy\n  geom_segment(data = arrow_df, \n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")), color = \"black\", size = 1, show.legend = FALSE) +\n  labs(\n    title = latex2exp::TeX(r\"(Ilustracja kierunku dyskryminacyjnego $a = S^{-1} (\\bar{y}_1 - \\bar{y}_2)$)\"),\n    x = latex2exp::TeX(r\"($X_1$)\"),\n    y = latex2exp::TeX(r\"($X_2$)\")\n  ) +\n  coord_equal() +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#zaÅ‚oÅ¼enia-i-testowane-hipotezy",
    "href": "multi_tests.html#zaÅ‚oÅ¼enia-i-testowane-hipotezy",
    "title": "Testy wielowymiarowe",
    "section": "ZaÅ‚oÅ¼enia i testowane hipotezy",
    "text": "ZaÅ‚oÅ¼enia i testowane hipotezy\nZakÅ‚ada siÄ™, Å¼e obserwacje sÄ… niezaleÅ¼ne i pochodzÄ… z wielowymiarowego rozkÅ‚adu normalnego w kaÅ¼dej grupie, tj.:\n\\[\n\\boldsymbol{y}_{ij} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}), \\quad i = 1, \\dots, g,\\ j = 1, \\dots, n_i,\n\\]\ngdzie:\n\n\n\\(\\boldsymbol{y}_{ij} \\in \\mathbb{R}^p\\) â€” wektor obserwacji w grupie \\(i\\),\n\n\\(\\boldsymbol{\\mu}_i\\) â€” wektor Å›rednich dla grupy \\(i\\),\n\n\\(\\boldsymbol{\\Sigma}\\) â€” wspÃ³lna macierz kowariancji we wszystkich grupach (zaÅ‚oÅ¼enie homogenicznoÅ›ci).\n\nTestowana jest hipoteza zerowa:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\dots = \\boldsymbol{\\mu}_g\n\\]\nwobec alternatywy:\n\\[\nH_1: \\exists\\ i,j\\ \\text{takie, Å¼e}\\ \\boldsymbol{\\mu}_i \\ne \\boldsymbol{\\mu}_j.\n\\]\nModel MANOVA opiera siÄ™ na kilku fundamentalnych zaÅ‚oÅ¼eniach dotyczÄ…cych danych, ktÃ³rych speÅ‚nienie warunkuje poprawnoÅ›Ä‡ i wiarygodnoÅ›Ä‡ uzyskanych wynikÃ³w. Ich naruszenie moÅ¼e prowadziÄ‡ do faÅ‚szywych wnioskÃ³w, zbyt duÅ¼ej liczby odrzuceÅ„ hipotezy zerowej lub do bÅ‚Ä™dnych ocen efektÃ³w czynnikÃ³w. PoniÅ¼ej przedstawiono szczegÃ³Å‚owo kaÅ¼de z tych zaÅ‚oÅ¼eÅ„.\n\nPierwszym kluczowym zaÅ‚oÅ¼eniem jest odpowiednia wielkoÅ›Ä‡ prÃ³by. Przyjmuje siÄ™, Å¼e liczba obserwacji w kaÅ¼dej grupie (komÃ³rce) powinna przekraczaÄ‡ liczbÄ™ zmiennych zaleÅ¼nych, ktÃ³re sÄ… jednoczeÅ›nie analizowane. To praktyczne zalecenie pozwala uniknÄ…Ä‡ problemÃ³w z oszacowaniem macierzy kowariancji i zapewnia dostatecznÄ… moc statystycznÄ….\nKolejnym istotnym zaÅ‚oÅ¼eniem jest niezaleÅ¼noÅ›Ä‡ obserwacji. Oznacza to, Å¼e kaÅ¼da jednostka obserwacyjna (np. osoba) powinna przynaleÅ¼eÄ‡ wyÅ‚Ä…cznie do jednej grupy. Obserwacje wewnÄ…trz i pomiÄ™dzy grupami nie mogÄ… byÄ‡ ze sobÄ… powiÄ…zane. W szczegÃ³lnoÅ›ci, model MANOVA nie jest odpowiedni dla danych z pomiarami powtarzanymi u tych samych obiektÃ³w. DobÃ³r prÃ³by powinien byÄ‡ dokonany w sposÃ³b losowy, bez systematycznych zaleÅ¼noÅ›ci.\nTrzecim wymogiem jest brak obserwacji odstajÄ…cych, zarÃ³wno w sensie jednowymiarowym (dla kaÅ¼dej zmiennej z osobna), jak i wielowymiarowym (dla kombinacji wszystkich zmiennych zaleÅ¼nych). Obserwacje odstajÄ…ce mogÄ… silnie znieksztaÅ‚caÄ‡ wartoÅ›ci Å›rednich i macierzy kowariancji, przez co wyniki MANOVA stajÄ… siÄ™ niestabilne.\nFundamentalnym zaÅ‚oÅ¼eniem MANOVA jest wielowymiarowa normalnoÅ›Ä‡ rozkÅ‚adu danych w kaÅ¼dej z grup. Oznacza to, Å¼e wektor zmiennych zaleÅ¼nych w kaÅ¼dej grupie powinien mieÄ‡ rozkÅ‚ad wielowymiarowy normalny. W R moÅ¼na zastosowaÄ‡ funkcjÄ™ mshapiro_test() z pakietu rstatix, aby przeprowadziÄ‡ test Shapiroâ€“Wilka dla sprawdzenia normalnoÅ›ci wielowymiarowej.\nKolejne zaÅ‚oÅ¼enie dotyczy braku wspÃ³Å‚liniowoÅ›ci. Oczekuje siÄ™, Å¼e zmienne zaleÅ¼ne bÄ™dÄ… ze sobÄ… skorelowane w umiarkowany sposÃ³b, ale nie nadmiernie. WartoÅ›ci wspÃ³Å‚czynnikÃ³w korelacji przekraczajÄ…ce \\(r = 0,90\\) sÄ… uznawane za niepoÅ¼Ä…dane i mogÄ… powodowaÄ‡ problemy numeryczne oraz bÅ‚Ä™dnÄ… interpretacjÄ™ wynikÃ³w. Jak podajÄ… Tabachnick i Fidell (2012), zmienne powinny wnosiÄ‡ unikalne informacje do modelu.\nWaÅ¼nym wymogiem jest rÃ³wnieÅ¼ liniowoÅ›Ä‡ zaleÅ¼noÅ›ci miÄ™dzy zmiennymi zaleÅ¼nymi w kaÅ¼dej grupie. Oznacza to, Å¼e zaleÅ¼noÅ›ci pomiÄ™dzy kaÅ¼dÄ… parÄ… zmiennych muszÄ… byÄ‡ dobrze opisane przez funkcjÄ™ liniowÄ… â€” jest to konieczne, aby poprawnie oszacowaÄ‡ strukturÄ™ kowariancji.\nDla poprawnego dziaÅ‚ania MANOVA zakÅ‚ada siÄ™ takÅ¼e jednorodnoÅ›Ä‡ wariancji dla kaÅ¼dej zmiennej zaleÅ¼nej miÄ™dzy grupami. MoÅ¼na to testowaÄ‡ za pomocÄ… testu Leveneâ€™a. Nieistotny wynik testu Leveneâ€™a sugeruje, Å¼e wariancje sÄ… porÃ³wnywalne w grupach.\nOstatnie, ale bardzo istotne, jest zaÅ‚oÅ¼enie o jednorodnoÅ›ci macierzy kowariancji (homogenicznoÅ›ci macierzy wariancjiâ€“kowariancji) pomiÄ™dzy grupami. Oznacza to, Å¼e struktura wspÃ³Å‚zaleÅ¼noÅ›ci miÄ™dzy zmiennymi powinna byÄ‡ podobna w kaÅ¼dej grupie. WeryfikacjÄ™ tego zaÅ‚oÅ¼enia umoÅ¼liwia test Boxa (Boxâ€™s M test), ktÃ³ry stanowi wielowymiarowy odpowiednik testu Leveneâ€™a. Ze wzglÄ™du na duÅ¼Ä… czuÅ‚oÅ›Ä‡ testu Boxa na odstÄ™pstwa od zaÅ‚oÅ¼eÅ„, przyjmuje siÄ™ konserwatywny poziom istotnoÅ›ci \\(\\alpha = 0,001\\) dla weryfikacji jego wyniku.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "href": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "title": "Testy wielowymiarowe",
    "section": "Konstrukcja modelu i statystyki testowe",
    "text": "Konstrukcja modelu i statystyki testowe\nPodobnie jak w jednowymiarowym przypadku, w MANOVA analizuje siÄ™ rozkÅ‚ad wariancji caÅ‚kowitej na wariancjÄ™ miÄ™dzygrupowÄ… i wewnÄ…trzgrupowÄ…, ale w postaci macierzy kowariancji:\n\nMacierz wariancji miÄ™dzygrupowej (ang. between-group SSCP):\n\n\\[\n\\mathbf{B} = \\sum_{i=1}^{g} n_i (\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})(\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})^\\top\n\\]\n\nMacierz wariancji wewnÄ…trzgrupowej (ang. within-group SSCP):\n\n\\[\n\\mathbf{W} = \\sum_{i=1}^{g} \\sum_{j=1}^{n_i} (\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)(\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)^\\top\n\\]\nMacierz wariancji caÅ‚kowitej to: \\(\\mathbf{T} = \\mathbf{B} + \\mathbf{W}\\).\nW celu przeprowadzenia testu MANOVA, wykorzystuje siÄ™ statystyki oparte na stosunku macierzy:\n\\[\n\\mathbf{W}^{-1} \\mathbf{B}\n\\]\nNajczÄ™Å›ciej spotykane statystyki testowe to:\n\nWilksâ€™ Lambda:\n\n\\[\n\\Lambda = \\frac{\\det(\\mathbf{W})}{\\det(\\mathbf{B} + \\mathbf{W})}\n\\]\n\nStatystyka Pillai-Bartletta (Trace):\n\n\\[\nV = \\mathrm{tr}\\left[(\\mathbf{B} + \\mathbf{W})^{-1} \\mathbf{B}\\right]\n\\]\n\nStatystyka Hotellingaâ€“Lawleya (Trace):\n\n\\[\nT = \\mathrm{tr}(\\mathbf{W}^{-1} \\mathbf{B})\n\\]\n\nNajwiÄ™kszy pierwiastek Royâ€™a:\n\n\\[\n\\theta_{\\text{max}} = \\text{najwiÄ™ksza wartoÅ›Ä‡ wÅ‚asna}\\ (\\mathbf{W}^{-1} \\mathbf{B})\n\\]\nWybÃ³r konkretnej statystyki zaleÅ¼y od liczebnoÅ›ci prÃ³b, wymiaru przestrzeni i liczby grup. W praktyce Wilksâ€™ Lambda jest najczÄ™Å›ciej stosowana.\n\nPrzykÅ‚ad 5.1 Na poziomie istotnoÅ›ci \\(\\alpha=0.05\\) zweryfikuj hipotezÄ™, Å¼e czynnik grupujÄ…cy (Group) istotnie rÃ³Å¼nicuje zmienne Actions i Thoughts jednoczeÅ›nie.\n\nKodlibrary(gtsummary)\nlibrary(rstatix)\nlibrary(easystats)\nlibrary(gt)\ndane &lt;- rio::import(\"data/OCD.dat\")\n\n\nStatystyki opisowe grup\n\nKoddane %&gt;% \n  tbl_summary(by = Group,\n              statistic = list(where(is.numeric) ~ \"{mean} ({sd})\"),\n              type = list(Actions ~ \"continuous\"),\n              digits = list(everything() ~ 2))\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nBT\nN = 101\n\n\nCBT\nN = 101\n\n\nNo Treatment Control\nN = 101\n\n\n\n\nActions\n3.70 (1.77)\n4.90 (1.20)\n5.00 (1.05)\n\n\nThoughts\n15.20 (2.10)\n13.40 (1.90)\n15.00 (2.36)\n\n\n\n\n1 Mean (SD)\n\n\n\n\nKodp &lt;- dane %&gt;% \n  select(-Group) %&gt;% \n  correlation()\n\np %&gt;% print_html()\n\n\n\n\n\n\nCorrelation Matrix (pearson-method)\n\n\nParameter1\nParameter2\nr\n95% CI\nt(28)\np\n\n\n\nActions\nThoughts\n0.06\n(-0.31, 0.41)\n0.31\n0.758\n\n\np-value adjustment method: Holm (1979); Observations: 30\n\n\n\n\n\nW kontekÅ›cie zmiennej Actions widzimy najwyÅ¼szy poziom w grupie No treatment, natomiast najniÅ¼szy w grupie BT. Dla zmiennej Thoughts najwyÅ¼szy poziom osiÄ…gniÄ™to w grupie BT a najniÅ¼szy w grupie CBT. Grupy rÃ³Å¼niÄ… siÄ™ rÃ³wnieÅ¼ zmiennoÅ›ciÄ… obu cech. ZwiÄ…zek pomiÄ™dzy zmiennymi Actions i Thoughts jest niemal niezauwaÅ¼alny. Korelacja pomiÄ™dzy tymi cechami jest nieistotnie rÃ³Å¼na od zera.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  ggplot(aes(x = Group, y = value, fill = name)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(fill = \"Variable\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\nPowyÅ¼sze wykresy potwierdzajÄ… znaczne rÃ³Å¼nice pomiÄ™dzy grupami w kontekÅ›cie analizowanych cech.\nZaÅ‚oÅ¼enia\n\nKoddane %&gt;% \n  group_split(Group) %&gt;% \n  map_df(~mshapiro_test(.x[,2:3])) %&gt;% \n  mutate(Group = dane %&gt;% \n           group_keys(Group) %&gt;% \n           pull(Group),\n         .before = statistic) %&gt;%\n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nstatistic\np.value\n\n\n\nBT\n0.891\n0.175\n\n\nCBT\n0.959\n0.777\n\n\nNo Treatment Control\n0.826\n0.030\n\n\n\n\n\n\nJedynie w grupie No treatment nie jest zachowana wielowymiarowa normalnoÅ›Ä‡ rozkÅ‚adu analizowanych cech. MoÅ¼na teÅ¼ przeprowadziÄ‡ testy normalnoÅ›ci poszczegÃ³lnych zmiennych, ale naleÅ¼y pamiÄ™taÄ‡, Å¼e brak podstaw do odrzucenia hipotezy o normalnoÅ›ci brzegowych zmiennych nie jest warunkiem dostatecznym, a jedynie koniecznym.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Actions) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nActions\n0.872\n0.106\n\n\nCBT\nActions\n0.952\n0.691\n\n\nNo Treatment Control\nActions\n0.859\n0.074\n\n\n\n\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nThoughts\n0.877\n0.120\n\n\nCBT\nThoughts\n0.914\n0.310\n\n\nNo Treatment Control\nThoughts\n0.826\n0.030\n\n\n\n\n\n\nPodobnie jak w przypadku wielowymiarowym brak normalnoÅ›ci zarysowaÅ‚ siÄ™ w grupie No treatment i to tylko dla zmiennej Thoughts. Teraz przechodzimy do testowania jednorodnoÅ›ci kowariancji.\n\nKodbox_m(dane[,2:3], dane$Group)  %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n8.893\n0.180\n6.000\nBox's M-test for Homogeneity of Covariance Matrices\n\n\n\n\n\nNa podstawie powyÅ¼szego testu moÅ¼na stwierdziÄ‡, iÅ¼ nie ma podstaw do odrzucenia hipotezy o jednorodnoÅ›ci macierzy kowariancji.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Actions) \n\n[1] Group      Actions    Thoughts   is.outlier is.extreme\n&lt;0 rows&gt; (or 0-length row.names)\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nActions\nThoughts\nis.outlier\nis.extreme\n\n\nNo Treatment Control\n4\n20\nTRUE\nFALSE\n\n\n\n\nKodwhich(dane$Actions == 4 & dane$Thoughts == 20)\n\n[1] 26\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  mahalanobis_distance() %&gt;% \n  filter(is.outlier==TRUE)\n\n# A tibble: 0 Ã— 4\n# â„¹ 4 variables: Actions &lt;int&gt;, Thoughts &lt;int&gt;, mahal.dist &lt;dbl&gt;,\n#   is.outlier &lt;lgl&gt;\n\n\nIstnieje jedna obserwacja odstajÄ…ca w grupie No treatment (obserwacja nr 26). Test wielowymiarowy nie wykryÅ‚ Å¼adnego elementu odstajÄ…cego.\nPomimo niespeÅ‚nienia zaÅ‚oÅ¼enia o wielowymiarowej normalnoÅ›ci cech w grupach, zastosujemy test MANOVA.\nManova\n\nKodmod &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane)\nManova(mod) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.32\n4\n54\n2.56\n0.049\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Wilk\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.70\n4\n52\n2.55\n0.050\n\n\nWilks test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Roy\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.33\n2\n27\n4.52\n0.020\n\n\nRoy test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Hotelling\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.41\n4\n50\n2.55\n0.051\n\n\nHotelling-Lawley test statistic Anova Table (Type 2 tests)\n\n\n\n\nKod# model bez obserawcji odstajÄ…cej\nmod2 &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane[-26,])\nManova(mod2) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.36\n4\n52\n2.87\n0.032\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\n\nAnalizujÄ… wszystkie rodzaje testÃ³w Manova, widzimy, Å¼e jedynie test Hotellinga-Laweya nie daje podstaw do odrzucenia hipotezy o rÃ³wnoÅ›ci wektorÃ³w Å›rednich. Natomiast poniewaÅ¼ co najmniej jeden z nich wskazaÅ‚ istotnoÅ›Ä‡ rÃ³Å¼nic, to przyjmujemy, Å¼e sÄ… podstawy aby odrzuciÄ‡ hipotezÄ™ o rÃ³wnoÅ›ci wektorÃ³w Å›rednich pomiÄ™dzy grupami. Test wykluczajÄ…cy obserwacjÄ™ odstajÄ…cÄ… rÃ³wnieÅ¼ kaÅ¼e odrzuciÄ‡ hipotezÄ™ \\(H_0\\).\nPrzeprowadzimy zatem analizÄ™ brzegowÄ….\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  anova_test(value~Group) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\nActions\nGroup\n2.000\n27.000\n2.771\n0.080\n\n0.170\n\n\nThoughts\nGroup\n2.000\n27.000\n2.154\n0.136\n\n0.138\n\n\n\n\n\n\nAnaliza brzegowa pokazuje ciekawy wynik, mianowicie, dla Å¼adnej z analizowanych cech testy brzegowe nie wykazaÅ‚y istotnych rÃ³Å¼nic. To pokazuje jak waÅ¼ne jest stosowanie testÃ³w wielowymiarowych w kontekÅ›cie porÃ³wnaÅ„ grup.\nPost-hoc\nPoniewaÅ¼ testy brzegowe ANOVA nie wykazaÅ‚y rÃ³Å¼nic, to testÃ³w post-hoc nie powinno siÄ™ wykonywaÄ‡, ale dla celÃ³w Ä‡wiczeniowych pokaÅ¼Ä™ jak je wykonaÄ‡.\n\nKodpwc &lt;- dane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  games_howell_test(value~Group)\npwc %&gt;% \n  select(-.y.) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\ngroup1\ngroup2\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\nActions\nBT\nCBT\n1.200\nâˆ’0.544\n2.944\n0.209\nns\n\n\nActions\nBT\nNo Treatment Control\n1.300\nâˆ’0.394\n2.994\n0.148\nns\n\n\nActions\nCBT\nNo Treatment Control\n0.100\nâˆ’1.189\n1.389\n0.979\nns\n\n\nThoughts\nBT\nCBT\nâˆ’1.800\nâˆ’4.085\n0.485\n0.138\nns\n\n\nThoughts\nBT\nNo Treatment Control\nâˆ’0.200\nâˆ’2.749\n2.349\n0.978\nns\n\n\nThoughts\nCBT\nNo Treatment Control\n1.600\nâˆ’0.852\n4.052\n0.244\nns\n\n\n\n\n\n\nTesty post-hoc potwierdzajÄ… wyniki testÃ³w brzegowych ANOVA, poniewaÅ¼ brakuje rÃ³Å¼nic pomiÄ™dzy poziomami zmiennych grupujÄ…cych.\n\n\n\n\n\nRencher, Alvin C. 1998. Multivariate statistical inference and applications. T. 635. Wiley New York.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Rencher, Alvin C. 1998. Multivariate Statistical Inference and\nApplications. Vol. 635. Wiley New York.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "cca.html",
    "href": "cca.html",
    "title": "Analiza kanoniczna",
    "section": "",
    "text": "Przypomnienie z algebry ğŸ˜‰\nAnaliza kanoniczna (ang. Canonical Correlation Analysis, CCA) jest klasycznÄ… technikÄ… statystycznÄ… sÅ‚uÅ¼Ä…cÄ… do badania zwiÄ…zkÃ³w pomiÄ™dzy dwoma zestawami zmiennych wielowymiarowych. Jej podstawowym celem jest znalezienie takich kombinacji liniowych zmiennych z obu zestawÃ³w, ktÃ³re maksymalizujÄ… wzajemnÄ… korelacjÄ™ â€“ sÄ… to tzw. kanoniczne zmienne lub kanoniczne skÅ‚adniki. Technika ta zostaÅ‚a wprowadzona przez Harolda Hotellinga w roku 1936, a wiÄ™c w okresie intensywnego rozwoju metod statystycznych opartych na algebrze macierzy.\nW tym samym czasie powstawaÅ‚y takÅ¼e inne fundamenty analizy wielowymiarowej, takie jak analiza skÅ‚adowych gÅ‚Ã³wnych (PCA) czy dyskryminacja liniowa (LDA)1. Analiza kanoniczna stanowi zatem jeden z filarÃ³w klasycznej statystyki wielowymiarowej i do dziÅ› pozostaje istotnym narzÄ™dziem eksploracji i modelowania zÅ‚oÅ¼onych zaleÅ¼noÅ›ci.\nW odrÃ³Å¼nieniu od regresji wielorakiej, ktÃ³ra przewiduje zestaw zmiennych zaleÅ¼nych na podstawie zestawu predyktorÃ³w, analiza kanoniczna traktuje obie grupy zmiennych symetrycznie â€“ nie zakÅ‚ada istnienia wyraÅºnego kierunku przyczynowego. Dlatego stosuje siÄ™ jÄ… w sytuacjach, gdy celem jest ogÃ³lna analiza wspÃ³Å‚zaleÅ¼noÅ›ci pomiÄ™dzy dwoma zbiorami zmiennych, a nie przewidywanie jednego zestawu na podstawie drugiego.\nTypowe zastosowania analizy kanonicznej obejmujÄ…:\nNa potrzeby definicji modeli kanonicznego potrzebne bÄ™dÄ… nam pewne twierdzenia z zakresu algebry.\nOto matematyczna definicja modelu CCA oraz dowÃ³d istnienia rozwiÄ…zania, sformuÅ‚owana Å›ciÅ›le w duchu Twojego tekstu:",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#sformuÅ‚owanie-problemu-wÅ‚asnego",
    "href": "cca.html#sformuÅ‚owanie-problemu-wÅ‚asnego",
    "title": "Analiza kanoniczna",
    "section": "SformuÅ‚owanie problemu wÅ‚asnego",
    "text": "SformuÅ‚owanie problemu wÅ‚asnego\nPrzeksztaÅ‚Ä‡my zmienne \\[\nc=\\Sigma_{XX}^{1/2}a,\\quad d=\\Sigma_{YY}^{1/2}b.\n\\]\nWÃ³wczas \\[\n\\rho(a,b)=\\frac{c^\\top\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}d}{\\sqrt{c^\\top c}\\sqrt{d^\\top d}}.\n\\]\nZ lematu Cauchyâ€™egoâ€“Buniakowskiegoâ€“Schwarza mamy \\[\n\\left|c^\\top \\mathbf{M} d\\right| \\le\n\\bigl(c^\\top \\mathbf{M}\\mathbf{M}^\\top c\\bigr)^{1/2}\\bigl(d^\\top d\\bigr)^{1/2},\n\\quad \\text{gdzie }\\mathbf{M}=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}.\n\\tag{5.1}\\]\nZatem \\[\n\\rho(a,b)^2 \\le\n\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}.\n\\]\nPoniewaÅ¼ \\(\\mathbf{M}\\mathbf{M}^\\top=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\) jest macierzÄ… symetrycznÄ… dodatnio okreÅ›lonÄ…, z lematu Rayleighaâ€“Ritza otrzymujemy \\[\n\\max_{c\\neq 0}\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}=\\lambda_1,\n\\] gdzie \\(\\lambda_1\\) to najwiÄ™ksza wartoÅ›Ä‡ wÅ‚asna tej macierzy, osiÄ…gana dla \\(c=e_1\\) â€“ jej wektora wÅ‚asnego.\nJeÅ›li \\[\nd \\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1\n\\] to RÃ³wnanieÂ 5.1 staje siÄ™ rÃ³wnoÅ›ciÄ….\nWracajÄ…c do oryginalnych wspÃ³Å‚czynnikÃ³w \\[\na_1=\\Sigma_{XX}^{-1/2}e_1,\\quad\nb_1\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1.\n\\]\nPierwsza korelacja kanoniczna wynosi wÃ³wczas \\[\n\\rho_1=\\sqrt{\\lambda_1}.\n\\]\nAnalogicznie dla kolejnych par, przy zaÅ‚oÅ¼eniu \\(c\\perp e_1,\\dots,e_{k-1}\\), mamy \\[\n\\rho_k=\\sqrt{\\lambda_k},\n\\] gdzie \\(\\lambda_k\\) to kolejne wartoÅ›ci wÅ‚asne macierzy \\(\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\), a odpowiadajÄ…ce im wektory wÅ‚asne \\(e_k\\) definiujÄ… kolejne wektory kanoniczne \\[\nU_k=e_k^\\top\\Sigma_{XX}^{-1/2}X,\\quad\nV_k=f_k^\\top\\Sigma_{YY}^{-1/2}Y,\\quad\nf_k\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_k.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wÅ‚asnoÅ›ci-rozwiÄ…zaÅ„",
    "href": "cca.html#wÅ‚asnoÅ›ci-rozwiÄ…zaÅ„",
    "title": "Analiza kanoniczna",
    "section": "WÅ‚asnoÅ›ci rozwiÄ…zaÅ„",
    "text": "WÅ‚asnoÅ›ci rozwiÄ…zaÅ„\nDla kaÅ¼dej pary \\((U_k,V_k)\\) zachodzi \\[\n\\mathrm{Var}(U_k)=\\mathrm{Var}(V_k)=1,\\quad\n\\mathrm{Cov}(U_k,U_l)=\\mathrm{Cov}(V_k,V_l)=\\mathrm{Cov}(U_k,V_l)=0\\quad (k\\neq l).\n\\]\n\n\n\n\n\n\nDowÃ³d powyÅ¼szych rÃ³wnoÅ›ci\n\n\n\nDla danej pary wektorÃ³w kanonicznych mamy\n\\[\nU_k = a_k^\\top X = e_k^\\top \\Sigma_{XX}^{-1/2} X,\n    \\qquad\n    V_k = b_k^\\top Y = f_k^\\top \\Sigma_{YY}^{-1/2} Y,\n\\] gdzie:\n\n\n\\(e_k\\) jest ortonormalnym6 wektorem wÅ‚asnym macierzy \\(\\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} \\Sigma_{XX}^{-1/2}\\)\n\n\n\\(f_k \\propto \\Sigma_{YY}^{-1/2} \\Sigma_{YX} \\Sigma_{XX}^{-1/2} e_k\\),\n\n\\(\\rho_k = \\sqrt{\\lambda_k}\\), gdzie \\(\\lambda_k\\) to odpowiadajÄ…ca wartoÅ›Ä‡ wÅ‚asna.\n\nMacierze \\(\\Sigma_{XX}\\), \\(\\Sigma_{YY}\\) sÄ… dodatnio okreÅ›lone, wiÄ™c moÅ¼na wprowadziÄ‡ transformacje \\[\n\\tilde{X} = \\Sigma_{XX}^{-1/2}X, \\quad \\tilde{Y} = \\Sigma_{YY}^{-1/2}Y.\n\\] Zatem \\[\nU_k = e_k^\\top \\tilde{X},\\quad V_k = f_k^\\top \\tilde{Y}.\n\\]\nNajpierw udowodnimy, Å¼e \\(\\operatorname{Var}(U_k) = \\operatorname{Var}(V_k) = 1\\).\nZmienna \\(U_k = e_k^\\top \\tilde{X}\\), wiÄ™c \\[\n\\operatorname{Var}(U_k) = \\operatorname{Var}(e_k^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_k.\n\\] ZauwaÅ¼my, Å¼e \\[\n\\operatorname{Var}(\\tilde{X}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XX} \\Sigma_{XX}^{-1/2} = I,\n\\] wiÄ™c \\[\n\\operatorname{Var}(U_k) = e_k^\\top I e_k = e_k^\\top e_k = 1.\n\\] Analogicznie \\[\n\\operatorname{Var}(V_k) = f_k^\\top \\operatorname{Var}(\\tilde{Y}) f_k = f_k^\\top f_k = 1,\n\\] poniewaÅ¼ \\(\\tilde{Y}\\) ma jednostkowÄ… macierz kowariancji, a \\(f_k\\) sÄ… znormalizowane.\nTeraz dowiedziemy, Å¼e \\(\\operatorname{Cov}(U_k, U_l) = 0\\) dla \\(k \\neq l\\)\n\\[\n\\operatorname{Cov}(U_k, U_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, e_l^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_l = e_k^\\top e_l.\n\\] PoniewaÅ¼ \\(e_k\\), \\(e_l\\) sÄ… ortonormalnymi wektorami wÅ‚asnymi symetrycznej macierzy, to \\[\ne_k^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, U_l) = 0.\n\\] Podobnie \\[\n\\operatorname{Cov}(V_k, V_l) = f_k^\\top f_l = 0 \\quad \\text{dla } k \\neq l.\n\\]\nNa koniec dowiedÅºmy, Å¼e \\(\\operatorname{Cov}(U_k, V_l) = 0\\) dla \\(k \\neq l\\) \\[\n\\operatorname{Cov}(U_k, V_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, f_l^\\top \\tilde{Y}) = e_k^\\top \\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) f_l.\n\\] Z definicji \\[\n\\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1/2} =: M.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = e_k^\\top M f_l.\n\\] Z poprzednich wyprowadzeÅ„ \\[\nf_l \\propto M^\\top e_l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) \\propto e_k^\\top M M^\\top e_l.\n\\] Ale macierz \\(MM^\\top\\) jest symetryczna, a \\(e_k\\) sÄ… jej ortonormalnymi wektorami wÅ‚asnymi, wiÄ™c \\[\ne_k^\\top MM^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = 0.\n\\]\n\n\n6Â ortonormalnoÅ›Ä‡ wynika z niezmienniczoÅ›ci korelacji wzglÄ™dem dÅ‚ugoÅ›ci wektorÃ³wPonadto korelacje kanoniczne sÄ… niezmiennicze wzglÄ™dem odwracalnych przeksztaÅ‚ceÅ„ liniowych \\(X\\) i \\(Y\\): \\[\nX^*=\\mathcal{U}^TX+u,\\quad Y^*=\\mathcal{V}^TY+v \\implies\n\\rho_i(X^*,Y^*)=\\rho_i(X,Y).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wniosek",
    "href": "cca.html#wniosek",
    "title": "Analiza kanoniczna",
    "section": "Wniosek",
    "text": "Wniosek\nPoniewaÅ¼ macierze kowariancji \\(\\Sigma_{XX}\\) i \\(\\Sigma_{YY}\\) sÄ… dodatnio okreÅ›lone, ich odwrotnoÅ›ci istniejÄ…. Macierze: \\[\n\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2},\\quad\n\\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}\n\\] sÄ… symetryczne i dodatnio okreÅ›lone, wiÄ™c majÄ… rzeczywiste, dodatnie wartoÅ›ci wÅ‚asne i ortonormalne wektory wÅ‚asne. Z lematu Rayleighaâ€“Ritza otrzymujemy maksymalizacjÄ™ ilorazu Rayleigha oraz gwarancjÄ™ istnienia rozwiÄ…zania. Tym samym wykazano, Å¼e pary \\((a_k,b_k)\\) istniejÄ…, a odpowiadajÄ…ce im \\(\\rho_k=\\sqrt{\\lambda_k}\\) sÄ… kanonicznymi korelacjami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#hipoteza-zerowa-i-alternatywna",
    "href": "cca.html#hipoteza-zerowa-i-alternatywna",
    "title": "Analiza kanoniczna",
    "section": "Hipoteza zerowa i alternatywna",
    "text": "Hipoteza zerowa i alternatywna\nDla zbiorÃ³w zmiennych \\(X \\in \\mathbb{R}^p\\), \\(Y \\in \\mathbb{R}^q\\), testujemy\n\n\n\\(H_0: \\rho_1 = \\rho_2 = \\cdots = \\rho_s = 0\\) â€“ brak istotnych korelacji kanonicznych (pierwiastkÃ³w),\n\n\\(H_1\\): istnieje co najmniej jedna istotna korelacja kanoniczna, tj. \\(\\exists i \\leq s \\ \\text{takie, Å¼e } \\rho_i \\ne 0\\).\n\ngdzie \\(s = \\min(p, q)\\), a \\(\\rho_i\\) to \\(i\\)-ta korelacja kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#statystyka-testowa-test-wilka",
    "href": "cca.html#statystyka-testowa-test-wilka",
    "title": "Analiza kanoniczna",
    "section": "Statystyka testowa â€“ test Wilka",
    "text": "Statystyka testowa â€“ test Wilka\nW celu przetestowania tej hipotezy, wykorzystuje siÄ™ statystykÄ™ Wilka, ktÃ³ra bazuje na iloczynie skÅ‚adnikÃ³w postaci (\\(1 - \\lambda_i\\)), gdzie \\(\\lambda_i\\) to wartoÅ›ci wÅ‚asne odpowiadajÄ…ce kwadratom korelacji kanonicznych \\[\n\\lambda_i = \\rho_i^2.\n\\] Statystyka Wilkas jest zdefiniowana jako \\[\n\\Lambda = \\prod_{i=1}^s (1 - \\lambda_i).\n\\]\nInterpretacja - im mniejsze wartoÅ›ci \\(\\Lambda\\), tym wiÄ™ksza zaleÅ¼noÅ›Ä‡ miÄ™dzy zbiorami \\(X\\) i \\(Y\\). DuÅ¼e wartoÅ›ci \\(\\lambda_i\\) (czyli silne korelacje kanoniczne) powodujÄ…, Å¼e \\(\\Lambda\\) dÄ…Å¼y do zera.\nW praktyce, dla prÃ³by \\(n\\)-elementowej, stosujemy wersjÄ™ testu bazujÄ…cÄ… na macierzach kowariancji estymowanych z prÃ³by \\(S_{XX}, S_{XY}, S_{YX}, S_{YY})\\) â€“ odpowiedniki \\(\\Sigma_{XX}, \\Sigma_{XY}, \\Sigma_{YX}, \\Sigma_{YY}\\).\nWÃ³wczas \\[\nT^2/n = \\left|I - S_{YY}^{-1} S_{YX} S_{XX}^{-1} S_{XY} \\right| = \\prod_{i=1}^s (1 - \\hat{\\lambda}_i),\n\\] gdzie \\(\\hat{\\lambda}_i\\) to prÃ³bkowe wartoÅ›ci wÅ‚asne (szacunki \\(\\lambda_i\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#rozkÅ‚ad-asymptotyczny-i-transformacja-do-rozkÅ‚adu-chi2",
    "href": "cca.html#rozkÅ‚ad-asymptotyczny-i-transformacja-do-rozkÅ‚adu-chi2",
    "title": "Analiza kanoniczna",
    "section": "RozkÅ‚ad asymptotyczny i transformacja do rozkÅ‚adu \\(\\chi^2\\)\n",
    "text": "RozkÅ‚ad asymptotyczny i transformacja do rozkÅ‚adu \\(\\chi^2\\)\n\nWielu autorÃ³w (np. Marriott i Gittins (1986)) sugeruje przeksztaÅ‚cenie statystyki Wilksa do postaci asymptotycznie zgodnej z rozkÅ‚adem \\(\\chi^2\\), np. za pomocÄ… transformacji\n\\[\n-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda) \\sim \\chi^2_{pq}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#procedura-testowania",
    "href": "cca.html#procedura-testowania",
    "title": "Analiza kanoniczna",
    "section": "Procedura testowania",
    "text": "Procedura testowania\n\nOszacuj wszystkie korelacje kanoniczne \\(\\hat{\\rho}_1, \\ldots, \\hat{\\rho}_p\\).\nOd \\(k = 0\\) do \\(p-1\\) oblicz \\(\\Lambda_k = \\prod_{i=k+1}^{p}(1 - \\hat{\\rho}_i^2)\\).\nOblicz transformacjÄ™ \\(\\chi^2_k=-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda_k)\\)\n\nPorÃ³wnaj z odpowiednim kwantylem rozkÅ‚adu \\(\\chi^2\\) z \\((q - k)(r - k)\\) stopniami swobody.\nJeÅ›li wartoÅ›Ä‡ statystyki przekracza ten kwantyl (\\(p&lt;\\alpha\\)), odrzuÄ‡ \\(H_0^{(k)}\\) i przejdÅº do \\(k+1\\). JeÅ›li nie, zatrzymaj siÄ™ â€“ kolejne korelacje uznajemy za nieistotne.\n\nOcena dopasowania modelu w analizie kanonicznej (CCA â€“ Canonical Correlation Analysis) obejmuje kilka istotnych wskaÅºnikÃ³w diagnostycznych, ktÃ³re pozwalajÄ… zrozumieÄ‡ siÅ‚Ä™ i strukturÄ™ relacji miÄ™dzy dwoma zbiorami zmiennych. PoniÅ¼ej omÃ³wione zostaÅ‚y trzy kluczowe miary: Å‚adunki czynnikowe, wariancja wyjaÅ›niona oraz redundancja.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#Å‚adunki-czynnikowe-ang.-canonical-loadings",
    "href": "cca.html#Å‚adunki-czynnikowe-ang.-canonical-loadings",
    "title": "Analiza kanoniczna",
    "section": "Åadunki czynnikowe (ang. canonical loadings)",
    "text": "Åadunki czynnikowe (ang. canonical loadings)\n\n\nDefinicja - korelacje pomiÄ™dzy zmiennymi kanonicznymi (czyli kombinacjami liniowymi wektorÃ³w \\(a_k'X\\) i \\(b_k'Y\\)) a oryginalnymi zmiennymi ze zbiorÃ³w \\(X\\) i \\(Y\\).\n\nInterpretacja:\n\nPokazujÄ…, ktÃ³re konkretne zmienne pierwotne w najwiÄ™kszym stopniu â€Å‚adujÄ… siÄ™â€ (czyli kontrybuujÄ…) na danÄ… zmiennÄ… kanonicznÄ….\nWysoka wartoÅ›Ä‡ (np. &gt; 0.7) wskazuje na silnÄ… zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennÄ… oryginalnÄ… a danÄ… zmiennÄ… kanonicznÄ….\nZnaki dodatnie/ujemne pozwalajÄ… wnioskowaÄ‡ o kierunku zwiÄ…zku.\n\n\n\nWzÃ³r:\n\nDla zbioru \\(X\\): \\[\n\\text{loadings}_X = \\mathrm{Corr}(X, U_k) = \\Sigma_{XX} a_k\n\\]\n\nDla zbioru \\(Y\\): \\[\n\\text{loadings}_Y = \\mathrm{Corr}(Y, V_k) = \\Sigma_{YY} b_k\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wariancja-wyjaÅ›niona-ang.-variance-explained",
    "href": "cca.html#wariancja-wyjaÅ›niona-ang.-variance-explained",
    "title": "Analiza kanoniczna",
    "section": "Wariancja wyjaÅ›niona (ang. variance explained)",
    "text": "Wariancja wyjaÅ›niona (ang. variance explained)\n\n\nDefinicja - Å›rednia kwadratÃ³w Å‚adunkÃ³w czynnikowych dla kaÅ¼dej zmiennej kanonicznej i kaÅ¼dego zbioru danych.\n\nInterpretacja:\n\nInformuje, jakÄ… czÄ™Å›Ä‡ wariancji oryginalnych zmiennych w danym zbiorze (\\(X\\) lub \\(Y\\)) wyjaÅ›nia dana zmienna kanoniczna.\nMoÅ¼na traktowaÄ‡ ten wskaÅºnik jako odpowiednik wspÃ³Å‚czynnika determinacji \\(R^2\\) dla pojedynczej zmiennej kanonicznej.\nWysoka wartoÅ›Ä‡ oznacza, Å¼e dana zmienna kanoniczna dobrze reprezentuje zbiÃ³r, z ktÃ³rego zostaÅ‚a utworzona.\n\n\n\nWzÃ³r: \\[\n\\text{Explained variance} = \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k)\n\\] gdzie \\(p\\) to liczba zmiennych w zbiorze \\(X\\), a \\(U_k\\) to \\(k\\)-ta zmienna kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#redundancja-ang.-redundancy-index",
    "href": "cca.html#redundancja-ang.-redundancy-index",
    "title": "Analiza kanoniczna",
    "section": "Redundancja (ang. redundancy index)",
    "text": "Redundancja (ang. redundancy index)\n\n\nDefinicja - iloczyn kwadratu korelacji kanonicznej \\(\\rho_k^2\\) oraz wariancji wyjaÅ›nionej przez danÄ… zmiennÄ… kanonicznÄ… we wÅ‚asnym zbiorze.\n\nInterpretacja:\n\nInformuje, jaka czÄ™Å›Ä‡ przeciÄ™tnej wariancji jednej grupy zmiennych jest wyjaÅ›niana przez zmiennÄ… kanonicznÄ… utworzonÄ… na podstawie drugiego zbioru.\nMiara ta pokazuje, czy dany zbiÃ³r zmiennych wnosi unikalnÄ… informacjÄ™ o drugim zbiorze.\nWysoka redundancja oznacza, Å¼e istnieje istotny zwiÄ…zek miÄ™dzy strukturami dwÃ³ch zbiorÃ³w zmiennych.\n\n\n\nWzÃ³r: \\[\n\\text{Redundancy}_X = \\rho_k^2 \\cdot \\left( \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k) \\right)\n\\] Analogicznie definiujemy redundancjÄ™ wzglÄ™dem \\(Y\\).\n\n\n\n\n\n\n\n\nMiara\nCo opisuje\nInterpretacja praktyczna\n\n\n\nÅadunki czynnikowe\nSiÅ‚Ä™ powiÄ…zania zmiennej oryginalnej z kanonicznÄ…\nWysoka wartoÅ›Ä‡ â‡’ silna reprezentacja zmiennej\n\n\nWariancja wyjaÅ›niona\nÅšrednia siÅ‚a reprezentacji zbioru przez zm. kanonicznÄ…\nMiara dopasowania struktury do zbioru\n\n\nRedundancja\nIloÅ›Ä‡ informacji o jednym zbiorze zawarta w drugim\nMiara istotnoÅ›ci relacji miÄ™dzy zbiorami",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#normalnoÅ›Ä‡-wielowymiarowa",
    "href": "cca.html#normalnoÅ›Ä‡-wielowymiarowa",
    "title": "Analiza kanoniczna",
    "section": "NormalnoÅ›Ä‡ wielowymiarowa",
    "text": "NormalnoÅ›Ä‡ wielowymiarowa\nZakÅ‚ada siÄ™, Å¼e obydwa zbiory zmiennych losowych â€“ \\(X\\) i \\(Y\\) â€“ sÄ… wspÃ³lnie rozkÅ‚adem normalnym wielowymiarowym jak podano w RÃ³wnanieÂ 4.1. NormalnoÅ›Ä‡ umoÅ¼liwia stosowanie testÃ³w statystycznych (np. testu Wilksa) do oceny liczby istotnych korelacji kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-wartoÅ›ci-odstajÄ…cych-outliers",
    "href": "cca.html#brak-wartoÅ›ci-odstajÄ…cych-outliers",
    "title": "Analiza kanoniczna",
    "section": "Brak wartoÅ›ci odstajÄ…cych (outliers)",
    "text": "Brak wartoÅ›ci odstajÄ…cych (outliers)\nZarÃ³wno obserwacje odstajÄ…ce jednowymiarowe, jak i wielowymiarowe mogÄ… istotnie zaburzaÄ‡ wynik analizy kanonicznej. OdstajÄ…ce wartoÅ›ci mogÄ… wpÅ‚ywaÄ‡ na macierze kowariancji, zmieniajÄ…c kierunki i siÅ‚y relacji miÄ™dzy zbiorami zmiennych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wystarczajÄ…ca-liczba-obserwacji",
    "href": "cca.html#wystarczajÄ…ca-liczba-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "WystarczajÄ…ca liczba obserwacji",
    "text": "WystarczajÄ…ca liczba obserwacji\nLiczba obserwacji powinna znaczÄ…co przekraczaÄ‡ liczbÄ™ zmiennych w kaÅ¼dym zbiorze. Liczba obserwacji \\(n\\) w kaÅ¼dej grupie powinna byÄ‡ wiÄ™ksza niÅ¼ suma liczby zmiennych w \\(X\\) i \\(Y\\) \\[\nn &gt; p + q\n\\] Zapewnia odwracalnoÅ›Ä‡ macierzy kowariancji oraz stabilnoÅ›Ä‡ estymatorÃ³w.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#liniowoÅ›Ä‡-zaleÅ¼noÅ›ci",
    "href": "cca.html#liniowoÅ›Ä‡-zaleÅ¼noÅ›ci",
    "title": "Analiza kanoniczna",
    "section": "LiniowoÅ›Ä‡ zaleÅ¼noÅ›ci",
    "text": "LiniowoÅ›Ä‡ zaleÅ¼noÅ›ci\nZakÅ‚ada siÄ™, Å¼e zwiÄ…zki miÄ™dzy wszystkimi parami zmiennych sÄ… liniowe. PoniewaÅ¼ CCA opiera siÄ™ na maksymalizacji liniowych kombinacji, nieliniowe zaleÅ¼noÅ›ci mogÄ… pozostaÄ‡ niewykryte.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-nadmiernej-wspÃ³Å‚liniowoÅ›ci-multikolinearnoÅ›ci",
    "href": "cca.html#brak-nadmiernej-wspÃ³Å‚liniowoÅ›ci-multikolinearnoÅ›ci",
    "title": "Analiza kanoniczna",
    "section": "Brak nadmiernej wspÃ³Å‚liniowoÅ›ci (multikolinearnoÅ›ci)",
    "text": "Brak nadmiernej wspÃ³Å‚liniowoÅ›ci (multikolinearnoÅ›ci)\nZmienne wewnÄ…trz kaÅ¼dego zbioru (w \\(X\\) lub w \\(Y\\)) nie powinny byÄ‡ nadmiernie skorelowane. Wysoka wspÃ³Å‚liniowoÅ›Ä‡ moÅ¼e prowadziÄ‡ do niestabilnych i trudnych do interpretacji wektorÃ³w kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#niezaleÅ¼noÅ›Ä‡-obserwacji",
    "href": "cca.html#niezaleÅ¼noÅ›Ä‡-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "NiezaleÅ¼noÅ›Ä‡ obserwacji",
    "text": "NiezaleÅ¼noÅ›Ä‡ obserwacji\nKaÅ¼da obserwacja powinna pochodziÄ‡ od innej jednostki (brak powtÃ³rzeÅ„ pomiarÃ³w). NiezaleÅ¼noÅ›Ä‡ warunkuje poprawnoÅ›Ä‡ estymatorÃ³w kowariancji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "fa.html",
    "href": "fa.html",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "Eksploracyjna analiza czynnikowa\nAnaliza czynnikowa naleÅ¼y do klasy metod wielowymiarowych, ktÃ³rych celem jest odkrywanie ukrytych struktur stojÄ…cych za obserwowanymi zmiennymi. W odrÃ³Å¼nieniu od metod takich jak analiza gÅ‚Ã³wnych skÅ‚adowych1, ktÃ³re opierajÄ… siÄ™ na czysto algebraicznych przeksztaÅ‚ceniach danych, analiza czynnikowa ma wyraÅºne odniesienie do modeli statystycznych i psychometrycznych, w ktÃ³rych zakÅ‚ada siÄ™ istnienie czynnikÃ³w latentnych â€“ czyli zmiennych ukrytych, niewidocznych bezpoÅ›rednio, ale wpÅ‚ywajÄ…cych na wartoÅ›ci zmiennych obserwowalnych. PrzykÅ‚adem moÅ¼e byÄ‡ konstrukt â€inteligencjaâ€, ktÃ³ry przejawia siÄ™ w wynikach testÃ³w logicznych, pamiÄ™ciowych czy jÄ™zykowych. GÅ‚Ã³wnym celem analizy czynnikowej jest redukcja wymiarowoÅ›ci poprzez reprezentacjÄ™ wielu zmiennych w postaci mniejszej liczby czynnikÃ³w oraz lepsze zrozumienie powiÄ…zaÅ„ miÄ™dzy zmiennymi poprzez ujawnienie wspÃ³lnych ÅºrÃ³deÅ‚ ich zmiennoÅ›ci.\nMoÅ¼na wyrÃ³Å¼niÄ‡ dwa podstawowe podejÅ›cia do analizy czynnikowej. Eksploracyjna analiza czynnikowa (EFA, Exploratory Factor Analysis) jest stosowana, gdy badacz nie ma wczeÅ›niej zdefiniowanych hipotez co do liczby czynnikÃ³w czy struktury powiÄ…zaÅ„ miÄ™dzy nimi. Jej celem jest odkrycie potencjalnych ukÅ‚adÃ³w zaleÅ¼noÅ›ci i zidentyfikowanie liczby czynnikÃ³w najlepiej opisujÄ…cych dane. Konfirmacyjna analiza czynnikowa (CFA, Confirmatory Factor Analysis) jest natomiast podejÅ›ciem dedukcyjnym â€“ badacz z gÃ³ry formuÅ‚uje model teoretyczny (np. Å¼e pewne zmienne mierzÄ… â€pamiÄ™Ä‡ roboczÄ…â€, a inne â€myÅ›lenie abstrakcyjneâ€) i testuje jego zgodnoÅ›Ä‡ z danymi empirycznymi. CFA jest szczegÃ³lnie istotna w kontekÅ›cie walidacji narzÄ™dzi badawczych, np. kwestionariuszy psychologicznych, i stanowi fundament bardziej zaawansowanych modeli strukturalnych (SEM).\nHistoria analizy czynnikowej siÄ™ga poczÄ…tkÃ³w XX wieku i jest Å›ciÅ›le zwiÄ…zana z psychometriÄ…. Jej pionierem byÅ‚ Charles Spearman, ktÃ³ry w 1904 roku zaproponowaÅ‚ model jednoczynnikowy, interpretujÄ…c zmienne poznawcze jako przejawy ogÃ³lnego czynnika inteligencji. W kolejnych dekadach metoda byÅ‚a rozwijana przez psychologÃ³w, takich jak Thurstone, ktÃ³ry wprowadziÅ‚ koncepcjÄ™ wieloczynnikowÄ… oraz przez statystykÃ³w, ktÃ³rzy rozwijali formalne podstawy estymacji czynnikÃ³w i rotacji macierzy Å‚adunkÃ³w. W latach 60. i 70. analiza czynnikowa staÅ‚a siÄ™ jednÄ… z najczÄ™Å›ciej stosowanych metod w badaniach psychologicznych i spoÅ‚ecznych, a wraz z rozwojem informatyki zyskaÅ‚a na popularnoÅ›ci takÅ¼e w ekonomii, biologii czy medycynie. DziÅ› analiza czynnikowa jest narzÄ™dziem interdyscyplinarnym, stosowanym zarÃ³wno do eksploracji struktur danych, jak i do testowania teorii opartych na zmiennych latentnych.\nFormalna postaÄ‡ modelu eksploracyjnej analizy czynnikowej (EFA) zakÅ‚ada, Å¼e zmienne obserwowalne \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)^\\top\\) moÅ¼na wyraziÄ‡ jako kombinacjÄ™ liniowÄ… czynnikÃ³w latentnych oraz skÅ‚adnikÃ³w specyficznych. Model przyjmuje postaÄ‡:\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\]\ngdzie:",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#efa",
    "href": "fa.html#efa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) â€“ wektor zmiennych obserwowalnych,\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) â€“ wektor Å›rednich,\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) â€“ macierz Å‚adunkÃ³w czynnikowych, ktÃ³rej element \\(\\lambda_{ij}\\) opisuje wpÅ‚yw czynnika \\(j\\) na zmiennÄ… \\(i\\),\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) â€“ wektor czynnikÃ³w latentnych (czynnikÃ³w wspÃ³lnych),\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) â€“ wektor skÅ‚adnikÃ³w specyficznych (unikalnych, bÅ‚Ä™dÃ³w pomiaru).\n\n\nZaÅ‚oÅ¼enia klasycznego modelu EFA\n\nRozkÅ‚ad czynnikÃ³w wspÃ³lnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne majÄ… Å›redniÄ… zero i macierz kowariancji rÃ³wnÄ… macierzy jednostkowej. To zaÅ‚oÅ¼enie oznacza, Å¼e czynniki sÄ… nieskorelowane i majÄ… wariancjÄ™ jednostkowÄ… (jest to standaryzacja wprowadzona dla identyfikowalnoÅ›ci modelu).\nRozkÅ‚ad skÅ‚adnikÃ³w specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzÄ… diagonalnÄ… o elementach dodatnich. Oznacza to, Å¼e bÅ‚Ä™dy sÄ… nieskorelowane miÄ™dzy sobÄ… oraz niezaleÅ¼ne od czynnikÃ³w \\(\\mathbf{f}\\).\nNiezaleÅ¼noÅ›Ä‡ czynnikÃ³w i bÅ‚Ä™dÃ³w \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyÅ¼szej konstrukcji wynika, Å¼e kowariancja zmiennych obserwowalnych jest sumÄ… czÄ™Å›ci wspÃ³lnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowÃ³d\n\n\n\nNiech losowy wektor obserwacji ma postaÄ‡ \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynnikÃ³w wspÃ³lnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor skÅ‚adnikÃ³w specyficznych. ZakÅ‚adamy, Å¼e \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazaÄ‡, Å¼e \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczegÃ³lnoÅ›ci przy \\(\\Phi=I_m\\), Å¼e mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a poniewaÅ¼ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancjÄ™ \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyraÅ¼amy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] KorzystajÄ…c z liniowoÅ›ci kowariancji i toÅ¼samoÅ›ci \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorÃ³w losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skoÅ„czonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDziÄ™ki zaÅ‚oÅ¼eniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikajÄ… i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] JeÅ›li dodatkowo przyjmiemy standardyzacjÄ™ czynnikÃ³w \\(\\Phi=I_m\\) (co jest konwencjÄ… identyfikacyjnÄ… modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego naleÅ¼aÅ‚o dowieÅ›Ä‡.\nWarto odnotowaÄ‡, Å¼e dowÃ³d nie wymaga niezaleÅ¼noÅ›ci \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym â€” wystarcza nieskorelowanie, aby zniknÄ™Å‚y skÅ‚adniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postaÄ‡ \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to moÅ¼na zastosowaÄ‡ tzw. whitening czynnikÃ³w \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co rÃ³wnowaÅ¼nie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), ktÃ³re prowadzÄ… do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to zwiÄ…zane z moÅ¼liwoÅ›ciÄ… przeprowadzania rÃ³Å¼nych transformacji czynnikÃ³w bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogÃ³lnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeÅ¼eli dokonamy transformacji ortogonalnej czynnikÃ³w \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzÄ… ortogonalnÄ…, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, Å¼e \\(\\Lambda\\) i \\(\\Phi\\) nie sÄ… jednoznacznie wyznaczone. RÃ³Å¼ne pary \\((\\Lambda, \\Phi)\\) mogÄ… prowadziÄ‡ do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczegÃ³lnoÅ›ci wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jednÄ… z takich rÃ³wnowaÅ¼nych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalnÄ… rolÄ™, poniewaÅ¼ jest miejscem, w ktÃ³rym spotykajÄ… siÄ™ dwa skÅ‚adniki zmiennoÅ›ci: wspÃ³lna i specyficzna. RozkÅ‚ad \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, Å¼e caÅ‚kowita wariancja i kowariancja obserwowanych zmiennych moÅ¼e byÄ‡ przedstawiona jako suma efektu wspÃ³lnych czynnikÃ³w oraz efektu specyficznego, indywidualnego dla kaÅ¼dej zmiennej.\nCzÄ™Å›Ä‡ \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspÃ³lne ÅºrÃ³dÅ‚o zmiennoÅ›ci, czyli wariancjÄ™ wyjaÅ›nianÄ… przez czynniki ukryte. To wÅ‚aÅ›nie ta czÄ™Å›Ä‡ umoÅ¼liwia redukcjÄ™ wymiaru â€“ wiele zmiennych obserwowanych moÅ¼na sprowadziÄ‡ do kilku czynnikÃ³w, ktÃ³re reprezentujÄ… gÅ‚Ã³wnÄ… strukturÄ™ zaleÅ¼noÅ›ci. Interpretacja czynnikÃ³w jako ukrytych wymiarÃ³w (np. inteligencja, poziom lÄ™ku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uproÅ›ciÄ‡ analizÄ™, ale takÅ¼e nadaÄ‡ jej znaczenie teoretyczne w danej dziedzinie badaÅ„.\nZ kolei \\(\\Psi\\) odpowiada za wariancjÄ™ unikalnÄ…, czyli tÄ™ czÄ™Å›Ä‡ zmiennoÅ›ci, ktÃ³ra nie jest wspÃ³Å‚dzielona z innymi zmiennymi. Obejmuje ona zarÃ³wno wariancjÄ™ czysto specyficznÄ… dla danej cechy, jak i wariancjÄ™ bÅ‚Ä™du pomiarowego. DziÄ™ki temu moÅ¼liwe jest odrÃ³Å¼nienie struktury gÅ‚Ä™bokiej (czynnikowej) od elementÃ³w przypadkowych i indywidualnych.\nPodsumowujÄ…c, znaczenie modelu czynnikowego polega na tym, Å¼e pozwala on wydzieliÄ‡ istotne, ukryte mechanizmy stojÄ…ce za wspÃ³Å‚zaleÅ¼noÅ›ciami zmiennych i oddzieliÄ‡ je od szumÃ³w specyficznych dla pojedynczych obserwacji. W praktyce oznacza to moÅ¼liwoÅ›Ä‡ redukcji liczby analizowanych zmiennych, uproszczenie opisu zÅ‚oÅ¼onych danych i pogÅ‚Ä™bienie interpretacji zjawisk spoÅ‚ecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynnikÃ³w w praktyce opiera siÄ™ przede wszystkim na analizie macierzy Å‚adunkÃ³w czynnikowych \\(\\Lambda\\). KaÅ¼dy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile zwiÄ…zku pomiÄ™dzy zmiennÄ… obserwowanÄ… \\(x_i\\) a czynnikiem \\(f_j\\). Im wyÅ¼sza wartoÅ›Ä‡ bezwzglÄ™dna Å‚adunku, tym wiÄ™kszy udziaÅ‚ danego czynnika w wyjaÅ›nianiu zmiennoÅ›ci konkretnej zmiennej. Na przykÅ‚ad w psychologii wysoki Å‚adunek czynnika na zmiennej opisujÄ…cej pamiÄ™Ä‡ krÃ³tkotrwaÅ‚Ä… i na zmiennej opisujÄ…cej zdolnoÅ›Ä‡ rozwiÄ…zywania problemÃ³w matematycznych moÅ¼e sugerowaÄ‡, Å¼e obie cechy sÄ… przejawem wspÃ³lnego czynnika â€“ inteligencji ogÃ³lnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#metody-estymacji-Å‚adunkÃ³w-czynnikowych",
    "href": "fa.html#metody-estymacji-Å‚adunkÃ³w-czynnikowych",
    "title": "Analiza czynnikowa",
    "section": "Metody estymacji Å‚adunkÃ³w czynnikowych",
    "text": "Metody estymacji Å‚adunkÃ³w czynnikowych\nMetoda najwiÄ™kszej wiarogodnoÅ›ci (ang. Maximal Likelihood, ML)\nZaÅ‚oÅ¼enia\nZakÅ‚adamy, Å¼e wektor zmiennych obserwowalnych\n\\[\n\\mathbf{x} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\Sigma),\n\\]\ngdzie kowariancja \\(\\Sigma\\) ma postaÄ‡ modelowÄ… \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nDla uproszczenia przyjmuje siÄ™ czÄ™sto, Å¼e czynniki \\(\\mathbf{f}\\) sÄ… standaryzowane i nieskorelowane, czyli \\(\\Phi = I_m\\). WÃ³wczas macierz kowariancji ma postaÄ‡\n\\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nFunkcja wiarygodnoÅ›ci\nDla prÃ³by \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) funkcja wiarygodnoÅ›ci rozkÅ‚adu normalnego wynosi\n\\[\nL(\\Lambda,\\Psi) = (2\\pi)^{-\\frac{np}{2}} |\\Sigma|^{-\\frac{n}{2}}\n\\exp\\left(-\\tfrac{1}{2}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{x}_i-\\mu)\\right).\n\\]\nczÄ™Å›ciej wyraÅ¼ana w postaci zlogarytmowanej\n\\[\n\\ell(\\Lambda,\\Psi) = -\\frac{n}{2} \\left[ \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) \\right] + C,\n\\]\ngdzie \\(S = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)(\\mathbf{x}_i-\\mu)^\\top\\) jest macierzÄ… kowariancji z prÃ³by.\nEstymacja parametrÃ³w\nEstymatory \\(\\hat{\\Lambda}, \\hat{\\Psi}\\) dobiera siÄ™ tak, aby maksymalizowaÅ‚y \\(\\ell(\\Lambda,\\Psi)\\), co odpowiada minimalizacji funkcji rozbieÅ¼noÅ›ci:\n\\[\nF(\\Lambda,\\Psi) = \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) - \\log |S| - p.\n\\]\nPowyÅ¼sza miara rozbieÅ¼noÅ›ci powstaje z odlegÅ‚oÅ›ci Kullbacka-Leiblera miÄ™dzy rozkÅ‚adami normalnymi \\(\\mathcal{N}_p(\\mu, \\Sigma)\\) i \\(\\mathcal{N}_p(\\mu, S)\\) i jest rÃ³wna dokÅ‚adnie \\(2D_{KL}(S||\\Sigma)\\).\nProcedura obliczeniowa\nW praktyce:\n\nWybiera siÄ™ liczbÄ™ \\(m\\) czynnikÃ³w2.\nUstala siÄ™ poczÄ…tkowe wartoÅ›ci \\(\\Lambda, \\Psi\\)3.\nIteracyjnie poprawia siÄ™ parametry, rozwiÄ…zujÄ…c rÃ³wnania warunkÃ³w pierwszego rzÄ™du\n\n2Â wybÃ³r liczby czynnikÃ³w zostanie przedstawiony nieco pÃ³Åºniej3Â spsoby ewstÄ™pnej estymacji zostanÄ… omÃ³wione w dalszej czÄ™Å›ci\\[\n\\frac{\\partial \\ell}{\\partial \\Lambda} = 0, \\quad \\frac{\\partial \\ell}{\\partial \\Psi} = 0.\n\\]\n\nTakie postÄ™powanie iteracyjne prowadzi siÄ™ aÅ¼ do zbieÅ¼noÅ›ci funkcji wiarygodnoÅ›ci.\nWÅ‚asnoÅ›ci\n\nEstymatory ML sÄ… efektywne przy speÅ‚nieniu zaÅ‚oÅ¼enia o normalnoÅ›ci wielowymiarowej danych pierwotnych.\nUmoÅ¼liwiaja testy istotnoÅ›ci liczby czynnikÃ³w:\n\nHipoteza \\(H_0: \\Sigma = \\Lambda\\Lambda^\\top + \\Psi\\) vs \\(H_1: \\Sigma\\) dowolna.\nStatystyka testowa ma w przybliÅ¼eniu rozkÅ‚ad \\(\\chi^2\\).\n\n\nPozwalajÄ… teÅ¼ konstruowaÄ‡ przedziaÅ‚y ufnoÅ›ci dla Å‚adunkÃ³w czynnikowych.\nOgraniczenia\n\nWymagaja duÅ¼ej prÃ³by i speÅ‚nienia zaÅ‚oÅ¼enia normalnoÅ›ci wielowymiarowej.\nMoÅ¼e byÄ‡ numerycznie niestabilne, zwÅ‚aszcza gdy liczba czynnikÃ³w jest duÅ¼a w stosunku do liczby zmiennych.\nPrzy maÅ‚ych prÃ³bach lub silnym naruszeniu normalnoÅ›ci wyniki mogÄ… byÄ‡ obciÄ…Å¼one.\nMetoda osi gÅ‚Ã³wnych (ang. Principal Axis Factoring, PAF)\nIdea metody PAF\nW metodzie PAF znanej rÃ³wnieÅ¼ jako metoda czynnikÃ³w gÅ‚Ã³wnych, zakÅ‚adamy klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad \\mathrm{Cov}(\\mathbf{x}) = \\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nCelem jest znalezienie takiego \\(\\Lambda\\) i \\(\\Psi\\), aby zbliÅ¼yÄ‡ siÄ™ do macierzy kowariancji prÃ³bkowej \\(S\\). W odrÃ³Å¼nieniu od ML, PAF nie opiera siÄ™ na funkcji wiarygodnoÅ›ci ani na rozbieÅ¼noÅ›ci Kullbackaâ€“Leiblera, lecz maksymalizuje wariancjÄ™ wspÃ³lnÄ… zmiennych, traktujÄ…c czÄ™Å›Ä‡ specyficznÄ… \\((\\Psi)\\) jako resztÄ™.\nMacierz zredukowanych korelacji\nW metodzie Principal Axis Factoring (PAF) kluczowÄ… rolÄ™ odgrywa macierz zredukowanych korelacji. Punktem wyjÅ›cia jest macierz korelacji \\(\\mathbf{R}\\) pomiÄ™dzy zmiennymi obserwowanymi \\(\\mathbf{x}\\). Na diagonali tej macierzy stojÄ… jedynki, odzwierciedlajÄ…ce fakt, Å¼e kaÅ¼da zmienna jest w peÅ‚ni skorelowana sama ze sobÄ…. Jednak w modelu czynnikowym zakÅ‚adamy, Å¼e caÅ‚kowita wariancja zmiennej \\(x_j\\) moÅ¼e zostaÄ‡ podzielona na czÄ™Å›Ä‡ wspÃ³lnÄ… (zasoby zmiennoÅ›ci wspÃ³lnej - ang. communalities) i czÄ™Å›Ä‡ swoistÄ… (zasoby zmiennoÅ›ci swoistej - ang. uniqness):\n\\[\n1 = h_j^2 + \\psi_j, \\quad j=1,\\dots,p,\n\\]\ngdzie \\(h_j^2\\) oznacza zasÃ³b zmiennoÅ›ci wspÃ³lnej, a \\(\\psi_j\\) wariancjÄ™ swoistÄ…. W konstrukcji macierzy zredukowanych korelacji zamiast jedynek wstawia siÄ™ w diagonali wÅ‚aÅ›nie wartoÅ›ci \\(h_j^2\\). Otrzymujemy w ten sposÃ³b macierz\n\\[\n\\mathbf{R}^* = [r_{ij}^*], \\quad r_{jj}^* = h_j^2.\n\\]\nMacierz \\(\\mathbf{R}^*\\) ma wiÄ™c charakter â€zredukowanyâ€, poniewaÅ¼ na jej diagonali pozostaje tylko ta czÄ™Å›Ä‡ wariancji zmiennej, ktÃ³rÄ… model czynnikowy ma szansÄ™ wyjaÅ›niÄ‡. DziÄ™ki temu macierz ta moÅ¼e byÄ‡ przybliÅ¼ana przez strukturÄ™ \\(\\Lambda \\Lambda^\\top\\), co odpowiada wspÃ³lnej wariancji wszystkich zmiennych.\n\n\n\n\n\n\nWstÄ™pne oszacowania zasobÃ³w zmiennoÅ›ci wspÃ³lnej\n\n\n\nProblem polega na tym, Å¼e wartoÅ›ci \\(h_j^2\\) nie sÄ… znane a priori. Dlatego w praktyce stosuje siÄ™ rÃ³Å¼ne metody wstÄ™pnego ich wyznaczania, ktÃ³re mogÄ… byÄ‡ nastÄ™pnie udoskonalane iteracyjnie w kolejnych krokach procedury PAF. Do najczÄ™Å›ciej stosowanych metod naleÅ¼Ä…:\n\nÅ›rednia arytmetyczna wspÃ³Å‚czynnikÃ³w korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\frac{1}{m}\\sum_{j'=1}^m r_{jj'},\\quad j\\ne j'\n\\]\nmaksymalna wartoÅ›Ä‡ bezwzglÄ™dna wspÃ³Å‚czynnikÃ³w korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\max_{j'}|r_{jj'}|, \\quad j\\ne j',\n\\]\nwspÃ³Å‚czynnik determinacji wielokrotnej danej zmiennej z innymi zmiennymi (najczÄ™Å›ciej stosowana i wykorzystywana przez R) \\[\nh_j^2=R^2_{j\\cdot 1,2,\\ldots,m},\n\\]\nformuÅ‚a triad \\[\nh_j^2=\\frac{r_{jj'}r_{jj''}}{r_{j'j''}}, \\quad j\\ne j' \\ne j''\n\\] gdzie \\(r_{jj'}, r_{jj''}\\) - dwie najwyÅ¼sze wartoÅ›ci wspÃ³Å‚czynnikÃ³w korelacji \\(j\\)-tej zmiennej z innymi zmiennymi.\n\n\n\nRozkÅ‚ad na wartoÅ›ci wÅ‚asne\nW metodzie PAF zakÅ‚adamy, Å¼e tylko czÄ™Å›Ä‡ wariancji kaÅ¼dej zmiennej jest wspÃ³lna. Oznacza to, Å¼e zamiast peÅ‚nej macierzy korelacji \\(\\mathbf{R}\\), rozwaÅ¼amy macierz zredukowanych korelacji: \\[\n\\mathbf{R}^* = \\mathbf{R} - \\Psi,\n\\] gdzie na diagonali znajdujÄ… siÄ™ oszacowane zasoby zmiennoÅ›ci wspÃ³lnej \\(\\hat{h}_j^2\\), zamiast jedynek.\nNastÄ™pnie wykonujemy dekompozycjÄ™ spektralnÄ… tej macierzy: \\[\n\\mathbf{R}^* = \\mathbf{Q}^* \\mathbf{D}^* {\\mathbf{Q}^*}^\\top,\n\\] gdzie \\(\\mathbf{Q}^*\\) i \\(\\mathbf{D}^*\\) sÄ… odpowiednio wektorami i wartoÅ›ciami wÅ‚asnymi macierzy \\(\\mathbf{R}^*\\).\nEstymator Å‚adunkÃ³w czynnikowych w PAF ma wiÄ™c postaÄ‡ \\[\n\\hat{\\Lambda} = \\mathbf{Q}^*_m (\\mathbf{D}^*_m)^{1/2},\n\\]\nbazujÄ…cÄ… na zmodyfikowanej macierzy korelacji, w ktÃ³rej uwzglÄ™dniono oszacowane komunalnoÅ›ci.\nPoniewaÅ¼ \\(\\hat{h}_j^2\\) same zaleÅ¼Ä… od Å‚adunkÃ³w (sÄ… ich sumÄ… kwadratÃ³w), w praktyce stosuje siÄ™ procedurÄ™ iteracyjnÄ…: zaczynamy od pewnych wartoÅ›ci poczÄ…tkowych, obliczamy dekompozycjÄ™ spektralnÄ…, aktualizujemy komunalnoÅ›ci i powtarzamy procedurÄ™ aÅ¼ do zbieÅ¼noÅ›ci.\nIteracyjna poprawa komunalnoÅ›ci\nPoniewaÅ¼ poczÄ…tkowe komunalnoÅ›ci sÄ… przybliÅ¼one, PAF stosuje procedurÄ™ iteracyjnÄ…:\n\nSzacujemy \\(\\Lambda\\) na podstawie bieÅ¼Ä…cego \\(\\mathbf{R}^*\\).\nObliczamy nowe zasoby zmiennoÅ›ci wspÃ³lnej \\(h_j^2 = \\sum_{k=1}^m \\lambda_{jk}^2\\).\nWstawiamy je na przekÄ…tnej \\(\\mathbf{R}^*\\) zamiast starych wartoÅ›ci.\nPowtarzamy rozkÅ‚ad wartoÅ›ci wÅ‚asnych.\n\nProces powtarza siÄ™ aÅ¼ do zbieÅ¼noÅ›ci, czyli stabilizacji Å‚adunkÃ³w czynnikowych i zasobÃ³w zmiennoÅ›ci wspÃ³lnej.\nWÅ‚asnoÅ›ci\n\n\nDopasowanie do wariancji wspÃ³lnej â€“ PAF minimalizuje rÃ³Å¼nice pomiÄ™dzy macierzÄ… zredukowanych korelacji \\(\\mathbf{R}^*\\) a aproksymacjÄ… \\(\\Lambda \\Lambda^\\top\\). Sskupia siÄ™ na wariancji wspÃ³lnej.\n\nIteracyjnoÅ›Ä‡ oszacowaÅ„ â€“ estymatory w PAF powstajÄ… w procesie iteracyjnym, w ktÃ³rym kolejne przybliÅ¼enia komunalnoÅ›ci sÄ… poprawiane na podstawie sumy kwadratÃ³w aktualnych Å‚adunkÃ³w czynnikowych. DziÄ™ki temu metoda zbiega do rozwiÄ…zaÅ„ lepiej oddajÄ…cych strukturÄ™ wspÃ³lnÄ… niÅ¼ proste metody jednorazowe.\n\nNiestandaryzowana postaÄ‡ estymatorÃ³w â€“ rozwiÄ…zania PAF mogÄ… zaleÅ¼eÄ‡ od przyjÄ™tych wartoÅ›ci poczÄ…tkowych \\(h_j^2\\). RÃ³Å¼ne wybory startowe mogÄ… prowadziÄ‡ do nieco innych estymatorÃ³w, choÄ‡ w praktyce po kilku iteracjach zbieÅ¼noÅ›Ä‡ do stabilnego rozwiÄ…zania jest zazwyczaj dobra.\n\nInterpretowalnoÅ›Ä‡ â€“ poniewaÅ¼ oszacowane Å‚adunki czynnikowe odzwierciedlajÄ… wyÅ‚Ä…cznie czÄ™Å›Ä‡ wspÃ³lnÄ… wariancji, interpretacja czynnikÃ³w uzyskanych metodÄ… PAF jest bliÅ¼sza teoretycznemu modelowi czynnikowemu niÅ¼ w przypadku metod opartych na PCA.\nOgraniczenia\n\n\nBrak optymalnoÅ›ci w sensie funkcji wiarygodnoÅ›ci â€“ w przeciwieÅ„stwie do metody najwiÄ™kszej wiarygodnoÅ›ci (ML), estymatory PAF nie majÄ… znanych wÅ‚asnoÅ›ci asymptotycznych, takich jak efektywnoÅ›Ä‡ czy zgodnoÅ›Ä‡ w sensie probabilistycznym. SÄ… bardziej heurystyczne niÅ¼ Å›ciÅ›le statystyczne.\n\nZaleÅ¼noÅ›Ä‡ od wartoÅ›ci poczÄ…tkowych komunalnoÅ›ci â€“ oszacowania poczÄ…tkowe wpÅ‚ywajÄ… na przebieg iteracji i mogÄ… prowadziÄ‡ do lokalnych minimÃ³w. W praktyce wybÃ³r metody startowej (np. \\(R^2\\), Å›rednia korelacja, â€¦) ma znaczenie dla szybkoÅ›ci i stabilnoÅ›ci algorytmu.\n\nMoÅ¼liwoÅ›Ä‡ uzyskania ujemnych komunalnoÅ›ci â€“ w niektÃ³rych przypadkach iteracje mogÄ… prowadziÄ‡ do oszacowaÅ„ \\(h_j^2 &lt; 0\\) (tzw. przypadek Haywooda), co jest sprzeczne z definicjÄ… wariancji wspÃ³lnej. WÃ³wczas konieczne stosowanie innych metod estymacji Å‚adunkÃ³w.\n\nMniejsza przydatnoÅ›Ä‡ przy maÅ‚ych prÃ³bach â€“ poniewaÅ¼ metoda nie opiera siÄ™ na peÅ‚nym modelu statystycznym, jej wÅ‚asnoÅ›ci sÄ… mniej stabilne przy niewielkich licznoÅ›ciach obserwacji. Wyniki mogÄ… byÄ‡ wÃ³wczas silnie zaleÅ¼ne od przypadkowych fluktuacji w danych.\n\nBrak testÃ³w statystycznych dopasowania modelu â€“ w odrÃ³Å¼nieniu od metody ML, PAF nie pozwala na formalne testowanie hipotez o liczbie czynnikÃ³w czy jakoÅ›ci dopasowania modelu do danych.\nMetoda sÅ‚adowych gÅ‚Ã³wnych (ang. Principal Component Method)\nMetoda sÅ‚adowych gÅ‚Ã³wnych naleÅ¼y do klasy metod wspÃ³lnotowych, czyli takich, ktÃ³re zakÅ‚adajÄ… klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\quad \\Sigma = \\Lambda\\Lambda^\\top + \\Psi.\n\\]\nCelem jest oszacowanie macierzy Å‚adunkÃ³w \\(\\Lambda\\), tak aby jak najlepiej odtworzyÄ‡ czÄ™Å›Ä‡ wspÃ³lnÄ… wariancji.\nIdea metody\nW metodzie PCM zakÅ‚adamy, Å¼e caÅ‚a wariancja zmiennej jest wariancjÄ… wspÃ³lnÄ…, tzn. \\[\nh_j^2 = 1, \\quad j=1,\\ldots,p.\n\\]\nOznacza to, Å¼e macierz zredukowanych korelacji jest po prostu zwykÅ‚Ä… macierzÄ… korelacji \\(\\mathbf{R}\\): \\[\n\\mathbf{R} = \\Lambda \\Lambda^\\top + \\Psi,\n\\] przy czym w PCM przyjmujemy \\(\\Psi = \\mathbf{0}\\).\nNastÄ™pnie wykonujemy dekompozycjÄ™ spektralnÄ… \\[\n\\mathbf{R} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^\\top,\n\\] gdzie:\n\n\n\\(\\mathbf{Q} = (q_1, q_2, \\ldots, q_p)\\) â€“ to macierz ortonormalnych wektorÃ³w wÅ‚asnych,\n\n\\(\\mathbf{D} = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p)\\) â€“ to macierz wartoÅ›ci wÅ‚asnych uporzÄ…dkowanych malejÄ…co.\n\nJeÅ›li chcemy oszacowaÄ‡ model z \\(m\\) czynnikami, to bierzemy najwiÄ™ksze \\(m\\) wartoÅ›ci wÅ‚asne i odpowiadajÄ…ce im wektory wÅ‚asne. Estymator Å‚adunkÃ³w czynnikowych jest wtedy rÃ³wny \\[\n\\hat{\\Lambda} = \\mathbf{Q}_m \\mathbf{D}_m^{1/2},\n\\] gdzie \\(\\mathbf{Q}_m = (q_1,\\ldots,q_m)\\), a \\(\\mathbf{D}_m = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_m)\\).\nWidzimy wiÄ™c, Å¼e w PCM Å‚adunki sÄ… wprost pierwiastkami z najwiÄ™kszych wartoÅ›ci wÅ‚asnych pomnoÅ¼onymi przez odpowiadajÄ…ce im wektory wÅ‚asne.\nProcedura estymacji4\n\n\nKonstruujemy macierz korelacji \\(\\mathbf{R}\\).\nObliczamy rozkÅ‚ad wartoÅ›ci i wektorÃ³w wÅ‚asnych macierzy \\(\\mathbf{R}\\).\nWybieramy \\(m\\) najwiÄ™kszych wartoÅ›ci wÅ‚asnych (odpowiadajÄ…cych liczbie czynnikÃ³w w modelu).\nNa tej podstawie konstruujemy macierz Å‚adunkÃ³w czynnikowych \\(\\Lambda\\).\n4Â tu widaÄ‡ najwiÄ™kszÄ… rÃ³Å¼nicÄ™ pomiÄ™cy PCM a PAF; w metodzie PCM wystÄ™pujÄ™ jedna iteracja estymacji Å‚adunkÃ³wWÅ‚asnoÅ›ci\n\n\nZgodnoÅ›Ä‡ z modelem czynnikowym â€“ metoda dÄ…Å¼y do aproksymacji struktury wspÃ³lnej wariancji, a nie caÅ‚kowitej wariancji.\n\nZbieÅ¼noÅ›Ä‡ do stabilnych oszacowaÅ„ â€“ iteracyjne poprawki komunalnoÅ›ci pozwalajÄ… uzyskaÄ‡ estymatory spÃ³jne z zaÅ‚oÅ¼eniami modelu.\n\nÅatwoÅ›Ä‡ interpretacji â€“ podobnie jak PCA, metoda bazuje na analizie spektralnej wartoÅ›ci wÅ‚asnych, co uÅ‚atwia intuicyjne rozumienie struktury danych.\nOgraniczenia\n\n\nBrak optymalnoÅ›ci statystycznej â€“ podobnie jak PAF, metoda nie ma wÅ‚asnoÅ›ci estymatorÃ³w opartych na funkcji wiarygodnoÅ›ci (ML).\n\nZaleÅ¼noÅ›Ä‡ od poczÄ…tkowych oszacowaÅ„ komunalnoÅ›ci â€“ nieprawidÅ‚owy wybÃ³r startowy moÅ¼e utrudniÄ‡ uzyskanie sensownych rozwiÄ…zaÅ„.\n\nHaywood case â€“ zdarza siÄ™, Å¼e zasoby zmiennoÅ›ci wspÃ³lnej mogÄ… przyjmowaÄ‡ wartoÅ›ci ujemne.\nMetoda minimalizacji reszt (ang. MINRES)\nIdea metody MINRES\nW modelu czynnikowym przyjmujemy, Å¼e macierz kowariancji (lub korelacji) ma postaÄ‡ \\[\n\\Sigma = \\Lambda \\Lambda' + \\Psi,\n\\] gdzie \\(\\Lambda\\) to macierz Å‚adunkÃ³w czynnikowych, a \\(\\Psi = \\mathrm{diag}(\\psi_1,\\ldots,\\psi_p)\\) to macierz wariancji swoistych.\nW metodzie MINRES nie prÃ³bujemy dokÅ‚adnie odtworzyÄ‡ caÅ‚ej macierzy \\(\\Sigma\\). Zamiast tego minimalizujemy reszty pozadiagonalne, czyli rÃ³Å¼nice miÄ™dzy obserwowanÄ… macierzÄ… korelacji \\(\\mathbf{R}\\) a macierzÄ… odtworzonÄ… z modelu \\(\\Lambda \\Lambda^\\top + \\Psi\\), przy czym skupiamy siÄ™ wyÅ‚Ä…cznie na elementach pozadiagonalnych.\nFunkcja kryterialna\nFormalnie minimalizowana jest suma kwadratÃ³w reszt poza przekÄ…tnÄ… \\[\nF(\\Lambda, \\Psi) = \\sum_{i \\neq j} \\Big( r_{ij} - \\hat{r}_{ij} \\Big)^2,\n\\] gdzie:\n\n\n\\(r_{ij}\\) to element macierzy korelacji empirycznej \\(\\mathbf{R}\\),\n\n\\(\\hat{r}_{ij}\\) to element macierzy odtworzonej \\(\\Lambda \\Lambda^\\top + \\Psi\\),\nelementy diagonalne nie sÄ… uwzglÄ™dniane (bo zawsze odtwarzane sÄ… przez normalizacjÄ™ zmiennych).\n\nMoÅ¼na to zapisaÄ‡ rÃ³wnowaÅ¼nie jako \\[\nF(\\Lambda) = | \\mathbf{R} - (\\Lambda \\Lambda' + \\Psi)|^2_{off},\n\\] gdzie \\(|\\cdot|_{off}\\) oznacza normÄ™ Frobeniusa liczona tylko na czÄ™Å›ciach pozadiagonalnych macierzy.\nProcedura estymacyjna\n\nZaczynamy od przybliÅ¼onych wartoÅ›ci komunalnoÅ›ci \\(\\hat{h}_j^2\\), tak jak w PAF.\nBudujemy macierz reszt \\[\n\\mathbf{U} = \\mathbf{R} - (\\Lambda \\Lambda^\\top + \\Psi).\n\\]\n\nSzukamy takich Å‚adunkÃ³w \\(\\Lambda\\), ktÃ³re minimalizujÄ… sumÄ™ kwadratÃ³w elementÃ³w \\(\\mathbf{U}\\) poza przekÄ…tnÄ….\nW praktyce problem redukuje siÄ™ do iteracyjnego rozwiÄ…zywania ukÅ‚adÃ³w rÃ³wnaÅ„ wÅ‚asnych, bardzo podobnie jak w PAF, ale z innym warunkiem minimalizacji (PAF dopasowuje wartoÅ›ci wÅ‚asne macierzy zredukowanych korelacji, MINRES â€“ reszty pozadiagonalne).\nZwiÄ…zek z dekompozycjÄ… spektralnÄ…\nW przeciwieÅ„stwie do PCM czy PAF, metoda MINRES nie ma bezpoÅ›redniego prostego rozwiÄ…zania w postaci pierwiastkÃ³w z wartoÅ›ci wÅ‚asnych. Wymaga zastosowania iteracyjnych algorytmÃ³w numerycznych, ktÃ³re szukajÄ… \\(\\Lambda\\) minimalizujÄ…cej \\(F(\\Lambda)\\). Jednak podobnie jak w PAF, punktem startowym mogÄ… byÄ‡ wektory wÅ‚asne macierzy zredukowanych korelacji. NastÄ™pnie algorytm minimalizacji dopasowuje Å‚adunki tak, by reszty pozadiagonalne byÅ‚y jak najmniejsze.\nWÅ‚aÅ›ciwoÅ›ci i ograniczenia\n\n\nMINRES skupia siÄ™ tylko na korelacjach pomiÄ™dzy zmiennymi, ignorujÄ…c elementy diagonalne â€“ co sprawia, Å¼e estymacja jest mniej wraÅ¼liwa na problem ujemnych komunalnoÅ›ci (tzw. Heywood cases).\nMetoda jest relatywnie stabilna numerycznie i dobrze sprawdza siÄ™ przy duÅ¼ej liczbie zmiennych.\nOgraniczeniem jest to, Å¼e wynik zaleÅ¼y od jakoÅ›ci poczÄ…tkowych oszacowaÅ„ zasobÃ³w zmiennoÅ›ci wspÃ³lnej. Przy zÅ‚ym wyborze startu moÅ¼liwa jest wolna zbieÅ¼noÅ›Ä‡ albo zbieÅ¼noÅ›Ä‡ do lokalnego minimum.\nMetoda uogÃ³lnionych najmniejszych kwadratÃ³w (ang. Generalized Least Squares, GLS)\nIdea metody\nGLS, podobnie jak MINRES czy ML, polega na porÃ³wnaniu macierzy obserwowanej \\(\\mathbf{S}\\) (kowariancji lub korelacji) z macierzÄ… odtworzonÄ… przez model czynnikowy \\(\\hat{\\Sigma} = \\Lambda \\Lambda^\\top + \\Psi\\). RÃ³Å¼nica w stosunku do MINRES polega na tym, Å¼e w GLS waÅ¼ymy reszty, czyli bÅ‚Ä™dy odwzorowania poszczegÃ³lnych elementÃ³w macierzy \\(\\mathbf{S}\\).\nFormalnie kryterium minimalizacji ma postaÄ‡ \\[\nF_{\\text{GLS}}(\\Lambda, \\Psi) = \\mathrm{tr}\\Big[ \\big( S - \\hat{\\Sigma} \\big) W \\big( S - \\hat{\\Sigma} \\big) W \\Big],\n\\]\ngdzie \\(W\\) to macierz wag, zwykle przyjmowana jako odwrotnoÅ›Ä‡ (lub pseudoodwrotnoÅ›Ä‡) wariancji estymatora elementÃ³w macierzy \\(\\mathbf{S}\\).\nW przeciwieÅ„stwie do MINRES (gdzie wszystkie reszty traktowane sÄ… jednakowo), w GLS rÃ³Å¼ne elementy macierzy kowariancji otrzymujÄ… rÃ³Å¼ne wagi. Wagi te wynikajÄ… z asymptotycznych wÅ‚asnoÅ›ci estymatora macierzy kowariancji i uwzglÄ™dniajÄ… fakt, Å¼e elementy macierzy nie sÄ… niezaleÅ¼ne i majÄ… rÃ³Å¼ne wariancje. DziÄ™ki temu GLS jest bardziej efektywny statystycznie niÅ¼ MINRES, ale jednoczeÅ›nie mniej wymagajÄ…cy niÅ¼ ML (ktÃ³ry zakÅ‚ada peÅ‚nÄ… normalnoÅ›Ä‡ wielowymiarowÄ…).\nWÅ‚asnoÅ›ci\n\nEstymatory GLS sÄ… spÃ³jne i asymptotycznie efektywne w klasie metod najmniejszych kwadratÃ³w, przy zaÅ‚oÅ¼eniu poprawnej specyfikacji modelu.\nGLS, podobnie jak ML, uwzglÄ™dnia strukturÄ™ wariancji elementÃ³w macierzy \\(\\mathbf{S}\\), co czyni go bardziej precyzyjnym niÅ¼ MINRES.\nZ drugiej strony GLS jest mniej czuÅ‚y na naruszenie zaÅ‚oÅ¼enia normalnoÅ›ci niÅ¼ ML, dlatego bywa rekomendowany przy wiÄ™kszych odchyleniach od normalnoÅ›ci.\nOgraniczenia\n\nProcedura GLS jest obliczeniowo trudniejsza niÅ¼ MINRES, poniewaÅ¼ wymaga oszacowania (lub przyjÄ™cia) odpowiedniej macierzy wag.\nW praktyce GLS bywa niestabilny przy maÅ‚ych prÃ³bach lub przy silnych wspÃ³Å‚liniowoÅ›ciach zmiennych.\nW implementacjach programowych czÄ™sto stosuje siÄ™ GLS jako kompromis pomiÄ™dzy prostym MINRES a wymagajÄ…cym ML.\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nIstniejÄ… rÃ³wnieÅ¼ inne metody estymacji Å‚adunkÃ³w czynnikowych, jak metody bayesowskie, czy metody z regularyzacjÄ… LASSO ale nie sÄ… one czÄ™Å›ciÄ… tego opracowania.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynnikÃ³w",
    "href": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynnikÃ³w",
    "title": "Analiza czynnikowa",
    "section": "Oceny dopasowania modelu i kryteria doboru liczby czynnikÃ³w",
    "text": "Oceny dopasowania modelu i kryteria doboru liczby czynnikÃ³w\nOcena dopasowania modelu EFA opiera siÄ™ na kilku uzupeÅ‚niajÄ…cych siÄ™ perspektywach: globalnym dopasowaniu implikowanej macierzy kowariancji do macierzy empirycznej, analizie reszt korelacyjnych, doborze liczby czynnikÃ³w, stabilnoÅ›ci rozwiÄ…zania oraz jakoÅ›ci lokalnej (Å‚adunki i zasoby zmiennoÅ›ci wspÃ³lnej). PoniÅ¼ej przedstawiam najwaÅ¼niejsze procedury wraz z ich interpretacjÄ… oraz typowymi puÅ‚apkami.\nProporcja wyjaÅ›nionej wariancji przez czynniki\nProporcja wariancji wyjaÅ›nionej przez model czynnikowy, czyli stosunek sumy wariancji wspÃ³lnej do caÅ‚kowitej wariancji wszystkich zmiennych, stanowi podstawowÄ… miarÄ™ jakoÅ›ci dopasowania. W przypadku standaryzowanych zmiennych caÅ‚kowita wariancja wynosi \\(p\\), wiÄ™c proporcja ta ma postaÄ‡ \\[\n\\text{Proporcja wyjaÅ›nionej wariancji} = \\frac{\\sum_{j=1}^p h_j^2}{p}.\n\\] WyÅ¼sze wartoÅ›ci (np. powyÅ¼ej \\(0,6\\)) wskazujÄ… na dobrÄ… reprezentacjÄ™ zmiennych przez czynniki, natomiast niskie wartoÅ›ci (np. poniÅ¼ej \\(0,4\\)) sugerujÄ…, Å¼e model nie uchwytuje istotnej czÄ™Å›ci struktury danych. Jednak sama proporcja nie uwzglÄ™dnia liczby czynnikÃ³w ani zÅ‚oÅ¼onoÅ›ci modelu, dlatego powinna byÄ‡ interpretowana w kontekÅ›cie innych wskaÅºnikÃ³w dopasowania.\nTest chi-kwadrat\nW metodzie ML zostaÅ‚ przedstawiony test dopasowania oparty na maximum likelihood. Przy zaÅ‚oÅ¼eniu normalnoÅ›ci wielowymiarowej i zidentyfikowanym modelu postaci \\[\n\\Sigma=\\Lambda\\Lambda^\\top + \\Psi\n\\] testujemy hipotezÄ™ \\(H_0:\\ \\Sigma(\\Lambda,\\Psi)=S\\) w populacji, gdzie \\(S\\) oznacza macierz kowariancji (lub korelacji) z prÃ³by. Statystyka \\(\\chi^2\\) roÅ›nie wraz z pogarszajÄ…cym siÄ™ dopasowaniem (niestety duÅ¼e prÃ³by sprzyjajÄ… odrzucaniu nawet dobrze dopasowanych modeli, a naruszenia normalnoÅ›ci mogÄ… zawyÅ¼aÄ‡ lub zaniÅ¼aÄ‡ wynik).\nWskaÅºnik RMSEA\nWskaÅºnik root mean square error of approximation (RMSEA) mierzy bÅ‚Ä…d aproksymacji na jednostkÄ™ stopnia swobody i moÅ¼na go interpretowaÄ‡ jako â€bÅ‚Ä…d w populacjiâ€, nie tylko w prÃ³bie. Definiujemy go jako \\[\n\\mathrm{RMSEA}=\\sqrt{\\max\\left\\{\\frac{\\chi^2-df}{df(n-1)},0\\right\\}},\n\\] a ocenÄ™ uzupeÅ‚niamy o przedziaÅ‚ ufnoÅ›ci oparty na niecentralnym rozkÅ‚adzie chi-kwadrat. WartoÅ›ci rzÄ™du \\(0,05-0,08\\) tradycyjnie uznawane sÄ… za akceptowalne, traktujÄ…c progi orientacyjnie: wzrost liczby zmiennych i stopni swobody sprzyja niÅ¼szym RMSEA, natomiast maÅ‚e prÃ³by destabilizujÄ… oszacowanie.\nAnaliza reszt\nAnaliza reszt macierzy korelacji stanowi podstawowÄ… kontrolÄ™ lokalnego dopasowania, niezaleÅ¼nie od sposobu estymacji. Wyznaczamy reszty \\(r_{ij}-\\hat r_{ij}\\) i przeglÄ…damy rozkÅ‚ad wartoÅ›ci bezwzglÄ™dnych, a dokÅ‚adnie odsetek przekraczajÄ…cych praktyczne progi (np. \\(0,05\\)). WskaÅºniki zbiorcze, takie jak RMSR (root mean square residual) oraz SRMR (standardized RMSR), agregujÄ… wielkoÅ›Ä‡ reszt poza diagonalÄ… - mniejsze wartoÅ›ci Å›wiadczÄ… o lepszym dopasowaniu. Mapa ciepÅ‚a reszt uÅ‚atwia wykrywanie klastrÃ³w niedopasowania sugerujÄ…cych brakujÄ…cy czynnik lub zbyt maÅ‚Ä… liczbÄ™ czynnikÃ³w.\nKryteria informacyjne\nKryteria informacyjne, takie jak AIC i BIC, sÅ‚uÅ¼Ä… do porÃ³wnywania modeli o rÃ³Å¼nej liczbie czynnikÃ³w, karzÄ…c nadmiernÄ… zÅ‚oÅ¼onoÅ›Ä‡. Definiujemy je przez logarytm funkcji wiarogodnoÅ›ci i liczbÄ™ parametrÃ³w. BIC silniej faworyzuje prostsze modele przy duÅ¼ych prÃ³bach. Bardzo waÅ¼ne jest aby uÅ¼ywaÄ‡ tych metod do porÃ³wnywania modeli otrzymanych tÄ… samÄ… metodÄ….\nInne wskaÅºniki dopasowania\nWskaÅºniki â€globalneâ€ starszej generacji, takie jak GFI i AGFI (goodness of fit index, adjusted GFI), oceniajÄ… proporcjÄ™ wariancji/kowariancji wyjaÅ›nionej przez model. SÄ… wraÅ¼liwe na rozmiar prÃ³by i liczbÄ™ zmiennych, skÅ‚onne do optymizmu w duÅ¼ych modelach i do pesymizmu przy maÅ‚ej liczbie stopni swobody. MoÅ¼emy je traktowaÄ‡ pomocniczo, kÅ‚adÄ…c wiÄ™kszy nacisk na RMSEA oraz analizÄ™ reszt.\nAnaliza wartoÅ›ci wÅ‚asnych macierzy reszt uzupeÅ‚nia powyÅ¼sze podejÅ›cia. Po wyodrÄ™bnieniu \\(m\\) czynnikÃ³w obliczamy resztowÄ… macierz korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) i badaÄ‡ jej wartoÅ›ci wÅ‚asne. DuÅ¼e dodatnie wartoÅ›ci wÅ‚asne sygnalizujÄ… pozostawionÄ… wspÃ³lnÄ… wariancjÄ™ (niedomiar czynnikÃ³w) lub struktury lokalne.\nJakoÅ›Ä‡ lokalnÄ… rozwiÄ…zania oceniaÄ‡ przez zasoby zmiennoÅ›ci wspÃ³lnej i swoistej. \\[\nh_j^2=\\sum_{k=1}^{m}\\lambda_{jk}^{2}\n\\] mierzÄ… czÄ™Å›Ä‡ wariancji zmiennej \\(x_j\\) wyjaÅ›nionÄ… przez czynniki, bardzo niskie \\(h_j^2\\) wskazujÄ… sÅ‚abÄ… reprezentacjÄ™ zmiennej, natomiast bardzo wysokie â€” wraz z ryzykiem ujemnych \\(\\Psi_j\\) (przypadki Haywooda) â€” mogÄ… sygnalizowaÄ‡ dopasowanie wymuszone lub niewÅ‚aÅ›ciwÄ… liczebnoÅ›Ä‡ czynnikÃ³w. Sumy kwadratÃ³w Å‚adunkÃ³w per czynnik odzwierciedlajÄ… wyjaÅ›nionÄ… wspÃ³lnÄ… wariancjÄ™ i sÅ‚uÅ¼Ä… do oceny rÃ³wnomiernoÅ›ci wkÅ‚adu czynnikÃ³w.\nW rozwiÄ…zaniach dopuszczajacych korelacje pomiedzy czynnikami dodatkowym aspektem dopasowania jest macierz korelacji czynnikÃ³w \\(\\Phi\\). Bardzo wysokie korelacje miÄ™dzy czynnikami sugerujÄ… nadmiarowoÅ›Ä‡ i potencjalne przeparametryzowanie. WÃ³wczas warto rozwaÅ¼yÄ‡ redukcjÄ™ liczby czynnikÃ³w lub alternatywne struktury.\nNajbardziej znane kryteria doboru liczby czynnikÃ³w to:\nKryterium wykresu osypiska (Scree plot, Cattell)\nNa osi poziomej odkÅ‚adamy kolejne wartoÅ›ci wÅ‚asne, a na pionowej ich wielkoÅ›Ä‡. Punktem granicznym jest miejsce, gdzie wykres â€zaÅ‚amuje siÄ™â€ i przechodzi w â€osypiskoâ€ â€“ od tego miejsca czynniki interpretowane sÄ… jako szum.\n\nZalety: wizualna intuicja, Å‚atwe zastosowanie.\nWady: czÄ™sto subiektywnoÅ›Ä‡ w okreÅ›leniu miejsca â€Å‚okciaâ€, szczegÃ³lnie gdy krzywa nie ma wyraÅºnego zaÅ‚amania.\nAnaliza rÃ³wnolegÅ‚a (Parallel analysis, Horn)\nPolega na porÃ³wnaniu wartoÅ›ci wÅ‚asnych dla danych empirycznych z wartoÅ›ciami wÅ‚asnymi uzyskanymi dla danych losowych o tej samej strukturze (ta sama liczba zmiennych i obserwacji). Zatrzymuje siÄ™ te czynniki, ktÃ³rych wartoÅ›ci wÅ‚asne przewyÅ¼szajÄ… np. 95. percentyl rozkÅ‚adu wartoÅ›ci losowych.\n\nZalety: jedna z najbardziej rekomendowanych metod, dobrze sprawdza siÄ™ w praktyce.\nWady: wymaga procedur symulacyjnych, wiÄ™kszej mocy obliczeniowej.\nKryterium MAP (Minimum Average Partial, Velicer)\nOpiera siÄ™ na analizie korelacji czÄ…stkowych. Stopniowo usuwa siÄ™ kolejne czynniki, a nastÄ™pnie oblicza Å›redniÄ… wartoÅ›Ä‡ kwadratu korelacji czÄ…stkowych. Liczba czynnikÃ³w odpowiadajÄ…ca minimum tej wartoÅ›ci uznawana jest za optymalnÄ….\n\nZalety: metoda oparta na minimalizacji resztowych zaleÅ¼noÅ›ci, obiektywna.\nWady: wraÅ¼liwa na naruszenia zaÅ‚oÅ¼eÅ„ modelu, mniej intuicyjna dla poczÄ…tkujÄ…cych.\nTesty statystyczne dopasowania (dla ML)\nPrzy estymacji metodÄ… najwiÄ™kszej wiarygodnoÅ›ci moÅ¼na zastosowaÄ‡ test chi-kwadrat dla porÃ³wnania modelu z \\(m\\) czynnikami z modelem peÅ‚nym. Sprawdza siÄ™, czy macierz implikowana przez model rÃ³Å¼ni siÄ™ istotnie od empirycznej. LiczbÄ™ czynnikÃ³w dobiera siÄ™ tak, aby model byÅ‚ jeszcze akceptowalny, ale nie przeparametryzowany.\n\nZalety: formalne podejÅ›cie statystyczne.\nWady: silna wraÅ¼liwoÅ›Ä‡ na licznoÅ›Ä‡ prÃ³by i zaÅ‚oÅ¼enie normalnoÅ›ci wielowymiarowej; w duÅ¼ych prÃ³bach nawet dobre modele mogÄ… byÄ‡ odrzucane.\nKryteria informacyjne (AIC, BIC, CAIC)\nPorÃ³wnujÄ… modele o rÃ³Å¼nej liczbie czynnikÃ³w, rÃ³wnowaÅ¼Ä…c dopasowanie (log-wiarygodnoÅ›Ä‡) i zÅ‚oÅ¼onoÅ›Ä‡ (liczbÄ™ parametrÃ³w). Optymalna liczba czynnikÃ³w to ta, dla ktÃ³rej wartoÅ›Ä‡ kryterium jest minimalna.\n\nZalety: uwzglÄ™dniajÄ… karÄ™ za nadmiernÄ… zÅ‚oÅ¼onoÅ›Ä‡, dobrze sprawdzajÄ… siÄ™ w porÃ³wnaniach.\nWady: wartoÅ›ci kryteriÃ³w sÄ… zaleÅ¼ne od metody estymacji, wiÄ™c porÃ³wnywaÄ‡ moÅ¼na tylko modele oszacowane tÄ… samÄ… metodÄ….\nAnaliza reszt i spektrum wartoÅ›ci wÅ‚asnych macierzy reszt\nPo przyjÄ™ciu liczby czynnikÃ³w oblicza siÄ™ macierz reszt korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) . JeÅ›li w resztach (poza przekÄ…tnÄ…) pozostajÄ… duÅ¼e (co do wartoÅ›ci bezwzglÄ™dnej) wartoÅ›ci wÅ‚asne, oznacza to, Å¼e nie wszystkie wspÃ³lne zaleÅ¼noÅ›ci zostaÅ‚y uchwycone i potrzebne sÄ… dodatkowe czynniki.\n\nZalety: pozwala oceniÄ‡ niedopasowanie â€lokalneâ€ i strukturalne.\nWady: wymaga bardziej zaawansowanej interpretacji.\nUdziaÅ‚ wyjaÅ›nionej wariancji\nW praktyce czÄ™sto wymaga siÄ™, aby caÅ‚kowita wyjaÅ›niona wariancja przekraczaÅ‚a okreÅ›lony prÃ³g (np. 50% w naukach spoÅ‚ecznych). Dodatkowo analizuje siÄ™ rÃ³wnomiernoÅ›Ä‡ wkÅ‚adu poszczegÃ³lnych czynnikÃ³w.\n\nZalety: intuicyjne i Å‚atwe do raportowania.\nWady: arbitralne progi, zaleÅ¼ne od liczby zmiennych i kontekstu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#eksploracyjna-analiza-czynnikowa",
    "href": "fa.html#eksploracyjna-analiza-czynnikowa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) â€“ wektor zmiennych obserwowalnych,\n\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) â€“ wektor Å›rednich,\n\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) â€“ macierz Å‚adunkÃ³w czynnikowych, ktÃ³rej element \\(\\lambda_{ij}\\) opisuje wpÅ‚yw czynnika \\(j\\) na zmiennÄ… \\(i\\),\n\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) â€“ wektor czynnikÃ³w latentnych (czynnikÃ³w wspÃ³lnych),\n\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) â€“ wektor skÅ‚adnikÃ³w specyficznych (unikalnych, bÅ‚Ä™dÃ³w pomiaru).\n\nZaÅ‚oÅ¼enia klasycznego modelu EFA\n\nRozkÅ‚ad czynnikÃ³w wspÃ³lnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne majÄ… Å›redniÄ… zero i macierz kowariancji rÃ³wnÄ… macierzy jednostkowej. To zaÅ‚oÅ¼enie oznacza, Å¼e czynniki sÄ… nieskorelowane i majÄ… wariancjÄ™ jednostkowÄ… (jest to standaryzacja wprowadzona dla identyfikowalnoÅ›ci modelu).\nRozkÅ‚ad skÅ‚adnikÃ³w specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzÄ… diagonalnÄ… o elementach dodatnich. Oznacza to, Å¼e bÅ‚Ä™dy sÄ… nieskorelowane miÄ™dzy sobÄ… oraz niezaleÅ¼ne od czynnikÃ³w \\(\\mathbf{f}\\).\nNiezaleÅ¼noÅ›Ä‡ czynnikÃ³w i bÅ‚Ä™dÃ³w \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyÅ¼szej konstrukcji wynika, Å¼e kowariancja zmiennych obserwowalnych jest sumÄ… czÄ™Å›ci wspÃ³lnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowÃ³d\n\n\n\nNiech losowy wektor obserwacji ma postaÄ‡ \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynnikÃ³w wspÃ³lnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor skÅ‚adnikÃ³w specyficznych. ZakÅ‚adamy, Å¼e \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazaÄ‡, Å¼e \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczegÃ³lnoÅ›ci przy \\(\\Phi=I_m\\), Å¼e mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a poniewaÅ¼ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancjÄ™ \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyraÅ¼amy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] KorzystajÄ…c z liniowoÅ›ci kowariancji i toÅ¼samoÅ›ci \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorÃ³w losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skoÅ„czonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDziÄ™ki zaÅ‚oÅ¼eniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikajÄ… i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] JeÅ›li dodatkowo przyjmiemy standardyzacjÄ™ czynnikÃ³w \\(\\Phi=I_m\\) (co jest konwencjÄ… identyfikacyjnÄ… modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego naleÅ¼aÅ‚o dowieÅ›Ä‡.\nWarto odnotowaÄ‡, Å¼e dowÃ³d nie wymaga niezaleÅ¼noÅ›ci \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym â€” wystarcza nieskorelowanie, aby zniknÄ™Å‚y skÅ‚adniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postaÄ‡ \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to moÅ¼na zastosowaÄ‡ tzw. whitening czynnikÃ³w \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co rÃ³wnowaÅ¼nie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), ktÃ³re prowadzÄ… do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to zwiÄ…zane z moÅ¼liwoÅ›ciÄ… przeprowadzania rÃ³Å¼nych transformacji czynnikÃ³w bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogÃ³lnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeÅ¼eli dokonamy transformacji ortogonalnej czynnikÃ³w \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzÄ… ortogonalnÄ…, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, Å¼e \\(\\Lambda\\) i \\(\\Phi\\) nie sÄ… jednoznacznie wyznaczone. RÃ³Å¼ne pary \\((\\Lambda, \\Phi)\\) mogÄ… prowadziÄ‡ do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczegÃ³lnoÅ›ci wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jednÄ… z takich rÃ³wnowaÅ¼nych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalnÄ… rolÄ™, poniewaÅ¼ jest miejscem, w ktÃ³rym spotykajÄ… siÄ™ dwa skÅ‚adniki zmiennoÅ›ci: wspÃ³lna i specyficzna. RozkÅ‚ad \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, Å¼e caÅ‚kowita wariancja i kowariancja obserwowanych zmiennych moÅ¼e byÄ‡ przedstawiona jako suma efektu wspÃ³lnych czynnikÃ³w oraz efektu specyficznego, indywidualnego dla kaÅ¼dej zmiennej.\nCzÄ™Å›Ä‡ \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspÃ³lne ÅºrÃ³dÅ‚o zmiennoÅ›ci, czyli wariancjÄ™ wyjaÅ›nianÄ… przez czynniki ukryte. To wÅ‚aÅ›nie ta czÄ™Å›Ä‡ umoÅ¼liwia redukcjÄ™ wymiaru â€“ wiele zmiennych obserwowanych moÅ¼na sprowadziÄ‡ do kilku czynnikÃ³w, ktÃ³re reprezentujÄ… gÅ‚Ã³wnÄ… strukturÄ™ zaleÅ¼noÅ›ci. Interpretacja czynnikÃ³w jako ukrytych wymiarÃ³w (np. inteligencja, poziom lÄ™ku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uproÅ›ciÄ‡ analizÄ™, ale takÅ¼e nadaÄ‡ jej znaczenie teoretyczne w danej dziedzinie badaÅ„.\nZ kolei \\(\\Psi\\) odpowiada za wariancjÄ™ unikalnÄ…, czyli tÄ™ czÄ™Å›Ä‡ zmiennoÅ›ci, ktÃ³ra nie jest wspÃ³Å‚dzielona z innymi zmiennymi. Obejmuje ona zarÃ³wno wariancjÄ™ czysto specyficznÄ… dla danej cechy, jak i wariancjÄ™ bÅ‚Ä™du pomiarowego. DziÄ™ki temu moÅ¼liwe jest odrÃ³Å¼nienie struktury gÅ‚Ä™bokiej (czynnikowej) od elementÃ³w przypadkowych i indywidualnych.\nPodsumowujÄ…c, znaczenie modelu czynnikowego polega na tym, Å¼e pozwala on wydzieliÄ‡ istotne, ukryte mechanizmy stojÄ…ce za wspÃ³Å‚zaleÅ¼noÅ›ciami zmiennych i oddzieliÄ‡ je od szumÃ³w specyficznych dla pojedynczych obserwacji. W praktyce oznacza to moÅ¼liwoÅ›Ä‡ redukcji liczby analizowanych zmiennych, uproszczenie opisu zÅ‚oÅ¼onych danych i pogÅ‚Ä™bienie interpretacji zjawisk spoÅ‚ecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynnikÃ³w w praktyce opiera siÄ™ przede wszystkim na analizie macierzy Å‚adunkÃ³w czynnikowych \\(\\Lambda\\). KaÅ¼dy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile zwiÄ…zku pomiÄ™dzy zmiennÄ… obserwowanÄ… \\(x_i\\) a czynnikiem \\(f_j\\). Im wyÅ¼sza wartoÅ›Ä‡ bezwzglÄ™dna Å‚adunku, tym wiÄ™kszy udziaÅ‚ danego czynnika w wyjaÅ›nianiu zmiennoÅ›ci konkretnej zmiennej. Na przykÅ‚ad w psychologii wysoki Å‚adunek czynnika na zmiennej opisujÄ…cej pamiÄ™Ä‡ krÃ³tkotrwaÅ‚Ä… i na zmiennej opisujÄ…cej zdolnoÅ›Ä‡ rozwiÄ…zywania problemÃ³w matematycznych moÅ¼e sugerowaÄ‡, Å¼e obie cechy sÄ… przejawem wspÃ³lnego czynnika â€“ inteligencji ogÃ³lnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#rotacje-czynnikÃ³w",
    "href": "fa.html#rotacje-czynnikÃ³w",
    "title": "Analiza czynnikowa",
    "section": "Rotacje czynnikÃ³w",
    "text": "Rotacje czynnikÃ³w\nRotacja czynnikÃ³w jest etapem analizy czynnikowej, ktÃ³rego celem jest poprawa interpretowalnoÅ›ci rozwiÄ…zania poprzez uproszczenie struktury Å‚adunkÃ³w czynnikowych. Matematycznie polega ona na zastosowaniu transformacji liniowej do macierzy Å‚adunkÃ³w \\(\\Lambda\\). JeÅ›li \\(\\Lambda\\) jest macierzÄ… \\(p \\times m\\) Å‚adunkÃ³w (gdzie \\(p\\) to liczba zmiennych, a \\(m\\) liczba czynnikÃ³w), to po rotacji otrzymujemy nowÄ… macierz Å‚adunkÃ³w \\[\n\\Lambda^* = \\Lambda T,\n\\] gdzie \\(T\\) jest macierzÄ… transformacji rotacyjnej o wymiarach \\(m \\times m\\). W zaleÅ¼noÅ›ci od wÅ‚asnoÅ›ci macierzy \\(T\\) wyrÃ³Å¼nia siÄ™ dwa gÅ‚Ã³wne typy rotacji: ortogonalne i skoÅ›ne (oblique).\nRotacje ortogonalne\nW przypadku rotacji ortogonalnych macierz \\(T\\) jest macierzÄ… ortogonalnÄ…, czyli speÅ‚nia warunek: \\[T^\\top T = TT^\\top = I_m.\\] Oznacza to, Å¼e czynniki po rotacji pozostajÄ… nieskorelowane (\\(\\Phi = I_m\\)).\nNajwaÅ¼niejsze rodzaje rotacji ortogonalnych:\n\nVarimax (Kaiser, 1958) - najczÄ™Å›ciej stosowana rotacja ortogonalna. Maksymalizuje wariancjÄ™ kwadratÃ³w Å‚adunkÃ³w w ramach kaÅ¼dego czynnika. Prowadzi do tego, Å¼e kaÅ¼da zmienna ma wysokie Å‚adunki tylko na jednym czynniku, a bliskie zeru na pozostaÅ‚ych. Funkcja celu \\[\nV = \\sum_{j=1}^m \\left[ \\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*4} - \\left(\\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*2}\\right)^2 \\right].\n\\]\nQuartimax - minimalizuje liczbÄ™ czynnikÃ³w potrzebnych do opisania kaÅ¼dej zmiennej, upraszczajÄ…c wiersze macierzy Å‚adunkÃ³w. Funkcja celu \\[\nQ = \\sum_{i=1}^p \\sum_{j=1}^m \\lambda_{ij}^{*4}.\n\\]\nEquamax - Å‚Ä…czy idee varimax i quartimax. Celem jest rÃ³wnowaÅ¼enie prostoty struktur wierszy i kolumn macierzy Å‚adunkÃ³w. Funkcja celu \\[\nE = \\frac12(Q + V).\n\\]\nBiquartimax - celem tej rotacji jest jednoczesne uproszczenie wierszy i kolumn macierzy Å‚adunkÃ³w. W praktyce Å‚Ä…czy zalety varimax i quartimax. Funkcja celu \\[\nBQ = \\alpha \\, Q + (1 - \\alpha) \\, V,\n\\] z modyfikacjÄ… wag, ktÃ³re rÃ³wnowaÅ¼Ä… wpÅ‚yw prostoty wierszy i kolumn. Zmienne majÄ… tendencjÄ™ do Å‚adowania siÄ™ mocno na jednym czynniku (jak w varimax), ale jednoczeÅ›nie ogranicza siÄ™ sytuacje, w ktÃ³rych jedna zmienna ma Å›rednie Å‚adunki na wielu czynnikach (jak w quartimax).\nRotacje skoÅ›ne (oblique)\nW przypadku rotacji skoÅ›nych macierz \\(T\\) nie musi byÄ‡ ortogonalna, wiÄ™c \\[\nT^\\top T \\neq I_m.\n\\] W efekcie rotowane czynniki mogÄ… byÄ‡ skorelowane, a macierz korelacji czynnikÃ³w \\(\\Phi\\) przyjmuje ogÃ³lnÄ… postaÄ‡ dodatnio okreÅ›lonÄ….\nPodstawowe rodzaje:\n\nOblimin (Jennrich & Sampson, 1966) - rodzina rotacji z parametrem \\(\\gamma\\), ktÃ³ry reguluje stopieÅ„ skoÅ›noÅ›ci. Dla \\(\\gamma = 0\\) rozwiÄ…zanie staje siÄ™ quartimax, a wiÄ™ksze \\(\\gamma\\) prowadzÄ… do wiÄ™kszej korelacji czynnikÃ³w. Funkcja celu \\[\nF(\\Lambda^*) = \\sum_{i=1}^p \\sum_{j=1}^m \\left(\\lambda_{ij}^{*2} - \\gamma \\frac{\\sum_{k=1}^m \\lambda_{ik}^{*2}}{m}\\right)^2.\n\\]\nPromax (Hendrickson & White, 1964) - rotacja skoÅ›na oparta na prostym podejÅ›ciu dwustopniowym. Najpierw stosuje siÄ™ rotacjÄ™ ortogonalnÄ… (najczÄ™Å›ciej varimax), nastÄ™pnie Å‚adunki sÄ… podnoszone do potÄ™gi \\(k\\) (zwykle 3 lub 4), aby wymusiÄ‡ prostÄ… strukturÄ™, i ponownie dopasowywane przy uÅ¼yciu metody najmniejszych kwadratÃ³w \\[\n\\tilde{\\lambda}{jk} = \\text{sign}(\\lambda^*_{jk}) \\cdot |\\lambda^*_{jk}|^p.\n\\] Rotacja promax pozwala uzyskaÄ‡ bardziej realistyczne struktury, gdy czynniki sÄ… rzeczywiÅ›cie skorelowane.\nGeomin (Yates, 1987) - minimalizuje Å›redniÄ… geometrycznÄ… kwadratÃ³w Å‚adunkÃ³w, co prowadzi do sytuacji, w ktÃ³rej kaÅ¼da zmienna ma niewiele istotnych Å‚adunkÃ³w. Funkcja celu \\[\nG(\\Lambda^*) = \\sum_{i=1}^p \\left( \\prod_{j=1}^m (\\lambda_{ij}^{*2} + \\epsilon) \\right)^{1/m},\n\\] gdzie \\(\\epsilon\\) to maÅ‚y parametr stabilizujÄ…cy.\nSimplimax (Kiers, 1994) - uogÃ³lnienie kryteriÃ³w prostoty, ktÃ³re minimalizuje liczbÄ™ duÅ¼ych i maÅ‚ych Å‚adunkÃ³w w macierzy, pozwalajÄ…c uÅ¼ytkownikowi sterowaÄ‡ liczbÄ… â€prostychâ€ elementÃ³w.\nWybÃ³r rodzaju rotacji\n\nRotacje ortogonalne sÄ… preferowane, gdy zakÅ‚adamy, Å¼e czynniki powinny byÄ‡ niezaleÅ¼ne teoretycznie.\nRotacje skoÅ›ne stosuje siÄ™, gdy istnieje uzasadnienie, Å¼e czynniki mogÄ… byÄ‡ skorelowane (co jest czÄ™ste w naukach spoÅ‚ecznych, psychologii czy biologii).\n\n\nNa potrzeby ilustracji budowy modelu EFA wykorzystamy dane z pakietu psych, ktÃ³re zawierajÄ… wyniki rÃ³Å¼nych testÃ³w poznawczych (Harman, 1976). Dane te sÄ… czÄ™sto uÅ¼ywane jako przykÅ‚ad w literaturze dotyczÄ…cej analizy czynnikowej.\n\nKodlibrary(psych)\n\n# Dane: macierz korelacji testÃ³w poznawczych (Harman, 1976)\ndata(\"Harman74.cor\")\n\n\n\n\n\n\n\n\n\nZmienna\nOpis\nKategoria testu\n\n\n\nVisualPerception\nRozpoznawanie i analiza relacji przestrzennych w figurach\nZdolnoÅ›ci przestrzenne / percepcyjne\n\n\nCubes\nManipulacja wyobraÅ¼eniowa bryÅ‚, rotacje przestrzenne\nZdolnoÅ›ci przestrzenne\n\n\nPaperFormBoard\nSkÅ‚adanie i dopasowywanie elementÃ³w figur\nZdolnoÅ›ci przestrzenne\n\n\nFlags\nRozpoznawanie wzorÃ³w i relacji symboli\nPercepcja wzrokowa / logiczne\n\n\nGeneralInformation\nOgÃ³lna wiedza faktograficzna\nZdolnoÅ›ci werbalne\n\n\nPargraphComprehension\nRozumienie tekstÃ³w pisanych\nZdolnoÅ›ci werbalne\n\n\nSentenceCompletion\nUzupeÅ‚nianie zdaÅ„ brakujÄ…cymi sÅ‚owami\nZdolnoÅ›ci werbalne\n\n\nWordClassification\nGrupowanie sÅ‚Ã³w wedÅ‚ug znaczenia\nZdolnoÅ›ci werbalne / semantyczne\n\n\nWordMeaning\nZnajomoÅ›Ä‡ i rozumienie znaczeÅ„ sÅ‚Ã³w\nZdolnoÅ›ci werbalne\n\n\nAddition\nWykonywanie prostych dziaÅ‚aÅ„ arytmetycznych\nZdolnoÅ›ci numeryczne\n\n\nCode\nDopasowywanie symboli do liczb wedÅ‚ug klucza\nSzybkoÅ›Ä‡ przetwarzania / percepcja\n\n\nCountingDots\nLiczenie elementÃ³w wzrokowych\nSzybkoÅ›Ä‡ percepcji / numeryczne\n\n\nStraightCurvedCapitals\nRozpoznawanie prostych i zakrzywionych liter\nPercepcja wizualna / szybkoÅ›Ä‡\n\n\nWordRecognition\nRozpoznawanie sÅ‚Ã³w z listy\nPamiÄ™Ä‡ i zdolnoÅ›ci werbalne\n\n\nNumberRecognition\nRozpoznawanie liczb z listy\nPamiÄ™Ä‡ / percepcja numeryczna\n\n\nFigureRecognition\nRozpoznawanie i identyfikacja figur\nPamiÄ™Ä‡ wizualna / percepcja\n\n\nObjectNumber\nDopasowywanie obiektÃ³w do liczb\nZÅ‚oÅ¼one zdolnoÅ›ci percepcyjno-num.\n\n\nNumberFigure\nDopasowywanie liczb do figur\nZÅ‚oÅ¼one zdolnoÅ›ci percepcyjno-num.\n\n\nFigureWord\nDopasowywanie figur do sÅ‚Ã³w\nÅÄ…czenie informacji wizualno-werbalnych\n\n\nDeduction\nRozwiÄ…zywanie zadaÅ„ logicznych, wnioskowanie\nRozumowanie logiczne\n\n\nNumericalPuzzles\nZadania numeryczne o charakterze problemowym\nZdolnoÅ›ci numeryczne / logiczne\n\n\nProblemReasoning\nRozwiÄ…zywanie zÅ‚oÅ¼onych problemÃ³w\nRozumowanie ogÃ³lne\n\n\nSeriesCompletion\nUzupeÅ‚nianie szeregÃ³w logicznych lub numerycznych\nRozumowanie abstrakcyjne / numeryczne\n\n\nArithmeticProblems\nRozwiÄ…zywanie zadaÅ„ arytmetycznych o wiÄ™kszej trudnoÅ›ci\nZdolnoÅ›ci numeryczne\n\n\n\nWidaÄ‡, Å¼e testy moÅ¼na grupowaÄ‡ w piÄ™Ä‡ gÅ‚Ã³wnych obszarÃ³w: przestrzenne/percepcyjne (np. Cubes, VisualPerception), werbalne (np. WordMeaning, SentenceCompletion), numeryczne (np. Addition, ArithmeticProblems), pamiÄ™ciowe (np. WordRecognition, NumberRecognition), oraz rozumowania i logiczne (np. Deduction, SeriesCompletion). To wÅ‚aÅ›nie takie powiÄ…zania w macierzy korelacji uzasadniajÄ… zastosowanie analizy czynnikowej w celu identyfikacji ukrytych wymiarÃ³w inteligencji.\nNajpierw sprawdzimy czy dane nadajÄ… siÄ™ do analizy czynnikowej, obliczajÄ…c test KMO i test sferycznoÅ›ci Bartletta.\n\nKodlibrary(tidyverse)\nlibrary(easystats)\n\ncheck_factorstructure(Harman74.cor$cov, n = 145) \n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(276) = 1545.86, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.88). The individual KMO scores are: VisualPerception (0.90), Cubes (0.84), PaperFormBoard (0.78), Flags (0.85), GeneralInformation (0.88), PargraphComprehension (0.89), SentenceCompletion (0.89), WordClassification (0.92), WordMeaning (0.88), Addition (0.81), Code (0.85), CountingDots (0.84), StraightCurvedCapitals (0.89), WordRecognition (0.85), NumberRecognition (0.88), FigureRecognition (0.89), ObjectNumber (0.85), NumberFigure (0.88), FigureWord (0.83), Deduction (0.93), NumericalPuzzles (0.91), ProblemReasoning (0.93), SeriesCompletion (0.91), ArithmeticProblems (0.92).\n\n\nTest sferycznoÅ›ci Bartletta dostarcza podstawowego potwierdzenia, Å¼e w zbiorze danych wystÄ™pujÄ… istotne statystycznie korelacje pomiÄ™dzy zmiennymi. Wynik \\(\\chi^2(276) = 1545.86,\\ p &lt; 0.001\\) oznacza, Å¼e hipoteza zerowa o macierzy korelacji rÃ³wnej macierzy jednostkowej zostaje odrzucona. Innymi sÅ‚owy, zmienne nie sÄ… niezaleÅ¼ne, a ich struktura korelacyjna uzasadnia dalsze poszukiwanie wspÃ³lnych czynnikÃ³w. Gdyby test okazaÅ‚ siÄ™ nieistotny, sugerowaÅ‚by brak uzasadnienia do stosowania analizy czynnikowej, poniewaÅ¼ nie byÅ‚oby wystarczajÄ…cych zaleÅ¼noÅ›ci miÄ™dzy zmiennymi.\nMiara adekwatnoÅ›ci prÃ³by KMO (Kaiserâ€“Meyerâ€“Olkin) wskazuje, na ile obserwowane korelacje mogÄ… byÄ‡ wyjaÅ›nione przez czynniki wspÃ³lne w porÃ³wnaniu z korelacjami czÄ…stkowymi. Wynik ogÃ³lny KMO = 0.88 mieÅ›ci siÄ™ w przedziale uznawanym za â€bardzo dobryâ€ (powyÅ¼ej 0.80). Oznacza to, Å¼e dane dobrze nadajÄ… siÄ™ do analizy czynnikowej i moÅ¼emy oczekiwaÄ‡ stabilnych, interpretowalnych rozwiÄ…zaÅ„. WartoÅ›ci indywidualne dla poszczegÃ³lnych zmiennych mieszczÄ… siÄ™ miÄ™dzy 0.78 a 0.93, a wiÄ™c wszystkie osiÄ…gajÄ… poziom â€dobryâ€ lub â€bardzo dobryâ€. NajwyÅ¼sze wartoÅ›ci, takie jak Deduction (0.93), ProblemReasoning (0.93) czy ArithmeticProblems (0.92), wskazujÄ… na wyjÄ…tkowo silnÄ… reprezentacjÄ™ tych testÃ³w w przestrzeni czynnikowej. Z kolei najniÅ¼sze, jak PaperFormBoard (0.78), sÄ… nadal akceptowalne, ale sugerujÄ… nieco sÅ‚abszÄ… integracjÄ™ tej zmiennej z pozostaÅ‚ymi. CaÅ‚oÅ›ciowo zarÃ³wno wynik globalny, jak i rozkÅ‚ad wartoÅ›ci czÄ…stkowych KMO jednoznacznie potwierdzajÄ… zasadnoÅ›Ä‡ prowadzenia analizy czynnikowej na tym zbiorze danych.\n\nKod# Parallel analysis\nfa.parallel(Harman74.cor$cov, n.obs = 145, fa = \"fa\")\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\nSamo kryterium paralelne wskazuje na 4 czynniki, choÄ‡ gdyby braÄ‡ pod uwagÄ™ samo kryterium osypiska to rozwiÄ…zanie z 5 czynnikami teÅ¼ wydaje siÄ™ byÄ‡ wÅ‚aÅ›ciwe.\n\nKod# Kryterium MAP\nVSS(Harman74.cor$cov, n.obs = 145, plot = F)\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.8  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.85  with  2  factors\n\nThe Velicer MAP achieves a minimum of 0.02  with  4  factors \nBIC achieves a minimum of  -731.36  with  3  factors\nSample Size adjusted BIC achieves a minimum of  -112  with  5  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq    prob sqresid  fit RMSEA  BIC SABIC complex\n1 0.80 0.00 0.025 252   626 8.0e-34    16.8 0.80 0.101 -628   170     1.0\n2 0.55 0.85 0.022 229   428 3.1e-14    12.7 0.85 0.077 -711    13     1.5\n3 0.46 0.79 0.017 207   299 3.0e-05    10.0 0.88 0.055 -731   -76     1.8\n4 0.42 0.74 0.017 186   228 1.9e-02     8.0 0.90 0.039 -698  -109     1.9\n5 0.40 0.71 0.021 166   189 1.1e-01     7.2 0.91 0.030 -637  -112     2.0\n6 0.40 0.71 0.024 147   162 1.8e-01     6.3 0.92 0.026 -569  -104     2.0\n7 0.40 0.70 0.028 129   138 2.7e-01     5.6 0.93 0.021 -504   -95     2.2\n8 0.41 0.70 0.030 112   111 5.0e-01     5.0 0.94 0.000 -446   -92     2.3\n  eChisq  SRMR eCRMS eBIC\n1    748 0.097 0.101 -506\n2    422 0.073 0.080 -718\n3    240 0.055 0.063 -790\n4    133 0.041 0.050 -792\n5    105 0.036 0.047 -721\n6     81 0.032 0.044 -651\n7     62 0.028 0.041 -580\n8     44 0.023 0.037 -514\n\n\n\n\n\n\n\n\nWskaÅºnik\nInterpretacja\n\n\n\nvss1\nDopasowanie Very Simple Structure przy zaÅ‚oÅ¼eniu jednego czynnika na zmiennÄ…; wyÅ¼sze = lepsze.\n\n\nvss2\nDopasowanie VSS przy zaÅ‚oÅ¼eniu maksymalnie dwÃ³ch czynnikÃ³w na zmiennÄ…; wyÅ¼sze = lepsze.\n\n\nmap\nKryterium Velicera; minimum wskazuje optymalnÄ… liczbÄ™ czynnikÃ³w (eliminuje korelacje czÄ…stkowe).\n\n\ndof\nStopnie swobody testu dopasowania chi-kwadrat.\n\n\nchisq\nWartoÅ›Ä‡ statystyki chi-kwadrat; niska w relacji do df sugeruje dobre dopasowanie.\n\n\nprob\nWartoÅ›Ä‡ p testu chi-kwadrat; wysoka oznacza brak podstaw do odrzucenia poprawnego dopasowania.\n\n\nsqresid\nSuma kwadratÃ³w reszt (rÃ³Å¼nice R âˆ’ RÌ‚); niÅ¼sze wartoÅ›ci = lepsze odwzorowanie danych.\n\n\nfit\nProporcja wyjaÅ›nionej wariancji w macierzy korelacji; wyÅ¼sze wartoÅ›ci = lepsze dopasowanie.\n\n\nRMSEA\nBÅ‚Ä…d aproksymacji w populacji; &lt; 0.05 bardzo dobre, 0.05â€“0.08 akceptowalne, &gt; 0.10 sÅ‚abe.\n\n\nBIC\nKryterium informacyjne; niÅ¼sze wartoÅ›ci = lepszy kompromis dopasowania i prostoty.\n\n\nSABIC\nWersja BIC korygowana o wielkoÅ›Ä‡ prÃ³by; lepsza przy mniejszych prÃ³bach.\n\n\ncomplex\nÅšrednia liczba czynnikÃ³w na ktÃ³re Å‚adujÄ… siÄ™ zmienne; niÅ¼sze = prostsza struktura.\n\n\neChisq\nEstymowana statystyka chi-kwadrat w alternatywnej estymacji; interpretacja analogiczna jak chisq.\n\n\nSRMR\nStandardized Root Mean Square Residual; niski poziom (&lt; 0.08) wskazuje dobre dopasowanie.\n\n\neCRMS\nEstymowany bÅ‚Ä…d resztowy analogiczny do RMSEA; mniejsze wartoÅ›ci = lepsze dopasowanie.\n\n\neBIC\nEstymowana wersja kryterium BIC; niÅ¼sze wartoÅ›ci = lepszy model.\n\n\n\nKryterium MAP Velicera wskazuje, Å¼e minimalna wartoÅ›Ä‡ statystyki zostaÅ‚a osiÄ…gniÄ™ta przy czterech czynnikach (MAP = 0.017). Oznacza to, Å¼e w ujÄ™ciu tego kryterium, czynniki te najlepiej redukujÄ… korelacje czÄ…stkowe miÄ™dzy zmiennymi â€“ czyli eliminujÄ… najwiÄ™kszÄ… czÄ™Å›Ä‡ wariancji niepowiÄ…zanej ze wspÃ³lnÄ… strukturÄ… czynnikowÄ…. Innymi sÅ‚owy, przy czterech czynnikach model najefektywniej odwzorowuje wspÃ³lne zaleÅ¼noÅ›ci bez pozostawiania nadmiernych reszt.\nWarto jednak zauwaÅ¼yÄ‡, Å¼e rÃ³Å¼ne kryteria sugerujÄ… odmienne liczby czynnikÃ³w. Kryterium BIC wskazuje na trzy czynniki jako najbardziej oczekiwane rozwiÄ…zanie, natomiast skorygowany BIC (SABIC) preferuje piÄ™Ä‡ czynnikÃ³w. Z kolei wskaÅºniki VSS (Very Simple Structure) sugerujÄ… jednoâ€“ lub dwuczynnikowe rozwiÄ…zania, maksymalizujÄ…ce prostotÄ™ struktury. Ostateczna decyzja wymaga zatem kompromisu: MAP sugeruje cztery czynniki jako najpeÅ‚niej oddajÄ…ce wspÃ³lnÄ… strukturÄ™ zmiennych, BIC preferuje trzy jako prostsze, a SABIC wskazuje na piÄ™Ä‡. Interpretacja powinna uwzglÄ™dniaÄ‡ nie tylko statystyki, lecz takÅ¼e sensownoÅ›Ä‡ teoretycznÄ… i interpretowalnoÅ›Ä‡ uzyskanych czynnikÃ³w w kontekÅ›cie badanego materiaÅ‚u.\n\nKod# Analiza czynnikowa\nfa_model &lt;- fa(Harman74.cor$cov, nfactors = 4, n.obs = 145, \n               fm = \"ml\", rotate = \"varimax\")\n\nfa_model\n\nFactor Analysis using method =  ml\nCall: fa(r = Harman74.cor$cov, nfactors = 4, n.obs = 145, rotate = \"varimax\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                        ML1   ML3   ML2  ML4   h2   u2 com\nVisualPerception       0.16  0.69  0.19 0.16 0.56 0.44 1.4\nCubes                  0.12  0.44  0.08 0.10 0.22 0.78 1.3\nPaperFormBoard         0.14  0.57 -0.02 0.11 0.36 0.64 1.2\nFlags                  0.23  0.53  0.10 0.08 0.35 0.65 1.5\nGeneralInformation     0.74  0.19  0.21 0.15 0.65 0.35 1.4\nPargraphComprehension  0.77  0.20  0.07 0.23 0.69 0.31 1.4\nSentenceCompletion     0.81  0.20  0.15 0.07 0.72 0.28 1.2\nWordClassification     0.57  0.34  0.24 0.13 0.51 0.49 2.2\nWordMeaning            0.81  0.20  0.04 0.23 0.74 0.26 1.3\nAddition               0.17 -0.12  0.83 0.17 0.76 0.24 1.2\nCode                   0.18  0.12  0.51 0.37 0.45 0.55 2.2\nCountingDots           0.02  0.21  0.72 0.09 0.56 0.44 1.2\nStraightCurvedCapitals 0.19  0.44  0.53 0.08 0.51 0.49 2.3\nWordRecognition        0.20  0.05  0.08 0.55 0.35 0.65 1.3\nNumberRecognition      0.12  0.12  0.07 0.52 0.30 0.70 1.3\nFigureRecognition      0.07  0.41  0.06 0.53 0.45 0.55 2.0\nObjectNumber           0.14  0.06  0.22 0.57 0.40 0.60 1.4\nNumberFigure           0.03  0.29  0.34 0.46 0.41 0.59 2.6\nFigureWord             0.15  0.24  0.16 0.37 0.24 0.76 2.6\nDeduction              0.38  0.40  0.12 0.30 0.41 0.59 3.0\nNumericalPuzzles       0.17  0.38  0.44 0.22 0.42 0.58 2.8\nProblemReasoning       0.37  0.40  0.12 0.30 0.40 0.60 3.1\nSeriesCompletion       0.37  0.50  0.24 0.24 0.50 0.50 2.9\nArithmeticProblems     0.37  0.16  0.50 0.30 0.50 0.50 2.8\n\n                       ML1  ML3  ML2  ML4\nSS loadings           3.65 2.87 2.66 2.29\nProportion Var        0.15 0.12 0.11 0.10\nCumulative Var        0.15 0.27 0.38 0.48\nProportion Explained  0.32 0.25 0.23 0.20\nCumulative Proportion 0.32 0.57 0.80 1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  276  with the objective function =  11.44 with Chi Square =  1545.86\ndf of  the model are 186  and the objective function was  1.71 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  145 with the empirical chi square  135.74  with prob &lt;  1 \nThe total n.obs was  145  with Likelihood Chi Square =  226.68  with prob &lt;  0.022 \n\nTucker Lewis Index of factoring reliability =  0.951\nRMSEA index =  0.038  and the 90 % confidence intervals are  0.016 0.056\nBIC =  -698.99\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2  ML4\nCorrelation of (regression) scores with factors   0.93 0.87 0.91 0.82\nMultiple R square of scores with factors          0.87 0.76 0.83 0.68\nMinimum correlation of possible factor scores     0.73 0.52 0.66 0.36\n\n\nModel czteroczynnikowy oszacowany metodÄ… najwiÄ™kszej wiarygodnoÅ›ci na macierzy korelacji Harman74.cor$cov dobrze odwzorowuje strukturÄ™ danych i dostarcza interpretowalnych wynikÃ³w.\nPierwszy czynnik (ML1) skupia siÄ™ na kompetencjach werbalnych i wiedzy ogÃ³lnej. NajwyÅ¼sze Å‚adunki uzyskano dla zmiennych takich jak WordMeaning (0.81), SentenceCompletion (0.81), ParagraphComprehension (0.77) czy GeneralInformation (0.74). Wskazuje to, Å¼e ML1 reprezentuje wymiar wiedzy jÄ™zykowej i rozumienia tekstu. Zasoby zmiennoÅ›ci wspÃ³lej dla tych zmiennych sÄ… wysokie (powyÅ¼ej 0.65), co oznacza, Å¼e znaczna czÄ™Å›Ä‡ ich wariancji zostaÅ‚a uchwycona przez model.\nDrugi czynnik (ML2) odzwierciedla zdolnoÅ›ci arytmetyczne i numeryczne. Najsilniejsze Å‚adunki dotyczÄ… zmiennych Addition (0.83), CountingDots (0.72) i ArithmeticProblems (0.50). Oznacza to, Å¼e ML2 reprezentuje wymiar obliczeniowy, obejmujÄ…cy zarÃ³wno proste dziaÅ‚ania matematyczne, jak i bardziej zÅ‚oÅ¼one zadania wymagajÄ…ce operowania na liczbach. Wysokie wartoÅ›ci \\(h_j^2\\) (np. 0.76 dla Addition) sugerujÄ… dobrÄ… reprezentacjÄ™ tych zmiennych.\nTrzeci czynnik (ML3) moÅ¼na interpretowaÄ‡ jako zdolnoÅ›ci wzrokowo-przestrzenne i percepcyjne. Najsilniejsze Å‚adunki wystÄ…piÅ‚y dla VisualPerception (0.69), PaperFormBoard (0.57), Flags (0.53) oraz SeriesCompletion (0.50). Grupa ta obejmuje zadania zwiÄ…zane z manipulacjÄ… figurami, rozpoznawaniem wzorÃ³w i orientacjÄ… przestrzennÄ….\nCzwarty czynnik (ML4) wydaje siÄ™ zwiÄ…zany z rozpoznawaniem wzrokowym i pamiÄ™ciÄ… wzrokowÄ…. NajwiÄ™ksze Å‚adunki dotyczÄ… zmiennych takich jak WordRecognition (0.55), NumberRecognition (0.52), FigureRecognition (0.53) czy ObjectNumber (0.57). Sugeruje to wymiar rozpoznawania i szybkiego identyfikowania bodÅºcÃ³w wzrokowych.\nÅÄ…cznie cztery czynniki wyjaÅ›niajÄ… 48% wariancji caÅ‚kowitej, co w psychometrii jest uznawane za wartoÅ›Ä‡ akceptowalnÄ… przy tego typu danych. Dopasowanie globalne modelu rÃ³wnieÅ¼ jest dobre: RMSEA = 0.038 (z przedziaÅ‚em ufnoÅ›ci 0.016â€“0.056) wskazuje na bardzo dobre dopasowanie, a Tucker-Lewis Index wynosi 0.951, co rÃ³wnieÅ¼ Å›wiadczy o wysokiej jakoÅ›ci modelu. Niskie wartoÅ›ci RMSR (0.04) oraz wysoka zgodnoÅ›Ä‡ dopasowania poza przekÄ…tnÄ… (0.98) potwierdzajÄ…, Å¼e model trafnie odwzorowuje strukturÄ™ korelacji miÄ™dzy zmiennymi.\nOstatecznie wyniki wskazujÄ…, Å¼e struktura czteroczynnikowa jest dobrze uzasadniona empirycznie i teoretycznie. KaÅ¼dy czynnik odpowiada odmiennym zdolnoÅ›ciom poznawczym â€“ werbalnym, numerycznym, przestrzennym i percepcyjno-pamiÄ™ciowym â€“ a ich interpretacje sÄ… zgodne z psychologicznymi ujÄ™ciami inteligencji wielowymiarowej.\nDla wiÄ™kszej czytelnoÅ›ci przedstawiamy Å‚adunki czynnikowe po rotacji varimax w formie tabelarycznej, z wyciÄ™tymi Å‚adunkami o niskich wartoÅ›ciach.\n\nKodmodel_parameters(fa_model, sort = TRUE, threshold = \"max\")\n\n# Rotated loadings from Factor Analysis (varimax-rotation)\n\nVariable               |  ML1 |  ML3 |  ML2 |  ML4 | Complexity | Uniqueness\n----------------------------------------------------------------------------\nWordMeaning            | 0.81 |      |      |      |       1.30 |       0.26\nSentenceCompletion     | 0.81 |      |      |      |       1.21 |       0.28\nPargraphComprehension  | 0.77 |      |      |      |       1.35 |       0.31\nGeneralInformation     | 0.74 |      |      |      |       1.39 |       0.35\nWordClassification     | 0.57 |      |      |      |       2.17 |       0.49\nVisualPerception       |      | 0.69 |      |      |       1.38 |       0.44\nPaperFormBoard         |      | 0.57 |      |      |       1.20 |       0.64\nFlags                  |      | 0.53 |      |      |       1.51 |       0.65\nSeriesCompletion       |      | 0.50 |      |      |       2.87 |       0.50\nCubes                  |      | 0.44 |      |      |       1.33 |       0.78\nDeduction              |      | 0.40 |      |      |       3.05 |       0.59\nProblemReasoning       |      | 0.40 |      |      |       3.08 |       0.60\nAddition               |      |      | 0.83 |      |       1.21 |       0.24\nCountingDots           |      |      | 0.72 |      |       1.21 |       0.44\nStraightCurvedCapitals |      |      | 0.53 |      |       2.27 |       0.49\nCode                   |      |      | 0.51 |      |       2.25 |       0.55\nArithmeticProblems     |      |      | 0.50 |      |       2.83 |       0.50\nNumericalPuzzles       |      |      | 0.44 |      |       2.84 |       0.58\nObjectNumber           |      |      |      | 0.57 |       1.45 |       0.60\nWordRecognition        |      |      |      | 0.55 |       1.32 |       0.65\nFigureRecognition      |      |      |      | 0.53 |       1.96 |       0.55\nNumberRecognition      |      |      |      | 0.52 |       1.26 |       0.70\nNumberFigure           |      |      |      | 0.46 |       2.62 |       0.59\nFigureWord             |      |      |      | 0.37 |       2.56 |       0.76\n\nThe 4 latent factors (varimax rotation) accounted for 47.78% of the total variance of the original data (ML1 = 15.20%, ML3 = 11.97%, ML2 = 11.07%, ML4 = 9.54%).\n\n\nMoÅ¼emy teÅ¼ przedstawiÄ‡ model graficznie.\n\nKodfa.diagram(fa_model, marg = c(1,5,1,1), rsize = 2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  }
]