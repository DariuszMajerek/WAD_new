[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wielowymiarowa analiza danych",
    "section": "",
    "text": "WstÄ™p\nWielowymiarowa analiza danych stanowi jeden z filarÃ³w wspÃ³Å‚czesnej statystyki i eksploracji danych, oferujÄ…c metody pozwalajÄ…ce zrozumieÄ‡ strukturÄ™ i zaleÅ¼noÅ›ci w zbiorach danych, w ktÃ³rych kaÅ¼da obserwacja jest opisana wieloma zmiennymi jednoczeÅ›nie. W dobie powszechnego dostÄ™pu do danych oraz rosnÄ…cego zapotrzebowania na ich zaawansowanÄ… analizÄ™, umiejÄ™tnoÅ›Ä‡ stosowania metod wielowymiarowych staje siÄ™ nieodzowna zarÃ³wno w badaniach naukowych, jak i w analizie danych stosowanej w przemyÅ›le, finansach, biologii, medycynie czy naukach spoÅ‚ecznych.\nNiniejsza ksiÄ…Å¼ka zostaÅ‚a opracowana z myÅ›lÄ… o dwÃ³ch kierunkach ksztaÅ‚cenia akademickiego: matematyce oraz inÅ¼ynierii i analizie danych. Jej celem jest zapewnienie solidnych podstaw teoretycznych oraz praktycznych umiejÄ™tnoÅ›ci niezbÄ™dnych do stosowania metod wielowymiarowych w rzeczywistych problemach badawczych i aplikacyjnych. Zakres tematyczny ksiÄ…Å¼ki zostaÅ‚ dobrany tak, aby uwzglÄ™dniaÄ‡ zarÃ³wno klasyczne metody statystyczne, jak i techniki wykorzystywane we wspsÃ³Å‚czesnej analizie danych.\nW pierwszej czÄ™Å›ci ksiÄ…Å¼ki omÃ³wione zostanÄ… testy wielowymiarowe, ktÃ³re stanowiÄ… rozszerzenie klasycznych metod statystycznych na przypadki, w ktÃ³rych kaÅ¼da obserwacja opisana jest wieloma zmiennymi. SzczegÃ³lna uwaga zostanie poÅ›wiÄ™cona testowi Hotellinga TÂ², bÄ™dÄ…cemu odpowiednikiem testu t dla wielu zmiennych, oraz analizie wariancji dla wielu zmiennych (MANOVA), pozwalajÄ…cej na badanie rÃ³Å¼nic miÄ™dzy grupami z uwzglÄ™dnieniem wspÃ³Å‚zaleÅ¼noÅ›ci zmiennych. Celem tej czÄ™Å›ci bÄ™dzie zrozumienie podstaw inferencji w przestrzeni wielowymiarowej i interpretacji wynikÃ³w testÃ³w z uwzglÄ™dnieniem macierzy kowariancji.\nNastÄ™pnie przedstawiona zostanie analiza kanoniczna, ktÃ³ra sÅ‚uÅ¼y do badania zaleÅ¼noÅ›ci pomiÄ™dzy dwoma zestawami zmiennych. Czytelnik pozna konstrukcjÄ™ zmiennych kanonicznych, sposoby ich interpretacji oraz znaczenie wag i korelacji kanonicznych. Analiza ta ma kluczowe znaczenie wszÄ™dzie tam, gdzie celem jest znalezienie skorelowanych struktur w dwÃ³ch grupach cech, np. w badaniach biologicznych, spoÅ‚ecznych lub psychometrycznych.\nKolejna czÄ™Å›Ä‡ ksiÄ…Å¼ki bÄ™dzie poÅ›wiÄ™cona analizie czynnikowej (FA), ktÃ³ra umoÅ¼liwia modelowanie wspÃ³Å‚zmiennoÅ›ci zestawu zmiennych za pomocÄ… mniejszej liczby zmiennych ukrytych, zwanych czynnikami. Przedstawione zostanÄ… metody estymacji, kryteria wyboru liczby czynnikÃ³w oraz techniki rotacji, ktÃ³re sÅ‚uÅ¼Ä… lepszej interpretacji wynikÃ³w. Analiza czynnikowa jest czÄ™sto stosowana w badaniach ankietowych i psychometrycznych, ale znajduje rÃ³wnieÅ¼ zastosowanie w analizie danych ekonomicznych i marketingowych.\nW dalszej kolejnoÅ›ci wprowadzony zostanie model Å›cieÅ¼kowy oraz jego uogÃ³lnienie w postaci modeli rÃ³wnaÅ„ strukturalnych (SEM). Modele te pozwalajÄ… na modelowanie zarÃ³wno obserwowalnych, jak i ukrytych zmiennych oraz relacji przyczynowych pomiÄ™dzy nimi. Czytelnik pozna strukturÄ™ modelu Å›cieÅ¼kowego, pojÄ™cie identyfikowalnoÅ›ci, miary dopasowania oraz techniki estymacji parametrÃ³w. Modele SEM sÄ… obecnie szeroko stosowane w naukach spoÅ‚ecznych, biologii, psychologii i ekonomii.\nNastÄ™pnie omÃ³wione zostanÄ… metody redukcji wymiarowoÅ›ci, ktÃ³rych celem jest uproszczenie reprezentacji danych bez utraty istotnej informacji. KluczowÄ… technikÄ… bÄ™dzie analiza skÅ‚adowych gÅ‚Ã³wnych (PCA), ktÃ³ra pozwala na znalezienie nowych osi zmiennoÅ›ci w danych. Kolejno zaprezentowana zostanie analiza niezaleÅ¼nych skÅ‚adowych (ICA), ktÃ³ra poszukuje skÅ‚adnikÃ³w statystycznie niezaleÅ¼nych, co jest szczegÃ³lnie uÅ¼yteczne w analizie sygnaÅ‚Ã³w. Obie metody znajdÄ… zastosowanie zarÃ³wno w przygotowaniu danych, jak i w ich eksploracji.\nKolejna czÄ™Å›Ä‡ ksiÄ…Å¼ki poÅ›wiÄ™cona bÄ™dzie metodom skalowania wielowymiarowego (Multidimensional Scaling, MDS), ktÃ³re umoÅ¼liwiajÄ… odwzorowanie relacji odlegÅ‚oÅ›ciowych pomiÄ™dzy obiektami w przestrzeni o mniejszym wymiarze. Wariant metric zakÅ‚ada zachowanie rzeczywistych wartoÅ›ci odlegÅ‚oÅ›ci, natomiast non-metric koncentruje siÄ™ na porzÄ…dku dystansÃ³w. Metody te pozwalajÄ… uzyskaÄ‡ intuicyjne wizualizacje struktur danych, szczegÃ³lnie przydatne w psychologii, socjologii czy analizie rynku.\nW uzupeÅ‚nieniu do klasycznych technik przedstawione zostanÄ… nieliniowe metody redukcji wymiarowoÅ›ci, takie jak t-distributed Stochastic Neighbor Embedding (t-SNE) oraz Uniform Manifold Approximation and Projection (UMAP). Obie techniki pozwalajÄ… na odwzorowanie skomplikowanych struktur danych w przestrzeniach dwu- lub trÃ³jwymiarowych, zachowujÄ…c lokalne sÄ…siedztwa. ChoÄ‡ sÄ… to metody przede wszystkim eksploracyjne i wizualizacyjne, ich wartoÅ›Ä‡ w analizie duÅ¼ych zbiorÃ³w danych jest trudna do przecenienia.\nNastÄ™pnie przedstawiona zostanie analiza skupieÅ„, ktÃ³rej celem jest odkrywanie naturalnych grup w zbiorze danych. OmÃ³wione zostanÄ… zarÃ³wno metody hierarchiczne, jak i niehierarchiczne, w tym popularna metoda k-Å›rednich. Poruszona zostanie problematyka doboru liczby skupieÅ„ oraz oceny stabilnoÅ›ci i jakoÅ›ci otrzymanych rozwiÄ…zaÅ„. Analiza skupieÅ„ znajduje zastosowanie w segmentacji rynku, biologii molekularnej, diagnostyce medycznej i wielu innych dziedzinach.\nKolejna czÄ™Å›Ä‡ ksiÄ…Å¼ki poÅ›wiÄ™cona bÄ™dzie analizie korespondencji, stosowanej do eksploracji zwiÄ…zkÃ³w pomiÄ™dzy zmiennymi jakoÅ›ciowymi przedstawionymi w postaci tablicy kontyngencji. Przedstawiona zostanie zarÃ³wno analiza korespondencji prosta (dla dwÃ³ch zmiennych), jak i zÅ‚oÅ¼ona (dla wiÄ™cej niÅ¼ dwÃ³ch). OmÃ³wione zostanÄ… interpretacja map percepcyjnych, odwzorowanie profili oraz zwiÄ…zki z metodami takimi jak PCA czy MDS.\nOstatni rozdziaÅ‚ poÅ›wiÄ™cony bÄ™dzie analizie log-liniowej, ktÃ³ra umoÅ¼liwia modelowanie czÄ™stoÅ›ci w tablicach wielodzielczych na podstawie interakcji pomiÄ™dzy zmiennymi kategorycznymi. ZostanÄ… zaprezentowane modele peÅ‚ne i uproszczone, zasady testowania zÅ‚oÅ¼onoÅ›ci modeli oraz interpretacji parametrÃ³w. Analiza log-liniowa jest szczegÃ³lnie przydatna przy badaniu wielowymiarowych zaleÅ¼noÅ›ci miÄ™dzy zmiennymi kategorycznymi w badaniach spoÅ‚ecznych, medycznych oraz w analizie zachowaÅ„ konsumenckich.\nWszystkie metody zostanÄ… zilustrowane przykÅ‚adami praktycznymi, realizowanymi w jÄ™zyku R. Pozwoli to Czytelnikowi nie tylko zrozumieÄ‡ teoretyczne podstawy omawianych technik, ale takÅ¼e nabyÄ‡ umiejÄ™tnoÅ›Ä‡ ich stosowania w praktyce analitycznej.",
    "crumbs": [
      "WstÄ™p"
    ]
  },
  {
    "objectID": "multi_tests.html",
    "href": "multi_tests.html",
    "title": "Testy wielowymiarowe",
    "section": "",
    "text": "Test Hotellinga dla znanej macierzy kowariancji (Anderson 1992)\nW tradycyjnej analizie statystycznej czÄ™sto koncentrujemy siÄ™ na porÃ³wnywaniu grup ze wzglÄ™du na jednÄ… zmiennÄ… â€“ np. porÃ³wnujemy Å›redni wzrost kobiet i mÄ™Å¼czyzn, wykorzystujÄ…c test t-Studenta. Jednak w rzeczywistoÅ›ci badawczej rzadko interesuje nas tylko jedna cecha. PrzykÅ‚adowo, porÃ³wnujÄ…c grupy pacjentÃ³w, moÅ¼emy jednoczeÅ›nie rozwaÅ¼aÄ‡ poziom ciÅ›nienia, cholesterolu i BMI. Albo, analizujÄ…c dane socjologiczne, chcemy porÃ³wnaÄ‡ grupy pod wzglÄ™dem dochodÃ³w, wyksztaÅ‚cenia i poziomu zadowolenia z Å¼ycia.\nUÅ¼ycie wielu testÃ³w jednowymiarowych wydaje siÄ™ kuszÄ…ce â€“ testujemy kaÅ¼dÄ… zmiennÄ… osobno. Jednak prowadzi to do trzech istotnych problemÃ³w (Huberty i Morris 1989):\n\\[\n\\mathbb{P}(\\text{co najmniej jeden bÅ‚Ä…d I rodzaju}) = 1 - (1 - \\alpha)^p = 1 - 0.95^{10} \\approx 0.40\n\\]\nPowyÅ¼sze problemy uzasadniajÄ… potrzebÄ™ stosowania testÃ³w wielowymiarowych â€“ uwzglÄ™dniajÄ…cych strukturÄ™ wspÃ³Å‚zmiennoÅ›ci miÄ™dzy cechami oraz pozwalajÄ…cych na testowanie hipotez dotyczÄ…cych caÅ‚ych wektorÃ³w Å›rednich.\nRozwaÅ¼my prÃ³bkÄ™ \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\ldots, \\boldsymbol{y}_n \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), gdzie \\(\\boldsymbol{\\Sigma}\\) jest znana. Oznacza to, Å¼e mamy do czynienia z ciÄ…giem niezaleÅ¼nych losowych wektorÃ³w \\(\\boldsymbol{y}_i\\), z ktÃ³rych kaÅ¼dy ma ten sam wielowymiarowy rozkÅ‚ad normalny o wymiarze \\(p\\), Å›redniej \\(\\boldsymbol{\\mu}\\) i macierzy kowariancji \\(\\boldsymbol{\\Sigma}\\). KaÅ¼dy wektor \\(\\boldsymbol{y}_i\\) moÅ¼na interpretowaÄ‡ jako punkt w przestrzeni \\(\\mathbb{R}^p\\), opisujÄ…cy \\(p\\) cech (zmiennych) dla jednej obserwacji. Formalnie jest to wektor kolumnowy postaci \\(\\boldsymbol{y}_i = [y_{i1}, y_{i2}, \\ldots, y_{ip}]^\\top\\), gdzie indeks \\(i\\) numeruje jednostki (np. osoby, obiekty pomiaru), a indeksy \\(j = 1, \\ldots, p\\) odpowiadajÄ… poszczegÃ³lnym zmiennym. Wektor ten traktowany jest jako zmienna losowa, poniewaÅ¼ jego wartoÅ›ci sÄ… wynikiem losowego procesu generujÄ…cego dane. RozkÅ‚ad \\(\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) jest wielowymiarowÄ… wersjÄ… rozkÅ‚adu normalnego. Opisuje on sytuacjÄ™, w ktÃ³rej kaÅ¼da kombinacja liniowa zmiennych losowych w \\(\\boldsymbol{y}_i\\) rÃ³wnieÅ¼ ma rozkÅ‚ad normalny, a funkcja gÄ™stoÅ›ci prawdopodobieÅ„stwa ma postaÄ‡ zaleÅ¼nÄ… od wartoÅ›ci wektora Å›rednich oraz struktury kowariancji. Jest to fundamentalne zaÅ‚oÅ¼enie w klasycznej analizie statystycznej, pozwalajÄ…ce na stosowanie wielu narzÄ™dzi statystycznych.\nParametr \\(\\boldsymbol{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_p]^\\top\\) to wektor wartoÅ›ci oczekiwanych kaÅ¼dej z analizowanych zmiennych. Oznacza on przeciÄ™tny poziom zmiennej \\(y_{ij}\\) w populacji dla kaÅ¼dej cechy \\(j\\). Jest to parametr istotny z punktu widzenia testowania hipotez, poniewaÅ¼ wiele testÃ³w statystycznych dotyczy wÅ‚aÅ›nie rÃ³wnoÅ›ci lub rÃ³Å¼nic wektorÃ³w Å›rednich miÄ™dzy grupami.\nZ kolei macierz \\(\\boldsymbol{\\Sigma}\\) to dodatnio okreÅ›lona, symetryczna macierz kowariancji o wymiarach \\(p \\times p\\). Jej elementy \\(\\sigma_{jj}\\) opisujÄ… wariancje poszczegÃ³lnych zmiennych, natomiast elementy poza gÅ‚Ã³wnÄ… przekÄ…tnÄ… \\(\\sigma_{jk}\\) (dla \\(j \\ne k\\)) opisujÄ… kowariancje, czyli wspÃ³Å‚zmiennoÅ›Ä‡ pomiÄ™dzy zmiennymi \\(y_{ij}\\) i \\(y_{ik}\\). W analizie wielowymiarowej uwzglÄ™dnienie tych zaleÅ¼noÅ›ci miÄ™dzy cechami jest kluczowe, poniewaÅ¼ pozwala lepiej zrozumieÄ‡ strukturÄ™ danych i dokonywaÄ‡ bardziej trafnych wnioskÃ³w statystycznych.\nO prÃ³bie \\(\\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n\\) zakÅ‚adamy, Å¼e wszystkie obserwacje sÄ… niezaleÅ¼ne oraz pochodzÄ… z tego samego rozkÅ‚adu. Oznacza to, Å¼e mamy do czynienia z prÃ³bÄ… losowÄ…, niezaleÅ¼nÄ… o identycznych rozkÅ‚adach (i.i.d.), co jest podstawowym zaÅ‚oÅ¼eniem wielu testÃ³w i metod estymacji. CaÅ‚Ä… prÃ³bkÄ™ moÅ¼na przedstawiÄ‡ jako macierz danych o wymiarach \\(n \\times p\\), w ktÃ³rej wiersze odpowiadajÄ… jednostkom, a kolumny cechom.\nW przypadku, gdy macierz kowariancji \\(\\boldsymbol{\\Sigma}\\) jest znana, moÅ¼emy zastosowaÄ‡ uproszczone wersje testÃ³w statystycznych, takie jak klasyczny test Hotellinga \\(T^2\\) dla jednej prÃ³by. Jest to jednak sytuacja czysto teoretyczna, poniewaÅ¼ w praktyce \\(\\boldsymbol{\\Sigma}\\) musi byÄ‡ zazwyczaj estymowana na podstawie danych. Pomimo tego, przypadek znanej macierzy jest uÅ¼yteczny do budowania intuicji, zrozumienia rÃ³l poszczegÃ³lnych parametrÃ³w i wyprowadzania wÅ‚asnoÅ›ci statystyk testowych.\nChcemy przetestowaÄ‡ hipotezÄ™:\n\\[\nH_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu} \\neq \\boldsymbol{\\mu}_0\n\\]\nStatystyka testowa opiera siÄ™ na uogÃ³lnionej odlegÅ‚oÅ›ci Mahalanobisa:\n\\[\nT^2 = n (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Sigma}^{-1} (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)\n\\]\nPod warunkiem speÅ‚nienia \\(H_0\\), statystyka \\(T^2 \\sim \\chi^2_p\\). Zatem moÅ¼emy porÃ³wnaÄ‡ wartoÅ›Ä‡ \\(T^2\\) z odpowiednim kwantylem rozkÅ‚adu chi-kwadrat.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#zaÅ‚oÅ¼enia",
    "href": "multi_tests.html#zaÅ‚oÅ¼enia",
    "title": "Testy wielowymiarowe",
    "section": "ZaÅ‚oÅ¼enia",
    "text": "ZaÅ‚oÅ¼enia\n\nPrÃ³by sÄ… niezaleÅ¼ne;\nObserwacje w kaÅ¼dej grupie pochodzÄ… z rozkÅ‚adu wielowymiarowego normalnego;\n\nMacierze kowariancji sÄ… rÃ³wne \\(\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}\\). Jest to kluczowe zaÅ‚oÅ¼enie umoÅ¼liwiajÄ…ce zbudowanie wspÃ³lnego estymatora kowariancji i zastosowanie rozkÅ‚adu \\(T^2\\) Hotellinga. ChoÄ‡ moÅ¼e byÄ‡ ono naruszone w praktyce, to dla duÅ¼ych prÃ³b test zachowuje swoje wÅ‚aÅ›ciwoÅ›ci asymptotyczne (Rencher 1998).\n\nWektory Å›rednich z prÃ³by wyraÅ¼amy jako:\n\\[\n\\bar{\\boldsymbol{y}}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} \\boldsymbol{y}_{1,i}, \\quad\n\\bar{\\boldsymbol{y}}_2 = \\frac{1}{n_2} \\sum_{i=1}^{n_2} \\boldsymbol{y}_{2,i}\n\\]\nNieobciÄ…Å¼onym estymatorem macierzy kowariancji (\\(\\boldsymbol{\\Sigma}\\)) jest tzw. poÅ‚Ä…czony estymator kowariancji:\n\\[\n\\mathbf{S} =\n\\frac{(n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2}{n_1 + n_2 - 2}\n\\] gdzie \\[\n\\mathbf{S}_1 = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top\n\\]\n\\[\n\\mathbf{S}_2 = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top\n\\]\nZatem:\n\\[\n\\mathbf{S} = \\frac{\\mathbf{W}_1 + \\mathbf{W}_2}{n_1 + n_2 - 2}\n\\] gdzie: \\[\n\\mathbf{W}_1 = \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top = (n_1 - 1)\\mathbf{S}_1\n\\]\n\\[\n\\mathbf{W}_2 = \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top = (n_2 - 1)\\mathbf{S}_2\n\\] Statystyka testowa Hotellinga wÃ³wczas ma postaÄ‡:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)\n\\]\nA gdy \\(H_0\\) jest prawdziwa, to po przeksztaÅ‚ceniu: \\[\nF = \\frac{(n_1 + n_2 - p - 1)}{p(n_1 + n_2 - 2)} T^2 \\sim F_{p, n_1 + n_2 - p - 1}\n\\]\nAlternatywnie, moÅ¼na zapisaÄ‡, Å¼e: \\[\nT^2 \\sim \\frac{p(n_1 + n_2 - 2)}{n_1 + n_2 - p - 1} F_{p, n_1 + n_2 - p - 1}\n\\]\nHipotezÄ™ zerowÄ… \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) odrzucamy na poziomie istotnoÅ›ci \\(\\alpha\\), jeÅ›li: \\[\nT^2 &gt; T^2_{\\alpha, p, n_1 + n_2 - 2}\n\\] lub rÃ³wnowaÅ¼nie: \\[\nF &gt; F_{\\alpha, p, n_1 + n_2 - p - 1}\n\\]\nAby test byÅ‚ moÅ¼liwy do przeprowadzenia, konieczne jest, aby \\(n_1 + n_2 - 2 &gt; p\\), czyli liczba stopni swobody w estymacji wspÃ³lnej kowariancji byÅ‚a wiÄ™ksza niÅ¼ wymiar przestrzeni cech.\n\n\n\n\n\n\nAdnotacja\n\n\n\nW praktyce istotne jest, aby przed zastosowaniem testu \\(T^2\\) Hotellinga dla dwÃ³ch prÃ³b zweryfikowaÄ‡ zaÅ‚oÅ¼enie o rÃ³wnoÅ›ci macierzy kowariancji â€” np. za pomocÄ… testu Boxa. Test M Boxa (ang. Boxâ€™s M test) sÅ‚uÅ¼y do statystycznej weryfikacji hipotezy rÃ³wnoÅ›ci macierzy kowariancji w wielu grupach.\nZaÅ‚Ã³Å¼my, Å¼e mamy \\(G\\) niezaleÅ¼nych prÃ³b z wielowymiarowego rozkÅ‚adu normalnego:\n\\[\n\\boldsymbol{y}_{g} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_g, \\boldsymbol{\\Sigma}_g), \\quad g = 1, \\ldots, G\n\\] Testujemy hipotezÄ™: \\[\nH_0: \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\ldots = \\boldsymbol{\\Sigma}_G = \\boldsymbol{\\Sigma}\n\\] przeciwko alternatywie: \\[\nH_1: \\exists\\, g, h: \\boldsymbol{\\Sigma}_g \\ne \\boldsymbol{\\Sigma}_h\n\\] Niech\n\n\n\\(\\mathbf{S}_g\\) â€“ macierz kowariancji w grupie \\(g\\),\n\n\\(n_g\\) â€“ liczba obserwacji w grupie \\(g\\),\n\n\\(\\mathbf{S}_p\\) â€“ poÅ‚Ä…czony estymator macierzy kowariancji:\n\n\\[\n\\mathbf{S}_p = \\frac{1}{N - G} \\sum_{g=1}^G (n_g - 1)\\mathbf{S}_g\n\\] gdzie \\(N = \\sum_{g=1}^G n_g\\) â€“ Å‚Ä…czna liczba obserwacji. WÃ³wczas, statystyka testowa Boxa ma postaÄ‡: \\[\nM = (N - G) \\cdot \\ln|\\mathbf{S}_p| - \\sum_{g=1}^G (n_g - 1) \\cdot \\ln|\\mathbf{S}_g|\n\\] Poprawka na skoÅ„czonÄ… prÃ³bkÄ™ prowadzi do statystyki: \\[\nC = \\left(1 - c\\right) \\cdot M\n\\] gdzie: \\[\nc = \\frac{1}{3(p + 1)(G - 1)} \\left[ \\sum_{g=1}^G \\frac{1}{n_g - 1} - \\frac{1}{N - G} \\right]\n\\] Statystyka \\(C\\) jest asymptotycznie zbierzna do rozkÅ‚adu \\(\\chi^2\\left(\\frac{p}{2}(p + 1)(G - 1)\\right)\\) liczbÄ… stopni swobody.\nHipotezÄ™ \\(H_0\\) o rÃ³wnoÅ›ci macierzy kowariancji odrzuca siÄ™, jeÅ›li: \\[\nC &gt; \\chi^2_{1 - \\alpha, df}\n\\] lub gdy \\(p\\) testu jest mniejsza od poziomu istotnoÅ›ci \\(\\alpha\\).\nUwagi praktyczne\n\nTest M Boxa jest wraÅ¼liwy na odchylenia od normalnoÅ›ci â€“ jeÅ›li dane nie sÄ… zbliÅ¼one do normalnych, test moÅ¼e dawaÄ‡ mylÄ…ce wyniki.\nW duÅ¼ych prÃ³bach nawet drobne rÃ³Å¼nice miÄ™dzy macierzami kowariancji mogÄ… prowadziÄ‡ do odrzucenia \\(H_0\\), choÄ‡ nie majÄ… istotnego wpÅ‚ywu praktycznego.\nW maÅ‚ych prÃ³bach test moÅ¼e byÄ‡ niestabilny â€“ zaleca siÄ™ ostroÅ¼noÅ›Ä‡ przy interpretacji.\n\n\n\n\nPrzykÅ‚ad 4.1 (PorÃ³wnanie dwÃ³ch grup za pomocÄ… testu \\(T^2\\) Hotellinga) Â \n\nKodlibrary(MASS)\n# Parametry symulacji\nset.seed(44)\np &lt;- 2          # liczba zmiennych\nn1 &lt;- 30        # liczba obserwacji w grupie 1\nn2 &lt;- 35        # liczba obserwacji w grupie 2\n\n# Parametry rozkÅ‚adu\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1, 1)\nSigma &lt;- matrix(c(1, 0.5,\n                  0.5, 1), nrow = 2)\n\n# Generowanie danych\nY1 &lt;- mvrnorm(n1, mu = mu1, Sigma = Sigma)\nY2 &lt;- mvrnorm(n2, mu = mu2, Sigma = Sigma)\n\n# Åšrednie z prÃ³by\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymatory kowariancji\nS1 &lt;- cov(Y1)\nS2 &lt;- cov(Y2)\n\n# WspÃ³lna kowariancja (poÅ‚Ä…czona)\nSp &lt;- ((n1 - 1)*S1 + (n2 - 1)*S2) / (n1 + n2 - 2)\n\n# Statystyka testowa Hotellinga T^2\ndiff_mean &lt;- y1_bar - y2_bar\nT2 &lt;- (n1 * n2) / (n1 + n2) * t(diff_mean) %*% solve(Sp) %*% diff_mean\nT2 &lt;- as.numeric(T2)\n\n# PrzeksztaÅ‚cenie do F\ndf1 &lt;- p\ndf2 &lt;- n1 + n2 - p - 1\nF_stat &lt;- (df2 / (df1 * (n1 + n2 - 2))) * T2\n\n# WartoÅ›Ä‡ krytyczna\nalpha &lt;- 0.05\nF_crit &lt;- qf(1 - alpha, df1, df2)\n\n# p-wartoÅ›Ä‡\np_val &lt;- 1 - pf(F_stat, df1, df2)\n\n# Wynik testu\nsprintf(\"Statystyka TÂ² = %.3f\", T2)\n\n[1] \"Statystyka TÂ² = 17.340\"\n\nKodsprintf(\"Statystyka F = %.3f\", F_stat)\n\n[1] \"Statystyka F = 8.532\"\n\nKodsprintf(\"WartoÅ›Ä‡ krytyczna F = %.3f\", F_crit)\n\n[1] \"WartoÅ›Ä‡ krytyczna F = 3.145\"\n\nKodsprintf(\"p-value = %e\", p_val)\n\n[1] \"p-value = 5.330310e-04\"\n\nKod# Wizualizacja \ndf1 &lt;- as.data.frame(Y1) %&gt;%\n  mutate(grupa = \"Grupa 1\")\n\ndf2 &lt;- as.data.frame(Y2) %&gt;%\n  mutate(grupa = \"Grupa 2\")\n\ndf_all &lt;- bind_rows(df1, df2)\ncolnames(df_all)[1:2] &lt;- c(\"X1\", \"X2\")\n\n# Ramka danych ze Å›rednimi\nmeans &lt;- data.frame(\n  X1 = c(y1_bar[1], y2_bar[1]),\n  X2 = c(y1_bar[2], y2_bar[2]),\n  grupa = c(\"Grupa 1\", \"Grupa 2\")\n)\n\n# Wykres\nggplot(df_all, aes(x = X1, y = X2, color = grupa, shape = grupa)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_point(data = means, aes(x = X1, y = X2),\n             shape = c(1, 2), size = 5, stroke = 1.2, show.legend = FALSE) +\n  scale_shape_manual(values = c(16, 17)) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  coord_equal() +\n  theme_minimal() +\n  labs(title = \"PorÃ³wnanie dwÃ³ch grup\",\n       x = \"X1\", y = \"X2\", color = \"Grupa\", shape = \"Grupa\")\n\n\n\n\n\n\n\n\nKod# Alternatywnie, uÅ¼ycie gotowej funkcji z pakietu ICSNP\nlibrary(ICSNP)\nHotellingsT2(rbind(Y1, Y2)~factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Hotelling's two sample T2-test\n\ndata:  rbind(Y1, Y2) by factor(c(rep(1, n1), rep(2, n2)))\nT.2 = 8.5321, df1 = 2, df2 = 62, p-value = 0.000533\nalternative hypothesis: true location difference is not equal to c(0,0)\n\n\nW analizowanym przykÅ‚adzie zdefiniowaliÅ›my macierze kowariancji identycznie ale w rzeczywistoÅ›ci naleÅ¼aÅ‚oby testowaÄ‡ hipotezÄ™ o rÃ³wnoÅ›ci macierzy kowariancji. Tylko dla celÃ³w Ä‡wiczeniowych pokaÅ¼Ä™ jak to zrobiÄ‡.\n\nKod# Test Boxa na rÃ³wnoÅ›Ä‡ macierzy kowariancji\nlibrary(biotools)\nboxM(rbind(Y1, Y2), factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  rbind(Y1, Y2)\nChi-Sq (approx.) = 2.1261, df = 3, p-value = 0.5466\n\nKod# lub z wykorzystaniem pakietu rstatix\nlibrary(rstatix)\nbox_m(df_all[,-3], df_all[,3])\n\n# A tibble: 1 Ã— 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      2.13   0.547         3 Box's M-test for Homogeneity of Covariance Matricâ€¦\n\n\n\n\n\n\n\n\n\nWskazÃ³wka\n\n\n\nW sytuacji, gdy zaÅ‚oÅ¼enie o rÃ³wnoÅ›ci macierzy kowariancji jest naruszone, moÅ¼na stosowaÄ‡ alternatywne metody, takie jak:\n\nTesty permutacyjne (Hotelling::hotelling.test(Y~Group, perm = TRUE, B = 5000)).\nUogÃ³lniony test Hotellinga - test Jamesa (ang. Jamesâ€™s second-order test) lub czasami nazywany rÃ³wnieÅ¼ testem Welch-type Hotelling test (Hotelling::hotelling.test(Y~Group, var.equal = FALSE)).\nW przypadku danych charakteryzujÄ…cych siÄ™ duÅ¼Ä… liczbÄ… zmiennych w stosunku do liczby obserwacji, moÅ¼na rozwaÅ¼yÄ‡ uÅ¼ycie estymatora Jamesa-Steina do stabilizaji macierzy kowariancji (Hotelling::hotelling.test(Y~Group, shrinkage = TRUE)).\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nOdrzucenie hipotezy zerowej \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) w teÅ›cie Hotellinga \\(T^2\\) oznacza, Å¼e mamy statystycznie istotny dowÃ³d na to, iÅ¼ rozkÅ‚ady Å›rednich wektorÃ³w dwÃ³ch populacji wielowymiarowych rÃ³Å¼niÄ… siÄ™, biorÄ…c pod uwagÄ™ wspÃ³Å‚zmiennoÅ›Ä‡ miÄ™dzy zmiennymi. W kontekÅ›cie zastosowaÅ„ praktycznych, oznacza to, Å¼e przynajmniej jedna zmienna (lub kombinacja zmiennych) odrÃ³Å¼nia grupy, nawet jeÅ›li nie da siÄ™ tego wykazaÄ‡ za pomocÄ… testÃ³w jednowymiarowych.\nW sytuacji, gdy mamy dane wielowymiarowe \\(\\boldsymbol{y}_{gi} \\in \\mathbb{R}^p\\) z dwÃ³ch niezaleÅ¼nych grup (\\(g = 1,2\\)), testujemy:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu}_1 \\neq \\boldsymbol{\\mu}_2.\n\\]\nOdrzucenie \\(H_0\\) sugeruje, Å¼e istnieje rÃ³Å¼nica pomiÄ™dzy Å›rednimi wektorami, ale nie musi oznaczaÄ‡, Å¼e ktÃ³rakolwiek ze Å›rednich poszczegÃ³lnych zmiennych \\(\\mu_{1j}, \\mu_{2j}\\) rÃ³Å¼ni siÄ™ istotnie â€” zwÅ‚aszcza jeÅ›li uwzglÄ™dnimy korelacje miÄ™dzy zmiennymi.\nTesty jednowymiarowe ignorujÄ… te wspÃ³Å‚zaleÅ¼noÅ›ci, dlatego mogÄ… nie wykazaÄ‡ istotnych rÃ³Å¼nic, mimo Å¼e ogÃ³lny profil wielowymiarowy siÄ™ rÃ³Å¼ni. Innymi sÅ‚owy, moÅ¼e nie istnieÄ‡ Å¼adna istotna rÃ³Å¼nica w poszczegÃ³lnych zmiennych, ale pewna kombinacja liniowa tych zmiennych pozwala na rozrÃ³Å¼nienie grup.\nRozwaÅ¼my kombinacjÄ™ liniowÄ…:\n\\[\nz = \\boldsymbol{a}^\\top \\boldsymbol{y},\n\\]\ngdzie \\(\\boldsymbol{a} \\in \\mathbb{R}^p\\) to niezerowy wektor wag. WÃ³wczas \\(z\\) jest jednowymiarowÄ… zmiennÄ… losowÄ… bÄ™dÄ…cÄ… projekcjÄ… obserwacji \\(\\boldsymbol{y}\\) na kierunek \\(\\boldsymbol{a}\\).\nJeÅ›li hipoteza \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) jest faÅ‚szywa, to istnieje taki wektor \\(\\boldsymbol{a}\\), dla ktÃ³rego:\n\\[\nH_0: \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_1 = \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_2\n\\]\nzostanie odrzucona w teÅ›cie jednowymiarowym. Dla takiej kombinacji liniowej moÅ¼emy zdefiniowaÄ‡ statystykÄ™ \\(t\\)-Studenta:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\bar{z}_1 - \\bar{z}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) s_z^2}},\n\\]\ngdzie:\n\n\n\\(\\bar{z}_g = \\boldsymbol{a}^\\top \\bar{\\boldsymbol{y}}_g\\) â€“ Å›rednia z projekcji grupy \\(g\\),\n\n\\(s_z^2\\) â€“ nieobciÄ…Å¼ony estymator wariancji \\(z\\), czyli:\n\n\\[\ns_z^2 = \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a},\n\\]\na \\(\\mathbf{S}\\) to wspÃ³lna macierz kowariancji.\nStÄ…d peÅ‚na postaÄ‡ statystyki:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\boldsymbol{a}^\\top (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a}}}.\n\\]\nPoniewaÅ¼ statystyka \\(t(\\boldsymbol{a})\\) moÅ¼e byÄ‡ zarÃ³wno dodatnia, jak i ujemna, stosuje siÄ™ czÄ™sto jej kwadrat jako miarÄ™ istotnoÅ›ci:\n\\[\nT^2 = t^2(\\boldsymbol{a}).\n\\]\nStatystyka Hotellinga \\(T^2\\) przyjmuje postaÄ‡:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nJest to forma uogÃ³lnionej odlegÅ‚oÅ›ci Mahalanobisa miÄ™dzy Å›rednimi wektorami. MoÅ¼na pokazaÄ‡, Å¼e istnieje taki wektor \\(\\boldsymbol{a}\\), ktÃ³ry maksymalizuje rÃ³Å¼nicÄ™ \\(t(\\boldsymbol{a})\\) â€” to tzw. funkcja dyskryminacyjna:\n\\[\n\\boldsymbol{a} = \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nDla tej wartoÅ›ci wektora \\(\\boldsymbol{a}\\), statystyka \\(T^2\\) przyjmuje najwiÄ™kszÄ… wartoÅ›Ä‡ i jest najbardziej czuÅ‚a na rÃ³Å¼nice miÄ™dzy grupami.\nOdrzucenie \\(H_0\\) oznacza wiÄ™c, Å¼e w przestrzeni \\(\\mathbb{R}^p\\) istnieje kierunek \\(\\boldsymbol{a}\\), dla ktÃ³rego grupy majÄ… rÃ³Å¼ne Å›rednie projekcje. To otwiera drogÄ™ do:\n\nkonstrukcji funkcji dyskryminacyjnych (jak w analizie dyskryminacyjnej),\nidentyfikacji zmiennych lub ich kombinacji odpowiedzialnych za rÃ³Å¼nicÄ™,\ndalszych analiz jednowymiarowych dla projekcji \\(z = \\boldsymbol{a}^\\top \\boldsymbol{y}\\).\n\n\nKodlibrary(gridExtra)  # dla strzaÅ‚ki jako warstwy\n\nset.seed(42)\n\n# Parametry\nn1 &lt;- n2 &lt;- 100\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1.5, 0.5)\nSigma &lt;- matrix(c(1, 0.8, 0.8, 1), ncol = 2)\n\n# Dane\nY1 &lt;- mvrnorm(n1, mu1, Sigma)\nY2 &lt;- mvrnorm(n2, mu2, Sigma)\n\n# Åšrednie\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymacja wspÃ³lnej kowariancji\nS_pooled &lt;- ((n1 - 1) * cov(Y1) + (n2 - 1) * cov(Y2)) / (n1 + n2 - 2)\n\n# Kierunek dyskryminacyjny a\ndiff &lt;- y1_bar - y2_bar\na &lt;- solve(S_pooled, diff)\na_norm &lt;- a / sqrt(sum(a^2))  # normalizacja\n\n# Punkt startowy strzaÅ‚ki (Å›rodek miÄ™dzy Å›rednimi)\norigin &lt;- (y1_bar + y2_bar) / 2\nscale &lt;- 3  # dÅ‚ugoÅ›Ä‡ strzaÅ‚ki\narrow_end &lt;- origin + scale * a_norm\n\n# Ramka danych do wykresu\ndf &lt;- rbind(\n  data.frame(X1 = Y1[,1], X2 = Y1[,2], Grupa = \"Grupa 1\"),\n  data.frame(X1 = Y2[,1], X2 = Y2[,2], Grupa = \"Grupa 2\")\n)\n\n# Wektory Å›rednich i strzaÅ‚ka\nmeans_df &lt;- data.frame(rbind(y1_bar, y2_bar))\ncolnames(means_df) &lt;- c(\"X1\", \"X2\")\nmeans_df$Grupa &lt;- c(\"Grupa 1\", \"Grupa 2\")  # te same etykiety co w danych punktÃ³w\n\narrow_df &lt;- data.frame(\n  x = origin[1],\n  y = origin[2],\n  xend = arrow_end[1],\n  yend = arrow_end[2]\n)\n\n# Wykres\nggplot(df, aes(x = X1, y = X2, color = Grupa)) +\n  geom_point(alpha = 0.5) +\n  geom_point(data = means_df, aes(x = X1, y = X2),\n             size = 4, shape = 16, show.legend = FALSE) +  # Å›rednie tym samym kolorem; bez legendy\n  geom_segment(data = arrow_df, \n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")), color = \"black\", size = 1, show.legend = FALSE) +\n  labs(\n    title = latex2exp::TeX(r\"(Ilustracja kierunku dyskryminacyjnego $a = S^{-1} (\\bar{y}_1 - \\bar{y}_2)$)\"),\n    x = latex2exp::TeX(r\"($X_1$)\"),\n    y = latex2exp::TeX(r\"($X_2$)\")\n  ) +\n  coord_equal() +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#zaÅ‚oÅ¼enia-i-testowane-hipotezy",
    "href": "multi_tests.html#zaÅ‚oÅ¼enia-i-testowane-hipotezy",
    "title": "Testy wielowymiarowe",
    "section": "ZaÅ‚oÅ¼enia i testowane hipotezy",
    "text": "ZaÅ‚oÅ¼enia i testowane hipotezy\nZakÅ‚ada siÄ™, Å¼e obserwacje sÄ… niezaleÅ¼ne i pochodzÄ… z wielowymiarowego rozkÅ‚adu normalnego w kaÅ¼dej grupie, tj.:\n\\[\n\\boldsymbol{y}_{ij} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}), \\quad i = 1, \\dots, g,\\ j = 1, \\dots, n_i,\n\\]\ngdzie:\n\n\n\\(\\boldsymbol{y}_{ij} \\in \\mathbb{R}^p\\) â€” wektor obserwacji w grupie \\(i\\),\n\n\\(\\boldsymbol{\\mu}_i\\) â€” wektor Å›rednich dla grupy \\(i\\),\n\n\\(\\boldsymbol{\\Sigma}\\) â€” wspÃ³lna macierz kowariancji we wszystkich grupach (zaÅ‚oÅ¼enie homogenicznoÅ›ci).\n\nTestowana jest hipoteza zerowa:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\dots = \\boldsymbol{\\mu}_g\n\\]\nwobec alternatywy:\n\\[\nH_1: \\exists\\ i,j\\ \\text{takie, Å¼e}\\ \\boldsymbol{\\mu}_i \\ne \\boldsymbol{\\mu}_j.\n\\]\nModel MANOVA opiera siÄ™ na kilku fundamentalnych zaÅ‚oÅ¼eniach dotyczÄ…cych danych, ktÃ³rych speÅ‚nienie warunkuje poprawnoÅ›Ä‡ i wiarygodnoÅ›Ä‡ uzyskanych wynikÃ³w. Ich naruszenie moÅ¼e prowadziÄ‡ do faÅ‚szywych wnioskÃ³w, zbyt duÅ¼ej liczby odrzuceÅ„ hipotezy zerowej lub do bÅ‚Ä™dnych ocen efektÃ³w czynnikÃ³w. PoniÅ¼ej przedstawiono szczegÃ³Å‚owo kaÅ¼de z tych zaÅ‚oÅ¼eÅ„.\n\nPierwszym kluczowym zaÅ‚oÅ¼eniem jest odpowiednia wielkoÅ›Ä‡ prÃ³by. Przyjmuje siÄ™, Å¼e liczba obserwacji w kaÅ¼dej grupie (komÃ³rce) powinna przekraczaÄ‡ liczbÄ™ zmiennych zaleÅ¼nych, ktÃ³re sÄ… jednoczeÅ›nie analizowane. To praktyczne zalecenie pozwala uniknÄ…Ä‡ problemÃ³w z oszacowaniem macierzy kowariancji i zapewnia dostatecznÄ… moc statystycznÄ….\nKolejnym istotnym zaÅ‚oÅ¼eniem jest niezaleÅ¼noÅ›Ä‡ obserwacji. Oznacza to, Å¼e kaÅ¼da jednostka obserwacyjna (np. osoba) powinna przynaleÅ¼eÄ‡ wyÅ‚Ä…cznie do jednej grupy. Obserwacje wewnÄ…trz i pomiÄ™dzy grupami nie mogÄ… byÄ‡ ze sobÄ… powiÄ…zane. W szczegÃ³lnoÅ›ci, model MANOVA nie jest odpowiedni dla danych z pomiarami powtarzanymi u tych samych obiektÃ³w. DobÃ³r prÃ³by powinien byÄ‡ dokonany w sposÃ³b losowy, bez systematycznych zaleÅ¼noÅ›ci.\nTrzecim wymogiem jest brak obserwacji odstajÄ…cych, zarÃ³wno w sensie jednowymiarowym (dla kaÅ¼dej zmiennej z osobna), jak i wielowymiarowym (dla kombinacji wszystkich zmiennych zaleÅ¼nych). Obserwacje odstajÄ…ce mogÄ… silnie znieksztaÅ‚caÄ‡ wartoÅ›ci Å›rednich i macierzy kowariancji, przez co wyniki MANOVA stajÄ… siÄ™ niestabilne.\nFundamentalnym zaÅ‚oÅ¼eniem MANOVA jest wielowymiarowa normalnoÅ›Ä‡ rozkÅ‚adu danych w kaÅ¼dej z grup. Oznacza to, Å¼e wektor zmiennych zaleÅ¼nych w kaÅ¼dej grupie powinien mieÄ‡ rozkÅ‚ad wielowymiarowy normalny. W R moÅ¼na zastosowaÄ‡ funkcjÄ™ mshapiro_test() z pakietu rstatix, aby przeprowadziÄ‡ test Shapiroâ€“Wilka dla sprawdzenia normalnoÅ›ci wielowymiarowej.\nKolejne zaÅ‚oÅ¼enie dotyczy braku wspÃ³Å‚liniowoÅ›ci. Oczekuje siÄ™, Å¼e zmienne zaleÅ¼ne bÄ™dÄ… ze sobÄ… skorelowane w umiarkowany sposÃ³b, ale nie nadmiernie. WartoÅ›ci wspÃ³Å‚czynnikÃ³w korelacji przekraczajÄ…ce \\(r = 0,90\\) sÄ… uznawane za niepoÅ¼Ä…dane i mogÄ… powodowaÄ‡ problemy numeryczne oraz bÅ‚Ä™dnÄ… interpretacjÄ™ wynikÃ³w. Jak podajÄ… Tabachnick i Fidell (2012), zmienne powinny wnosiÄ‡ unikalne informacje do modelu.\nWaÅ¼nym wymogiem jest rÃ³wnieÅ¼ liniowoÅ›Ä‡ zaleÅ¼noÅ›ci miÄ™dzy zmiennymi zaleÅ¼nymi w kaÅ¼dej grupie. Oznacza to, Å¼e zaleÅ¼noÅ›ci pomiÄ™dzy kaÅ¼dÄ… parÄ… zmiennych muszÄ… byÄ‡ dobrze opisane przez funkcjÄ™ liniowÄ… â€” jest to konieczne, aby poprawnie oszacowaÄ‡ strukturÄ™ kowariancji.\nDla poprawnego dziaÅ‚ania MANOVA zakÅ‚ada siÄ™ takÅ¼e jednorodnoÅ›Ä‡ wariancji dla kaÅ¼dej zmiennej zaleÅ¼nej miÄ™dzy grupami. MoÅ¼na to testowaÄ‡ za pomocÄ… testu Leveneâ€™a. Nieistotny wynik testu Leveneâ€™a sugeruje, Å¼e wariancje sÄ… porÃ³wnywalne w grupach.\nOstatnie, ale bardzo istotne, jest zaÅ‚oÅ¼enie o jednorodnoÅ›ci macierzy kowariancji (homogenicznoÅ›ci macierzy wariancjiâ€“kowariancji) pomiÄ™dzy grupami. Oznacza to, Å¼e struktura wspÃ³Å‚zaleÅ¼noÅ›ci miÄ™dzy zmiennymi powinna byÄ‡ podobna w kaÅ¼dej grupie. WeryfikacjÄ™ tego zaÅ‚oÅ¼enia umoÅ¼liwia test Boxa (Boxâ€™s M test), ktÃ³ry stanowi wielowymiarowy odpowiednik testu Leveneâ€™a. Ze wzglÄ™du na duÅ¼Ä… czuÅ‚oÅ›Ä‡ testu Boxa na odstÄ™pstwa od zaÅ‚oÅ¼eÅ„, przyjmuje siÄ™ konserwatywny poziom istotnoÅ›ci \\(\\alpha = 0,001\\) dla weryfikacji jego wyniku.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "href": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "title": "Testy wielowymiarowe",
    "section": "Konstrukcja modelu i statystyki testowe",
    "text": "Konstrukcja modelu i statystyki testowe\nPodobnie jak w jednowymiarowym przypadku, w MANOVA analizuje siÄ™ rozkÅ‚ad wariancji caÅ‚kowitej na wariancjÄ™ miÄ™dzygrupowÄ… i wewnÄ…trzgrupowÄ…, ale w postaci macierzy kowariancji:\n\nMacierz wariancji miÄ™dzygrupowej (ang. between-group SSCP):\n\n\\[\n\\mathbf{B} = \\sum_{i=1}^{g} n_i (\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})(\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})^\\top\n\\]\n\nMacierz wariancji wewnÄ…trzgrupowej (ang. within-group SSCP):\n\n\\[\n\\mathbf{W} = \\sum_{i=1}^{g} \\sum_{j=1}^{n_i} (\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)(\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)^\\top\n\\]\nMacierz wariancji caÅ‚kowitej to: \\(\\mathbf{T} = \\mathbf{B} + \\mathbf{W}\\).\nW celu przeprowadzenia testu MANOVA, wykorzystuje siÄ™ statystyki oparte na stosunku macierzy:\n\\[\n\\mathbf{W}^{-1} \\mathbf{B}\n\\]\nNajczÄ™Å›ciej spotykane statystyki testowe to:\n\nWilksâ€™ Lambda:\n\n\\[\n\\Lambda = \\frac{\\det(\\mathbf{W})}{\\det(\\mathbf{B} + \\mathbf{W})}\n\\]\n\nStatystyka Pillai-Bartletta (Trace):\n\n\\[\nV = \\mathrm{tr}\\left[(\\mathbf{B} + \\mathbf{W})^{-1} \\mathbf{B}\\right]\n\\]\n\nStatystyka Hotellingaâ€“Lawleya (Trace):\n\n\\[\nT = \\mathrm{tr}(\\mathbf{W}^{-1} \\mathbf{B})\n\\]\n\nNajwiÄ™kszy pierwiastek Royâ€™a:\n\n\\[\n\\theta_{\\text{max}} = \\text{najwiÄ™ksza wartoÅ›Ä‡ wÅ‚asna}\\ (\\mathbf{W}^{-1} \\mathbf{B})\n\\]\nWybÃ³r konkretnej statystyki zaleÅ¼y od liczebnoÅ›ci prÃ³b, wymiaru przestrzeni i liczby grup. W praktyce Wilksâ€™ Lambda jest najczÄ™Å›ciej stosowana.\n\nPrzykÅ‚ad 5.1 Na poziomie istotnoÅ›ci \\(\\alpha=0.05\\) zweryfikuj hipotezÄ™, Å¼e czynnik grupujÄ…cy (Group) istotnie rÃ³Å¼nicuje zmienne Actions i Thoughts jednoczeÅ›nie.\n\nKodlibrary(gtsummary)\nlibrary(rstatix)\nlibrary(easystats)\nlibrary(gt)\ndane &lt;- rio::import(\"data/OCD.dat\")\n\n\nStatystyki opisowe grup\n\nKoddane %&gt;% \n  tbl_summary(by = Group,\n              statistic = list(where(is.numeric) ~ \"{mean} ({sd})\"),\n              type = list(Actions ~ \"continuous\"),\n              digits = list(everything() ~ 2))\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nBT\nN = 101\n\n\nCBT\nN = 101\n\n\nNo Treatment Control\nN = 101\n\n\n\n\nActions\n3.70 (1.77)\n4.90 (1.20)\n5.00 (1.05)\n\n\nThoughts\n15.20 (2.10)\n13.40 (1.90)\n15.00 (2.36)\n\n\n\n\n1 Mean (SD)\n\n\n\n\nKodp &lt;- dane %&gt;% \n  select(-Group) %&gt;% \n  correlation()\n\np %&gt;% print_html()\n\n\n\n\n\n\nCorrelation Matrix (pearson-method)\n\n\nParameter1\nParameter2\nr\n95% CI\nt(28)\np\n\n\n\nActions\nThoughts\n0.06\n(-0.31, 0.41)\n0.31\n0.758\n\n\np-value adjustment method: Holm (1979); Observations: 30\n\n\n\n\n\nW kontekÅ›cie zmiennej Actions widzimy najwyÅ¼szy poziom w grupie No treatment, natomiast najniÅ¼szy w grupie BT. Dla zmiennej Thoughts najwyÅ¼szy poziom osiÄ…gniÄ™to w grupie BT a najniÅ¼szy w grupie CBT. Grupy rÃ³Å¼niÄ… siÄ™ rÃ³wnieÅ¼ zmiennoÅ›ciÄ… obu cech. ZwiÄ…zek pomiÄ™dzy zmiennymi Actions i Thoughts jest niemal niezauwaÅ¼alny. Korelacja pomiÄ™dzy tymi cechami jest nieistotnie rÃ³Å¼na od zera.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  ggplot(aes(x = Group, y = value, fill = name)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(fill = \"Variable\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\nPowyÅ¼sze wykresy potwierdzajÄ… znaczne rÃ³Å¼nice pomiÄ™dzy grupami w kontekÅ›cie analizowanych cech.\nZaÅ‚oÅ¼enia\n\nKoddane %&gt;% \n  group_split(Group) %&gt;% \n  map_df(~mshapiro_test(.x[,2:3])) %&gt;% \n  mutate(Group = dane %&gt;% \n           group_keys(Group) %&gt;% \n           pull(Group),\n         .before = statistic) %&gt;%\n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nstatistic\np.value\n\n\n\nBT\n0.891\n0.175\n\n\nCBT\n0.959\n0.777\n\n\nNo Treatment Control\n0.826\n0.030\n\n\n\n\n\n\nJedynie w grupie No treatment nie jest zachowana wielowymiarowa normalnoÅ›Ä‡ rozkÅ‚adu analizowanych cech. MoÅ¼na teÅ¼ przeprowadziÄ‡ testy normalnoÅ›ci poszczegÃ³lnych zmiennych, ale naleÅ¼y pamiÄ™taÄ‡, Å¼e brak podstaw do odrzucenia hipotezy o normalnoÅ›ci brzegowych zmiennych nie jest warunkiem dostatecznym, a jedynie koniecznym.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Actions) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nActions\n0.872\n0.106\n\n\nCBT\nActions\n0.952\n0.691\n\n\nNo Treatment Control\nActions\n0.859\n0.074\n\n\n\n\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nThoughts\n0.877\n0.120\n\n\nCBT\nThoughts\n0.914\n0.310\n\n\nNo Treatment Control\nThoughts\n0.826\n0.030\n\n\n\n\n\n\nPodobnie jak w przypadku wielowymiarowym brak normalnoÅ›ci zarysowaÅ‚ siÄ™ w grupie No treatment i to tylko dla zmiennej Thoughts. Teraz przechodzimy do testowania jednorodnoÅ›ci kowariancji.\n\nKodbox_m(dane[,2:3], dane$Group)  %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n8.893\n0.180\n6.000\nBox's M-test for Homogeneity of Covariance Matrices\n\n\n\n\n\nNa podstawie powyÅ¼szego testu moÅ¼na stwierdziÄ‡, iÅ¼ nie ma podstaw do odrzucenia hipotezy o jednorodnoÅ›ci macierzy kowariancji.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Actions) \n\n[1] Group      Actions    Thoughts   is.outlier is.extreme\n&lt;0 wierszy&gt; (lub 'row.names' o zerowej dÅ‚ugoÅ›ci)\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nActions\nThoughts\nis.outlier\nis.extreme\n\n\nNo Treatment Control\n4\n20\nTRUE\nFALSE\n\n\n\n\nKodwhich(dane$Actions == 4 & dane$Thoughts == 20)\n\n[1] 26\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  mahalanobis_distance() %&gt;% \n  filter(is.outlier==TRUE)\n\n# A tibble: 0 Ã— 4\n# â„¹ 4 variables: Actions &lt;int&gt;, Thoughts &lt;int&gt;, mahal.dist &lt;dbl&gt;,\n#   is.outlier &lt;lgl&gt;\n\n\nIstnieje jedna obserwacja odstajÄ…ca w grupie No treatment (obserwacja nr 26). Test wielowymiarowy nie wykryÅ‚ Å¼adnego elementu odstajÄ…cego.\nPomimo niespeÅ‚nienia zaÅ‚oÅ¼enia o wielowymiarowej normalnoÅ›ci cech w grupach, zastosujemy test MANOVA.\nManova\n\nKodmod &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane)\nManova(mod) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.32\n4\n54\n2.56\n0.049\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Wilk\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.70\n4\n52\n2.55\n0.050\n\n\nWilks test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Roy\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.33\n2\n27\n4.52\n0.020\n\n\nRoy test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Hotelling\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.41\n4\n50\n2.55\n0.051\n\n\nHotelling-Lawley test statistic Anova Table (Type 2 tests)\n\n\n\n\nKod# model bez obserawcji odstajÄ…cej\nmod2 &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane[-26,])\nManova(mod2) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.36\n4\n52\n2.87\n0.032\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\n\nAnalizujÄ… wszystkie rodzaje testÃ³w Manova, widzimy, Å¼e jedynie test Hotellinga-Laweya nie daje podstaw do odrzucenia hipotezy o rÃ³wnoÅ›ci wektorÃ³w Å›rednich. Natomiast poniewaÅ¼ co najmniej jeden z nich wskazaÅ‚ istotnoÅ›Ä‡ rÃ³Å¼nic, to przyjmujemy, Å¼e sÄ… podstawy aby odrzuciÄ‡ hipotezÄ™ o rÃ³wnoÅ›ci wektorÃ³w Å›rednich pomiÄ™dzy grupami. Test wykluczajÄ…cy obserwacjÄ™ odstajÄ…cÄ… rÃ³wnieÅ¼ kaÅ¼e odrzuciÄ‡ hipotezÄ™ \\(H_0\\).\nPrzeprowadzimy zatem analizÄ™ brzegowÄ….\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  anova_test(value~Group) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\nActions\nGroup\n2.000\n27.000\n2.771\n0.080\n\n0.170\n\n\nThoughts\nGroup\n2.000\n27.000\n2.154\n0.136\n\n0.138\n\n\n\n\n\n\nAnaliza brzegowa pokazuje ciekawy wynik, mianowicie, dla Å¼adnej z analizowanych cech testy brzegowe nie wykazaÅ‚y istotnych rÃ³Å¼nic. To pokazuje jak waÅ¼ne jest stosowanie testÃ³w wielowymiarowych w kontekÅ›cie porÃ³wnaÅ„ grup.\nPost-hoc\nPoniewaÅ¼ testy brzegowe ANOVA nie wykazaÅ‚y rÃ³Å¼nic, to testÃ³w post-hoc nie powinno siÄ™ wykonywaÄ‡, ale dla celÃ³w Ä‡wiczeniowych pokaÅ¼Ä™ jak je wykonaÄ‡.\n\nKodpwc &lt;- dane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  games_howell_test(value~Group)\npwc %&gt;% \n  select(-.y.) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\ngroup1\ngroup2\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\nActions\nBT\nCBT\n1.200\nâˆ’0.544\n2.944\n0.209\nns\n\n\nActions\nBT\nNo Treatment Control\n1.300\nâˆ’0.394\n2.994\n0.148\nns\n\n\nActions\nCBT\nNo Treatment Control\n0.100\nâˆ’1.189\n1.389\n0.979\nns\n\n\nThoughts\nBT\nCBT\nâˆ’1.800\nâˆ’4.085\n0.485\n0.138\nns\n\n\nThoughts\nBT\nNo Treatment Control\nâˆ’0.200\nâˆ’2.749\n2.349\n0.978\nns\n\n\nThoughts\nCBT\nNo Treatment Control\n1.600\nâˆ’0.852\n4.052\n0.244\nns\n\n\n\n\n\n\nTesty post-hoc potwierdzajÄ… wyniki testÃ³w brzegowych ANOVA, poniewaÅ¼ brakuje rÃ³Å¼nic pomiÄ™dzy poziomami zmiennych grupujÄ…cych.\n\n\n\n\n\nAnderson, T. W. 1992. â€Introduction to Hotelling (1931) The Generalization of Studentâ€™s Ratioâ€. W, 45â€“53. Springer New York. https://doi.org/10.1007/978-1-4612-0919-5_3.\n\n\nHotelling, Harold. 1992. â€The generalization of Studentâ€™s ratioâ€. W, 54â€“65. Springer.\n\n\nHuberty, Carl J., i John D. Morris. 1989. â€Multivariate Analysis Versus Multiple Univariate Analyses.â€ Psychological Bulletin 105 (2): 302â€“8. https://doi.org/10.1037/0033-2909.105.2.302.\n\n\nâ€Omnibus MANOVA Testsâ€. 1985. W, 14â€“39. SAGE Publications, Inc. https://doi.org/10.4135/9781412985222.d16.\n\n\nRencher, Alvin C. 1998. Multivariate statistical inference and applications. T. 635. Wiley New York.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "cca.html",
    "href": "cca.html",
    "title": "Analiza kanoniczna",
    "section": "",
    "text": "Przypomnienie z algebry ğŸ˜‰\nAnaliza kanoniczna (ang. Canonical Correlation Analysis, CCA) jest klasycznÄ… technikÄ… statystycznÄ… sÅ‚uÅ¼Ä…cÄ… do badania zwiÄ…zkÃ³w pomiÄ™dzy dwoma zestawami zmiennych wielowymiarowych. Jej podstawowym celem jest znalezienie takich kombinacji liniowych zmiennych z obu zestawÃ³w, ktÃ³re maksymalizujÄ… wzajemnÄ… korelacjÄ™ â€“ sÄ… to tzw. kanoniczne zmienne lub kanoniczne skÅ‚adniki. Technika ta zostaÅ‚a wprowadzona przez Harolda Hotellinga w roku 1936, a wiÄ™c w okresie intensywnego rozwoju metod statystycznych opartych na algebrze macierzy (Hotelling 1936).\nW tym samym czasie powstawaÅ‚y takÅ¼e inne fundamenty analizy wielowymiarowej, takie jak analiza skÅ‚adowych gÅ‚Ã³wnych (PCA) czy dyskryminacja liniowa (LDA)1. Analiza kanoniczna stanowi zatem jeden z filarÃ³w klasycznej statystyki wielowymiarowej i do dziÅ› pozostaje istotnym narzÄ™dziem eksploracji i modelowania zÅ‚oÅ¼onych zaleÅ¼noÅ›ci.\nW odrÃ³Å¼nieniu od regresji wielorakiej, ktÃ³ra przewiduje zestaw zmiennych zaleÅ¼nych na podstawie zestawu predyktorÃ³w, analiza kanoniczna traktuje obie grupy zmiennych symetrycznie â€“ nie zakÅ‚ada istnienia wyraÅºnego kierunku przyczynowego. Dlatego stosuje siÄ™ jÄ… w sytuacjach, gdy celem jest ogÃ³lna analiza wspÃ³Å‚zaleÅ¼noÅ›ci pomiÄ™dzy dwoma zbiorami zmiennych, a nie przewidywanie jednego zestawu na podstawie drugiego.\nTypowe zastosowania analizy kanonicznej obejmujÄ…:\nNa potrzeby definicji modeli kanonicznego potrzebne bÄ™dÄ… nam pewne twierdzenia z zakresu algebry.\nOto matematyczna definicja modelu CCA oraz dowÃ³d istnienia rozwiÄ…zania, sformuÅ‚owana Å›ciÅ›le w duchu Twojego tekstu:",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#sformuÅ‚owanie-problemu-wÅ‚asnego",
    "href": "cca.html#sformuÅ‚owanie-problemu-wÅ‚asnego",
    "title": "Analiza kanoniczna",
    "section": "SformuÅ‚owanie problemu wÅ‚asnego",
    "text": "SformuÅ‚owanie problemu wÅ‚asnego\nPrzeksztaÅ‚Ä‡my zmienne \\[\nc=\\Sigma_{XX}^{1/2}a,\\quad d=\\Sigma_{YY}^{1/2}b.\n\\]\nWÃ³wczas \\[\n\\rho(a,b)=\\frac{c^\\top\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}d}{\\sqrt{c^\\top c}\\sqrt{d^\\top d}}.\n\\]\nZ lematu Cauchyâ€™egoâ€“Buniakowskiegoâ€“Schwarza mamy \\[\n\\left|c^\\top \\mathbf{M} d\\right| \\le\n\\bigl(c^\\top \\mathbf{M}\\mathbf{M}^\\top c\\bigr)^{1/2}\\bigl(d^\\top d\\bigr)^{1/2},\n\\quad \\text{gdzie }\\mathbf{M}=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}.\n\\tag{5.1}\\]\nZatem \\[\n\\rho(a,b)^2 \\le\n\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}.\n\\]\nPoniewaÅ¼ \\(\\mathbf{M}\\mathbf{M}^\\top=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\) jest macierzÄ… symetrycznÄ… dodatnio okreÅ›lonÄ…, z lematu Rayleighaâ€“Ritza otrzymujemy \\[\n\\max_{c\\neq 0}\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}=\\lambda_1,\n\\] gdzie \\(\\lambda_1\\) to najwiÄ™ksza wartoÅ›Ä‡ wÅ‚asna tej macierzy, osiÄ…gana dla \\(c=e_1\\) â€“ jej wektora wÅ‚asnego.\nJeÅ›li \\[\nd \\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1\n\\] to RÃ³wnanieÂ 5.1 staje siÄ™ rÃ³wnoÅ›ciÄ….\nWracajÄ…c do oryginalnych wspÃ³Å‚czynnikÃ³w \\[\na_1=\\Sigma_{XX}^{-1/2}e_1,\\quad\nb_1\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1.\n\\]\nPierwsza korelacja kanoniczna wynosi wÃ³wczas \\[\n\\rho_1=\\sqrt{\\lambda_1}.\n\\]\nAnalogicznie dla kolejnych par, przy zaÅ‚oÅ¼eniu \\(c\\perp e_1,\\dots,e_{k-1}\\), mamy \\[\n\\rho_k=\\sqrt{\\lambda_k},\n\\] gdzie \\(\\lambda_k\\) to kolejne wartoÅ›ci wÅ‚asne macierzy \\(\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\), a odpowiadajÄ…ce im wektory wÅ‚asne \\(e_k\\) definiujÄ… kolejne wektory kanoniczne \\[\nU_k=e_k^\\top\\Sigma_{XX}^{-1/2}X,\\quad\nV_k=f_k^\\top\\Sigma_{YY}^{-1/2}Y,\\quad\nf_k\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_k.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wÅ‚asnoÅ›ci-rozwiÄ…zaÅ„",
    "href": "cca.html#wÅ‚asnoÅ›ci-rozwiÄ…zaÅ„",
    "title": "Analiza kanoniczna",
    "section": "WÅ‚asnoÅ›ci rozwiÄ…zaÅ„",
    "text": "WÅ‚asnoÅ›ci rozwiÄ…zaÅ„\nDla kaÅ¼dej pary \\((U_k,V_k)\\) zachodzi \\[\n\\mathrm{Var}(U_k)=\\mathrm{Var}(V_k)=1,\\quad\n\\mathrm{Cov}(U_k,U_l)=\\mathrm{Cov}(V_k,V_l)=\\mathrm{Cov}(U_k,V_l)=0\\quad (k\\neq l).\n\\]\n\n\n\n\n\n\nAdnotacjaDowÃ³d powyÅ¼szych rÃ³wnoÅ›ci\n\n\n\nDla danej pary wektorÃ³w kanonicznych mamy\n\\[\nU_k = a_k^\\top X = e_k^\\top \\Sigma_{XX}^{-1/2} X,\n    \\qquad\n    V_k = b_k^\\top Y = f_k^\\top \\Sigma_{YY}^{-1/2} Y,\n\\] gdzie:\n\n\n\\(e_k\\) jest ortonormalnym6 wektorem wÅ‚asnym macierzy \\(\\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} \\Sigma_{XX}^{-1/2}\\)\n\n\n\\(f_k \\propto \\Sigma_{YY}^{-1/2} \\Sigma_{YX} \\Sigma_{XX}^{-1/2} e_k\\),\n\n\\(\\rho_k = \\sqrt{\\lambda_k}\\), gdzie \\(\\lambda_k\\) to odpowiadajÄ…ca wartoÅ›Ä‡ wÅ‚asna.\n\nMacierze \\(\\Sigma_{XX}\\), \\(\\Sigma_{YY}\\) sÄ… dodatnio okreÅ›lone, wiÄ™c moÅ¼na wprowadziÄ‡ transformacje \\[\n\\tilde{X} = \\Sigma_{XX}^{-1/2}X, \\quad \\tilde{Y} = \\Sigma_{YY}^{-1/2}Y.\n\\] Zatem \\[\nU_k = e_k^\\top \\tilde{X},\\quad V_k = f_k^\\top \\tilde{Y}.\n\\]\nNajpierw udowodnimy, Å¼e \\(\\operatorname{Var}(U_k) = \\operatorname{Var}(V_k) = 1\\).\nZmienna \\(U_k = e_k^\\top \\tilde{X}\\), wiÄ™c \\[\n\\operatorname{Var}(U_k) = \\operatorname{Var}(e_k^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_k.\n\\] ZauwaÅ¼my, Å¼e \\[\n\\operatorname{Var}(\\tilde{X}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XX} \\Sigma_{XX}^{-1/2} = I,\n\\] wiÄ™c \\[\n\\operatorname{Var}(U_k) = e_k^\\top I e_k = e_k^\\top e_k = 1.\n\\] Analogicznie \\[\n\\operatorname{Var}(V_k) = f_k^\\top \\operatorname{Var}(\\tilde{Y}) f_k = f_k^\\top f_k = 1,\n\\] poniewaÅ¼ \\(\\tilde{Y}\\) ma jednostkowÄ… macierz kowariancji, a \\(f_k\\) sÄ… znormalizowane.\nTeraz dowiedziemy, Å¼e \\(\\operatorname{Cov}(U_k, U_l) = 0\\) dla \\(k \\neq l\\)\n\\[\n\\operatorname{Cov}(U_k, U_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, e_l^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_l = e_k^\\top e_l.\n\\] PoniewaÅ¼ \\(e_k\\), \\(e_l\\) sÄ… ortonormalnymi wektorami wÅ‚asnymi symetrycznej macierzy, to \\[\ne_k^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, U_l) = 0.\n\\] Podobnie \\[\n\\operatorname{Cov}(V_k, V_l) = f_k^\\top f_l = 0 \\quad \\text{dla } k \\neq l.\n\\]\nNa koniec dowiedÅºmy, Å¼e \\(\\operatorname{Cov}(U_k, V_l) = 0\\) dla \\(k \\neq l\\) \\[\n\\operatorname{Cov}(U_k, V_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, f_l^\\top \\tilde{Y}) = e_k^\\top \\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) f_l.\n\\] Z definicji \\[\n\\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1/2} =: M.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = e_k^\\top M f_l.\n\\] Z poprzednich wyprowadzeÅ„ \\[\nf_l \\propto M^\\top e_l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) \\propto e_k^\\top M M^\\top e_l.\n\\] Ale macierz \\(MM^\\top\\) jest symetryczna, a \\(e_k\\) sÄ… jej ortonormalnymi wektorami wÅ‚asnymi, wiÄ™c \\[\ne_k^\\top MM^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = 0.\n\\]\n\n\n6Â ortonormalnoÅ›Ä‡ wynika z niezmienniczoÅ›ci korelacji wzglÄ™dem dÅ‚ugoÅ›ci wektorÃ³wPonadto korelacje kanoniczne sÄ… niezmiennicze wzglÄ™dem odwracalnych przeksztaÅ‚ceÅ„ liniowych \\(X\\) i \\(Y\\): \\[\nX^*=\\mathcal{U}^TX+u,\\quad Y^*=\\mathcal{V}^TY+v \\implies\n\\rho_i(X^*,Y^*)=\\rho_i(X,Y).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#hipoteza-zerowa-i-alternatywna",
    "href": "cca.html#hipoteza-zerowa-i-alternatywna",
    "title": "Analiza kanoniczna",
    "section": "Hipoteza zerowa i alternatywna",
    "text": "Hipoteza zerowa i alternatywna\nDla zbiorÃ³w zmiennych \\(X \\in \\mathbb{R}^p\\), \\(Y \\in \\mathbb{R}^q\\), testujemy\n\n\n\\(H_0: \\rho_1 = \\rho_2 = \\cdots = \\rho_s = 0\\) â€“ brak istotnych korelacji kanonicznych (pierwiastkÃ³w),\n\n\\(H_1\\): istnieje co najmniej jedna istotna korelacja kanoniczna, tj. \\(\\exists i \\leq s \\ \\text{takie, Å¼e } \\rho_i \\ne 0\\).\n\ngdzie \\(s = \\min(p, q)\\), a \\(\\rho_i\\) to \\(i\\)-ta korelacja kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#statystyka-testowa-test-wilka",
    "href": "cca.html#statystyka-testowa-test-wilka",
    "title": "Analiza kanoniczna",
    "section": "Statystyka testowa â€“ test Wilka",
    "text": "Statystyka testowa â€“ test Wilka\nW celu przetestowania tej hipotezy, wykorzystuje siÄ™ statystykÄ™ Wilka, ktÃ³ra bazuje na iloczynie skÅ‚adnikÃ³w postaci (\\(1 - \\lambda_i\\)), gdzie \\(\\lambda_i\\) to wartoÅ›ci wÅ‚asne odpowiadajÄ…ce kwadratom korelacji kanonicznych \\[\n\\lambda_i = \\rho_i^2.\n\\] Statystyka Wilkas jest zdefiniowana jako \\[\n\\Lambda = \\prod_{i=1}^s (1 - \\lambda_i).\n\\]\nInterpretacja - im mniejsze wartoÅ›ci \\(\\Lambda\\), tym wiÄ™ksza zaleÅ¼noÅ›Ä‡ miÄ™dzy zbiorami \\(X\\) i \\(Y\\). DuÅ¼e wartoÅ›ci \\(\\lambda_i\\) (czyli silne korelacje kanoniczne) powodujÄ…, Å¼e \\(\\Lambda\\) dÄ…Å¼y do zera.\nW praktyce, dla prÃ³by \\(n\\)-elementowej, stosujemy wersjÄ™ testu bazujÄ…cÄ… na macierzach kowariancji estymowanych z prÃ³by \\(S_{XX}, S_{XY}, S_{YX}, S_{YY})\\) â€“ odpowiedniki \\(\\Sigma_{XX}, \\Sigma_{XY}, \\Sigma_{YX}, \\Sigma_{YY}\\).\nWÃ³wczas \\[\nT^2/n = \\left|I - S_{YY}^{-1} S_{YX} S_{XX}^{-1} S_{XY} \\right| = \\prod_{i=1}^s (1 - \\hat{\\lambda}_i),\n\\] gdzie \\(\\hat{\\lambda}_i\\) to prÃ³bkowe wartoÅ›ci wÅ‚asne (szacunki \\(\\lambda_i\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#rozkÅ‚ad-asymptotyczny-i-transformacja-do-rozkÅ‚adu-chi2",
    "href": "cca.html#rozkÅ‚ad-asymptotyczny-i-transformacja-do-rozkÅ‚adu-chi2",
    "title": "Analiza kanoniczna",
    "section": "RozkÅ‚ad asymptotyczny i transformacja do rozkÅ‚adu \\(\\chi^2\\)\n",
    "text": "RozkÅ‚ad asymptotyczny i transformacja do rozkÅ‚adu \\(\\chi^2\\)\n\nWielu autorÃ³w (np. Marriott i Gittins (1986)) sugeruje przeksztaÅ‚cenie statystyki Wilksa do postaci asymptotycznie zgodnej z rozkÅ‚adem \\(\\chi^2\\), np. za pomocÄ… transformacji\n\\[\n-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda) \\sim \\chi^2_{pq}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#procedura-testowania",
    "href": "cca.html#procedura-testowania",
    "title": "Analiza kanoniczna",
    "section": "Procedura testowania",
    "text": "Procedura testowania\n\nOszacuj wszystkie korelacje kanoniczne \\(\\hat{\\rho}_1, \\ldots, \\hat{\\rho}_p\\).\nOd \\(k = 0\\) do \\(p-1\\) oblicz \\(\\Lambda_k = \\prod_{i=k+1}^{p}(1 - \\hat{\\rho}_i^2)\\).\nOblicz transformacjÄ™ \\(\\chi^2_k=-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda_k)\\)\n\nPorÃ³wnaj z odpowiednim kwantylem rozkÅ‚adu \\(\\chi^2\\) z \\((q - k)(r - k)\\) stopniami swobody.\nJeÅ›li wartoÅ›Ä‡ statystyki przekracza ten kwantyl (\\(p&lt;\\alpha\\)), odrzuÄ‡ \\(H_0^{(k)}\\) i przejdÅº do \\(k+1\\). JeÅ›li nie, zatrzymaj siÄ™ â€“ kolejne korelacje uznajemy za nieistotne.\n\nOcena dopasowania modelu w analizie kanonicznej (CCA â€“ Canonical Correlation Analysis) obejmuje kilka istotnych wskaÅºnikÃ³w diagnostycznych, ktÃ³re pozwalajÄ… zrozumieÄ‡ siÅ‚Ä™ i strukturÄ™ relacji miÄ™dzy dwoma zbiorami zmiennych. PoniÅ¼ej omÃ³wione zostaÅ‚y trzy kluczowe miary: Å‚adunki czynnikowe, wariancja wyjaÅ›niona oraz redundancja.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#Å‚adunki-czynnikowe-ang.-canonical-loadings",
    "href": "cca.html#Å‚adunki-czynnikowe-ang.-canonical-loadings",
    "title": "Analiza kanoniczna",
    "section": "Åadunki czynnikowe (ang. canonical loadings)",
    "text": "Åadunki czynnikowe (ang. canonical loadings)\n\n\nDefinicja - korelacje pomiÄ™dzy zmiennymi kanonicznymi (czyli kombinacjami liniowymi wektorÃ³w \\(a_k'X\\) i \\(b_k'Y\\)) a oryginalnymi zmiennymi ze zbiorÃ³w \\(X\\) i \\(Y\\).\n\nInterpretacja:\n\nPokazujÄ…, ktÃ³re konkretne zmienne pierwotne w najwiÄ™kszym stopniu â€Å‚adujÄ… siÄ™â€ (czyli kontrybuujÄ…) na danÄ… zmiennÄ… kanonicznÄ….\nWysoka wartoÅ›Ä‡ (np. &gt; 0.7) wskazuje na silnÄ… zaleÅ¼noÅ›Ä‡ miÄ™dzy zmiennÄ… oryginalnÄ… a danÄ… zmiennÄ… kanonicznÄ….\nZnaki dodatnie/ujemne pozwalajÄ… wnioskowaÄ‡ o kierunku zwiÄ…zku.\n\n\n\nWzÃ³r:\n\nDla zbioru \\(X\\): \\[\n\\text{loadings}_X = \\mathrm{Corr}(X, U_k) = \\Sigma_{XX} a_k\n\\]\n\nDla zbioru \\(Y\\): \\[\n\\text{loadings}_Y = \\mathrm{Corr}(Y, V_k) = \\Sigma_{YY} b_k\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wariancja-wyjaÅ›niona-ang.-variance-explained",
    "href": "cca.html#wariancja-wyjaÅ›niona-ang.-variance-explained",
    "title": "Analiza kanoniczna",
    "section": "Wariancja wyjaÅ›niona (ang. variance explained)",
    "text": "Wariancja wyjaÅ›niona (ang. variance explained)\n\n\nDefinicja - Å›rednia kwadratÃ³w Å‚adunkÃ³w czynnikowych dla kaÅ¼dej zmiennej kanonicznej i kaÅ¼dego zbioru danych.\n\nInterpretacja:\n\nInformuje, jakÄ… czÄ™Å›Ä‡ wariancji oryginalnych zmiennych w danym zbiorze (\\(X\\) lub \\(Y\\)) wyjaÅ›nia dana zmienna kanoniczna.\nMoÅ¼na traktowaÄ‡ ten wskaÅºnik jako odpowiednik wspÃ³Å‚czynnika determinacji \\(R^2\\) dla pojedynczej zmiennej kanonicznej.\nWysoka wartoÅ›Ä‡ oznacza, Å¼e dana zmienna kanoniczna dobrze reprezentuje zbiÃ³r, z ktÃ³rego zostaÅ‚a utworzona.\n\n\n\nWzÃ³r: \\[\n\\text{Explained variance} = \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k)\n\\] gdzie \\(p\\) to liczba zmiennych w zbiorze \\(X\\), a \\(U_k\\) to \\(k\\)-ta zmienna kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#redundancja-ang.-redundancy-index",
    "href": "cca.html#redundancja-ang.-redundancy-index",
    "title": "Analiza kanoniczna",
    "section": "Redundancja (ang. redundancy index)",
    "text": "Redundancja (ang. redundancy index)\n\n\nDefinicja - iloczyn kwadratu korelacji kanonicznej \\(\\rho_k^2\\) oraz wariancji wyjaÅ›nionej przez danÄ… zmiennÄ… kanonicznÄ… we wÅ‚asnym zbiorze.\n\nInterpretacja:\n\nInformuje, jaka czÄ™Å›Ä‡ przeciÄ™tnej wariancji jednej grupy zmiennych jest wyjaÅ›niana przez zmiennÄ… kanonicznÄ… utworzonÄ… na podstawie drugiego zbioru.\nMiara ta pokazuje, czy dany zbiÃ³r zmiennych wnosi unikalnÄ… informacjÄ™ o drugim zbiorze.\nWysoka redundancja oznacza, Å¼e istnieje istotny zwiÄ…zek miÄ™dzy strukturami dwÃ³ch zbiorÃ³w zmiennych.\n\n\n\nWzÃ³r: \\[\n\\text{Redundancy}_X = \\rho_k^2 \\cdot \\left( \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k) \\right)\n\\] Analogicznie definiujemy redundancjÄ™ wzglÄ™dem \\(Y\\).\n\n\n\n\n\n\n\n\nMiara\nCo opisuje\nInterpretacja praktyczna\n\n\n\nÅadunki czynnikowe\nSiÅ‚Ä™ powiÄ…zania zmiennej oryginalnej z kanonicznÄ…\nWysoka wartoÅ›Ä‡ â‡’ silna reprezentacja zmiennej\n\n\nWariancja wyjaÅ›niona\nÅšrednia siÅ‚a reprezentacji zbioru przez zm. kanonicznÄ…\nMiara dopasowania struktury do zbioru\n\n\nRedundancja\nIloÅ›Ä‡ informacji o jednym zbiorze zawarta w drugim\nMiara istotnoÅ›ci relacji miÄ™dzy zbiorami",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#normalnoÅ›Ä‡-wielowymiarowa",
    "href": "cca.html#normalnoÅ›Ä‡-wielowymiarowa",
    "title": "Analiza kanoniczna",
    "section": "NormalnoÅ›Ä‡ wielowymiarowa",
    "text": "NormalnoÅ›Ä‡ wielowymiarowa\nZakÅ‚ada siÄ™, Å¼e obydwa zbiory zmiennych losowych â€“ \\(X\\) i \\(Y\\) â€“ sÄ… wspÃ³lnie rozkÅ‚adem normalnym wielowymiarowym jak podano w RÃ³wnanieÂ 4.1. NormalnoÅ›Ä‡ umoÅ¼liwia stosowanie testÃ³w statystycznych (np. testu Wilksa) do oceny liczby istotnych korelacji kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-wartoÅ›ci-odstajÄ…cych-outliers",
    "href": "cca.html#brak-wartoÅ›ci-odstajÄ…cych-outliers",
    "title": "Analiza kanoniczna",
    "section": "Brak wartoÅ›ci odstajÄ…cych (outliers)",
    "text": "Brak wartoÅ›ci odstajÄ…cych (outliers)\nZarÃ³wno obserwacje odstajÄ…ce jednowymiarowe, jak i wielowymiarowe mogÄ… istotnie zaburzaÄ‡ wynik analizy kanonicznej. OdstajÄ…ce wartoÅ›ci mogÄ… wpÅ‚ywaÄ‡ na macierze kowariancji, zmieniajÄ…c kierunki i siÅ‚y relacji miÄ™dzy zbiorami zmiennych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wystarczajÄ…ca-liczba-obserwacji",
    "href": "cca.html#wystarczajÄ…ca-liczba-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "WystarczajÄ…ca liczba obserwacji",
    "text": "WystarczajÄ…ca liczba obserwacji\nLiczba obserwacji powinna znaczÄ…co przekraczaÄ‡ liczbÄ™ zmiennych w kaÅ¼dym zbiorze. Liczba obserwacji \\(n\\) w kaÅ¼dej grupie powinna byÄ‡ wiÄ™ksza niÅ¼ suma liczby zmiennych w \\(X\\) i \\(Y\\) \\[\nn &gt; p + q\n\\] Zapewnia odwracalnoÅ›Ä‡ macierzy kowariancji oraz stabilnoÅ›Ä‡ estymatorÃ³w.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#liniowoÅ›Ä‡-zaleÅ¼noÅ›ci",
    "href": "cca.html#liniowoÅ›Ä‡-zaleÅ¼noÅ›ci",
    "title": "Analiza kanoniczna",
    "section": "LiniowoÅ›Ä‡ zaleÅ¼noÅ›ci",
    "text": "LiniowoÅ›Ä‡ zaleÅ¼noÅ›ci\nZakÅ‚ada siÄ™, Å¼e zwiÄ…zki miÄ™dzy wszystkimi parami zmiennych sÄ… liniowe. PoniewaÅ¼ CCA opiera siÄ™ na maksymalizacji liniowych kombinacji, nieliniowe zaleÅ¼noÅ›ci mogÄ… pozostaÄ‡ niewykryte.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-nadmiernej-wspÃ³Å‚liniowoÅ›ci-multikolinearnoÅ›ci",
    "href": "cca.html#brak-nadmiernej-wspÃ³Å‚liniowoÅ›ci-multikolinearnoÅ›ci",
    "title": "Analiza kanoniczna",
    "section": "Brak nadmiernej wspÃ³Å‚liniowoÅ›ci (multikolinearnoÅ›ci)",
    "text": "Brak nadmiernej wspÃ³Å‚liniowoÅ›ci (multikolinearnoÅ›ci)\nZmienne wewnÄ…trz kaÅ¼dego zbioru (w \\(X\\) lub w \\(Y\\)) nie powinny byÄ‡ nadmiernie skorelowane. Wysoka wspÃ³Å‚liniowoÅ›Ä‡ moÅ¼e prowadziÄ‡ do niestabilnych i trudnych do interpretacji wektorÃ³w kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#niezaleÅ¼noÅ›Ä‡-obserwacji",
    "href": "cca.html#niezaleÅ¼noÅ›Ä‡-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "NiezaleÅ¼noÅ›Ä‡ obserwacji",
    "text": "NiezaleÅ¼noÅ›Ä‡ obserwacji\nKaÅ¼da obserwacja powinna pochodziÄ‡ od innej jednostki (brak powtÃ³rzeÅ„ pomiarÃ³w). NiezaleÅ¼noÅ›Ä‡ warunkuje poprawnoÅ›Ä‡ estymatorÃ³w kowariancji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "fa.html",
    "href": "fa.html",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "Eksploracyjna analiza czynnikowa\nAnaliza czynnikowa naleÅ¼y do klasy metod wielowymiarowych, ktÃ³rych celem jest odkrywanie ukrytych struktur stojÄ…cych za obserwowanymi zmiennymi. W odrÃ³Å¼nieniu od metod takich jak analiza gÅ‚Ã³wnych skÅ‚adowych1, ktÃ³re opierajÄ… siÄ™ na czysto algebraicznych przeksztaÅ‚ceniach danych, analiza czynnikowa ma wyraÅºne odniesienie do modeli statystycznych i psychometrycznych, w ktÃ³rych zakÅ‚ada siÄ™ istnienie czynnikÃ³w latentnych â€“ czyli zmiennych ukrytych, niewidocznych bezpoÅ›rednio, ale wpÅ‚ywajÄ…cych na wartoÅ›ci zmiennych obserwowalnych. PrzykÅ‚adem moÅ¼e byÄ‡ konstrukt â€inteligencjaâ€, ktÃ³ry przejawia siÄ™ w wynikach testÃ³w logicznych, pamiÄ™ciowych czy jÄ™zykowych. GÅ‚Ã³wnym celem analizy czynnikowej jest redukcja wymiarowoÅ›ci poprzez reprezentacjÄ™ wielu zmiennych w postaci mniejszej liczby czynnikÃ³w oraz lepsze zrozumienie powiÄ…zaÅ„ miÄ™dzy zmiennymi poprzez ujawnienie wspÃ³lnych ÅºrÃ³deÅ‚ ich zmiennoÅ›ci.\nMoÅ¼na wyrÃ³Å¼niÄ‡ dwa podstawowe podejÅ›cia do analizy czynnikowej. Eksploracyjna analiza czynnikowa (EFA, Exploratory Factor Analysis) jest stosowana, gdy badacz nie ma wczeÅ›niej zdefiniowanych hipotez co do liczby czynnikÃ³w czy struktury powiÄ…zaÅ„ miÄ™dzy nimi. Jej celem jest odkrycie potencjalnych ukÅ‚adÃ³w zaleÅ¼noÅ›ci i zidentyfikowanie liczby czynnikÃ³w najlepiej opisujÄ…cych dane. Konfirmacyjna analiza czynnikowa (CFA, Confirmatory Factor Analysis) jest natomiast podejÅ›ciem dedukcyjnym â€“ badacz z gÃ³ry formuÅ‚uje model teoretyczny (np. Å¼e pewne zmienne mierzÄ… â€pamiÄ™Ä‡ roboczÄ…â€, a inne â€myÅ›lenie abstrakcyjneâ€) i testuje jego zgodnoÅ›Ä‡ z danymi empirycznymi. CFA jest szczegÃ³lnie istotna w kontekÅ›cie walidacji narzÄ™dzi badawczych, np. kwestionariuszy psychologicznych, i stanowi fundament bardziej zaawansowanych modeli strukturalnych (SEM).\nHistoria analizy czynnikowej siÄ™ga poczÄ…tkÃ³w XX wieku i jest Å›ciÅ›le zwiÄ…zana z psychometriÄ…. Jej pionierem byÅ‚ Charles Spearman, ktÃ³ry w 1904 roku zaproponowaÅ‚ model jednoczynnikowy, interpretujÄ…c zmienne poznawcze jako przejawy ogÃ³lnego czynnika inteligencji. W kolejnych dekadach metoda byÅ‚a rozwijana przez psychologÃ³w, takich jak Thurstone, ktÃ³ry wprowadziÅ‚ koncepcjÄ™ wieloczynnikowÄ… oraz przez statystykÃ³w, ktÃ³rzy rozwijali formalne podstawy estymacji czynnikÃ³w i rotacji macierzy Å‚adunkÃ³w. W latach 60. i 70. analiza czynnikowa staÅ‚a siÄ™ jednÄ… z najczÄ™Å›ciej stosowanych metod w badaniach psychologicznych i spoÅ‚ecznych, a wraz z rozwojem informatyki zyskaÅ‚a na popularnoÅ›ci takÅ¼e w ekonomii, biologii czy medycynie. DziÅ› analiza czynnikowa jest narzÄ™dziem interdyscyplinarnym, stosowanym zarÃ³wno do eksploracji struktur danych, jak i do testowania teorii opartych na zmiennych latentnych.\nFormalna postaÄ‡ modelu eksploracyjnej analizy czynnikowej (EFA) zakÅ‚ada, Å¼e zmienne obserwowalne \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)^\\top\\) moÅ¼na wyraziÄ‡ jako kombinacjÄ™ liniowÄ… czynnikÃ³w latentnych oraz skÅ‚adnikÃ³w specyficznych. Model przyjmuje postaÄ‡ (â€Introduction to Factor Analysisâ€ 2020):\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\]\ngdzie:",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#eksploracyjna-analiza-czynnikowa",
    "href": "fa.html#eksploracyjna-analiza-czynnikowa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) â€“ wektor zmiennych obserwowalnych,\n\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) â€“ wektor Å›rednich,\n\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) â€“ macierz Å‚adunkÃ³w czynnikowych, ktÃ³rej element \\(\\lambda_{ij}\\) opisuje wpÅ‚yw czynnika \\(j\\) na zmiennÄ… \\(i\\),\n\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) â€“ wektor czynnikÃ³w latentnych (czynnikÃ³w wspÃ³lnych),\n\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) â€“ wektor skÅ‚adnikÃ³w specyficznych (unikalnych, bÅ‚Ä™dÃ³w pomiaru).\n\nZaÅ‚oÅ¼enia klasycznego modelu EFA\n\nRozkÅ‚ad czynnikÃ³w wspÃ³lnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne majÄ… Å›redniÄ… zero i macierz kowariancji rÃ³wnÄ… macierzy jednostkowej. To zaÅ‚oÅ¼enie oznacza, Å¼e czynniki sÄ… nieskorelowane i majÄ… wariancjÄ™ jednostkowÄ… (jest to standaryzacja wprowadzona dla identyfikowalnoÅ›ci modelu).\nRozkÅ‚ad skÅ‚adnikÃ³w specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzÄ… diagonalnÄ… o elementach dodatnich. Oznacza to, Å¼e bÅ‚Ä™dy sÄ… nieskorelowane miÄ™dzy sobÄ… oraz niezaleÅ¼ne od czynnikÃ³w \\(\\mathbf{f}\\).\nNiezaleÅ¼noÅ›Ä‡ czynnikÃ³w i bÅ‚Ä™dÃ³w \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyÅ¼szej konstrukcji wynika, Å¼e kowariancja zmiennych obserwowalnych jest sumÄ… czÄ™Å›ci wspÃ³lnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nAdnotacjaDowÃ³d\n\n\n\nNiech losowy wektor obserwacji ma postaÄ‡ \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynnikÃ³w wspÃ³lnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor skÅ‚adnikÃ³w specyficznych. ZakÅ‚adamy, Å¼e \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazaÄ‡, Å¼e \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczegÃ³lnoÅ›ci przy \\(\\Phi=I_m\\), Å¼e mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a poniewaÅ¼ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancjÄ™ \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyraÅ¼amy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] KorzystajÄ…c z liniowoÅ›ci kowariancji i toÅ¼samoÅ›ci \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorÃ³w losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skoÅ„czonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDziÄ™ki zaÅ‚oÅ¼eniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikajÄ… i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] JeÅ›li dodatkowo przyjmiemy standardyzacjÄ™ czynnikÃ³w \\(\\Phi=I_m\\) (co jest konwencjÄ… identyfikacyjnÄ… modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego naleÅ¼aÅ‚o dowieÅ›Ä‡.\nWarto odnotowaÄ‡, Å¼e dowÃ³d nie wymaga niezaleÅ¼noÅ›ci \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym â€” wystarcza nieskorelowanie, aby zniknÄ™Å‚y skÅ‚adniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postaÄ‡ \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to moÅ¼na zastosowaÄ‡ tzw. whitening czynnikÃ³w \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co rÃ³wnowaÅ¼nie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), ktÃ³re prowadzÄ… do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to zwiÄ…zane z moÅ¼liwoÅ›ciÄ… przeprowadzania rÃ³Å¼nych transformacji czynnikÃ³w bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogÃ³lnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeÅ¼eli dokonamy transformacji ortogonalnej czynnikÃ³w \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzÄ… ortogonalnÄ…, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, Å¼e \\(\\Lambda\\) i \\(\\Phi\\) nie sÄ… jednoznacznie wyznaczone. RÃ³Å¼ne pary \\((\\Lambda, \\Phi)\\) mogÄ… prowadziÄ‡ do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczegÃ³lnoÅ›ci wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jednÄ… z takich rÃ³wnowaÅ¼nych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalnÄ… rolÄ™, poniewaÅ¼ jest miejscem, w ktÃ³rym spotykajÄ… siÄ™ dwa skÅ‚adniki zmiennoÅ›ci: wspÃ³lna i specyficzna. RozkÅ‚ad \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, Å¼e caÅ‚kowita wariancja i kowariancja obserwowanych zmiennych moÅ¼e byÄ‡ przedstawiona jako suma efektu wspÃ³lnych czynnikÃ³w oraz efektu specyficznego, indywidualnego dla kaÅ¼dej zmiennej.\nCzÄ™Å›Ä‡ \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspÃ³lne ÅºrÃ³dÅ‚o zmiennoÅ›ci, czyli wariancjÄ™ wyjaÅ›nianÄ… przez czynniki ukryte. To wÅ‚aÅ›nie ta czÄ™Å›Ä‡ umoÅ¼liwia redukcjÄ™ wymiaru â€“ wiele zmiennych obserwowanych moÅ¼na sprowadziÄ‡ do kilku czynnikÃ³w, ktÃ³re reprezentujÄ… gÅ‚Ã³wnÄ… strukturÄ™ zaleÅ¼noÅ›ci. Interpretacja czynnikÃ³w jako ukrytych wymiarÃ³w (np. inteligencja, poziom lÄ™ku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uproÅ›ciÄ‡ analizÄ™, ale takÅ¼e nadaÄ‡ jej znaczenie teoretyczne w danej dziedzinie badaÅ„.\nZ kolei \\(\\Psi\\) odpowiada za wariancjÄ™ unikalnÄ…, czyli tÄ™ czÄ™Å›Ä‡ zmiennoÅ›ci, ktÃ³ra nie jest wspÃ³Å‚dzielona z innymi zmiennymi. Obejmuje ona zarÃ³wno wariancjÄ™ czysto specyficznÄ… dla danej cechy, jak i wariancjÄ™ bÅ‚Ä™du pomiarowego. DziÄ™ki temu moÅ¼liwe jest odrÃ³Å¼nienie struktury gÅ‚Ä™bokiej (czynnikowej) od elementÃ³w przypadkowych i indywidualnych.\nPodsumowujÄ…c, znaczenie modelu czynnikowego polega na tym, Å¼e pozwala on wydzieliÄ‡ istotne, ukryte mechanizmy stojÄ…ce za wspÃ³Å‚zaleÅ¼noÅ›ciami zmiennych i oddzieliÄ‡ je od szumÃ³w specyficznych dla pojedynczych obserwacji. W praktyce oznacza to moÅ¼liwoÅ›Ä‡ redukcji liczby analizowanych zmiennych, uproszczenie opisu zÅ‚oÅ¼onych danych i pogÅ‚Ä™bienie interpretacji zjawisk spoÅ‚ecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynnikÃ³w w praktyce opiera siÄ™ przede wszystkim na analizie macierzy Å‚adunkÃ³w czynnikowych \\(\\Lambda\\). KaÅ¼dy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile zwiÄ…zku pomiÄ™dzy zmiennÄ… obserwowanÄ… \\(x_i\\) a czynnikiem \\(f_j\\). Im wyÅ¼sza wartoÅ›Ä‡ bezwzglÄ™dna Å‚adunku, tym wiÄ™kszy udziaÅ‚ danego czynnika w wyjaÅ›nianiu zmiennoÅ›ci konkretnej zmiennej. Na przykÅ‚ad w psychologii wysoki Å‚adunek czynnika na zmiennej opisujÄ…cej pamiÄ™Ä‡ krÃ³tkotrwaÅ‚Ä… i na zmiennej opisujÄ…cej zdolnoÅ›Ä‡ rozwiÄ…zywania problemÃ³w matematycznych moÅ¼e sugerowaÄ‡, Å¼e obie cechy sÄ… przejawem wspÃ³lnego czynnika â€“ inteligencji ogÃ³lnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#zaÅ‚oÅ¼enia-dotyczÄ…ce-danych",
    "href": "fa.html#zaÅ‚oÅ¼enia-dotyczÄ…ce-danych",
    "title": "Analiza czynnikowa",
    "section": "ZaÅ‚oÅ¼enia dotyczÄ…ce danych",
    "text": "ZaÅ‚oÅ¼enia dotyczÄ…ce danych\nAby estymacja modelu eksploracyjnej analizy czynnikowej (EFA) byÅ‚a uzasadniona, dane powinny speÅ‚niaÄ‡ szereg zaÅ‚oÅ¼eÅ„ teoretycznych i praktycznych.\n\nPo pierwsze, podstawÄ… jest istnienie istotnej struktury korelacyjnej pomiÄ™dzy zmiennymi obserwowalnymi. JeÅ¼eli zmienne sÄ… w zasadzie nieskorelowane, nie da siÄ™ wydzieliÄ‡ wspÃ³lnych czynnikÃ³w. Warunek ten weryfikuje siÄ™ wstÄ™pnie testem sferycznoÅ›ci Bartletta (Bartlett 1951) oraz miarÄ… adekwatnoÅ›ci prÃ³by KMO (Kaiser 1970).\nPo drugie, zakÅ‚ada siÄ™ odpowiedniÄ… wielkoÅ›Ä‡ prÃ³by. ChoÄ‡ w literaturze nie istnieje jednoznaczna reguÅ‚a, rekomenduje siÄ™ co najmniej 5â€“10 obserwacji na zmiennÄ… oraz Å‚Ä…cznÄ… liczebnoÅ›Ä‡ rzÄ™du â‰¥100â€“200 jednostek, aby uzyskaÄ‡ stabilne rozwiÄ…zania i wiarygodne oszacowania Å‚adunkÃ³w czynnikowych.\nPo trzecie, dane powinny pochodziÄ‡ z rozkÅ‚adu wielowymiarowego normalnego, szczegÃ³lnie jeÅ¼eli korzysta siÄ™ z estymacji metodÄ… najwiÄ™kszej wiarygodnoÅ›ci. Naruszenia normalnoÅ›ci mogÄ… prowadziÄ‡ do zawyÅ¼enia bÅ‚Ä™dÃ³w standardowych, problemÃ³w z testami istotnoÅ›ci oraz bÅ‚Ä™dnych przedziaÅ‚Ã³w ufnoÅ›ci.\nPo czwarte, model EFA wymaga, aby zmienne byÅ‚y ciÄ…gÅ‚e lub co najmniej traktowane jako przybliÅ¼enie zmiennych ciÄ…gÅ‚ych. W przypadku zmiennych kategorycznych naleÅ¼y siÄ™gnÄ…Ä‡ po odpowiednie uogÃ³lnienia (np. analizy czynnikowe dla danych porzÄ…dkowych).\nPo piÄ…te, zakÅ‚ada siÄ™ addytywnoÅ›Ä‡ wariancji. Oznacza to, Å¼e wariancja kaÅ¼dej zmiennej obserwowanej rozkÅ‚ada siÄ™ na czÄ™Å›Ä‡ wspÃ³lnÄ…, wyjaÅ›nianÄ… przez czynniki latentne, oraz czÄ™Å›Ä‡ swoistÄ… (unikalnÄ… dla danej zmiennej), zgodnie z postaciÄ…: \\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\nI na koniec, naleÅ¼y zadbaÄ‡ o brak nadmiernej wspÃ³Å‚liniowoÅ›ci oraz o to, by liczba czynnikÃ³w nie przekraczaÅ‚a liczby zmiennych â€“ w przeciwnym wypadku model byÅ‚by nieidentyfikowalny.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#metody-estymacji-Å‚adunkÃ³w-czynnikowych",
    "href": "fa.html#metody-estymacji-Å‚adunkÃ³w-czynnikowych",
    "title": "Analiza czynnikowa",
    "section": "Metody estymacji Å‚adunkÃ³w czynnikowych",
    "text": "Metody estymacji Å‚adunkÃ³w czynnikowych\nMetoda najwiÄ™kszej wiarogodnoÅ›ci (ang. Maximal Likelihood, ML) (Lawley 1940)\n\nZaÅ‚oÅ¼enia\nZakÅ‚adamy, Å¼e wektor zmiennych obserwowalnych\n\\[\n\\mathbf{x} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\Sigma),\n\\]\ngdzie kowariancja \\(\\Sigma\\) ma postaÄ‡ modelowÄ… \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nDla uproszczenia przyjmuje siÄ™ czÄ™sto, Å¼e czynniki \\(\\mathbf{f}\\) sÄ… standaryzowane i nieskorelowane, czyli \\(\\Phi = I_m\\). WÃ³wczas macierz kowariancji ma postaÄ‡\n\\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nFunkcja wiarygodnoÅ›ci\nDla prÃ³by \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) funkcja wiarygodnoÅ›ci rozkÅ‚adu normalnego wynosi\n\\[\nL(\\Lambda,\\Psi) = (2\\pi)^{-\\frac{np}{2}} |\\Sigma|^{-\\frac{n}{2}}\n\\exp\\left(-\\tfrac{1}{2}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{x}_i-\\mu)\\right).\n\\]\nczÄ™Å›ciej wyraÅ¼ana w postaci zlogarytmowanej\n\\[\n\\ell(\\Lambda,\\Psi) = -\\frac{n}{2} \\left[ \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) \\right] + C,\n\\]\ngdzie \\(S = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)(\\mathbf{x}_i-\\mu)^\\top\\) jest macierzÄ… kowariancji z prÃ³by.\nEstymacja parametrÃ³w\nEstymatory \\(\\hat{\\Lambda}, \\hat{\\Psi}\\) dobiera siÄ™ tak, aby maksymalizowaÅ‚y \\(\\ell(\\Lambda,\\Psi)\\), co odpowiada minimalizacji funkcji rozbieÅ¼noÅ›ci:\n\\[\nF(\\Lambda,\\Psi) = \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) - \\log |S| - p.\n\\]\nPowyÅ¼sza miara rozbieÅ¼noÅ›ci powstaje z odlegÅ‚oÅ›ci Kullbacka-Leiblera miÄ™dzy rozkÅ‚adami normalnymi \\(\\mathcal{N}_p(\\mu, \\Sigma)\\) i \\(\\mathcal{N}_p(\\mu, S)\\) i jest rÃ³wna dokÅ‚adnie \\(2D_{KL}(S||\\Sigma)\\).\nProcedura obliczeniowa\nW praktyce:\n\nWybiera siÄ™ liczbÄ™ \\(m\\) czynnikÃ³w2.\nUstala siÄ™ poczÄ…tkowe wartoÅ›ci \\(\\Lambda, \\Psi\\)3.\nIteracyjnie poprawia siÄ™ parametry, rozwiÄ…zujÄ…c rÃ³wnania warunkÃ³w pierwszego rzÄ™du\n\n2Â wybÃ³r liczby czynnikÃ³w zostanie przedstawiony nieco pÃ³Åºniej3Â W metodzie najwiÄ™kszej wiarygodnoÅ›ci estymacja macierzy Å‚adunkÃ³w czynnikowych \\(\\Lambda\\) oraz macierzy wariancji specyficznych \\(\\Psi\\) jest procedurÄ… iteracyjnÄ… i wymaga ustalenia wartoÅ›ci startowych. W praktyce wartoÅ›ci te nie wynikajÄ… z jednej Å›cisÅ‚ej reguÅ‚y, lecz z podejÅ›cia zapewniajÄ…cego stabilnoÅ›Ä‡ i szybkie zbieganie algorytmu. W typowych implementacjach w R (np. w stats::factanal lub psych::fa z metodÄ… ml) stosuje siÄ™ kilka sprawdzonych metod inicjalizacji. WartoÅ›ci poczÄ…tkowe macierzy \\(\\Lambda\\) ustala siÄ™ najczÄ™Å›ciej poprzez wykorzystanie wynikÃ³w analizy gÅ‚Ã³wnych skÅ‚adowych. Ustalenie polega na wyznaczeniu wartoÅ›ci wÅ‚asnych i wektorÃ³w wÅ‚asnych macierzy korelacji, a nastÄ™pnie pobraniu pierwszych \\(m\\) kolumn tej macierzy wektorÃ³w wÅ‚asnych jako punktu startowego. Zastosowanie PCA jako inicjalizacji wynika z tego, Å¼e wektory wÅ‚asne dobrze odzwierciedlajÄ… dominujÄ…ce kierunki wariancji, co pozwala rozpoczÄ…Ä‡ optymalizacjÄ™ w okolicy sensownego rozwiÄ…zania. WartoÅ›ci poczÄ…tkowe macierzy \\(\\Psi\\) wyznacza siÄ™ poprzez oszacowanie wariancji niewyjaÅ›nionej przez czynniki wstÄ™pne. Standardowym zabiegiem jest przypisanie \\(\\Psi\\) jako macierzy diagonalnej, w ktÃ³rej diagonalne elementy rÃ³wne sÄ… rÃ³Å¼nicom pomiÄ™dzy jednoÅ›ciÄ… a komunalnoÅ›ciami wynikajÄ…cymi z inicjalnych Å‚adunkÃ³w z PCA. Oznacza to, Å¼e jeÅ¼eli zmienna ma poczÄ…tkowÄ… komunalnoÅ›Ä‡ rÃ³wnÄ… \\(c\\), to jej wariancja specyficzna w punkcie startowym wynosi \\(1 â€“ c\\). W niektÃ³rych implementacjach ustala siÄ™ \\(\\Psi\\) jako macierz jednostkowÄ…, co jest rozwiÄ…zaniem neutralnym, jednak prowadzi zwykle do wolniejszej zbieÅ¼noÅ›ci.\\[\n\\frac{\\partial \\ell}{\\partial \\Lambda} = 0, \\quad \\frac{\\partial \\ell}{\\partial \\Psi} = 0.\n\\]\n\nTakie postÄ™powanie iteracyjne prowadzi siÄ™ aÅ¼ do zbieÅ¼noÅ›ci funkcji wiarygodnoÅ›ci.\nWÅ‚asnoÅ›ci\n\nEstymatory ML sÄ… efektywne przy speÅ‚nieniu zaÅ‚oÅ¼enia o normalnoÅ›ci wielowymiarowej danych pierwotnych.\nUmoÅ¼liwiaja testy istotnoÅ›ci liczby czynnikÃ³w:\n\nHipoteza \\(H_0: \\Sigma = \\Lambda\\Lambda^\\top + \\Psi\\) vs \\(H_1: \\Sigma\\) dowolna.\nStatystyka testowa ma w przybliÅ¼eniu rozkÅ‚ad \\(\\chi^2\\).\n\n\nPozwalajÄ… teÅ¼ konstruowaÄ‡ przedziaÅ‚y ufnoÅ›ci dla Å‚adunkÃ³w czynnikowych.\nOgraniczenia\n\nWymagaja duÅ¼ej prÃ³by i speÅ‚nienia zaÅ‚oÅ¼enia normalnoÅ›ci wielowymiarowej.\nMoÅ¼e byÄ‡ numerycznie niestabilne, zwÅ‚aszcza gdy liczba czynnikÃ³w jest duÅ¼a w stosunku do liczby zmiennych.\nPrzy maÅ‚ych prÃ³bach lub silnym naruszeniu normalnoÅ›ci wyniki mogÄ… byÄ‡ obciÄ…Å¼one.\nMetoda osi gÅ‚Ã³wnych (ang. Principal Axis Factoring, PAF) (Grieder i Steiner 2020)\n\nIdea metody PAF\nW metodzie PAF znanej rÃ³wnieÅ¼ jako metoda czynnikÃ³w gÅ‚Ã³wnych, zakÅ‚adamy klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad \\mathrm{Cov}(\\mathbf{x}) = \\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nCelem jest znalezienie takiego \\(\\Lambda\\) i \\(\\Psi\\), aby zbliÅ¼yÄ‡ siÄ™ do macierzy kowariancji prÃ³bkowej \\(S\\). W odrÃ³Å¼nieniu od ML, PAF nie opiera siÄ™ na funkcji wiarygodnoÅ›ci ani na rozbieÅ¼noÅ›ci Kullbackaâ€“Leiblera, lecz maksymalizuje wariancjÄ™ wspÃ³lnÄ… zmiennych, traktujÄ…c czÄ™Å›Ä‡ specyficznÄ… \\((\\Psi)\\) jako resztÄ™.\nMacierz zredukowanych korelacji\nW metodzie Principal Axis Factoring (PAF) kluczowÄ… rolÄ™ odgrywa macierz zredukowanych korelacji. Punktem wyjÅ›cia jest macierz korelacji \\(\\mathbf{R}\\) pomiÄ™dzy zmiennymi obserwowanymi \\(\\mathbf{x}\\). Na diagonali tej macierzy stojÄ… jedynki, odzwierciedlajÄ…ce fakt, Å¼e kaÅ¼da zmienna jest w peÅ‚ni skorelowana sama ze sobÄ…. Jednak w modelu czynnikowym zakÅ‚adamy, Å¼e caÅ‚kowita wariancja zmiennej \\(x_j\\) moÅ¼e zostaÄ‡ podzielona na czÄ™Å›Ä‡ wspÃ³lnÄ… (zasoby zmiennoÅ›ci wspÃ³lnej - ang. communalities) i czÄ™Å›Ä‡ swoistÄ… (zasoby zmiennoÅ›ci swoistej - ang. uniqness):\n\\[\n1 = h_j^2 + \\psi_j, \\quad j=1,\\dots,p,\n\\]\ngdzie \\(h_j^2\\) oznacza zasÃ³b zmiennoÅ›ci wspÃ³lnej, a \\(\\psi_j\\) wariancjÄ™ swoistÄ…. W konstrukcji macierzy zredukowanych korelacji zamiast jedynek wstawia siÄ™ w diagonali wÅ‚aÅ›nie wartoÅ›ci \\(h_j^2\\). Otrzymujemy w ten sposÃ³b macierz\n\\[\n\\mathbf{R}^* = [r_{ij}^*], \\quad r_{jj}^* = h_j^2.\n\\]\nMacierz \\(\\mathbf{R}^*\\) ma wiÄ™c charakter â€zredukowanyâ€, poniewaÅ¼ na jej diagonali pozostaje tylko ta czÄ™Å›Ä‡ wariancji zmiennej, ktÃ³rÄ… model czynnikowy ma szansÄ™ wyjaÅ›niÄ‡. DziÄ™ki temu macierz ta moÅ¼e byÄ‡ przybliÅ¼ana przez strukturÄ™ \\(\\Lambda \\Lambda^\\top\\), co odpowiada wspÃ³lnej wariancji wszystkich zmiennych.\nRozkÅ‚ad na wartoÅ›ci wÅ‚asne\nW metodzie PAF zakÅ‚adamy, Å¼e tylko czÄ™Å›Ä‡ wariancji kaÅ¼dej zmiennej jest wspÃ³lna. Oznacza to, Å¼e zamiast peÅ‚nej macierzy korelacji \\(\\mathbf{R}\\), rozwaÅ¼amy macierz zredukowanych korelacji: \\[\n\\mathbf{R}^* = \\mathbf{R} - \\Psi,\n\\] gdzie na diagonali znajdujÄ… siÄ™ oszacowane zasoby zmiennoÅ›ci wspÃ³lnej \\(\\hat{h}_j^2\\), zamiast jedynek.\nNastÄ™pnie wykonujemy dekompozycjÄ™ spektralnÄ… tej macierzy: \\[\n\\mathbf{R}^* = \\mathbf{Q}^* \\mathbf{D}^* {\\mathbf{Q}^*}^\\top,\n\\] gdzie \\(\\mathbf{Q}^*\\) i \\(\\mathbf{D}^*\\) sÄ… odpowiednio wektorami i wartoÅ›ciami wÅ‚asnymi macierzy \\(\\mathbf{R}^*\\).\nEstymator Å‚adunkÃ³w czynnikowych w PAF ma wiÄ™c postaÄ‡ \\[\n\\hat{\\Lambda} = \\mathbf{Q}^*_m (\\mathbf{D}^*_m)^{1/2},\n\\]\nbazujÄ…cÄ… na zmodyfikowanej macierzy korelacji, w ktÃ³rej uwzglÄ™dniono oszacowane komunalnoÅ›ci, przy czym \\(\\mathbf{Q}^*_m\\) i \\(\\mathbf{D}^*_m\\) oznaczajÄ… zredukowane macierze \\(\\mathbf{Q}\\) i \\(\\mathbf{D}\\) do pierwszych \\(m\\) kolumn odpowiadajÄ…cych pierwszym \\(m\\) czynnikom.\nPoniewaÅ¼ \\(\\hat{h}_j^2\\) same zaleÅ¼Ä… od Å‚adunkÃ³w (sÄ… ich sumÄ… kwadratÃ³w), w praktyce stosuje siÄ™ procedurÄ™ iteracyjnÄ…: zaczynamy od pewnych wartoÅ›ci poczÄ…tkowych, obliczamy dekompozycjÄ™ spektralnÄ…, aktualizujemy komunalnoÅ›ci i powtarzamy procedurÄ™ aÅ¼ do zbieÅ¼noÅ›ci.\n\n\n\n\n\n\nAdnotacjaWstÄ™pne oszacowania zasobÃ³w zmiennoÅ›ci wspÃ³lnej\n\n\n\nProblem polega na tym, Å¼e wartoÅ›ci \\(h_j^2\\) nie sÄ… znane a priori. Dlatego w praktyce stosuje siÄ™ rÃ³Å¼ne metody wstÄ™pnego ich wyznaczania, ktÃ³re mogÄ… byÄ‡ nastÄ™pnie udoskonalane iteracyjnie w kolejnych krokach procedury PAF. Do najczÄ™Å›ciej stosowanych metod naleÅ¼Ä…:\n\nÅ›rednia arytmetyczna wspÃ³Å‚czynnikÃ³w korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\frac{1}{m}\\sum_{j'=1}^m r_{jj'},\\quad j\\ne j'\n\\]\nmaksymalna wartoÅ›Ä‡ bezwzglÄ™dna wspÃ³Å‚czynnikÃ³w korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\max_{j'}|r_{jj'}|, \\quad j\\ne j',\n\\]\nwspÃ³Å‚czynnik determinacji wielokrotnej danej zmiennej z innymi zmiennymi (najczÄ™Å›ciej stosowana i wykorzystywana przez R) \\[\nh_j^2=R^2_{j\\cdot 1,2,\\ldots,m},\n\\]\nformuÅ‚a triad \\[\nh_j^2=\\frac{r_{jj'}r_{jj''}}{r_{j'j''}}, \\quad j\\ne j' \\ne j''\n\\] gdzie \\(r_{jj'}, r_{jj''}\\) - dwie najwyÅ¼sze wartoÅ›ci wspÃ³Å‚czynnikÃ³w korelacji \\(j\\)-tej zmiennej z innymi zmiennymi.\n\n\n\nIteracyjna poprawa komunalnoÅ›ci\nPoniewaÅ¼ poczÄ…tkowe komunalnoÅ›ci sÄ… przybliÅ¼one, PAF stosuje procedurÄ™ iteracyjnÄ…:\n\nSzacujemy \\(\\Lambda\\) na podstawie bieÅ¼Ä…cego \\(\\mathbf{R}^*\\).\nObliczamy nowe zasoby zmiennoÅ›ci wspÃ³lnej \\(h_j^2 = \\sum_{k=1}^m \\lambda_{jk}^2\\).\nWstawiamy je na przekÄ…tnej \\(\\mathbf{R}^*\\) zamiast starych wartoÅ›ci.\nPowtarzamy rozkÅ‚ad wartoÅ›ci wÅ‚asnych.\n\nProces powtarza siÄ™ aÅ¼ do zbieÅ¼noÅ›ci, czyli stabilizacji Å‚adunkÃ³w czynnikowych i zasobÃ³w zmiennoÅ›ci wspÃ³lnej.\nWÅ‚asnoÅ›ci\n\n\nDopasowanie do wariancji wspÃ³lnej â€“ PAF minimalizuje rÃ³Å¼nice pomiÄ™dzy macierzÄ… zredukowanych korelacji \\(\\mathbf{R}^*\\) a aproksymacjÄ… \\(\\Lambda \\Lambda^\\top\\). Skupia siÄ™ na wariancji wspÃ³lnej.\n\nIteracyjnoÅ›Ä‡ oszacowaÅ„ â€“ estymatory w PAF powstajÄ… w procesie iteracyjnym, w ktÃ³rym kolejne przybliÅ¼enia komunalnoÅ›ci sÄ… poprawiane na podstawie sumy kwadratÃ³w aktualnych Å‚adunkÃ³w czynnikowych. DziÄ™ki temu metoda zbiega do rozwiÄ…zaÅ„ lepiej oddajÄ…cych strukturÄ™ wspÃ³lnÄ… niÅ¼ proste metody jednorazowe.\n\nNiestandaryzowana postaÄ‡ estymatorÃ³w â€“ rozwiÄ…zania PAF mogÄ… zaleÅ¼eÄ‡ od przyjÄ™tych wartoÅ›ci poczÄ…tkowych \\(h_j^2\\). RÃ³Å¼ne wybory startowe mogÄ… prowadziÄ‡ do nieco innych estymatorÃ³w, choÄ‡ w praktyce po kilku iteracjach zbieÅ¼noÅ›Ä‡ do stabilnego rozwiÄ…zania jest zazwyczaj dobra.\n\nInterpretowalnoÅ›Ä‡ â€“ poniewaÅ¼ oszacowane Å‚adunki czynnikowe odzwierciedlajÄ… wyÅ‚Ä…cznie czÄ™Å›Ä‡ wspÃ³lnÄ… wariancji, interpretacja czynnikÃ³w uzyskanych metodÄ… PAF jest bliÅ¼sza teoretycznemu modelowi czynnikowemu niÅ¼ w przypadku metod opartych na PCA.\nOgraniczenia\n\n\nBrak optymalnoÅ›ci w sensie funkcji wiarygodnoÅ›ci â€“ w przeciwieÅ„stwie do metody najwiÄ™kszej wiarygodnoÅ›ci (ML), estymatory PAF nie majÄ… znanych wÅ‚asnoÅ›ci asymptotycznych, takich jak efektywnoÅ›Ä‡ czy zgodnoÅ›Ä‡ w sensie probabilistycznym. SÄ… bardziej heurystyczne niÅ¼ Å›ciÅ›le statystyczne.\n\nZaleÅ¼noÅ›Ä‡ od wartoÅ›ci poczÄ…tkowych komunalnoÅ›ci â€“ oszacowania poczÄ…tkowe wpÅ‚ywajÄ… na przebieg iteracji i mogÄ… prowadziÄ‡ do lokalnych minimÃ³w. W praktyce wybÃ³r metody startowej (np. \\(R^2\\), Å›rednia korelacja, â€¦) ma znaczenie dla szybkoÅ›ci i stabilnoÅ›ci algorytmu.\n\nMoÅ¼liwoÅ›Ä‡ uzyskania ujemnych komunalnoÅ›ci â€“ w niektÃ³rych przypadkach iteracje mogÄ… prowadziÄ‡ do oszacowaÅ„ \\(h_j^2 &lt; 0\\) (tzw. przypadek Haywooda), co jest sprzeczne z definicjÄ… wariancji wspÃ³lnej. WÃ³wczas konieczne stosowanie innych metod estymacji Å‚adunkÃ³w.\n\nMniejsza przydatnoÅ›Ä‡ przy maÅ‚ych prÃ³bach â€“ poniewaÅ¼ metoda nie opiera siÄ™ na peÅ‚nym modelu statystycznym, jej wÅ‚asnoÅ›ci sÄ… mniej stabilne przy niewielkich licznoÅ›ciach obserwacji. Wyniki mogÄ… byÄ‡ wÃ³wczas silnie zaleÅ¼ne od przypadkowych fluktuacji w danych.\n\nBrak testÃ³w statystycznych dopasowania modelu â€“ w odrÃ³Å¼nieniu od metody ML, PAF nie pozwala na formalne testowanie hipotez o liczbie czynnikÃ³w czy jakoÅ›ci dopasowania modelu do danych.\nMetoda sÅ‚adowych gÅ‚Ã³wnych (ang. Principal Component Method) (Wang i in. 2008)\n\nMetoda sÅ‚adowych gÅ‚Ã³wnych naleÅ¼y do klasy metod wspÃ³lnotowych, czyli takich, ktÃ³re zakÅ‚adajÄ… klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\quad \\Sigma = \\Lambda\\Lambda^\\top + \\Psi.\n\\]\nCelem jest oszacowanie macierzy Å‚adunkÃ³w \\(\\Lambda\\), tak aby jak najlepiej odtworzyÄ‡ czÄ™Å›Ä‡ wspÃ³lnÄ… wariancji.\nIdea metody\nW metodzie PCM zakÅ‚adamy, Å¼e caÅ‚a wariancja zmiennej jest wariancjÄ… wspÃ³lnÄ…, tzn. \\[\nh_j^2 = 1, \\quad j=1,\\ldots,p.\n\\] W praktyce przyjÄ™cie takiego zaÅ‚oÅ¼enia sprawia, Å¼e estymacja Å‚adunkÃ³w czynnikowych staje siÄ™ rÃ³wnoznaczna z pobraniem wektorÃ³w wÅ‚asnych macierzy korelacji lub kowariancji, odpowiednio do liczby czynnikÃ³w. Oznacza to, Å¼e metoda Principal Component Method nie estymuje parametrÃ³w modelu czynnikowego w sensie statystycznym, lecz wykonuje dekompozycjÄ™ macierzy korelacji i interpretuje jej wynik jako osadzenia czynnikowe. Oznacza to teÅ¼, Å¼e macierz zredukowanych korelacji \\(\\mathbf{R}^*\\) jest po prostu zwykÅ‚Ä… macierzÄ… korelacji \\(\\mathbf{R}\\): \\[\n\\mathbf{R} = \\Lambda \\Lambda^\\top + \\Psi,\n\\] przy czym w PCM przyjmujemy \\(\\Psi = \\mathbf{0}\\).\nNastÄ™pnie wykonujemy dekompozycjÄ™ spektralnÄ… \\[\n\\mathbf{R} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^\\top,\n\\] gdzie:\n\n\n\\(\\mathbf{Q} = (q_1, q_2, \\ldots, q_p)\\) â€“ to macierz ortonormalnych wektorÃ³w wÅ‚asnych,\n\n\\(\\mathbf{D} = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p)\\) â€“ to macierz wartoÅ›ci wÅ‚asnych uporzÄ…dkowanych malejÄ…co.\n\nJeÅ›li chcemy oszacowaÄ‡ model z \\(m\\) czynnikami, to bierzemy najwiÄ™ksze \\(m\\) wartoÅ›ci wÅ‚asne i odpowiadajÄ…ce im wektory wÅ‚asne. Estymator Å‚adunkÃ³w czynnikowych jest wtedy rÃ³wny \\[\n\\hat{\\Lambda} = \\mathbf{Q}_m \\mathbf{D}_m^{1/2},\n\\] gdzie \\(\\mathbf{Q}_m = (q_1,\\ldots,q_m)\\), a \\(\\mathbf{D}_m = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_m)\\).\nWidzimy wiÄ™c, Å¼e w PCM Å‚adunki sÄ… wprost pierwiastkami z najwiÄ™kszych wartoÅ›ci wÅ‚asnych pomnoÅ¼onymi przez odpowiadajÄ…ce im wektory wÅ‚asne.\nProcedura estymacji4\n\n\nKonstruujemy macierz korelacji \\(\\mathbf{R}\\).\nObliczamy rozkÅ‚ad wartoÅ›ci i wektorÃ³w wÅ‚asnych macierzy \\(\\mathbf{R}\\).\nWybieramy \\(m\\) najwiÄ™kszych wartoÅ›ci wÅ‚asnych (odpowiadajÄ…cych liczbie czynnikÃ³w w modelu).\nNa tej podstawie konstruujemy macierz Å‚adunkÃ³w czynnikowych \\(\\Lambda\\).\n4Â tu widaÄ‡ najwiÄ™kszÄ… rÃ³Å¼nicÄ™ pomiÄ™cy PCM a PAF; w metodzie PCM wystÄ™pujÄ™ jedna iteracja estymacji Å‚adunkÃ³wWÅ‚asnoÅ›ci\n\n\nZgodnoÅ›Ä‡ z modelem czynnikowym â€“ metoda dÄ…Å¼y do aproksymacji struktury wspÃ³lnej wariancji, a nie caÅ‚kowitej wariancji.\n\nZbieÅ¼noÅ›Ä‡ do stabilnych oszacowaÅ„ â€“ iteracyjne poprawki komunalnoÅ›ci pozwalajÄ… uzyskaÄ‡ estymatory spÃ³jne z zaÅ‚oÅ¼eniami modelu.\n\nÅatwoÅ›Ä‡ interpretacji â€“ podobnie jak PCA, metoda bazuje na analizie spektralnej wartoÅ›ci wÅ‚asnych, co uÅ‚atwia intuicyjne rozumienie struktury danych.\nOgraniczenia\n\n\nBrak optymalnoÅ›ci statystycznej â€“ podobnie jak PAF, metoda nie ma wÅ‚asnoÅ›ci estymatorÃ³w opartych na funkcji wiarygodnoÅ›ci (ML).\n\nZaleÅ¼noÅ›Ä‡ od poczÄ…tkowych oszacowaÅ„ komunalnoÅ›ci â€“ nieprawidÅ‚owy wybÃ³r startowy moÅ¼e utrudniÄ‡ uzyskanie sensownych rozwiÄ…zaÅ„.\n\nHaywood case â€“ zdarza siÄ™, Å¼e zasoby zmiennoÅ›ci wspÃ³lnej mogÄ… przyjmowaÄ‡ wartoÅ›ci ujemne.\nMetoda minimalizacji reszt (ang. MINRES) (Harman i Jones 1966)\n\nIdea metody MINRES\nW modelu czynnikowym przyjmujemy, Å¼e macierz kowariancji (lub korelacji) ma postaÄ‡ \\[\n\\Sigma = \\Lambda \\Lambda' + \\Psi,\n\\] gdzie \\(\\Lambda\\) to macierz Å‚adunkÃ³w czynnikowych, a \\(\\Psi = \\mathrm{diag}(\\psi_1,\\ldots,\\psi_p)\\) to macierz wariancji swoistych.\nW metodzie MINRES nie prÃ³bujemy dokÅ‚adnie odtworzyÄ‡ caÅ‚ej macierzy \\(\\Sigma\\). Zamiast tego minimalizujemy reszty pozadiagonalne, czyli rÃ³Å¼nice miÄ™dzy obserwowanÄ… macierzÄ… korelacji \\(\\mathbf{R}\\) a macierzÄ… odtworzonÄ… z modelu \\(\\Lambda \\Lambda^\\top + \\Psi\\), przy czym skupiamy siÄ™ wyÅ‚Ä…cznie na elementach pozadiagonalnych.\nFunkcja kryterialna\nFormalnie minimalizowana jest suma kwadratÃ³w reszt poza przekÄ…tnÄ… \\[\nF(\\Lambda, \\Psi) = \\sum_{i \\neq j} \\Big( r_{ij} - \\hat{r}_{ij} \\Big)^2,\n\\] gdzie:\n\n\n\\(r_{ij}\\) to element macierzy korelacji empirycznej \\(\\mathbf{R}\\),\n\n\\(\\hat{r}_{ij}\\) to element macierzy odtworzonej \\(\\Lambda \\Lambda^\\top + \\Psi\\),\nelementy diagonalne nie sÄ… uwzglÄ™dniane (bo zawsze odtwarzane sÄ… przez normalizacjÄ™ zmiennych).\n\nMoÅ¼na to zapisaÄ‡ rÃ³wnowaÅ¼nie jako \\[\nF(\\Lambda) = | \\mathbf{R} - (\\Lambda \\Lambda' + \\Psi)|^2_{off},\n\\] gdzie \\(|\\cdot|_{off}\\) oznacza normÄ™ Frobeniusa liczona tylko na czÄ™Å›ciach pozadiagonalnych macierzy.\nProcedura estymacyjna\n\nZaczynamy od przybliÅ¼onych wartoÅ›ci komunalnoÅ›ci \\(\\hat{h}_j^2\\), tak jak w PAF.\nBudujemy macierz reszt \\[\n\\mathbf{U} = \\mathbf{R} - (\\Lambda \\Lambda^\\top + \\Psi).\n\\]\n\nSzukamy takich Å‚adunkÃ³w \\(\\Lambda\\), ktÃ³re minimalizujÄ… sumÄ™ kwadratÃ³w elementÃ³w \\(\\mathbf{U}\\) poza przekÄ…tnÄ…. Realizowane jest to w dwÃ³ch krokach:\n\nPierwszy krok polega na ustaleniu bieÅ¼Ä…cych komunalnoÅ›ci jako sum kwadratÃ³w Å‚adunkÃ³w dla kaÅ¼dej zmiennej, co zapewnia zgodnoÅ›Ä‡ z ograniczeniem modelu. Te komunalnoÅ›ci wprowadza siÄ™ nastÄ™pnie do macierzy korelacji w celu uzyskania nowej macierzy zredukowanej.\nDrugi element obejmuje rozwiÄ…zanie problemu wartoÅ›ci wÅ‚asnych tej macierzy, ktÃ³re prowadzi do nowej macierzy osadzeÅ„. Taki krok aktualizuje Å‚adunki w sposÃ³b zmniejszajÄ…cy reszty pozadiagonalne bez uÅ¼ycia pochodnych funkcji \\(F\\).\n\nW algorytmie MINRES nie wyznacza siÄ™ gradientu funkcji \\(F\\), lecz wykorzystuje siÄ™ fakt, Å¼e macierz reszt moÅ¼na zredukowaÄ‡, dopasowujÄ…c kierunki wÅ‚asne macierzy korelacji zredukowanej do struktury reszt. Aktualizacja Å‚adunkÃ³w jest wiÄ™c realizowana przez wybÃ³r tych wektorÃ³w wÅ‚asnych, ktÃ³re najlepiej redukujÄ… rÃ³Å¼nice poza przekÄ…tnÄ…. Po kaÅ¼dej aktualizacji ponownie oblicza siÄ™ macierz odtworzonÄ…, wyznacza macierz reszt i sprawdza, czy zmiany funkcji \\(F\\) sÄ… wystarczajÄ…co maÅ‚e.\n\nW praktyce problem redukuje siÄ™ do iteracyjnego rozwiÄ…zywania ukÅ‚adÃ³w rÃ³wnaÅ„ wÅ‚asnych, bardzo podobnie jak w PAF, ale z innym warunkiem minimalizacji (PAF dopasowuje wartoÅ›ci wÅ‚asne macierzy zredukowanych korelacji, MINRES â€“ reszty pozadiagonalne).\nWÅ‚aÅ›ciwoÅ›ci i ograniczenia\n\n\nMINRES skupia siÄ™ tylko na korelacjach pomiÄ™dzy zmiennymi, ignorujÄ…c elementy diagonalne â€“ co sprawia, Å¼e estymacja jest mniej wraÅ¼liwa na problem ujemnych komunalnoÅ›ci (tzw. Heywood cases).\nMetoda jest relatywnie stabilna numerycznie i dobrze sprawdza siÄ™ przy duÅ¼ej liczbie zmiennych.\nOgraniczeniem jest to, Å¼e wynik zaleÅ¼y od jakoÅ›ci poczÄ…tkowych oszacowaÅ„ zasobÃ³w zmiennoÅ›ci wspÃ³lnej. Przy zÅ‚ym wyborze startu moÅ¼liwa jest wolna zbieÅ¼noÅ›Ä‡ albo zbieÅ¼noÅ›Ä‡ do lokalnego minimum.\nMetoda uogÃ³lnionych najmniejszych kwadratÃ³w (ang. Generalized Least Squares, GLS) (JÃ¶reskog i Goldberger 1972)\n\nIdea metody\nGLS, podobnie jak MINRES czy ML, polega na porÃ³wnaniu macierzy obserwowanej \\(\\mathbf{S}\\) (kowariancji lub korelacji) z macierzÄ… odtworzonÄ… przez model czynnikowy \\(\\hat{\\Sigma} = \\Lambda \\Lambda^\\top + \\Psi.\\) RÃ³Å¼nica w stosunku do MINRES polega na tym, Å¼e w GLS waÅ¼ymy reszty, czyli bÅ‚Ä™dy odwzorowania poszczegÃ³lnych elementÃ³w macierzy \\(\\mathbf{S}\\).\nFormalnie kryterium minimalizacji ma postaÄ‡ \\[\nF_{\\text{GLS}}(\\Lambda, \\Psi) = \\mathrm{tr}\\Big[ \\big( S - \\hat{\\Sigma} \\big) W \\big( S - \\hat{\\Sigma} \\big) W \\Big],\n\\]\ngdzie \\(W\\) to macierz wag, zwykle przyjmowana jako odwrotnoÅ›Ä‡ (lub pseudoodwrotnoÅ›Ä‡) wariancji estymatora elementÃ³w macierzy \\(\\mathbf{S}\\).\nEstymacja metodÄ… GLS przebiega poprzez iteracyjne aktualizowanie \\(\\Lambda\\) i \\(\\Psi\\). W pierwszej kolejnoÅ›ci przyjmuje siÄ™ poczÄ…tkowÄ… postaÄ‡ \\(\\Psi\\) (np. oszacowanÄ… z wariancji resztowych lub metodÄ… principal factor/axis). NastÄ™pnie przeksztaÅ‚ca siÄ™ zmienne przy uÅ¼yciu macierzy \\(\\Psi^{-1/2}\\) w celu uzyskania tzw. zmiennych zwaÅ¼onych. Dla tak przeksztaÅ‚conych danych stosuje siÄ™ klasycznÄ… metodÄ™ estymacji, poniewaÅ¼ przeksztaÅ‚cenie usuwa heteroskedastycznoÅ›Ä‡ skÅ‚adnikÃ³w specyficznych. PrzykÅ‚adowo, po przeprowadzeniu transformacji \\(Y = \\Psi^{-1/2}X\\), model przyjmuje postaÄ‡\n\\[\nY = (\\Psi^{-1/2}\\Lambda)f+\\Psi^{-1/2}\\varepsilon,\n\\] gdzie skÅ‚adnik losowy ma kowariancjÄ™ rÃ³wnÄ… macierzy jednostkowej. Åadunki czynnikowe szacujemy wiÄ™c przez zwykÅ‚Ä… metodÄ™ najmniejszych kwadratÃ³w dla zmiennych \\(Y\\). NastÄ™pnie uzyskane Å‚adunki transformujemy z powrotem, otrzymujÄ…c aktualizacjÄ™ \\(\\Lambda\\). Po tej aktualizacji wyznaczamy nowÄ… \\(\\Psi\\) i powtarzamy procedurÄ™ aÅ¼ do zbieÅ¼noÅ›ci.\nMetoda GLS pozwala wÅ‚Ä…czaÄ‡ do analizy informacjÄ™ o strukturze wariancji specyficznych i ograniczaÄ‡ wpÅ‚yw zmiennych o maÅ‚ej precyzji pomiaru. Takie podejÅ›cie zapewnia lepszÄ… stabilnoÅ›Ä‡ estymacji i wiÄ™kszÄ… zgodnoÅ›Ä‡ modelu z empirycznÄ… strukturÄ… kowariancji, zwÅ‚aszcza gdy zmienne charakteryzujÄ… siÄ™ wyraÅºnie zrÃ³Å¼nicowanÄ… wariancjÄ… bÅ‚Ä™du.\nW przeciwieÅ„stwie do MINRES (gdzie wszystkie reszty traktowane sÄ… jednakowo), w GLS rÃ³Å¼ne elementy macierzy kowariancji otrzymujÄ… rÃ³Å¼ne wagi. Wagi te wynikajÄ… z asymptotycznych wÅ‚asnoÅ›ci estymatora macierzy kowariancji i uwzglÄ™dniajÄ… fakt, Å¼e elementy macierzy nie sÄ… niezaleÅ¼ne i majÄ… rÃ³Å¼ne wariancje. DziÄ™ki temu GLS jest bardziej efektywny statystycznie niÅ¼ MINRES, ale jednoczeÅ›nie mniej wymagajÄ…cy niÅ¼ ML (ktÃ³ry zakÅ‚ada peÅ‚nÄ… normalnoÅ›Ä‡ wielowymiarowÄ…).\nWÅ‚asnoÅ›ci\n\nEstymatory GLS sÄ… spÃ³jne i asymptotycznie efektywne w klasie metod najmniejszych kwadratÃ³w, przy zaÅ‚oÅ¼eniu poprawnej specyfikacji modelu.\nGLS, podobnie jak ML, uwzglÄ™dnia strukturÄ™ wariancji elementÃ³w macierzy \\(\\mathbf{S}\\), co czyni go bardziej precyzyjnym niÅ¼ MINRES.\nZ drugiej strony GLS jest mniej czuÅ‚y na naruszenie zaÅ‚oÅ¼enia normalnoÅ›ci niÅ¼ ML, dlatego bywa rekomendowany przy wiÄ™kszych odchyleniach od normalnoÅ›ci.\nOgraniczenia\n\nProcedura GLS jest obliczeniowo trudniejsza niÅ¼ MINRES, poniewaÅ¼ wymaga oszacowania (lub przyjÄ™cia) odpowiedniej macierzy wag.\nW praktyce GLS bywa niestabilny przy maÅ‚ych prÃ³bach lub przy silnych wspÃ³Å‚liniowoÅ›ciach zmiennych.\nW implementacjach programowych czÄ™sto stosuje siÄ™ GLS jako kompromis pomiÄ™dzy prostym MINRES a wymagajÄ…cym ML.\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nIstniejÄ… rÃ³wnieÅ¼ inne metody estymacji Å‚adunkÃ³w czynnikowych, jak metody bayesowskie (Lu, Chow, i Loken 2016), czy metody z regularyzacjÄ… LASSO ale nie sÄ… one czÄ™Å›ciÄ… tego opracowania (Jacobucci i Grimm 2018).",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynnikÃ³w",
    "href": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynnikÃ³w",
    "title": "Analiza czynnikowa",
    "section": "Oceny dopasowania modelu i kryteria doboru liczby czynnikÃ³w",
    "text": "Oceny dopasowania modelu i kryteria doboru liczby czynnikÃ³w\nOcena dopasowania modelu EFA opiera siÄ™ na kilku uzupeÅ‚niajÄ…cych siÄ™ perspektywach: globalnym dopasowaniu implikowanej macierzy kowariancji do macierzy empirycznej, analizie reszt korelacyjnych, doborze liczby czynnikÃ³w, stabilnoÅ›ci rozwiÄ…zania oraz jakoÅ›ci lokalnej (Å‚adunki i zasoby zmiennoÅ›ci wspÃ³lnej). PoniÅ¼ej przedstawiam najwaÅ¼niejsze procedury wraz z ich interpretacjÄ… oraz typowymi puÅ‚apkami.\nProporcja wyjaÅ›nionej wariancji przez czynniki\nProporcja wariancji wyjaÅ›nionej przez model czynnikowy, czyli stosunek sumy wariancji wspÃ³lnej do caÅ‚kowitej wariancji wszystkich zmiennych, stanowi podstawowÄ… miarÄ™ jakoÅ›ci dopasowania. W przypadku standaryzowanych zmiennych caÅ‚kowita wariancja wynosi \\(p\\), wiÄ™c proporcja wariancji wyjaÅ›niona przez model skÅ‚adajÄ…cy siÄ™ z \\(m\\) czynnikÃ³w ma postaÄ‡ \\[\n\\text{Proporcja wyjaÅ›nionej wariancji} = \\frac{\\operatorname{tr}(\\Lambda_m\\Lambda_m^\\top)}{\\operatorname{tr}(S)}=\\frac{\\sum_{j=1}^p h_j^2}{p}.\n\\]WyÅ¼sze wartoÅ›ci (np. powyÅ¼ej \\(0,6\\)) wskazujÄ… na dobrÄ… reprezentacjÄ™ zmiennych przez czynniki, natomiast niskie wartoÅ›ci (np. poniÅ¼ej \\(0,4\\)) sugerujÄ…, Å¼e model nie uchwytuje istotnej czÄ™Å›ci struktury danych. Jednak sama proporcja nie uwzglÄ™dnia liczby czynnikÃ³w ani zÅ‚oÅ¼onoÅ›ci modelu, dlatego powinna byÄ‡ interpretowana w kontekÅ›cie innych wskaÅºnikÃ³w dopasowania.\nTest chi-kwadrat\nW metodzie ML zostaÅ‚ przedstawiony test dopasowania oparty na maximum likelihood. Przy zaÅ‚oÅ¼eniu normalnoÅ›ci wielowymiarowej i zidentyfikowanym modelu postaci \\[\n\\Sigma=\\Lambda\\Lambda^\\top + \\Psi\n\\] testujemy hipotezÄ™ \\(H_0:\\ \\Sigma(\\Lambda,\\Psi)=S\\) w populacji, gdzie \\(S\\) oznacza macierz kowariancji (lub korelacji) z prÃ³by. Statystyka \\(\\chi^2\\) roÅ›nie wraz z pogarszajÄ…cym siÄ™ dopasowaniem (niestety duÅ¼e prÃ³by sprzyjajÄ… odrzucaniu nawet dobrze dopasowanych modeli, a naruszenia normalnoÅ›ci mogÄ… zawyÅ¼aÄ‡ lub zaniÅ¼aÄ‡ wynik).\nWskaÅºnik RMSEA\nWskaÅºnik root mean square error of approximation (RMSEA) mierzy bÅ‚Ä…d aproksymacji na jednostkÄ™ stopnia swobody i moÅ¼na go interpretowaÄ‡ jako â€bÅ‚Ä…d w populacjiâ€, nie tylko w prÃ³bie. Definiujemy go jako \\[\n\\mathrm{RMSEA}=\\sqrt{\\max\\left\\{\\frac{\\chi^2-df}{df(n-1)},0\\right\\}},\n\\] a ocenÄ™ uzupeÅ‚niamy o przedziaÅ‚ ufnoÅ›ci oparty na niecentralnym rozkÅ‚adzie chi-kwadrat. WartoÅ›ci rzÄ™du \\(0,05-0,08\\) tradycyjnie uznawane sÄ… za akceptowalne, traktujÄ…c progi orientacyjnie: wzrost liczby zmiennych i stopni swobody sprzyja niÅ¼szym RMSEA, natomiast maÅ‚e prÃ³by destabilizujÄ… oszacowanie.\nAnaliza reszt\nAnaliza reszt macierzy korelacji stanowi podstawowÄ… kontrolÄ™ lokalnego dopasowania, niezaleÅ¼nie od sposobu estymacji. Wyznaczamy reszty \\(r_{ij}-\\hat r_{ij}\\) i przeglÄ…damy rozkÅ‚ad wartoÅ›ci bezwzglÄ™dnych, a dokÅ‚adnie odsetek przekraczajÄ…cych praktyczne progi (np. \\(0,05\\) lub \\(0,1\\)). WskaÅºniki zbiorcze, takie jak RMSR (root mean square residual) oraz SRMR (standardized RMSR), agregujÄ… wielkoÅ›Ä‡ reszt poza diagonalÄ… - mniejsze wartoÅ›ci Å›wiadczÄ… o lepszym dopasowaniu. Mapa ciepÅ‚a reszt uÅ‚atwia wykrywanie klastrÃ³w niedopasowania sugerujÄ…cych brakujÄ…cy czynnik lub zbyt maÅ‚Ä… liczbÄ™ czynnikÃ³w.\nKryteria informacyjne\nKryteria informacyjne, takie jak AIC i BIC, sÅ‚uÅ¼Ä… do porÃ³wnywania modeli o rÃ³Å¼nej liczbie czynnikÃ³w, karzÄ…c nadmiernÄ… zÅ‚oÅ¼onoÅ›Ä‡. Definiujemy je przez logarytm funkcji wiarogodnoÅ›ci i liczbÄ™ parametrÃ³w. BIC silniej faworyzuje prostsze modele przy duÅ¼ych prÃ³bach. Bardzo waÅ¼ne jest aby uÅ¼ywaÄ‡ tych metod do porÃ³wnywania modeli otrzymanych tÄ… samÄ… metodÄ….\nInne wskaÅºniki dopasowania\nWskaÅºniki â€globalneâ€ starszej generacji, takie jak GFI i AGFI (goodness of fit index, adjusted GFI), oceniajÄ… proporcjÄ™ wariancji/kowariancji wyjaÅ›nionej przez model. SÄ… wraÅ¼liwe na rozmiar prÃ³by i liczbÄ™ zmiennych, skÅ‚onne do optymizmu w duÅ¼ych modelach i do pesymizmu przy maÅ‚ej liczbie stopni swobody. MoÅ¼emy je traktowaÄ‡ pomocniczo, kÅ‚adÄ…c wiÄ™kszy nacisk na RMSEA oraz analizÄ™ reszt.\nAnaliza wartoÅ›ci wÅ‚asnych macierzy reszt uzupeÅ‚nia powyÅ¼sze podejÅ›cia. Po wyodrÄ™bnieniu \\(m\\) czynnikÃ³w obliczamy resztowÄ… macierz korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) i badaÄ‡ jej wartoÅ›ci wÅ‚asne. DuÅ¼e dodatnie wartoÅ›ci wÅ‚asne sygnalizujÄ… pozostawionÄ… wspÃ³lnÄ… wariancjÄ™ (niedomiar czynnikÃ³w) lub struktury lokalne.\nJakoÅ›Ä‡ lokalnÄ… rozwiÄ…zania oceniaÄ‡ przez zasoby zmiennoÅ›ci wspÃ³lnej i swoistej. \\[\nh_j^2=\\sum_{k=1}^{m}\\lambda_{jk}^{2}\n\\] mierzÄ… czÄ™Å›Ä‡ wariancji zmiennej \\(x_j\\) wyjaÅ›nionÄ… przez czynniki, bardzo niskie \\(h_j^2\\) wskazujÄ… sÅ‚abÄ… reprezentacjÄ™ zmiennej, natomiast bardzo wysokie â€” wraz z ryzykiem ujemnych \\(\\Psi_j\\) (przypadki Haywooda) â€” mogÄ… sygnalizowaÄ‡ dopasowanie wymuszone lub niewÅ‚aÅ›ciwÄ… liczebnoÅ›Ä‡ czynnikÃ³w. Sumy kwadratÃ³w Å‚adunkÃ³w per czynnik odzwierciedlajÄ… wyjaÅ›nionÄ… wspÃ³lnÄ… wariancjÄ™ i sÅ‚uÅ¼Ä… do oceny rÃ³wnomiernoÅ›ci wkÅ‚adu czynnikÃ³w.\nW rozwiÄ…zaniach dopuszczajacych korelacje pomiedzy czynnikami dodatkowym aspektem dopasowania jest macierz korelacji czynnikÃ³w \\(\\Phi\\). Bardzo wysokie korelacje miÄ™dzy czynnikami sugerujÄ… nadmiarowoÅ›Ä‡ i potencjalne przeparametryzowanie. WÃ³wczas warto rozwaÅ¼yÄ‡ redukcjÄ™ liczby czynnikÃ³w lub alternatywne struktury.\nNajbardziej znane kryteria doboru liczby czynnikÃ³w to:\nKryterium wykresu osypiska (Scree plot, Cattell (1966))\nNa osi poziomej odkÅ‚adamy kolejne wartoÅ›ci wÅ‚asne, a na pionowej ich wielkoÅ›Ä‡. Punktem granicznym jest miejsce, gdzie wykres â€zaÅ‚amuje siÄ™â€ i przechodzi w â€osypiskoâ€ â€“ od tego miejsca czynniki interpretowane sÄ… jako szum.\n\nZalety: wizualna intuicja, Å‚atwe zastosowanie.\nWady: czÄ™sto subiektywnoÅ›Ä‡ w okreÅ›leniu miejsca â€Å‚okciaâ€, szczegÃ³lnie gdy krzywa nie ma wyraÅºnego zaÅ‚amania.\nAnaliza rÃ³wnolegÅ‚a (Parallel analysis, Horn (1965))\nPolega na porÃ³wnaniu wartoÅ›ci wÅ‚asnych dla danych empirycznych z wartoÅ›ciami wÅ‚asnymi uzyskanymi dla danych losowych o tej samej strukturze (ta sama liczba zmiennych i obserwacji). Zatrzymuje siÄ™ te czynniki, ktÃ³rych wartoÅ›ci wÅ‚asne przewyÅ¼szajÄ… np. 95. percentyl rozkÅ‚adu wartoÅ›ci losowych.\n\nZalety: jedna z najbardziej rekomendowanych metod, dobrze sprawdza siÄ™ w praktyce.\nWady: wymaga procedur symulacyjnych, wiÄ™kszej mocy obliczeniowej.\nKryterium MAP (Minimum Average Partial, Velicer (1976))\nOpiera siÄ™ na analizie korelacji czÄ…stkowych. Stopniowo usuwa siÄ™ kolejne czynniki, a nastÄ™pnie oblicza Å›redniÄ… wartoÅ›Ä‡ kwadratu korelacji czÄ…stkowych. Liczba czynnikÃ³w odpowiadajÄ…ca minimum tej wartoÅ›ci uznawana jest za optymalnÄ….\n\nZalety: metoda oparta na minimalizacji resztowych zaleÅ¼noÅ›ci, obiektywna.\nWady: wraÅ¼liwa na naruszenia zaÅ‚oÅ¼eÅ„ modelu, mniej intuicyjna dla poczÄ…tkujÄ…cych.\nTesty statystyczne dopasowania (dla ML)\nPrzy estymacji metodÄ… najwiÄ™kszej wiarygodnoÅ›ci moÅ¼na zastosowaÄ‡ test chi-kwadrat dla porÃ³wnania modelu z \\(m\\) czynnikami z modelem peÅ‚nym. Sprawdza siÄ™, czy macierz implikowana przez model rÃ³Å¼ni siÄ™ istotnie od empirycznej. LiczbÄ™ czynnikÃ³w dobiera siÄ™ tak, aby model byÅ‚ jeszcze akceptowalny, ale nie przeparametryzowany.\n\nZalety: formalne podejÅ›cie statystyczne.\nWady: silna wraÅ¼liwoÅ›Ä‡ na licznoÅ›Ä‡ prÃ³by i zaÅ‚oÅ¼enie normalnoÅ›ci wielowymiarowej; w duÅ¼ych prÃ³bach nawet dobre modele mogÄ… byÄ‡ odrzucane.\nKryteria informacyjne (AIC, BIC, CAIC)\nPorÃ³wnujÄ… modele o rÃ³Å¼nej liczbie czynnikÃ³w, rÃ³wnowaÅ¼Ä…c dopasowanie (log-wiarygodnoÅ›Ä‡) i zÅ‚oÅ¼onoÅ›Ä‡ (liczbÄ™ parametrÃ³w). Optymalna liczba czynnikÃ³w to ta, dla ktÃ³rej wartoÅ›Ä‡ kryterium jest minimalna.\n\nZalety: uwzglÄ™dniajÄ… karÄ™ za nadmiernÄ… zÅ‚oÅ¼onoÅ›Ä‡, dobrze sprawdzajÄ… siÄ™ w porÃ³wnaniach.\nWady: wartoÅ›ci kryteriÃ³w sÄ… zaleÅ¼ne od metody estymacji, wiÄ™c porÃ³wnywaÄ‡ moÅ¼na tylko modele oszacowane tÄ… samÄ… metodÄ….\nAnaliza reszt i spektrum wartoÅ›ci wÅ‚asnych macierzy reszt\nPo przyjÄ™ciu liczby czynnikÃ³w oblicza siÄ™ macierz reszt korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) . JeÅ›li w resztach (poza przekÄ…tnÄ…) pozostajÄ… duÅ¼e (co do wartoÅ›ci bezwzglÄ™dnej) wartoÅ›ci wÅ‚asne, oznacza to, Å¼e nie wszystkie wspÃ³lne zaleÅ¼noÅ›ci zostaÅ‚y uchwycone i potrzebne sÄ… dodatkowe czynniki.\n\nZalety: pozwala oceniÄ‡ niedopasowanie â€lokalneâ€ i strukturalne.\nWady: wymaga bardziej zaawansowanej interpretacji.\nUdziaÅ‚ wyjaÅ›nionej wariancji\nW praktyce czÄ™sto wymaga siÄ™, aby caÅ‚kowita wyjaÅ›niona wariancja przekraczaÅ‚a okreÅ›lony prÃ³g (np. 50% w naukach spoÅ‚ecznych). Dodatkowo analizuje siÄ™ rÃ³wnomiernoÅ›Ä‡ wkÅ‚adu poszczegÃ³lnych czynnikÃ³w.\n\nZalety: intuicyjne i Å‚atwe do raportowania.\nWady: arbitralne progi, zaleÅ¼ne od liczby zmiennych i kontekstu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#rotacje-czynnikÃ³w",
    "href": "fa.html#rotacje-czynnikÃ³w",
    "title": "Analiza czynnikowa",
    "section": "Rotacje czynnikÃ³w",
    "text": "Rotacje czynnikÃ³w\nRotacja czynnikÃ³w jest etapem analizy czynnikowej, ktÃ³rego celem jest poprawa interpretowalnoÅ›ci rozwiÄ…zania poprzez uproszczenie struktury Å‚adunkÃ³w czynnikowych. Matematycznie polega ona na zastosowaniu transformacji liniowej do macierzy Å‚adunkÃ³w \\(\\Lambda\\). JeÅ›li \\(\\Lambda\\) jest macierzÄ… \\(p \\times m\\) Å‚adunkÃ³w (gdzie \\(p\\) to liczba zmiennych, a \\(m\\) liczba czynnikÃ³w), to po rotacji otrzymujemy nowÄ… macierz Å‚adunkÃ³w \\[\n\\Lambda^* = \\Lambda T,\n\\] gdzie \\(T\\) jest macierzÄ… transformacji rotacyjnej o wymiarach \\(m \\times m\\). W zaleÅ¼noÅ›ci od wÅ‚asnoÅ›ci macierzy \\(T\\) wyrÃ³Å¼nia siÄ™ dwa gÅ‚Ã³wne typy rotacji: ortogonalne i skoÅ›ne (oblique).\nRotacje ortogonalne\nW przypadku rotacji ortogonalnych macierz \\(T\\) jest macierzÄ… ortogonalnÄ…, czyli speÅ‚nia warunek: \\[T^\\top T = TT^\\top = I_m.\\] Oznacza to, Å¼e czynniki po rotacji pozostajÄ… nieskorelowane (\\(\\Phi = I_m\\)).\nNajwaÅ¼niejsze rodzaje rotacji ortogonalnych:\n\nVarimax (Kaiser 1958) - najczÄ™Å›ciej stosowana rotacja ortogonalna. Maksymalizuje wariancjÄ™ kwadratÃ³w Å‚adunkÃ³w w ramach kaÅ¼dego czynnika. Prowadzi do tego, Å¼e kaÅ¼da zmienna ma wysokie Å‚adunki tylko na jednym czynniku, a bliskie zeru na pozostaÅ‚ych. Funkcja celu \\[\nV = \\sum_{j=1}^m \\left[ \\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*4} - \\left(\\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*2}\\right)^2 \\right].\n\\]\nQuartimax - minimalizuje liczbÄ™ czynnikÃ³w potrzebnych do opisania kaÅ¼dej zmiennej, upraszczajÄ…c wiersze macierzy Å‚adunkÃ³w. Funkcja celu \\[\nQ = \\sum_{i=1}^p \\sum_{j=1}^m \\lambda_{ij}^{*4}.\n\\]\nEquamax - Å‚Ä…czy idee varimax i quartimax. Celem jest rÃ³wnowaÅ¼enie prostoty struktur wierszy i kolumn macierzy Å‚adunkÃ³w. Funkcja celu \\[\nE = \\frac12(Q + V).\n\\]\nBiquartimax - celem tej rotacji jest jednoczesne uproszczenie wierszy i kolumn macierzy Å‚adunkÃ³w. W praktyce Å‚Ä…czy zalety varimax i quartimax. Funkcja celu \\[\nBQ = \\alpha \\, Q + (1 - \\alpha) \\, V,\n\\] z modyfikacjÄ… wag, ktÃ³re rÃ³wnowaÅ¼Ä… wpÅ‚yw prostoty wierszy i kolumn. Zmienne majÄ… tendencjÄ™ do Å‚adowania siÄ™ mocno na jednym czynniku (jak w varimax), ale jednoczeÅ›nie ogranicza siÄ™ sytuacje, w ktÃ³rych jedna zmienna ma Å›rednie Å‚adunki na wielu czynnikach (jak w quartimax).\nRotacje skoÅ›ne (oblique)\nW przypadku rotacji skoÅ›nych macierz \\(T\\) nie musi byÄ‡ ortogonalna, wiÄ™c \\[\nT^\\top T \\neq I_m.\n\\] W efekcie rotowane czynniki mogÄ… byÄ‡ skorelowane, a macierz korelacji czynnikÃ³w \\(\\Phi\\) przyjmuje ogÃ³lnÄ… postaÄ‡ dodatnio okreÅ›lonÄ….\nPodstawowe rodzaje:\n\nOblimin (Jennrich i Sampson 1966) - rodzina rotacji z parametrem \\(\\gamma\\), ktÃ³ry reguluje stopieÅ„ skoÅ›noÅ›ci. Dla \\(\\gamma = 0\\) rozwiÄ…zanie staje siÄ™ quartimax, a wiÄ™ksze \\(\\gamma\\) prowadzÄ… do wiÄ™kszej korelacji czynnikÃ³w. Funkcja celu \\[\nF(\\Lambda^*) = \\sum_{i=1}^p \\sum_{j=1}^m \\left(\\lambda_{ij}^{*2} - \\gamma \\frac{\\sum_{k=1}^m \\lambda_{ik}^{*2}}{m}\\right)^2.\n\\]\nPromax (Hendrickson i White 1964) - rotacja skoÅ›na oparta na prostym podejÅ›ciu dwustopniowym. Najpierw stosuje siÄ™ rotacjÄ™ ortogonalnÄ… (najczÄ™Å›ciej varimax), nastÄ™pnie Å‚adunki sÄ… podnoszone do potÄ™gi \\(k\\) (zwykle 3 lub 4), aby wymusiÄ‡ prostÄ… strukturÄ™, i ponownie dopasowywane przy uÅ¼yciu metody najmniejszych kwadratÃ³w \\[\n\\tilde{\\lambda}{jk} = \\text{sign}(\\lambda^*_{jk}) \\cdot |\\lambda^*_{jk}|^p.\n\\] Rotacja promax pozwala uzyskaÄ‡ bardziej realistyczne struktury, gdy czynniki sÄ… rzeczywiÅ›cie skorelowane.\nGeomin (Everitt i Yates 1989) - minimalizuje Å›redniÄ… geometrycznÄ… kwadratÃ³w Å‚adunkÃ³w, co prowadzi do sytuacji, w ktÃ³rej kaÅ¼da zmienna ma niewiele istotnych Å‚adunkÃ³w. Funkcja celu \\[\nG(\\Lambda^*) = \\sum_{i=1}^p \\left( \\prod_{j=1}^m (\\lambda_{ij}^{*2} + \\epsilon) \\right)^{1/m},\n\\] gdzie \\(\\epsilon\\) to maÅ‚y parametr stabilizujÄ…cy.\nSimplimax (Kiers 1994) - uogÃ³lnienie kryteriÃ³w upraszczajÄ…cych jak varimax czy quartimax, ktÃ³re minimalizuje liczbÄ™ duÅ¼ych i maÅ‚ych Å‚adunkÃ³w w macierzy, pozwalajÄ…c uÅ¼ytkownikowi sterowaÄ‡ liczbÄ… â€prostychâ€ elementÃ³w.\nWybÃ³r rodzaju rotacji\n\nRotacje ortogonalne sÄ… preferowane, gdy zakÅ‚adamy, Å¼e czynniki powinny byÄ‡ niezaleÅ¼ne teoretycznie.\nRotacje skoÅ›ne stosuje siÄ™, gdy istnieje uzasadnienie, Å¼e czynniki mogÄ… byÄ‡ skorelowane (co jest czÄ™ste w naukach spoÅ‚ecznych, psychologii czy biologii).\n\n\nPrzykÅ‚ad 3.1 Na potrzeby ilustracji budowy modelu EFA wykorzystamy dane z pakietu psych, ktÃ³re zawierajÄ… wyniki rÃ³Å¼nych testÃ³w poznawczych (Horn 1969). Dane te sÄ… czÄ™sto uÅ¼ywane jako przykÅ‚ad w literaturze dotyczÄ…cej analizy czynnikowej.\n\nKodlibrary(psych)\n\n# Dane: macierz korelacji testÃ³w poznawczych\ndata(\"Harman74.cor\")\n\n\n\n\n\n\n\n\n\nZmienna\nOpis\nKategoria testu\n\n\n\nVisualPerception\nRozpoznawanie i analiza relacji przestrzennych w figurach\nZdolnoÅ›ci przestrzenne / percepcyjne\n\n\nCubes\nManipulacja wyobraÅ¼eniowa bryÅ‚, rotacje przestrzenne\nZdolnoÅ›ci przestrzenne\n\n\nPaperFormBoard\nSkÅ‚adanie i dopasowywanie elementÃ³w figur\nZdolnoÅ›ci przestrzenne\n\n\nFlags\nRozpoznawanie wzorÃ³w i relacji symboli\nPercepcja wzrokowa / logiczne\n\n\nGeneralInformation\nOgÃ³lna wiedza faktograficzna\nZdolnoÅ›ci werbalne\n\n\nPargraphComprehension\nRozumienie tekstÃ³w pisanych\nZdolnoÅ›ci werbalne\n\n\nSentenceCompletion\nUzupeÅ‚nianie zdaÅ„ brakujÄ…cymi sÅ‚owami\nZdolnoÅ›ci werbalne\n\n\nWordClassification\nGrupowanie sÅ‚Ã³w wedÅ‚ug znaczenia\nZdolnoÅ›ci werbalne / semantyczne\n\n\nWordMeaning\nZnajomoÅ›Ä‡ i rozumienie znaczeÅ„ sÅ‚Ã³w\nZdolnoÅ›ci werbalne\n\n\nAddition\nWykonywanie prostych dziaÅ‚aÅ„ arytmetycznych\nZdolnoÅ›ci numeryczne\n\n\nCode\nDopasowywanie symboli do liczb wedÅ‚ug klucza\nSzybkoÅ›Ä‡ przetwarzania / percepcja\n\n\nCountingDots\nLiczenie elementÃ³w wzrokowych\nSzybkoÅ›Ä‡ percepcji / numeryczne\n\n\nStraightCurvedCapitals\nRozpoznawanie prostych i zakrzywionych liter\nPercepcja wizualna / szybkoÅ›Ä‡\n\n\nWordRecognition\nRozpoznawanie sÅ‚Ã³w z listy\nPamiÄ™Ä‡ i zdolnoÅ›ci werbalne\n\n\nNumberRecognition\nRozpoznawanie liczb z listy\nPamiÄ™Ä‡ / percepcja numeryczna\n\n\nFigureRecognition\nRozpoznawanie i identyfikacja figur\nPamiÄ™Ä‡ wizualna / percepcja\n\n\nObjectNumber\nDopasowywanie obiektÃ³w do liczb\nZÅ‚oÅ¼one zdolnoÅ›ci percepcyjno-num.\n\n\nNumberFigure\nDopasowywanie liczb do figur\nZÅ‚oÅ¼one zdolnoÅ›ci percepcyjno-num.\n\n\nFigureWord\nDopasowywanie figur do sÅ‚Ã³w\nÅÄ…czenie informacji wizualno-werbalnych\n\n\nDeduction\nRozwiÄ…zywanie zadaÅ„ logicznych, wnioskowanie\nRozumowanie logiczne\n\n\nNumericalPuzzles\nZadania numeryczne o charakterze problemowym\nZdolnoÅ›ci numeryczne / logiczne\n\n\nProblemReasoning\nRozwiÄ…zywanie zÅ‚oÅ¼onych problemÃ³w\nRozumowanie ogÃ³lne\n\n\nSeriesCompletion\nUzupeÅ‚nianie szeregÃ³w logicznych lub numerycznych\nRozumowanie abstrakcyjne / numeryczne\n\n\nArithmeticProblems\nRozwiÄ…zywanie zadaÅ„ arytmetycznych o wiÄ™kszej trudnoÅ›ci\nZdolnoÅ›ci numeryczne\n\n\n\nWidaÄ‡, Å¼e testy moÅ¼na grupowaÄ‡ w piÄ™Ä‡ gÅ‚Ã³wnych obszarÃ³w: przestrzenne/percepcyjne (np. Cubes, VisualPerception), werbalne (np. WordMeaning, SentenceCompletion), numeryczne (np. Addition, ArithmeticProblems), pamiÄ™ciowe (np. WordRecognition, NumberRecognition), oraz rozumowania i logiczne (np. Deduction, SeriesCompletion). To wÅ‚aÅ›nie takie powiÄ…zania w macierzy korelacji uzasadniajÄ… zastosowanie analizy czynnikowej w celu identyfikacji ukrytych wymiarÃ³w inteligencji.\nNajpierw sprawdzimy czy dane nadajÄ… siÄ™ do analizy czynnikowej, obliczajÄ…c test KMO i test sferycznoÅ›ci Bartletta.\n\nKodlibrary(tidyverse)\nlibrary(easystats)\n\ncheck_factorstructure(Harman74.cor$cov, n = 145) \n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(276) = 1545.86, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.88). The individual KMO scores are: VisualPerception (0.90), Cubes (0.84), PaperFormBoard (0.78), Flags (0.85), GeneralInformation (0.88), PargraphComprehension (0.89), SentenceCompletion (0.89), WordClassification (0.92), WordMeaning (0.88), Addition (0.81), Code (0.85), CountingDots (0.84), StraightCurvedCapitals (0.89), WordRecognition (0.85), NumberRecognition (0.88), FigureRecognition (0.89), ObjectNumber (0.85), NumberFigure (0.88), FigureWord (0.83), Deduction (0.93), NumericalPuzzles (0.91), ProblemReasoning (0.93), SeriesCompletion (0.91), ArithmeticProblems (0.92).\n\n\nTest sferycznoÅ›ci Bartletta dostarcza podstawowego potwierdzenia, Å¼e w zbiorze danych wystÄ™pujÄ… istotne statystycznie korelacje pomiÄ™dzy zmiennymi. Wynik \\(\\chi^2(276) = 1545.86,\\ p &lt; 0.001\\) oznacza, Å¼e hipoteza zerowa o macierzy korelacji rÃ³wnej macierzy jednostkowej zostaje odrzucona. Innymi sÅ‚owy, zmienne nie sÄ… niezaleÅ¼ne, a ich struktura korelacyjna uzasadnia dalsze poszukiwanie wspÃ³lnych czynnikÃ³w. Gdyby test okazaÅ‚ siÄ™ nieistotny, sugerowaÅ‚by brak uzasadnienia do stosowania analizy czynnikowej, poniewaÅ¼ nie byÅ‚oby wystarczajÄ…cych zaleÅ¼noÅ›ci miÄ™dzy zmiennymi.\nMiara adekwatnoÅ›ci prÃ³by KMO (Kaiserâ€“Meyerâ€“Olkin) wskazuje, na ile obserwowane korelacje mogÄ… byÄ‡ wyjaÅ›nione przez czynniki wspÃ³lne w porÃ³wnaniu z korelacjami czÄ…stkowymi. Wynik ogÃ³lny KMO = 0.88 mieÅ›ci siÄ™ w przedziale uznawanym za â€bardzo dobryâ€ (powyÅ¼ej 0.80). Oznacza to, Å¼e dane dobrze nadajÄ… siÄ™ do analizy czynnikowej i moÅ¼emy oczekiwaÄ‡ stabilnych, interpretowalnych rozwiÄ…zaÅ„. WartoÅ›ci indywidualne dla poszczegÃ³lnych zmiennych mieszczÄ… siÄ™ miÄ™dzy 0.78 a 0.93, a wiÄ™c wszystkie osiÄ…gajÄ… poziom â€dobryâ€ lub â€bardzo dobryâ€. NajwyÅ¼sze wartoÅ›ci, takie jak Deduction (0.93), ProblemReasoning (0.93) czy ArithmeticProblems (0.92), wskazujÄ… na wyjÄ…tkowo silnÄ… reprezentacjÄ™ tych testÃ³w w przestrzeni czynnikowej. Z kolei najniÅ¼sze, jak PaperFormBoard (0.78), sÄ… nadal akceptowalne, ale sugerujÄ… nieco sÅ‚abszÄ… integracjÄ™ tej zmiennej z pozostaÅ‚ymi. CaÅ‚oÅ›ciowo zarÃ³wno wynik globalny, jak i rozkÅ‚ad wartoÅ›ci czÄ…stkowych KMO jednoznacznie potwierdzajÄ… zasadnoÅ›Ä‡ prowadzenia analizy czynnikowej na tym zbiorze danych.\n\nKod# Parallel analysis\nfa.parallel(Harman74.cor$cov, n.obs = 145, fa = \"fa\")\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\nSamo kryterium paralelne wskazuje na 4 czynniki, choÄ‡ gdyby braÄ‡ pod uwagÄ™ samo kryterium osypiska to rozwiÄ…zanie z 5 czynnikami teÅ¼ wydaje siÄ™ byÄ‡ wÅ‚aÅ›ciwe.\n\nKod# Kryterium MAP\nVSS(Harman74.cor$cov, n.obs = 145, plot = F)\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.8  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.85  with  2  factors\n\nThe Velicer MAP achieves a minimum of 0.02  with  4  factors \nBIC achieves a minimum of  -731.36  with  3  factors\nSample Size adjusted BIC achieves a minimum of  -112  with  5  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq    prob sqresid  fit RMSEA  BIC SABIC complex\n1 0.80 0.00 0.025 252   626 8.0e-34    16.8 0.80 0.101 -628   170     1.0\n2 0.55 0.85 0.022 229   428 3.1e-14    12.7 0.85 0.077 -711    13     1.5\n3 0.46 0.79 0.017 207   299 3.0e-05    10.0 0.88 0.055 -731   -76     1.8\n4 0.42 0.74 0.017 186   228 1.9e-02     8.0 0.90 0.039 -698  -109     1.9\n5 0.40 0.71 0.021 166   189 1.1e-01     7.2 0.91 0.030 -637  -112     2.0\n6 0.40 0.71 0.024 147   162 1.8e-01     6.3 0.92 0.026 -569  -104     2.0\n7 0.40 0.70 0.028 129   138 2.7e-01     5.6 0.93 0.021 -504   -95     2.2\n8 0.41 0.70 0.030 112   111 5.0e-01     5.0 0.94 0.000 -446   -92     2.3\n  eChisq  SRMR eCRMS eBIC\n1    748 0.097 0.101 -506\n2    422 0.073 0.080 -718\n3    240 0.055 0.063 -790\n4    133 0.041 0.050 -792\n5    105 0.036 0.047 -721\n6     81 0.032 0.044 -651\n7     62 0.028 0.041 -580\n8     44 0.023 0.037 -514\n\n\n\n\n\n\n\n\nWskaÅºnik\nInterpretacja\n\n\n\nvss1\nDopasowanie Very Simple Structure przy zaÅ‚oÅ¼eniu jednego czynnika na zmiennÄ…; wyÅ¼sze = lepsze.\n\n\nvss2\nDopasowanie VSS przy zaÅ‚oÅ¼eniu maksymalnie dwÃ³ch czynnikÃ³w na zmiennÄ…; wyÅ¼sze = lepsze.\n\n\nmap\nKryterium Velicera; minimum wskazuje optymalnÄ… liczbÄ™ czynnikÃ³w (eliminuje korelacje czÄ…stkowe).\n\n\ndof\nStopnie swobody testu dopasowania chi-kwadrat.\n\n\nchisq\nWartoÅ›Ä‡ statystyki chi-kwadrat; niska w relacji do df sugeruje dobre dopasowanie.\n\n\nprob\nWartoÅ›Ä‡ p testu chi-kwadrat; wysoka oznacza brak podstaw do odrzucenia poprawnego dopasowania.\n\n\nsqresid\nSuma kwadratÃ³w reszt (rÃ³Å¼nice R âˆ’ RÌ‚); niÅ¼sze wartoÅ›ci = lepsze odwzorowanie danych.\n\n\nfit\nProporcja wyjaÅ›nionej wariancji w macierzy korelacji; wyÅ¼sze wartoÅ›ci = lepsze dopasowanie.\n\n\nRMSEA\nBÅ‚Ä…d aproksymacji w populacji; &lt; 0.05 bardzo dobre, 0.05â€“0.08 akceptowalne, &gt; 0.10 sÅ‚abe.\n\n\nBIC\nKryterium informacyjne; niÅ¼sze wartoÅ›ci = lepszy kompromis dopasowania i prostoty.\n\n\nSABIC\nWersja BIC korygowana o wielkoÅ›Ä‡ prÃ³by; lepsza przy mniejszych prÃ³bach.\n\n\ncomplex\nÅšrednia liczba czynnikÃ³w na ktÃ³re Å‚adujÄ… siÄ™ zmienne; niÅ¼sze = prostsza struktura.\n\n\neChisq\nEstymowana statystyka chi-kwadrat w alternatywnej estymacji; interpretacja analogiczna jak chisq.\n\n\nSRMR\nStandardized Root Mean Square Residual; niski poziom (&lt; 0.08) wskazuje dobre dopasowanie.\n\n\neCRMS\nEstymowany bÅ‚Ä…d resztowy analogiczny do RMSEA; mniejsze wartoÅ›ci = lepsze dopasowanie.\n\n\neBIC\nEstymowana wersja kryterium BIC; niÅ¼sze wartoÅ›ci = lepszy model.\n\n\n\nKryterium MAP Velicera wskazuje, Å¼e minimalna wartoÅ›Ä‡ statystyki zostaÅ‚a osiÄ…gniÄ™ta przy czterech czynnikach (MAP = 0.017). Oznacza to, Å¼e w ujÄ™ciu tego kryterium, czynniki te najlepiej redukujÄ… korelacje czÄ…stkowe miÄ™dzy zmiennymi â€“ czyli eliminujÄ… najwiÄ™kszÄ… czÄ™Å›Ä‡ wariancji niepowiÄ…zanej ze wspÃ³lnÄ… strukturÄ… czynnikowÄ…. Innymi sÅ‚owy, przy czterech czynnikach model najefektywniej odwzorowuje wspÃ³lne zaleÅ¼noÅ›ci bez pozostawiania nadmiernych reszt.\nWarto jednak zauwaÅ¼yÄ‡, Å¼e rÃ³Å¼ne kryteria sugerujÄ… odmienne liczby czynnikÃ³w. Kryterium BIC wskazuje na trzy czynniki jako najbardziej oczekiwane rozwiÄ…zanie, natomiast skorygowany BIC (SABIC) preferuje piÄ™Ä‡ czynnikÃ³w. Z kolei wskaÅºniki VSS (Very Simple Structure) sugerujÄ… jednoâ€“ lub dwuczynnikowe rozwiÄ…zania, maksymalizujÄ…ce prostotÄ™ struktury. Ostateczna decyzja wymaga zatem kompromisu: MAP sugeruje cztery czynniki jako najpeÅ‚niej oddajÄ…ce wspÃ³lnÄ… strukturÄ™ zmiennych, BIC preferuje trzy jako prostsze, a SABIC wskazuje na piÄ™Ä‡. Interpretacja powinna uwzglÄ™dniaÄ‡ nie tylko statystyki, lecz takÅ¼e sensownoÅ›Ä‡ teoretycznÄ… i interpretowalnoÅ›Ä‡ uzyskanych czynnikÃ³w w kontekÅ›cie badanego materiaÅ‚u.\n\nKod# Analiza czynnikowa\nfa_model &lt;- fa(Harman74.cor$cov, nfactors = 4, n.obs = 145, \n               fm = \"ml\", rotate = \"varimax\")\n\nfa_model\n\nFactor Analysis using method =  ml\nCall: fa(r = Harman74.cor$cov, nfactors = 4, n.obs = 145, rotate = \"varimax\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                        ML1   ML3   ML2  ML4   h2   u2 com\nVisualPerception       0.16  0.69  0.19 0.16 0.56 0.44 1.4\nCubes                  0.12  0.44  0.08 0.10 0.22 0.78 1.3\nPaperFormBoard         0.14  0.57 -0.02 0.11 0.36 0.64 1.2\nFlags                  0.23  0.53  0.10 0.08 0.35 0.65 1.5\nGeneralInformation     0.74  0.19  0.21 0.15 0.65 0.35 1.4\nPargraphComprehension  0.77  0.20  0.07 0.23 0.69 0.31 1.4\nSentenceCompletion     0.81  0.20  0.15 0.07 0.72 0.28 1.2\nWordClassification     0.57  0.34  0.24 0.13 0.51 0.49 2.2\nWordMeaning            0.81  0.20  0.04 0.23 0.74 0.26 1.3\nAddition               0.17 -0.12  0.83 0.17 0.76 0.24 1.2\nCode                   0.18  0.12  0.51 0.37 0.45 0.55 2.2\nCountingDots           0.02  0.21  0.72 0.09 0.56 0.44 1.2\nStraightCurvedCapitals 0.19  0.44  0.53 0.08 0.51 0.49 2.3\nWordRecognition        0.20  0.05  0.08 0.55 0.35 0.65 1.3\nNumberRecognition      0.12  0.12  0.07 0.52 0.30 0.70 1.3\nFigureRecognition      0.07  0.41  0.06 0.53 0.45 0.55 2.0\nObjectNumber           0.14  0.06  0.22 0.57 0.40 0.60 1.4\nNumberFigure           0.03  0.29  0.34 0.46 0.41 0.59 2.6\nFigureWord             0.15  0.24  0.16 0.37 0.24 0.76 2.6\nDeduction              0.38  0.40  0.12 0.30 0.41 0.59 3.0\nNumericalPuzzles       0.17  0.38  0.44 0.22 0.42 0.58 2.8\nProblemReasoning       0.37  0.40  0.12 0.30 0.40 0.60 3.1\nSeriesCompletion       0.37  0.50  0.24 0.24 0.50 0.50 2.9\nArithmeticProblems     0.37  0.16  0.50 0.30 0.50 0.50 2.8\n\n                       ML1  ML3  ML2  ML4\nSS loadings           3.65 2.87 2.66 2.29\nProportion Var        0.15 0.12 0.11 0.10\nCumulative Var        0.15 0.27 0.38 0.48\nProportion Explained  0.32 0.25 0.23 0.20\nCumulative Proportion 0.32 0.57 0.80 1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  276  with the objective function =  11.44 with Chi Square =  1545.86\ndf of  the model are 186  and the objective function was  1.71 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  145 with the empirical chi square  135.74  with prob &lt;  1 \nThe total n.obs was  145  with Likelihood Chi Square =  226.68  with prob &lt;  0.022 \n\nTucker Lewis Index of factoring reliability =  0.951\nRMSEA index =  0.038  and the 90 % confidence intervals are  0.016 0.056\nBIC =  -698.99\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2  ML4\nCorrelation of (regression) scores with factors   0.93 0.87 0.91 0.82\nMultiple R square of scores with factors          0.87 0.76 0.83 0.68\nMinimum correlation of possible factor scores     0.73 0.52 0.66 0.36\n\n\nModel czteroczynnikowy oszacowany metodÄ… najwiÄ™kszej wiarygodnoÅ›ci na macierzy korelacji Harman74.cor$cov dobrze odwzorowuje strukturÄ™ danych i dostarcza interpretowalnych wynikÃ³w.\nPierwszy czynnik (ML1) skupia siÄ™ na kompetencjach werbalnych i wiedzy ogÃ³lnej. NajwyÅ¼sze Å‚adunki uzyskano dla zmiennych takich jak WordMeaning (0.81), SentenceCompletion (0.81), ParagraphComprehension (0.77) czy GeneralInformation (0.74). Wskazuje to, Å¼e ML1 reprezentuje wymiar wiedzy jÄ™zykowej i rozumienia tekstu. Zasoby zmiennoÅ›ci wspÃ³lej dla tych zmiennych sÄ… wysokie (powyÅ¼ej 0.65), co oznacza, Å¼e znaczna czÄ™Å›Ä‡ ich wariancji zostaÅ‚a uchwycona przez model.\nDrugi czynnik (ML2) odzwierciedla zdolnoÅ›ci arytmetyczne i numeryczne. Najsilniejsze Å‚adunki dotyczÄ… zmiennych Addition (0.83), CountingDots (0.72) i ArithmeticProblems (0.50). Oznacza to, Å¼e ML2 reprezentuje wymiar obliczeniowy, obejmujÄ…cy zarÃ³wno proste dziaÅ‚ania matematyczne, jak i bardziej zÅ‚oÅ¼one zadania wymagajÄ…ce operowania na liczbach. Wysokie wartoÅ›ci \\(h_j^2\\) (np. 0.76 dla Addition) sugerujÄ… dobrÄ… reprezentacjÄ™ tych zmiennych.\nTrzeci czynnik (ML3) moÅ¼na interpretowaÄ‡ jako zdolnoÅ›ci wzrokowo-przestrzenne i percepcyjne. Najsilniejsze Å‚adunki wystÄ…piÅ‚y dla VisualPerception (0.69), PaperFormBoard (0.57), Flags (0.53) oraz SeriesCompletion (0.50). Grupa ta obejmuje zadania zwiÄ…zane z manipulacjÄ… figurami, rozpoznawaniem wzorÃ³w i orientacjÄ… przestrzennÄ….\nCzwarty czynnik (ML4) wydaje siÄ™ zwiÄ…zany z rozpoznawaniem wzrokowym i pamiÄ™ciÄ… wzrokowÄ…. NajwiÄ™ksze Å‚adunki dotyczÄ… zmiennych takich jak WordRecognition (0.55), NumberRecognition (0.52), FigureRecognition (0.53) czy ObjectNumber (0.57). Sugeruje to wymiar rozpoznawania i szybkiego identyfikowania bodÅºcÃ³w wzrokowych.\nÅÄ…cznie cztery czynniki wyjaÅ›niajÄ… 48% wariancji caÅ‚kowitej, co w psychometrii jest uznawane za wartoÅ›Ä‡ akceptowalnÄ… przy tego typu danych. Dopasowanie globalne modelu rÃ³wnieÅ¼ jest dobre: RMSEA = 0.038 (z przedziaÅ‚em ufnoÅ›ci 0.016â€“0.056) wskazuje na bardzo dobre dopasowanie, a Tucker-Lewis Index wynosi 0.951, co rÃ³wnieÅ¼ Å›wiadczy o wysokiej jakoÅ›ci modelu. Niskie wartoÅ›ci RMSR (0.04) oraz wysoka zgodnoÅ›Ä‡ dopasowania poza przekÄ…tnÄ… (0.98) potwierdzajÄ…, Å¼e model trafnie odwzorowuje strukturÄ™ korelacji miÄ™dzy zmiennymi.\nOstatecznie wyniki wskazujÄ…, Å¼e struktura czteroczynnikowa jest dobrze uzasadniona empirycznie i teoretycznie. KaÅ¼dy czynnik odpowiada odmiennym zdolnoÅ›ciom poznawczym â€“ werbalnym, numerycznym, przestrzennym i percepcyjno-pamiÄ™ciowym â€“ a ich interpretacje sÄ… zgodne z psychologicznymi ujÄ™ciami inteligencji wielowymiarowej.\nPoniÅ¼ej zostaÅ‚a przedstawiona macierz wartoÅ›ci bezwzglÄ™dnych reszt. Jak widaÄ‡ warunek \\(|R-\\hat{R}\\)|&lt;0.1$ jest speÅ‚niony dla prawie wszystkich reszt.\n\nKod# macierz reszt (R - R_hat)\nres_mat &lt;- fa_model$residual\n\n# interesuje nas |R - R_hat|\nres_abs &lt;- abs(res_mat)\n\n# diagonalna nie jest informacyjna\ndiag(res_abs) &lt;- NA\n\n# ramka danych w formacie dÅ‚ugim\nres_df &lt;- as.data.frame(res_abs)\nres_df$var1 &lt;- rownames(res_df)\n\nres_long &lt;- res_df |&gt;\n  pivot_longer(\n    cols      = -var1,\n    names_to  = \"var2\",\n    values_to = \"resid\"\n  )\n\n# heatmap\nggplot(res_long, aes(x = var1, y = var2, fill = resid)) +\n  geom_tile() +\n  geom_text(aes(label = ifelse(is.na(resid), \"\", round(resid, 2))), size = 3)+\n  scale_fill_gradient(low = \"white\", high = \"red\", na.value = \"grey90\") +\n  coord_equal() +\n  scale_x_discrete(position = \"bottom\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 0, vjust = -0.1),\n    axis.title  = element_blank()\n  ) +\n  labs(fill = latex2exp::TeX(\"$|R - \\\\hat{R}|$\"))\n\n\n\n\n\n\n\nDla wiÄ™kszej czytelnoÅ›ci przedstawiamy Å‚adunki czynnikowe po rotacji varimax w formie tabelarycznej, z wyciÄ™tymi Å‚adunkami o niskich wartoÅ›ciach.\n\nKodmodel_parameters(fa_model, sort = TRUE, threshold = \"max\") %&gt;% \n  print_html()\n\n\n\n\n\n\nRotated loadings from Factor Analysis (varimax-rotation)\n\n\nVariable\nML1\nML3\nML2\nML4\nComplexity\nUniqueness\n\n\n\n\nWordMeaning\n0.81\n\n\n\n1.30\n0.26\n\n\nSentenceCompletion\n0.81\n\n\n\n1.21\n0.28\n\n\nPargraphComprehension\n0.77\n\n\n\n1.35\n0.31\n\n\nGeneralInformation\n0.74\n\n\n\n1.39\n0.35\n\n\nWordClassification\n0.57\n\n\n\n2.17\n0.49\n\n\nVisualPerception\n\n0.69\n\n\n1.38\n0.44\n\n\nPaperFormBoard\n\n0.57\n\n\n1.20\n0.64\n\n\nFlags\n\n0.53\n\n\n1.51\n0.65\n\n\nSeriesCompletion\n\n0.50\n\n\n2.87\n0.50\n\n\nCubes\n\n0.44\n\n\n1.33\n0.78\n\n\nDeduction\n\n0.40\n\n\n3.05\n0.59\n\n\nProblemReasoning\n\n0.40\n\n\n3.08\n0.60\n\n\nAddition\n\n\n0.83\n\n1.21\n0.24\n\n\nCountingDots\n\n\n0.72\n\n1.21\n0.44\n\n\nStraightCurvedCapitals\n\n\n0.53\n\n2.27\n0.49\n\n\nCode\n\n\n0.51\n\n2.25\n0.55\n\n\nArithmeticProblems\n\n\n0.50\n\n2.83\n0.50\n\n\nNumericalPuzzles\n\n\n0.44\n\n2.84\n0.58\n\n\nObjectNumber\n\n\n\n0.57\n1.45\n0.60\n\n\nWordRecognition\n\n\n\n0.55\n1.32\n0.65\n\n\nFigureRecognition\n\n\n\n0.53\n1.96\n0.55\n\n\nNumberRecognition\n\n\n\n0.52\n1.26\n0.70\n\n\nNumberFigure\n\n\n\n0.46\n2.62\n0.59\n\n\nFigureWord\n\n\n\n0.37\n2.56\n0.76\n\n\n\nThe 4 latent factors (varimax rotation) accounted for 47.78% of the total variance of the original data (ML1 = 15.20%, ML3 = 11.97%, ML2 = 11.07%, ML4 = 9.54%).\n\n\n\n\n\nMoÅ¼emy teÅ¼ przedstawiÄ‡ model graficznie.\n\nKodfa.diagram(fa_model, marg = c(1,5,1,1), rsize = 2)\n\n\n\n\n\n\n\nMoÅ¼na sprÃ³bowaÄ‡ estymowaÄ‡ model z piÄ™cioma czynnikami, co odpowiadaÅ‚oby pierwotnemu rozpoznaniu obszarÃ³w.\n\nKodfa_model_5 &lt;- fa(Harman74.cor$cov, nfactors = 5, n.obs = 145, \n                 fm = \"ml\", rotate = \"varimax\")\nmodel_parameters(fa_model_5, sort = TRUE, threshold = \"max\") %&gt;% \n  print_html()\n\n\n\n\n\n\nRotated loadings from Factor Analysis (varimax-rotation)\n\n\nVariable\nML1\nML3\nML2\nML4\nML5\nComplexity\nUniqueness\n\n\n\n\nSentenceCompletion\n0.81\n\n\n\n\n1.21\n0.28\n\n\nWordMeaning\n0.80\n\n\n\n\n1.31\n0.26\n\n\nPargraphComprehension\n0.77\n\n\n\n\n1.39\n0.29\n\n\nGeneralInformation\n0.74\n\n\n\n\n1.40\n0.36\n\n\nWordClassification\n0.57\n\n\n\n\n2.17\n0.49\n\n\nVisualPerception\n\n0.66\n\n\n\n1.59\n0.45\n\n\nPaperFormBoard\n\n0.56\n\n\n\n1.30\n0.64\n\n\nSeriesCompletion\n\n0.55\n\n\n\n2.72\n0.44\n\n\nFlags\n\n0.53\n\n\n\n1.47\n0.65\n\n\nDeduction\n\n0.45\n\n\n\n3.33\n0.52\n\n\nCubes\n\n0.44\n\n\n\n1.32\n0.78\n\n\nProblemReasoning\n\n0.42\n\n\n\n3.10\n0.58\n\n\nAddition\n\n\n0.84\n\n\n1.21\n0.21\n\n\nCountingDots\n\n\n0.69\n\n\n1.34\n0.44\n\n\nArithmeticProblems\n\n\n0.50\n\n\n2.94\n0.48\n\n\nNumericalPuzzles\n\n\n0.44\n\n\n2.85\n0.56\n\n\nObjectNumber\n\n\n\n0.56\n\n1.46\n0.61\n\n\nWordRecognition\n\n\n\n0.56\n\n1.34\n0.64\n\n\nFigureRecognition\n\n\n\n0.53\n\n1.95\n0.55\n\n\nNumberRecognition\n\n\n\n0.51\n\n1.29\n0.71\n\n\nNumberFigure\n\n\n\n0.45\n\n2.65\n0.60\n\n\nCode\n\n\n\n0.45\n\n3.36\n0.39\n\n\nFigureWord\n\n\n\n0.36\n\n2.56\n0.76\n\n\nStraightCurvedCapitals\n\n\n\n\n0.56\n3.16\n0.26\n\n\n\nThe 5 latent factors (varimax rotation) accounted for 50.25% of the total variance of the original data (ML1 = 15.13%, ML3 = 12.35%, ML2 = 10.23%, ML4 = 9.77%, ML5 = 2.76%).\n\n\n\n\n\nChoÄ‡ wzrÃ³sÅ‚ nieco poziom wyjaÅ›nionej wariancji przez czynniki, to jednak rozwiÄ…zanie, w ktÃ³rym wystÄ™pujÄ… pojedyncze zmienne jako czynnik nie sÄ… porzÄ…dane. Dlatego pozostaniemy przy rozwiÄ…zaniu z czterema czynnikami.\n\n\n\n\n\n\n\nWskazÃ³wka\n\n\n\nLiczba czynnikÃ³w, moÅ¼e byÄ‡ szacowana na podstawie rÃ³Å¼nych kryteriÃ³w, z ktÃ³rych kaÅ¼de eksponuje inny aspekt, ale ostateczna decyzja o wyborze liczby czynnikÃ³w powinna byÄ‡ podyktowana gÅ‚Ã³wnie zgodnoÅ›ciÄ… otrzymany wynikÃ³w z teoriÄ… oraz interpretowalnoÅ›ciÄ…. OczywiÅ›cie nie powinno siÄ™ to dziaÄ‡ kosztem znacznego obniÅ¼enia poziomu dopasowania modelu.\n\n\nNa temat konfiramcyjnej analizy czynnikowej (CFA) zostnie poÅ›wiÄ™cony kolejny rozdziaÅ‚.\n\n\n\n\nBartlett, M. S. 1951. â€The Effect of Standardization on a Ï‡ 2 Approximation in Factor Analysisâ€. Biometrika 38 (3/4): 337. https://doi.org/10.2307/2332580.\n\n\nCattell, Raymond B. 1966. â€The Scree Test For The Number Of Factorsâ€. Multivariate Behavioral Research 1 (2): 245â€“76. https://doi.org/10.1207/s15327906mbr0102_10.\n\n\nEveritt, B. S., i A. Yates. 1989. â€Multivariate Exploratory Data Analysis: A Perspective on Exploratory Factor Analysis.â€ Biometrics 45 (1): 342. https://doi.org/10.2307/2532065.\n\n\nGrieder, Silvia, i Markus D. Steiner. 2020. â€Algorithmic Jingle Jungle: A Comparison of Implementations of Principal Axis Factoring and Promax Rotation in R and SPSSâ€. http://dx.doi.org/10.31234/osf.io/7hwrm.\n\n\nHarman, Harry H., i Wayne H. Jones. 1966. â€Factor Analysis by Minimizing Residuals (Minres)â€. Psychometrika 31 (3): 351â€“68. https://doi.org/10.1007/bf02289468.\n\n\nHendrickson, Alan E., i Paul Owen White. 1964. â€PROMAX: A QUICK METHOD FOR ROTATION TO OBLIQUE SIMPLE STRUCTUREâ€. British Journal of Statistical Psychology 17 (1): 65â€“70. https://doi.org/10.1111/j.2044-8317.1964.tb00244.x.\n\n\nHorn, John L. 1965. â€A Rationale and Test for the Number of Factors in Factor Analysisâ€. Psychometrika 30 (2): 179â€“85. https://doi.org/10.1007/bf02289447.\n\n\nâ€”â€”â€”. 1969. â€Harry H. Harman Modern Factor Analysis (Second Edition, Revised). Chicago and London: University of Chicago Press, 1967. Pp. Xx + 474. $12.50â€. Psychometrika 34 (1): 134â€“38. https://doi.org/10.1017/s0033312300004580.\n\n\nâ€Introduction to Factor Analysisâ€. 2020. W, 1â€“12. SAGE Publications, Inc. https://doi.org/10.4135/9781544339900.n4.\n\n\nJacobucci, Ross, i Kevin J. Grimm. 2018. â€Comparison of Frequentist and Bayesian Regularization in Structural Equation Modelingâ€. Structural Equation Modeling: A Multidisciplinary Journal 25 (4): 639â€“49. https://doi.org/10.1080/10705511.2017.1410822.\n\n\nJennrich, R. I., i P. F. Sampson. 1966. â€Rotation for Simple Loadingsâ€. Psychometrika 31 (3): 313â€“23. https://doi.org/10.1007/bf02289465.\n\n\nJÃ¶reskog, Karl G., i Arthur S. Goldberger. 1972. â€Factor Analysis by Generalized Least Squaresâ€. Psychometrika 37 (3): 243â€“60. https://doi.org/10.1007/bf02306782.\n\n\nKaiser, Henry F. 1958. â€The Varimax Criterion for Analytic Rotation in Factor Analysisâ€. Psychometrika 23 (3): 187â€“200. https://doi.org/10.1007/bf02289233.\n\n\nâ€”â€”â€”. 1970. â€A Second Generation Little Jiffyâ€. Psychometrika 35 (4): 401â€“15. https://doi.org/10.1007/bf02291817.\n\n\nKiers, Henk A. L. 1994. â€Simplimax: Oblique Rotation to an Optimal Target with Simple Structureâ€. Psychometrika 59 (4): 567â€“79. https://doi.org/10.1007/bf02294392.\n\n\nLawley, D. N. 1940. â€VI.The Estimation of Factor Loadings by the Method of Maximum Likelihoodâ€. Proceedings of the Royal Society of Edinburgh 60 (1): 64â€“82. https://doi.org/10.1017/s037016460002006x.\n\n\nLu, Zhao-Hua, Sy-Miin Chow, i Eric Loken. 2016. â€Bayesian Factor Analysis as a Variable-Selection Problem: Alternative Priors and Consequencesâ€. Multivariate Behavioral Research 51 (4): 519â€“39. https://doi.org/10.1080/00273171.2016.1168279.\n\n\nVelicer, Wayne F. 1976. â€Determining the Number of Components from the Matrix of Partial Correlationsâ€. Psychometrika 41 (3): 321â€“27. https://doi.org/10.1007/bf02293557.\n\n\nWang, Xiaojing, Candace M. Kammerer, Stewart Anderson, Jiang Lu, i Eleanor Feingold. 2008. â€A Comparison of Principal Component Analysis and Factor Analysis Strategies for Uncovering Pleiotropic Factorsâ€. Genetic Epidemiology 33 (4): 325â€“31. https://doi.org/10.1002/gepi.20384.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "sem.html",
    "href": "sem.html",
    "title": "Modele strukturalne",
    "section": "",
    "text": "Konfirmacyjna analiza czynnikowa\nModele strukturalne (ang. Structural Equation Models) stanowiÄ… uogÃ³lnienie klasycznych modeli regresyjnych do ukÅ‚adÃ³w wielu rÃ³wnaÅ„ z jednoczesnymi zaleÅ¼noÅ›ciami miÄ™dzy zmiennymi. W najprostszym wariancie, zwanym path analysis (PA), wszystkie zmienne sÄ… obserwowalne, a celem jest estymacja wspÃ³Å‚czynnikÃ³w Å›cieÅ¼ek, dekompozycja efektÃ³w na bezpoÅ›rednie i poÅ›rednie oraz wyjaÅ›nienie wspÃ³Å‚zmiennoÅ›ci. Konfirmacyjna analiza czynnikowa (CFA) rozszerza to ujÄ™cie o niewidoczne wprost czynniki latentne, modelujÄ…c relacjÄ™ wskaÅºnikâ€“czynnik i separujÄ…c wariancjÄ™ wspÃ³lnÄ… od swoistej. Modele strukturalne (SEM) integrujÄ… oba poziomy: pomiarowy (jak w CFA) i strukturalny (jak w path analysis), tworzÄ…c jednÄ… ramÄ™, w ktÃ³rej czynniki latentne i zmienne obserwowalne Å‚Ä…czÄ… siÄ™ w sieÄ‡ rÃ³wnaÅ„ opisujÄ…cych zaleÅ¼noÅ›ci przyczynowo-interpretacyjne.\nRys historyczny siÄ™ga prac Sewalla Wrighta z lat 1918â€“1934, ktÃ³ry wprowadziÅ‚ path analysis i reguÅ‚y Å›ledzenia Å›cieÅ¼ek, pozwalajÄ…ce dekomponowaÄ‡ kowariancje na sumy iloczynÃ³w wspÃ³Å‚czynnikÃ³w (Wright 1934). RÃ³wnolegle rozwijaÅ‚a siÄ™ analiza czynnikowa: Spearman (1961), ktÃ³ry postulowaÅ‚ czynnik ogÃ³lny, a Thurstone (1931) wprowadziÅ‚ czynniki wielowymiarowe. PrzeÅ‚omem byÅ‚ formalny opis CFA i ujÄ™cie SEM przez JÃ¶reskoga (koniec lat 60.), ktÃ³ry poÅ‚Ä…czyÅ‚ model pomiarowy i strukturalny w system LISREL (Tarka 2017). Lata 80. i 90. przyniosÅ‚y rozwÃ³j estymacji, wskaÅºnikÃ³w dopasowania i oprogramowania (m.in. EQS, AMOS), a podrÄ™cznikowa synteza Bollenâ€™a (1989) ugruntowaÅ‚a teoriÄ™ (Bollen 1989). W kolejnych dekadach pojawiaÅ‚y siÄ™ metody odporne i dla zmiennych porzÄ…dkowych oraz uogÃ³lnienia dla danych longitudalnych i wielopoziomowych, co uczyniÅ‚o z SEM uniwersalnÄ… ramÄ™ modelowania.\nKonfirmacyjna analiza czynnikowa (ang. Confirmatory Factor Analysis, CFA), stanowi ujÄ™cie modelu pomiarowego, w ktÃ³rym a priori narzuca strukturÄ™ zaleÅ¼noÅ›ci miÄ™dzy zmiennymi obserwowalnymi a czynnikami ukrytymi. W odrÃ³Å¼nieniu od eksploracyjnej analizy czynnikowej, gdzie pozwala danym â€odkrywaÄ‡â€ wzorzec Å‚adunkÃ³w, w CFA okreÅ›lamy, ktÃ³re zmienne Å‚adujÄ… siÄ™ na ktÃ³rych czynnikach, ktÃ³re Å‚adunki sÄ… rÃ³wne zeru, a ktÃ³re mogÄ… siÄ™ rÃ³Å¼niÄ‡, oraz czy dopuszczamy korelacje bÅ‚Ä™dÃ³w pomiaru. Celem jest weryfikacja hipotezy o poprawnej budowie narzÄ™dzia pomiarowego i o liczbie oraz treÅ›ci czynnikÃ³w, a nastÄ™pnie oceniamy dopasowanie modelu do macierzy kowariancji/Å›rednich w populacji.\nFormalnie przyjmujemy ten sam model pomiarowy co w EFA, lecz z naÅ‚oÅ¼onymi ograniczeniami strukturalnymi na macierz Å‚adunkÃ³w. Niech \\(\\mathbf{x}\\in\\mathbb{R}^p\\) oznacza wektor zmiennych obserwowalnych, \\(\\mathbf{f}\\in\\mathbb{R}^m\\) wektor czynnikÃ³w, \\(\\Lambda\\in\\mathbb{R}^{p\\times m}\\) macierz Å‚adunkÃ³w, \\(\\boldsymbol{\\epsilon}\\in\\mathbb{R}^p\\) wektor skÅ‚adnikÃ³w swoistych. Model przyjmuje wÃ³wczas postaÄ‡ \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\,\\mathbf{f}+\\boldsymbol{\\epsilon},\\qquad\n\\mathbb{E}(\\mathbf{f})=\\mathbf{0},\\ \\ \\mathbb{E}(\\boldsymbol{\\epsilon})=\\mathbf{0},\\ \\ \\mathrm{Cov}(\\mathbf{f})=\\Phi,\\ \\ \\mathrm{Cov}(\\boldsymbol{\\epsilon})=\\Psi,\n\\] gdzie \\(\\Phi\\) jest dodatnio okreÅ›lonÄ… macierzÄ… kowariancji czynnikÃ³w (dla rotacji skoÅ›nych) lub macierzÄ… jednostkowÄ… (dla czynnikÃ³w ortogonalnych), a \\(\\Psi\\) z reguÅ‚y jest macierzÄ… diagonalnÄ…, co odpowiada nieskorelowanym bÅ‚Ä™dom pomiaru. Macierz kowariancji implikowana przez model ma postaÄ‡ \\[\n\\Sigma(\\theta)=\\Lambda\\,\\Phi\\,\\Lambda^\\top + \\Psi,\n\\] gdzie \\(\\theta\\) reprezentuje wszystkie parametry modelu.\nKlucz identyfikacji w CFA polega na tym, Å¼e rotacyjna nieoznaczonoÅ›Ä‡ znika dziÄ™ki z gÃ³ry zdefiniowanemu wzorcowi zer w \\(\\Lambda\\) (kaÅ¼dy wskaÅºnik Å‚adujÄ…cy siÄ™ wyÅ‚Ä…cznie na â€wÅ‚asnymâ€ czynniku). Aby skalowaÄ‡ czynniki, przyjmujemy jednÄ… z rÃ³wnowaÅ¼nych konwencji: ustalamy wariancjÄ™ czynnika na 1 i estymujemy wszystkie Å‚adunki, albo ustalamy po jednym Å‚adunku na 1 w kaÅ¼dej kolumnie \\(\\Lambda\\) i estymujemy wariancje czynnikÃ³w. Praktycznie zapewniamy co najmniej trzy sensowne wskaÅºniki na czynnik; dwa wskaÅºniki bywajÄ… wystarczajÄ…ce przy dodatkowych ograniczeniach rÃ³wnoÅ›ci lub znanych bÅ‚Ä™dach pomiaru.\nEstymujemy parametry zwykle metodÄ… najwiÄ™kszej wiarygodnoÅ›ci, co przy normalnoÅ›ci wielowymiarowej oznacza minimalizowanie rozbieÅ¼noÅ›Ä‡ miÄ™dzy \\(S\\) a \\(\\Sigma(\\theta)\\) i umoÅ¼liwia wprowadzenie testu globalnego dopasowania \\(\\chi^2\\). Przy naruszeniach normalnoÅ›ci stosujemy wersje odporne lub waÅ¼one metody najmniejszych kwadratÃ³w dla danych porzÄ…dkowych (WLSMV) (Li 2015).\nInterpretacjÄ™ CFA opieramy na Å‚adunkach (\\(\\Lambda\\)) jako czuÅ‚oÅ›ciach wskaÅºnikÃ³w na czynniki, wariancjach i korelacjach czynnikÃ³w (\\(\\Phi\\)) jako sile i wspÃ³Å‚wystÄ™powaniu wymiarÃ³w latentnych oraz na resztach jako sygnaÅ‚ach lokalnego niedopasowania. SiÅ‚a CFA polega na tym, Å¼e pozwala wprost testowaÄ‡ hipotezy o narzÄ™dziu pomiarowym, porÃ³wnywaÄ‡ modele teoretycznie motywowane i zapewniaÄ‡ podstawÄ™ do dalszych modeli strukturalnych, SEM, w ktÃ³rych czynniki stajÄ… siÄ™ zmiennymi wyjaÅ›niajÄ…cymi i wyjaÅ›nianymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#konfirmacyjna-analiza-czynnikowa",
    "href": "sem.html#konfirmacyjna-analiza-czynnikowa",
    "title": "Modele strukturalne",
    "section": "",
    "text": "PrzykÅ‚ad 4.1 Przeprowadzimy CFA na danych pochodzÄ…cych z badania PISA 2009, dotyczÄ…cych strategii uczenia siÄ™ uczniÃ³w. Wybierzemy 13 pozycji z kwestionariusza ucznia, ktÃ³re majÄ… odzwierciedlaÄ‡ trzy strategie: zapamiÄ™tywania (M), opracowywania (E) i kontroli (C). Sprawdzimy, czy dane z Wielkiej Brytanii potwierdzajÄ… tÄ™ strukturÄ™ trÃ³jczynnikowÄ….\n\nKod#devtools::install_github(\"talbano/epmr\")\nlibrary(epmr)\nlibrary(gt)\nlibrary(lavaan)\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(sjPlot)\n\n# wybÃ³r itemÃ³w z testu (Å‚Ä…cznie 13), podzielonych wg zaÅ‚oÅ¼onej struktury, \n# ktÃ³rÄ… bÄ™dziemy weryfikowaÄ‡\n# strategie zapamiÄ™tywania, opracowywania i kontroli\nmitems &lt;- c(\"st27q01\", \"st27q03\", \"st27q05\", \"st27q07\")\neitems &lt;- c(\"st27q04\", \"st27q08\", \"st27q10\", \"st27q12\")\ncitems &lt;- c(\"st27q02\", \"st27q06\", \"st27q09\", \"st27q11\", \n  \"st27q13\")\nalitems &lt;- c(mitems, eitems, citems)\n\n# ZawÄ™zimy badania tylko go Wielkiej Brytanii\npisagbr &lt;- PISA09[PISA09$cnt == \"GBR\", alitems]\npisagbr &lt;- pisagbr[complete.cases(pisagbr[, c(mitems, \n  eitems, citems)]), ]\nplot_likert(pisagbr, groups = c(rep(\"ZapamiÄ™tywanie\", \n  length(mitems)), rep(\"Opracowywanie\", length(eitems)), \n  rep(\"Kontrola\", length(citems))))\n\n\n\n\n\n\n\nNa potrzeby budowy modelu konfirmacyjnego uÅ¼yjemy pakietu lavaan. Definiujemy model z trzema czynnikami, gdzie kaÅ¼dy czynnik jest Å‚adowany przez odpowiednie pozycje kwestionariusza. NastÄ™pnie estymujemy model metodÄ… najwiÄ™kszej wiarygodnoÅ›ci i sprawdzamy dopasowanie modelu do danych.\n\nKod# Definicja modelu CFA\nmodel_cfa &lt;- '\n  # Definicja czynnikÃ³w\n  ZapamiÄ™tywanie =~ st27q01 + st27q03 + st27q05 + st27q07\n  Opracowywanie =~ st27q04 + st27q08 + st27q10 + st27q12\n  Kontrola =~ st27q02 + st27q06 + st27q09 + st27q11 + st27q13\n'\n\n# Estymacja modelu CFA\nfit_cfa &lt;- cfa(model_cfa, data = pisagbr, auto.var = TRUE, auto.cov.lv.x = TRUE, std.lv = TRUE)\n\n\n\n\nauto.var = TRUE - estymuje wariancje czynnikÃ³w i bÅ‚Ä™dÃ³w pomiarowych, bez potrzeby rÄ™cznego ich dodawania.\n\nauto.cov.lv.x = TRUE - estymuje kowariancje pomiÄ™dzy wszystkimi czynnikami latentnymi.\n\nstd.lv = TRUE - ustawia wariancjÄ™ czynnikÃ³w latentnych na 1, co daje bezpoÅ›rednio interpretowalne Å‚adunki czynnikowe jako korelacje.\n\n\nKod# Podsumowanie dopasowania\nmodel_performance(fit_cfa, metrics = c(\"p_Chi2\", \"GFI\", \"AGFI\", \"NFI\", \"NNFI\", \"CFI\", \"RMSEA\", \"RMR\", \"SRMR\", \"RFI\")) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\np_Chi2\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMR\nSRMR\nRFI\n\n\n0.000\n0.936\n0.907\n0.881\n0.856\n0.885\n0.081\n0.042\n0.057\n0.850\n\n\n\n\n\nNajpierw ocenimy dopasowanie modelu:\n\nPo pierwsze, test chi-kwadrat (p_Chi2 = 0.000) jest istotny, co formalnie sugeruje, Å¼e model nie odtwarza idealnie macierzy kowariancji w populacji. JednakÅ¼e, przy wiÄ™kszych prÃ³bach test ten jest nadwraÅ¼liwy i czÄ™sto prowadzi do odrzucenia nawet dobrze dopasowanych modeli, dlatego nie naleÅ¼y go traktowaÄ‡ jako jedynego kryterium oceny.\nJeÅ›li chodzi o wskaÅºniki dopasowania absolutnego, wartoÅ›ci GFI = 0.936 oraz AGFI = 0.907 wskazujÄ… na przyzwoite dopasowanie â€“ oba mieszczÄ… siÄ™ powyÅ¼ej progu 0.90, choÄ‡ nie osiÄ…gajÄ… poziomu bardzo dobrego (â‰¥ 0.95). Podobnie RMR = 0.042 i SRMR = 0.057 sugerujÄ…, Å¼e przeciÄ™tne reszty miÄ™dzy obserwowanÄ… a implikowanÄ… macierzÄ… sÄ… umiarkowanie niskie â€“ SRMR &lt; 0.08 jest zwykle uznawane za akceptowalne.\nW przypadku wskaÅºnikÃ³w dopasowania przyrostowego (NFI = 0.881, NNFI = 0.856, CFI = 0.885, RFI = 0.850), wszystkie wartoÅ›ci sÄ… poniÅ¼ej konwencjonalnego progu 0.90, co wskazuje na pewne niedopasowanie.\nSzczegÃ³lnie waÅ¼ny jest RMSEA = 0.081, ktÃ³ry mieÅ›ci siÄ™ w strefie dopuszczalnej, ale nie idealnej (0.05â€“0.08 uznaje siÄ™ za akceptowalne dopasowanie, a &gt; 0.10 za sÅ‚abe). WartoÅ›Ä‡ 0.081 wskazuje na model na granicy akceptowalnoÅ›ci â€“ moÅ¼na go uznaÄ‡ za umiarkowanie dopasowany, ale istniejÄ… przesÅ‚anki do jego ulepszania (np. rozwaÅ¼enie korelacji bÅ‚Ä™dÃ³w pomiarowych, dodanie lub modyfikacja pozycji).\n\nTeraz przejdÅºmy do interpretacji parametrÃ³w modelu:\n\nKodmodel_parameters(fit_cfa, component = \"loading\", standardize = T) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nTo\nOperator\nFrom\nCoefficient\nSE\nCI_low\nCI_high\nz\np\nComponent\n\n\n\nZapamiÄ™tywanie\n=~\nst27q01\n0.585\n0.014\n0.557\n0.613\n40.467\n0.000\nLoading\n\n\nZapamiÄ™tywanie\n=~\nst27q03\n0.644\n0.014\n0.618\n0.671\n47.357\n0.000\nLoading\n\n\nZapamiÄ™tywanie\n=~\nst27q05\n0.597\n0.014\n0.569\n0.625\n41.809\n0.000\nLoading\n\n\nZapamiÄ™tywanie\n=~\nst27q07\n0.601\n0.014\n0.574\n0.629\n42.299\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q04\n0.534\n0.015\n0.505\n0.562\n36.319\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q08\n0.644\n0.013\n0.618\n0.669\n49.774\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q10\n0.706\n0.012\n0.682\n0.729\n58.794\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q12\n0.725\n0.012\n0.702\n0.748\n61.801\n0.000\nLoading\n\n\nKontrola\n=~\nst27q02\n0.550\n0.014\n0.523\n0.578\n39.524\n0.000\nLoading\n\n\nKontrola\n=~\nst27q06\n0.662\n0.012\n0.639\n0.685\n55.488\n0.000\nLoading\n\n\nKontrola\n=~\nst27q09\n0.657\n0.012\n0.633\n0.680\n54.562\n0.000\nLoading\n\n\nKontrola\n=~\nst27q11\n0.672\n0.012\n0.649\n0.695\n57.236\n0.000\nLoading\n\n\nKontrola\n=~\nst27q13\n0.588\n0.013\n0.562\n0.614\n44.259\n0.000\nLoading\n\n\n\n\n\nKodmodel_parameters(fit_cfa, component = \"correlation\") %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nTo\nOperator\nFrom\nCoefficient\nSE\nCI_low\nCI_high\nz\np\nComponent\n\n\n\nZapamiÄ™tywanie\n~~\nOpracowywanie\n0.368\n0.021\n0.327\n0.409\n17.565\n0.000\nCorrelation\n\n\nZapamiÄ™tywanie\n~~\nKontrola\n0.714\n0.015\n0.684\n0.744\n46.863\n0.000\nCorrelation\n\n\nOpracowywanie\n~~\nKontrola\n0.576\n0.017\n0.543\n0.609\n34.248\n0.000\nCorrelation\n\n\n\n\n\n\n\nW przypadku strategii zapamiÄ™tywania, wszystkie pozycje (st27q01, st27q03, st27q05, st27q07) Å‚adujÄ… siÄ™ umiarkowanie silnie na czynniku, z wartoÅ›ciami wspÃ³Å‚czynnikÃ³w standaryzowanych w przedziale 0.59â€“0.64. Oznacza to, Å¼e zmienne te sÄ… spÃ³jnymi wskaÅºnikami tego konstruktu i wnoszÄ… podobny wkÅ‚ad w jego pomiar. Wysokie istotnoÅ›ci statystyczne (p &lt; .001) potwierdzajÄ…, Å¼e kaÅ¼da z tych zmiennych odgrywa istotnÄ… rolÄ™ w budowie czynnika.\nDla strategii opracowywania obserwujemy wyÅ¼sze wartoÅ›ci Å‚adunkÃ³w czynnikowych â€“ od 0.53 dla st27q04 do 0.73 dla st27q12. Oznacza to, Å¼e ta grupa pytaÅ„ jest silnie zwiÄ…zana z konstruktem emocjonalnych strategii uczenia siÄ™, a zwÅ‚aszcza pozycje st27q10 i st27q12 okazujÄ… siÄ™ najbardziej reprezentatywne. InterpretowaÄ‡ to moÅ¼na jako silnÄ… spÃ³jnoÅ›Ä‡ wskaÅºnikÃ³w i duÅ¼Ä… trafnoÅ›Ä‡ pomiarowÄ… tego czynnika.\nJeÅ›li chodzi o strategie kontrolne, wszystkie pozycje majÄ… doÅ›Ä‡ wysokie Å‚adunki, od 0.55 do 0.67, co sugeruje dobrÄ… konsystencjÄ™ wewnÄ™trznÄ… tego konstruktu. SzczegÃ³lnie istotne sÄ… pozycje st27q06, st27q09 i st27q11, ktÃ³re majÄ… najwyÅ¼sze wartoÅ›ci wspÃ³Å‚czynnikÃ³w, a wiÄ™c najlepiej odzwierciedlajÄ… mechanizmy zwiÄ…zane z kontrolÄ… uczenia siÄ™.\nWyniki korelacji miÄ™dzy trzema strategiami uczenia siÄ™ wskazujÄ…, Å¼e sÄ… one ze sobÄ… istotnie powiÄ…zane, choÄ‡ w rÃ³Å¼nym stopniu. Najsilniejszy zwiÄ…zek wystÄ™puje miÄ™dzy ZapamiÄ™tywaniem a KontrolÄ… (r = 0.714, p &lt; 0.001), co sugeruje, Å¼e monitorowanie procesu uczenia siÄ™ jest Å›ciÅ›le powiÄ…zane ze stosowaniem technik zapamiÄ™tywania. Nieco sÅ‚absze, ale nadal istotne powiÄ…zania obserwuje siÄ™ miÄ™dzy Opracowywaniem a KontrolÄ… (r = 0.576, p &lt; 0.001) oraz miÄ™dzy ZapamiÄ™tywaniem a Opracowywaniem (r = 0.368, p &lt; 0.001), co wskazuje, Å¼e przetwarzanie materiaÅ‚u oraz kontrola uczenia siÄ™ sÄ… umiarkowanie powiÄ…zane z technikami pamiÄ™ciowymi. ÅÄ…cznie wyniki te potwierdzajÄ…, Å¼e strategie tworzÄ… spÃ³jny, ale zrÃ³Å¼nicowany zbiÃ³r powiÄ…zanych ze sobÄ… podejÅ›Ä‡ do uczenia siÄ™.\n\nModel moÅ¼emy rÃ³wnieÅ¼ przedstawiÄ‡ graficznie.\n\nKodlibrary(semPlot)\nlibrary(RColorBrewer)\n\n# wybieramy pastelowÄ… paletÄ™\npastel_cols &lt;- brewer.pal(3, \"Pastel2\")\n\nsemPaths(fit_cfa,\n         whatLabels = \"std\",\n         what = 'std',\n         layout = \"tree2\",\n         groups = \"latents\",\n         sizeMan = 6,\n         sizeLat = 8,\n         nCharNodes = 0,\n         style = \"lisrel\",\n         # kolory pastelowe dla zmiennych latentnych i obserwowanych\n         color = pastel_cols,\n         colorLat = pastel_cols[1],\n         colorMan = pastel_cols[2],\n         edge.color = \"grey70\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#path-analysis",
    "href": "sem.html#path-analysis",
    "title": "Modele strukturalne",
    "section": "Path analysis",
    "text": "Path analysis\nAnaliza Å›cieÅ¼kowa (ang. path analysis) jest jednÄ… z najwczeÅ›niejszych form modelowania strukturalnego i stanowi naturalne rozwiniÄ™cie regresji wielokrotnej. Jej gÅ‚Ã³wnym celem jest badanie zÅ‚oÅ¼onych ukÅ‚adÃ³w zaleÅ¼noÅ›ci przyczynowo-skutkowych miÄ™dzy zmiennymi obserwowalnymi, w tym ukÅ‚adÃ³w obejmujÄ…cych zmienne poÅ›redniczÄ…ce (mediatory). ZostaÅ‚a zaproponowana przez Sewalla Wrighta w latach 20. XX wieku jako narzÄ™dzie do formalizacji rÃ³wnaÅ„ przyczynowych w biologii, a nastÄ™pnie rozwinÄ™Å‚a siÄ™ jako fundament wspÃ³Å‚czesnych modeli SEM.\nFormalnie model analizy Å›cieÅ¼kowej moÅ¼na zapisaÄ‡ jako system rÃ³wnaÅ„ liniowych \\[\n\\mathbf{y} = B\\mathbf{y} + \\Gamma \\mathbf{x} + \\zeta,\n\\] gdzie:\n\n\n\\(\\mathbf{y}\\) to wektor zmiennych endogenicznych 1,\n\n\\(\\mathbf{x}\\) to wektor zmiennych egzogenicznych 2,\n\n\\(B\\) to macierz wspÃ³Å‚czynnikÃ³w regresji pomiÄ™dzy zmiennymi endogenicznymi,\n\n\\(\\Gamma\\) to macierz wspÃ³Å‚czynnikÃ³w regresji Å‚Ä…czÄ…cych zmienne egzogeniczne z endogenicznymi,\n\n\\(\\zeta\\) to wektor zakÅ‚Ã³ceÅ„ (bÅ‚Ä™dÃ³w strukturalnych).\n\n1Â Zmienna endogeniczna jest wielkoÅ›ciÄ…, ktÃ³rej wartoÅ›ci wynikajÄ… z procesÃ³w opisanych przez model. Jest to zmienna wyjaÅ›niana przez model, czyli taka, ktÃ³rej zmiennoÅ›Ä‡ tÅ‚umaczy siÄ™ innymi czynnikami. W modelu przyczynowym jest to zmienna pozostajÄ…ca wewnÄ…trz ukÅ‚adu zaleÅ¼noÅ›ci.2Â Zmienna egzogeniczna jest wielkoÅ›ciÄ…, ktÃ³rej wartoÅ›ci sÄ… zadane z zewnÄ…trz modelu i nie wynikajÄ… z jego struktury. Jest to zmienna wyjaÅ›niajÄ…ca, traktowana jako niezaleÅ¼na od procesu losowego opisujÄ…cego zmiennÄ… endogenicznÄ….ZaÅ‚oÅ¼enia analizy Å›cieÅ¼kowej sÄ… w duÅ¼ej mierze zbieÅ¼ne z klasycznymi zaÅ‚oÅ¼eniami regresji liniowej. ObejmujÄ… one liniowoÅ›Ä‡ zaleÅ¼noÅ›ci, brak silnej wspÃ³Å‚liniowoÅ›ci miÄ™dzy predyktorami, nieskorelowanie bÅ‚Ä™dÃ³w \\(\\zeta\\) z egzogenicznymi zmiennymi \\(\\mathbf{x}\\) oraz odpowiednio duÅ¼Ä… prÃ³bÄ™, aby zapewniÄ‡ stabilnoÅ›Ä‡ estymacji. Dodatkowo zakÅ‚ada siÄ™ poprawnoÅ›Ä‡ teoretycznÄ… modelu â€“ to badacz definiuje strukturÄ™ Å›cieÅ¼ek na podstawie teorii lub wczeÅ›niejszych wynikÃ³w, a analiza ma na celu jej statystycznÄ… weryfikacjÄ™.\nEstymacja parametrÃ³w w analizie Å›cieÅ¼kowej opiera siÄ™ najczÄ™Å›ciej na metodzie najwiÄ™kszej wiarygodnoÅ›ci (maximum likelihood, ML), ktÃ³ra minimalizuje rÃ³Å¼nicÄ™ miÄ™dzy macierzÄ… kowariancji obserwowanej a macierzÄ… kowariancji implikowanÄ… przez model. Alternatywnie stosuje siÄ™ metody oparte na najmniejszych kwadratach (generalized least squares, GLS, ordinary least squares, OLS) (Schweizer i DiStefano 2016), a w przypadku naruszenia normalnoÅ›ci rozkÅ‚adu dostÄ™pne sÄ… warianty odporne, takie jak robust ML (Schweizer i DiStefano 2016) czy estymacja asymptotycznie niezaleÅ¼na (asymptotically distribution free, ADF) (Huang i Bentler 2015). W nowszych zastosowaniach wykorzystuje siÄ™ rÃ³wnieÅ¼ metody bayesowskie, ktÃ³re pozwalajÄ… wprowadziÄ‡ rozkÅ‚ady a priori dla parametrÃ³w i prowadziÄ‡ wnioskowanie probabilistyczne o strukturze zaleÅ¼noÅ›ci.\n\nPrzykÅ‚ad 4.2 Wykonamy prostÄ… analizÄ™ Å›cieÅ¼kowÄ… na danych mtcars, aby zbadaÄ‡ wpÅ‚yw masy pojazdu (wt) na jego zuÅ¼ycie paliwa (mpg), za poÅ›rednictwem mocy silnika (hp). Hipoteza zakÅ‚ada, Å¼e masa wpÅ‚ywa na moc, ktÃ³ra z kolei wpÅ‚ywa na zuÅ¼ycie paliwa.\n\nKod# Model Å›cieÅ¼kowy (wyÅ‚Ä…cznie zmienne obserwowalne)\n# hp jest mediatorem miÄ™dzy wt a mpg\nmodel_pa &lt;- '\n  # rÃ³wnania regresji (czÄ™Å›Ä‡ strukturalna)\n  mpg ~ c*wt + b*hp\n  hp  ~ a*wt\n\n  # efekty poÅ›rednie i caÅ‚kowite\n  ind := a*b\n  tot := c + (a*b)\n'\n\nfit_pa &lt;- sem(model_pa, data = mtcars,\n              estimator = \"MLR\")                         # estymator odporny (robust ML)\n\n# Podsumowanie wynikÃ³w (standaryzacja, istotnoÅ›ci, efekty zdefiniowane)\nsummary(fit_pa, standardized = TRUE, ci = TRUE, rsquare = TRUE)\n\nlavaan 0.6-20 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                            32\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  mpg ~                                                                 \n    wt         (c)   -3.878    0.620   -6.255    0.000   -5.093   -2.663\n    hp         (b)   -0.032    0.007   -4.781    0.000   -0.045   -0.019\n  hp ~                                                                  \n    wt         (a)   46.160    5.734    8.051    0.000   34.922   57.398\n   Std.lv  Std.all\n                  \n   -3.878   -0.630\n   -0.032   -0.361\n                  \n   46.160    0.659\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .mpg               6.095    1.645    3.705    0.000    2.870    9.320\n   .hp             2577.777  996.624    2.587    0.010  624.430 4531.125\n   Std.lv  Std.all\n    6.095    0.173\n 2577.777    0.566\n\nR-Square:\n                   Estimate\n    mpg               0.827\n    hp                0.434\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind              -1.467    0.351   -4.179    0.000   -2.155   -0.779\n    tot              -5.344    0.634   -8.434    0.000   -6.587   -4.102\n   Std.lv  Std.all\n   -1.467   -0.238\n   -5.344   -0.868\n\nKod# Wizualizacja diagramu Å›cieÅ¼ek\nsemPaths(fit_pa,\n         what = \"std\", \n         whatLabels = \"std\",\n         layout = \"circle\",\n         style = \"lisrel\",\n         residuals = FALSE, intercepts = FALSE,\n         nCharNodes = 0, sizeMan = 7,\n         groups = \"manifests\",\n         color = brewer.pal(3, \"Pastel2\"),\n         edge.color = \"grey60\")\n\n\n\n\n\n\n\nModel zapisujemy jako: \\[\n\\mathbf{y} = B\\mathbf{y} + \\Gamma \\mathbf{x} + \\zeta,\n\\] gdzie:\n\n\n\\(\\mathbf{y} = \\begin{bmatrix} mpg \\\\ hp \\end{bmatrix}\\) to zmienne endogeniczne,\n\n\\(\\mathbf{x} = wt\\) to zmienna egzogeniczna,\n\n\\(B\\) to macierz regresji pomiÄ™dzy zmiennymi endogenicznymi,\n\n\\(\\Gamma\\) to macierz efektÃ³w zmiennych egzogenicznych na endogeniczne,\n\n\\(\\zeta\\) to wektor bÅ‚Ä™dÃ³w strukturalnych.\n\nEstymowane rÃ³wnania \\[\n\\begin{aligned}\nmpg &= c \\cdot wt + b \\cdot hp + \\zeta_{mpg}, \\\\\nhp  &= a \\cdot wt + \\zeta_{hp},\n\\end{aligned}\n\\] gdzie:\n\n\\(a = 46.160\\) (standaryzowane \\(0.659\\)) â€“ wpÅ‚yw masy (wt) na moc (hp),\n\\(b = -0.032\\) (standaryzowane \\(-0.361\\)) â€“ wpÅ‚yw mocy (hp) na spalanie (mpg),\n\\(c = -3.878\\) (standaryzowane \\(-0.630\\)) â€“ bezpoÅ›redni wpÅ‚yw masy (wt) na spalanie (mpg).\nMacierz \\(B\\) (zaleÅ¼noÅ›ci miÄ™dzy endogenicznymi): \\[\nB =\n\\begin{bmatrix}\n0 & b \\\\\n0 & 0\n\\end{bmatrix},\n\\quad b = -0.032.\n\\]\nMacierz \\(\\Gamma\\) (wpÅ‚ywy egzogenicznej zmiennej \\(wt\\)): \\[\n\\Gamma =\n\\begin{bmatrix}\nc \\\\\na\n\\end{bmatrix},\n\\quad c = -3.878, \\quad a = 46.160.\n\\]\nWariancje resztowe (bÅ‚Ä™dy strukturalne): \\[\n\\mathrm{Var}(\\zeta_{mpg}) = 6.095 \\; (17.3\\%),\n\\quad \\mathrm{Var}(\\zeta_{hp}) = 2577.777 \\; (56.6\\%).\n\\]\n\nOznacza to, Å¼e model wyjaÅ›nia 82.7% wariancji spalania i 43.4% wariancji mocy.\n\nEfekt poÅ›redni masy na spalanie przez moc \\[\nind = a \\cdot b = 46.160 \\cdot (-0.032) = -1.467\n\\] istotny statystycznie (\\(p &lt; 0.001\\)).\nEfekt caÅ‚kowity masy na spalanie: \\[\ntot = c + a \\cdot b = -3.878 + (-1.467) = -5.344.\n\\] Oznacza to, Å¼e wzrost masy samochodu (o jedno odchylenie standardowe tej zmiennej) zmniejsza spalanie o 0.868 odchylenia standardowego mpg â€“ w duÅ¼ej czÄ™Å›ci bezpoÅ›rednio, a w mniejszej poprzez wzrost mocy.\n\nModel Å›cieÅ¼kowy wskazuje, Å¼e masa samochodu (wt) ma silny negatywny wpÅ‚yw na oszczÄ™dnoÅ›Ä‡ paliwa (mpg), zarÃ³wno bezpoÅ›rednio, jak i poÅ›rednio poprzez zwiÄ™kszanie mocy silnika (hp). Moc natomiast sama w sobie pogarsza spalanie. WartoÅ›ci \\(R^2\\) potwierdzajÄ…, Å¼e model bardzo dobrze wyjaÅ›nia zmiennoÅ›Ä‡ mpg (83%), ale umiarkowanie sÅ‚abiej radzi sobie z hp (43%).",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#modele-strukturalne-sem",
    "href": "sem.html#modele-strukturalne-sem",
    "title": "Modele strukturalne",
    "section": "Modele strukturalne (SEM)",
    "text": "Modele strukturalne (SEM)\nModele typu covariance-based structural equation modeling (CB-SEM), okreÅ›lane po prostu jako SEM, stanowiÄ… rozwiniÄ™cie i uogÃ³lnienie dwÃ³ch podejÅ›Ä‡: analizy czynnikowej (CFA) oraz analizy Å›cieÅ¼kowej (path analysis). Istota SEM polega na tym, Å¼e pozwala ono jednoczeÅ›nie badamy trafnoÅ›Ä‡ pomiaru zmiennych latentnych oraz testujemy hipotezy dotyczÄ…ce relacji miÄ™dzy tymi zmiennymi. DziÄ™ki temu SEM stanowi narzÄ™dzie integrujÄ…ce w sobie modelowanie pomiarowe i strukturalne, umoÅ¼liwiajÄ…c analizÄ™ zÅ‚oÅ¼onych ukÅ‚adÃ³w zaleÅ¼noÅ›ci obserwowalnych i nieobserwowalnych.\nFormalnie model SEM zapisuje siÄ™ jako system rÃ³wnaÅ„ macierzowych. Model pomiarowy dla zmiennych egzogenicznych ma postaÄ‡ \\[\n\\mathbf{x} = \\Lambda_x \\boldsymbol{\\xi} + \\boldsymbol{\\delta},\n\\] gdzie \\(\\mathbf{x}\\) to wektor zmiennych obserwowalnych, \\(\\boldsymbol{\\xi}\\) â€“ wektor latentnych zmiennych egzogenicznych, \\(\\Lambda_x\\) â€“ macierz Å‚adunkÃ³w czynnikowych, a \\(\\boldsymbol{\\delta}\\) â€“ bÅ‚Ä™dy pomiarowe. Analogicznie model pomiarowy dla zmiennych endogenicznych przyjmuje formÄ™ \\[\n\\mathbf{y} = \\Lambda_y \\boldsymbol{\\eta} + \\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{y}\\) oznacza obserwowalne zmienne endogeniczne, \\(\\boldsymbol{\\eta}\\) â€“ latentne zmienne endogeniczne, \\(\\Lambda_y\\) â€“ macierz Å‚adunkÃ³w, a \\(\\boldsymbol{\\epsilon}\\) â€“ bÅ‚Ä™dy pomiaru. Trzecim elementem jest model strukturalny \\[\n\\boldsymbol{\\eta} = B \\boldsymbol{\\eta} + \\Gamma \\boldsymbol{\\xi} + \\boldsymbol{\\zeta},\n\\] ktÃ³ry opisuje relacje pomiÄ™dzy zmiennymi latentnymi endogenicznymi \\((B)\\) oraz wpÅ‚yw zmiennych egzogenicznych na endogeniczne \\((\\Gamma)\\), z uwzglÄ™dnieniem zakÅ‚Ã³ceÅ„ strukturalnych \\((\\boldsymbol{\\zeta})\\).\nW modelach SEM kluczowe znaczenie majÄ… zmienne latentne \\((\\xi, \\eta)\\), ktÃ³re reprezentujÄ… konstrukty teoretyczne trudne do bezpoÅ›redniego pomiaru, np. satysfakcjÄ™ z Å¼ycia czy strategie uczenia siÄ™. Zmienne obserwowalne \\((x, y)\\) stanowiÄ… wskaÅºniki tych konstruktÃ³w. Åadunki czynnikowe \\(\\Lambda\\) wskazujÄ…, jak silnie dana zmienna obserwowalna powiÄ…zana jest z konstruktem latentnym. Macierze \\(B\\) i \\(\\Gamma\\) opisujÄ… odpowiednio zaleÅ¼noÅ›ci miÄ™dzy konstruktami oraz ich uwarunkowania przez zmienne egzogeniczne. BÅ‚Ä™dy pomiarowe \\((\\delta, \\epsilon)\\) i zakÅ‚Ã³cenia strukturalne (\\(\\zeta\\)) odzwierciedlajÄ… niewyjaÅ›nionÄ… wariancjÄ™.\nParametry SEM mogÄ… byÄ‡ estymowane rÃ³Å¼nymi metodami. NajczÄ™Å›ciej stosuje siÄ™ metodÄ™ najwiÄ™kszej wiarygodnoÅ›ci (ML), ktÃ³ra minimalizuje rozbieÅ¼noÅ›Ä‡ miÄ™dzy macierzÄ… kowariancji modelowÄ… a empirycznÄ…. AlternatywÄ… sÄ… metody najmniejszych kwadratÃ³w: GLS (ang. Generalized Least Squares), ULS (ang. Unweighted Least Squares, mniej wymagajÄ…ca co do rozkÅ‚adÃ³w, lecz bez klasycznych testÃ³w istotnoÅ›ci) oraz DWLS (ang. Diagonally Weighted Least Squares), szczegÃ³lnie polecana przy danych porzÄ…dkowych. W przypadku naruszeÅ„ normalnoÅ›ci rozkÅ‚adu stosuje siÄ™ wersje odporne, takie jak MLR (ang. Maximum Likelihood Robust) czy MLM (ang. Maximum Likelihood Mean-adjusted), ktÃ³re korygujÄ… wariancje i bÅ‚Ä™dy standardowe (â€Supplemental Material for The Performance of ML, DWLS, and ULS Estimation With Robust Corrections in Structural Equation Models With Ordinal Variablesâ€ 2016; KILIÃ‡, UYSAL, i ATAR 2020; Li 2021; Kyriazos i Poga-Kyriazou 2023).\nZnaczenie SEM polega na tym, Å¼e Å‚Ä…czy ono analizÄ™ czynnikowÄ… i analizÄ™ Å›cieÅ¼kowÄ… w jeden spÃ³jny model. W czÄ™Å›ci pomiarowej pozwala sprawdziÄ‡, czy narzÄ™dzie badawcze dobrze odwzorowuje zamierzone konstrukty, natomiast w czÄ™Å›ci strukturalnej umoÅ¼liwia testowanie hipotez o zwiÄ…zkach miÄ™dzy zmiennymi ukrytymi. DziÄ™ki temu SEM jest traktowane jako zÅ‚oty standard w psychometrii, naukach spoÅ‚ecznych i zarzÄ…dzaniu, oferujÄ…c zarÃ³wno rzetelnÄ… ocenÄ™ jakoÅ›ci pomiaru, jak i analizÄ™ zaleÅ¼noÅ›ci przyczynowych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#zaÅ‚oÅ¼enia-modeli-sem",
    "href": "sem.html#zaÅ‚oÅ¼enia-modeli-sem",
    "title": "Modele strukturalne",
    "section": "ZaÅ‚oÅ¼enia modeli SEM",
    "text": "ZaÅ‚oÅ¼enia modeli SEM\n\nOparcie na teorii - SEM z definicji sÅ‚uÅ¼y testowaniu i potwierdzaniu modelu teoretycznego. Dlatego punktem wyjÅ›cia musi byÄ‡ koncepcja badawcza oparta na wczeÅ›niejszych badaniach i spÃ³jnej teorii. Model powinien odzwierciedlaÄ‡ hipotezy dotyczÄ…ce relacji miÄ™dzy konstruktami latentnymi i zmiennymi obserwowalnymi.\n\nWielkoÅ›Ä‡ prÃ³by - zaleca siÄ™ prÃ³by liczÄ…ce co najmniej okoÅ‚o 200 obserwacji, choÄ‡ ostateczny wymÃ³g zaleÅ¼y od trzech czynnikÃ³w:\n\nrozkÅ‚adu zmiennych,\nzÅ‚oÅ¼onoÅ›ci modelu,\nmetody estymacji.\n\nDuÅ¼e prÃ³by zwiÄ™kszajÄ… stabilnoÅ›Ä‡ wynikÃ³w i odpornoÅ›Ä‡ na naruszenia zaÅ‚oÅ¼eÅ„.\n\nNormalnoÅ›Ä‡ rozkÅ‚adu - poniewaÅ¼ SEM opiera siÄ™ na macierzy kowariancji, standardowo zakÅ‚ada siÄ™ wielowymiarowÄ… normalnoÅ›Ä‡ rozkÅ‚adu zmiennych. W praktyce odchylenia od normalnoÅ›ci moÅ¼na kompensowaÄ‡, stosujÄ…c estymatory odporne, np. robust ML czy WLSMV.\nLiniowoÅ›Ä‡ zwiÄ…zkÃ³w - zakÅ‚ada siÄ™, Å¼e relacje miÄ™dzy konstruktami latentnymi a wskaÅºnikami obserwowalnymi oraz miÄ™dzy zmiennymi latentnymi majÄ… charakter liniowy.\nBrak silnej wspÃ³Å‚liniowoÅ›ci - predyktory w modelu powinny byÄ‡ moÅ¼liwie niezaleÅ¼ne. ChoÄ‡ umiarkowana wspÃ³Å‚liniowoÅ›Ä‡ zwykle nie jest problemem, silne korelacje mogÄ… prowadziÄ‡ do trudnoÅ›ci w estymacji i interpretacji Å›cieÅ¼ek.\nKompletnoÅ›Ä‡ danych - modele SEM wymagajÄ… peÅ‚nych danych. MoÅ¼na to osiÄ…gnÄ…Ä‡ poprzez imputacjÄ™ (np. Å›redniÄ…, regresjÄ™) albo stosujÄ…c metody wykorzystujÄ…ce peÅ‚nÄ… informacjÄ™ przy brakach danych, jak FIML (Full Information Maximum Likelihood).\nNiezaleÅ¼noÅ›Ä‡ bÅ‚Ä™dÃ³w pomiarowych - standardowe zaÅ‚oÅ¼enie gÅ‚osi, Å¼e bÅ‚Ä™dy pomiarowe sÄ… nieskorelowane. W praktyce jednak niekiedy dopuszcza siÄ™ ich korelacje, zwÅ‚aszcza gdy sugerujÄ… to indeksy modyfikacyjne i uzasadnia teoria.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#ocena-dopasowania-modelu-sem",
    "href": "sem.html#ocena-dopasowania-modelu-sem",
    "title": "Modele strukturalne",
    "section": "Ocena dopasowania modelu SEM",
    "text": "Ocena dopasowania modelu SEM\n\n\n\n\n\n\n\nWskaÅºnik\nWartoÅ›Ä‡ idealna\nWartoÅ›Ä‡ akceptowalna\n\n\n\nChi-kwadrat (CMIN) *\np &gt; 0,05 (przy Î± = 0,05)\np &lt; 0,05 (przy Î± = 0,05)\n\n\nStandaryzowany chi-kwadrat (CMIN/df) *\n&lt; 3\n&lt; 5\n\n\nGFI (Goodness of Fit Index)\n&gt; 0,95\n&gt; 0,90\n\n\nAGFI (Adjusted GFI)\n&gt; 0,90\n&gt; 0,85\n\n\nCFI (Comparative Fit Index) *\n&gt; 0,95\n&gt; 0,90\n\n\nTLI (Tucker-Lewis Index, NNFI)\n&gt; 0,90\n&gt; 0,85\n\n\nNFI (Normed Fit Index)\n&gt; 0,95\n&gt; 0,90\n\n\nPGFI (Parsimonious GFI)\n&gt; 0,50\nbrak sztywnych progÃ³w\n\n\nPNFI (Parsimonious NFI)\n&gt; 0,50\nbrak sztywnych progÃ³w\n\n\nPCFI (Parsimonious CFI)\n&gt; 0,50\nbrak sztywnych progÃ³w\n\n\nSRMR (Standardized RMR) *\n&lt; 0,05\n&lt; 0,08\n\n\nRMSEA (Root Mean Square Error of Approximation) *\n&lt; 0,05 [90% CI]\n&lt; 0,10 [90% CI]\n\n\n\n\nPrzykÅ‚ad 4.3 ZbiÃ³r HolzingerSwineford1939 (pakietu lavaan) zawiera wyniki uczniÃ³w w dziewiÄ™ciu testach poznawczych oraz podstawowe cechy demograficzne i szkolne (Turney 1939). DziewiÄ™Ä‡ pozycji testowych tworzy trzy klasyczne domeny poznawcze: visual (postrzeganie wzrokowe), textual (kompetencje werbalne) i speed (szybkoÅ›Ä‡ przetwarzania), po trzy wskaÅºniki w kaÅ¼dej domenie. Oryginalne zmienne testowe oznaczone sÄ… jako x1â€“x9 i w literaturze przypisuje siÄ™ je do czynnikÃ³w: - x1, x2, x3 â†’ czynnik Visual, - x4, x5, x6 â†’ czynnik Textual, - x7, x8, x9 â†’ czynnik Speed.\nW danych moÅ¼na znaleÅºÄ‡ teÅ¼ zmienne: ageyr (wiek w latach), agemo (nadwyÅ¼ka miesiÄ™cy), sex (pÅ‚eÄ‡, kod 1 = chÅ‚opiec, 2 = dziewczynka), school (szkoÅ‚a), grade (klasa). Do modelu wprowadzony zostanie wiek w latach ciÄ…gÅ‚ych: age = ageyr + agemo/12, a pÅ‚eÄ‡ zostanie przekodowana binarnie (sex01: 0 = dziewczynka, 1 = chÅ‚opiec) dla przejrzystoÅ›ci interpretacji.\nHipotezy badawcze (wpÅ‚ywy bezpoÅ›rednie i poÅ›rednie)\nPrzyjmiemy klasycznÄ… trÃ³jczynnikowÄ… strukturÄ™ pomiarowÄ… (Visual, Textual, Speed), a w czÄ™Å›ci strukturalnej zaÅ‚oÅ¼ymy wpÅ‚ywy wieku i pÅ‚ci na latentne zdolnoÅ›ci oraz zaleÅ¼noÅ›Ä‡ miÄ™dzy zdolnoÅ›ciami:\n\n\n\\(H_0^1:\\) Wiek dodatnio wpÅ‚ywa na Visual i Textual oraz â€“ poÅ›rednio â€“ na Speed.\n\n\\(H_0^2:\\) PÅ‚eÄ‡ (kod 1 = chÅ‚opiec) rÃ³Å¼nicuje profile: dodatnio wpÅ‚ywa na Visual, natomiast sÅ‚abiej lub ujemnie na Textual; wpÅ‚yw na Speed wystÄ™puje poÅ›rednio poprzez Visual i Textual.\n\n\\(H_0^3:\\) Czynnik Speed zaleÅ¼y wprost od Visual i Textual; efekty wieku i pÅ‚ci na Speed bÄ™dÄ… zatem czÄ™Å›ciowo poÅ›redniczone przez Visual i Textual.\n\n\nKoddata(\"HolzingerSwineford1939\")\n\n# Przygotowanie zmiennych egzogenicznych\nhs &lt;- within(HolzingerSwineford1939, {\n  age &lt;- ageyr + agemo/12\n  sex01 &lt;- as.numeric(sex == 1)  # 1=boy, 0=girl (w razie innego kodowania dostosowaÄ‡)\n})\n\n# Specyfikacja modelu SEM: czÄ™Å›Ä‡ pomiarowa (CFA) + czÄ™Å›Ä‡ strukturalna\nmodel_sem &lt;- '\n\n# CzÄ™Å›Ä‡ pomiarowa (CFA)\n\nVisual  =~ x1 + x2 + x3\nTextual =~ x4 + x5 + x6\nSpeed   =~ x7 + x8 + x9\n\n# CzÄ™Å›Ä‡ strukturalna\n\nSpeed   ~ b1*Visual + b2*Textual + d1*age + d2*sex01\nVisual  ~ a1*age + a2*sex01\nTextual ~ a3*age + a4*sex01\n\n# Efekty poÅ›rednie wieku i pÅ‚ci na Speed\n\nind_age  := a1*b1 + a3*b2\nind_sex  := a2*b1 + a4*b2\n\n# Efekty bezpoÅ›rednie wieku i pÅ‚ci na Speed\n\ndir_age  := d1\ndir_sex  := d2\n\n# Efekty caÅ‚kowite wieku i pÅ‚ci na Speed\n\ntot_age  := dir_age + ind_age\ntot_sex  := dir_sex + ind_sex\n'\n\nfit_sem &lt;- sem(model_sem, data = hs,\n               estimator = \"MLR\",      # robust ML\n               meanstructure = TRUE)\n\nmodel_parameters(fit_sem)\n\n# Loading\n\nLink          | Coefficient |   SE |       95% CI |     z |      p\n------------------------------------------------------------------\nVisual =~ x1  |        1.00 | 0.00 | [1.00, 1.00] |       | &lt; .001\nVisual =~ x2  |        0.75 | 0.15 | [0.45, 1.04] |  4.95 | &lt; .001\nVisual =~ x3  |        1.12 | 0.23 | [0.66, 1.57] |  4.79 | &lt; .001\nTextual =~ x4 |        1.00 | 0.00 | [1.00, 1.00] |       | &lt; .001\nTextual =~ x5 |        1.14 | 0.07 | [1.00, 1.28] | 16.06 | &lt; .001\nTextual =~ x6 |        0.92 | 0.06 | [0.81, 1.04] | 15.76 | &lt; .001\nSpeed =~ x7   |        1.00 | 0.00 | [1.00, 1.00] |       | &lt; .001\nSpeed =~ x8   |        1.19 | 0.14 | [0.92, 1.46] |  8.66 | &lt; .001\nSpeed =~ x9   |        1.04 | 0.20 | [0.65, 1.43] |  5.25 | &lt; .001\n\n# Regression\n\nLink                 | Coefficient |   SE |         95% CI |     z |      p\n---------------------------------------------------------------------------\nSpeed ~ Visual (b1)  |        0.34 | 0.08 | [ 0.17,  0.50] |  4.04 | &lt; .001\nSpeed ~ Textual (b2) |        0.16 | 0.06 | [ 0.05,  0.27] |  2.94 | 0.003 \nSpeed ~ age (d1)     |        0.21 | 0.05 | [ 0.12,  0.31] |  4.47 | &lt; .001\nSpeed ~ sex01 (d2)   |       -0.19 | 0.09 | [-0.37, -0.02] | -2.13 | 0.033 \nVisual ~ age (a1)    |       -0.02 | 0.05 | [-0.12,  0.09] | -0.31 | 0.755 \nVisual ~ sex01 (a2)  |        0.32 | 0.11 | [ 0.11,  0.53] |  2.96 | 0.003 \nTextual ~ age (a3)   |       -0.23 | 0.06 | [-0.34, -0.12] | -4.18 | &lt; .001\nTextual ~ sex01 (a4) |       -0.07 | 0.12 | [-0.31,  0.16] | -0.60 | 0.547 \n\n# Correlation\n\nLink         | Coefficient | SE |       95% CI |      p\n-------------------------------------------------------\nage ~~ sex01 |        0.08 |  0 | [0.08, 0.08] | &lt; .001\n\n# Defined\n\nTo        | Coefficient |   SE |         95% CI |     z |      p\n----------------------------------------------------------------\n(ind_age) |       -0.04 | 0.03 | [-0.09,  0.01] | -1.74 | 0.083 \n(ind_sex) |        0.10 | 0.05 | [ 0.00,  0.20] |  1.87 | 0.062 \n(dir_age) |        0.21 | 0.05 | [ 0.12,  0.31] |  4.47 | &lt; .001\n(dir_sex) |       -0.19 | 0.09 | [-0.37, -0.02] | -2.13 | 0.033 \n(tot_age) |        0.17 | 0.05 | [ 0.08,  0.26] |  3.67 | &lt; .001\n(tot_sex) |       -0.10 | 0.09 | [-0.27,  0.08] | -1.09 | 0.275 \n\n\nW czÄ™Å›ci pomiarowej zdefiniowano trzy czynniki pierwszego rzÄ™du z klasycznym mapowaniem wskaÅºnikÃ³w. W czÄ™Å›ci strukturalnej zaÅ‚oÅ¼ono, Å¼e Visual i Textual determinujÄ… Speed, a age i sex01 oddziaÅ‚ujÄ… na Visual i Textual. Zdefiniowano takÅ¼e etykiety Å›cieÅ¼ek, aby policzyÄ‡ efekty poÅ›rednie i caÅ‚kowite (:=). Parametry raportowane sÄ… w skalach surowych i standaryzowanych.\n\nKodmodel_performance(fit_sem, c(\"Chi2\",\"Chi2_df\",\"p_Chi2\",\"CFI\",\"NNFI\",\"RMSEA\",\"RMSEA_CI_low\",\"RMSEA_CI_high\",\"SRMR\")) %&gt;% \n  print_html()\n\n\n\n\n\nChi2\nChi2_df\np_Chi2\nCFI\nNNFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nSRMR\n\n\n143.11\n37\n2.11e-14\n0.89\n0.84\n0.10\n0.08\n0.11\n0.10\n\n\n\n\n\nOcena dopasowania modelu SEM zawsze powinna byÄ‡ przeprowadzona z kilku perspektyw: testu chi-kwadrat, wskaÅºnikÃ³w dopasowania przyrostowych (incremental fit indices) oraz wskaÅºnikÃ³w bÅ‚Ä™du aproksymacji. Wyniki uzyskane w analizie wskazujÄ… na istotne sygnaÅ‚y niedopasowania modelu.\nTest chi-kwadrat dla modelu daÅ‚ wartoÅ›Ä‡ \\(\\chi^2 = 143,11\\) przy df = 37, co przy duÅ¼ej licznoÅ›ci prowadzi do p &lt; 0.001. Oznacza to, Å¼e w sensie dosÅ‚ownym odrzucamy hipotezÄ™ o peÅ‚nym zgodnym odwzorowaniu macierzy kowariancji w populacji przez model. Jednak test chi-kwadrat jest bardzo wraÅ¼liwy zarÃ³wno na rozmiar prÃ³by, jak i zÅ‚oÅ¼onoÅ›Ä‡ modelu, dlatego wynik ten traktuje siÄ™ raczej jako punkt wyjÅ›cia niÅ¼ rozstrzygajÄ…ce kryterium.\nWskaÅºniki przyrostowe pokazujÄ… umiarkowanie sÅ‚abe dopasowanie. WartoÅ›ci CFI = 0.89 i NNFI = 0.84 (znany rÃ³wnieÅ¼ jako TFI) sÄ… wyraÅºnie poniÅ¼ej rekomendowanego poziomu 0.90, a tym bardziej 0.95, ktÃ³re zwykle przyjmuje siÄ™ jako granicÄ™ bardzo dobrego dopasowania. To sugeruje, Å¼e model w obecnej postaci nie wyjaÅ›nia wystarczajÄ…co dobrze struktury zaleÅ¼noÅ›ci obserwowanych w danych i potencjalnie wymaga modyfikacji â€“ np. dodania powiÄ…zaÅ„ reszt, rewizji struktury Å›cieÅ¼ek lub przemyÅ›lenia samego modelu pomiarowego.\nWskaÅºnik bÅ‚Ä™du aproksymacji RMSEA = 0.10 (90% CI: 0.08â€“0.11) jest stosunkowo wysoki i wykracza poza granicÄ™ akceptowalnoÅ›ci (zwykle &lt; 0.08, a najlepiej &lt; 0.05). Taki wynik sugeruje, Å¼e model charakteryzuje siÄ™ zauwaÅ¼alnym bÅ‚Ä™dem przybliÅ¼enia w stosunku do danych populacyjnych. Z kolei wskaÅºnik SRMR = 0.10 jest powyÅ¼ej standardowego progu akceptowalnoÅ›ci 0.08, co dodatkowo wskazuje na problemy z odwzorowaniem korelacji obserwowanych przez model.\nChcÄ…c poprawiÄ‡ dopasowanie modelu moÅ¼na zaproponowaÄ‡ pewne modyfikacje 3.\n3Â Indeksy modyfikacyjne (modification indices, MI) wskazujÄ…, o ile zmniejszyÅ‚by siÄ™ chi-kwadrat modelu, gdyby wprowadzono danÄ… modyfikacjÄ™ (np. dodano Å›cieÅ¼kÄ™ lub skorelowano bÅ‚Ä™dy). Wysokie wartoÅ›ci MI sugerujÄ… potencjalne obszary niedopasowania modelu do danych i mogÄ… byÄ‡ punktem wyjÅ›cia do rozwaÅ¼aÅ„ nad jego ulepszeniem. NaleÅ¼y jednak podchodziÄ‡ do nich ostroÅ¼nie i zawsze w kontekÅ›cie teorii, aby uniknÄ…Ä‡ nadmiernego dopasowania modelu do konkretnego zbioru danych.\nKod# Pomocniczo: gdzie sÄ… najwiÄ™ksze niedopasowania?\nmodindices(fit_sem, sort.=TRUE, minimum.value = 10)[1:11, c(\"lhs\",\"op\",\"rhs\",\"mi\",\"epc\",\"sepc.all\")] %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = c(\"mi\",\"epc\",\"sepc.all\"), decimals = 3) \n\n\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.all\n\n\n\nTextual\n=~\nx1\n33.378\n0.371\n0.312\n\n\nVisual\n~\nTextual\n29.857\n0.310\n0.419\n\n\nTextual\n~\nSpeed\n29.857\n1.643\n1.025\n\n\nVisual\n~~\nTextual\n29.857\n0.280\n0.415\n\n\nTextual\n~\nVisual\n29.857\n0.556\n0.411\n\n\nVisual\n~\nSpeed\n29.856\n1.909\n1.607\n\n\nVisual\n=~\nx9\n25.003\n0.505\n0.368\n\n\nx7\n~~\nx8\n19.965\n0.363\n0.600\n\n\nVisual\n=~\nx7\n16.671\nâˆ’0.444\nâˆ’0.299\n\n\nx1\n~~\nx9\n11.465\n0.175\n0.251\n\n\nVisual\n=~\nx6\n10.830\n0.223\n0.148\n\n\n\n\n\n\nW powyÅ¼szej liÅ›cie wskaÅºnikÃ³w modyfikacji widaÄ‡ dwa zasadnicze problemy. Po pierwsze, silne sygnaÅ‚y sugerujÄ…, Å¼e czÄ™Å›Ä‡ niedopasowania pochodzi z bÅ‚Ä™dnie okreÅ›lonej lub zbyt restrykcyjnej czÄ™Å›ci pomiarowej (propozycje typu Textual =~ x1, Visual =~ x9, Visual =~ x7, Visual =~ x6). Po drugie, wiele propozycji o identycznym mi = 29.857 (Visual ~ Textual, Textual ~ Speed, Visual ~~ Textual, Textual ~ Visual, Visual ~ Speed) wskazuje, Å¼e model prÃ³buje â€odzyskaÄ‡â€ brakujÄ…cÄ… wspÃ³Å‚zmiennoÅ›Ä‡ miÄ™dzy konstruktami w czÄ™Å›ci strukturalnej.\nNie naleÅ¼y wprowadzaÄ‡ zbyt wiele modyfikacji jednoczeÅ›nie. W modelu z mediacjÄ… czÄ™Å›ciowÄ… sensowne jest dopuszczenie kowariancji reszt (zakÅ‚Ã³ceÅ„) miÄ™dzy mediatorami, czyli Visual ~~ Textual, poniewaÅ¼ oba konstrukty sÄ… endogeniczne (zaleÅ¼ne od age i sex01) i mogÄ… wspÃ³Å‚dzieliÄ‡ niewyjaÅ›nione czynniki.\n\nKodmodel_sem_mod1 &lt;- '\n  # CFA\n  Visual  =~ x1 + x2 + x3\n  Textual =~ x4 + x5 + x6\n  Speed   =~ x7 + x8 + x9\n\n  # Strukturalny (mediacja czÄ™Å›ciowa)\n  Speed   ~ b1*Visual + b2*Textual + d1*age + d2*sex01\n  Visual  ~ a1*age + a2*sex01\n  Textual ~ a3*age + a4*sex01\n\n  # Kowariancje reszt zmiennych latentnych\n  Visual  ~~ Textual\n\n  # Efekty poÅ›rednie, bezpoÅ›rednie i caÅ‚kowite\n  ind_age  := a1*b1 + a3*b2\n  ind_sex  := a2*b1 + a4*b2\n  dir_age  := d1\n  dir_sex  := d2\n  tot_age  := dir_age + ind_age\n  tot_sex  := dir_sex + ind_sex\n'\n\nfit_sem_mod1 &lt;- sem(model_sem_mod1, data = hs,\n                      estimator = \"MLR\",\n                      meanstructure = TRUE)\n\nmodel_performance(fit_sem_mod1, c(\"Chi2\",\"Chi2_df\",\"p_Chi2\",\"CFI\",\"NNFI\",\"RMSEA\",\"RMSEA_CI_low\",\"RMSEA_CI_high\",\"SRMR\")) %&gt;%\n  print_html()\n\n\n\n\n\nChi2\nChi2_df\np_Chi2\nCFI\nNNFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nSRMR\n\n\n105.31\n36\n1.02e-08\n0.93\n0.89\n0.08\n0.06\n0.10\n0.06\n\n\n\n\n\nPo wprowadzeniu kowariancji Visual ~~ Textual dopasowanie poprawiÅ‚o siÄ™, ale nadal nie jest ono w peÅ‚ni satysfakcjonujÄ…ce, zwÅ‚aszcza jeÅ¼eli traktowaÄ‡ kryteria dopasowania w sposÃ³b konserwatywny.\nTest \\(\\chi^2\\) jest istotny (p = 1.02e-08), co oznacza, Å¼e model jako caÅ‚oÅ›Ä‡ nadal odbiega od danych. Trzeba jednak pamiÄ™taÄ‡, Å¼e \\(\\chi^2\\) jest bardzo czuÅ‚y na liczebnoÅ›Ä‡ prÃ³by i na nawet niewielkie niedopasowania, wiÄ™c w praktyce ocenia siÄ™ go Å‚Ä…cznie z miarami przybliÅ¼onego dopasowania. CFI = 0.93 jest wynikiem â€umiarkowanie dobrymâ€, czÄ™sto uznawanym za akceptowalny w analizach aplikacyjnych, ale poniÅ¼ej typowego progu â€dobregoâ€ dopasowania (czÄ™sto 0.95). NNFI/TLI = 0.89 jest sÅ‚absze; to sygnaÅ‚, Å¼e w relacji do stopni swobody model nadal jest zbyt uproszczony lub wymusza zbyt wiele restrykcji. RMSEA = 0.08 jest na granicy dopasowania akceptowalnego; przedziaÅ‚ ufnoÅ›ci 0.06â€“0.10 sugeruje, Å¼e realne niedopasowanie moÅ¼e byÄ‡ od umiarkowanego do wyraÅºnego, bo gÃ³rna granica 0.10 jest wysoka. SRMR = 0.06 jest natomiast dobre (czÄ™sto wartoÅ›ci poniÅ¼ej 0.08 uznaje siÄ™ za akceptowalne), co wskazuje, Å¼e przeciÄ™tne reszty w macierzy kowariancji nie sÄ… duÅ¼e, choÄ‡ pozostajÄ… lokalne obszary niedopasowania, ktÃ³re psujÄ… CFI/TLI i RMSEA.\n\nKodmodindices(fit_sem_mod1, sort.=TRUE, minimum.value = 10)[, c(\"lhs\",\"op\",\"rhs\",\"mi\",\"epc\",\"sepc.all\")] %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = c(\"mi\",\"epc\",\"sepc.all\"), decimals = 3) \n\n\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.all\n\n\n\nVisual\n=~\nx9\n31.411\n0.526\n0.449\n\n\nx7\n~~\nx8\n19.271\n0.335\n0.551\n\n\nVisual\n=~\nx7\n18.170\nâˆ’0.425\nâˆ’0.335\n\n\nTextual\n=~\nx1\n12.160\n0.360\n0.306\n\n\nTextual\n=~\nx3\n11.825\nâˆ’0.298\nâˆ’0.262\n\n\n\n\n\n\nW tym momencie ranking mi jest znacznie â€czystszyâ€ i moÅ¼na z niego wyprowadziÄ‡ sensownÄ… kolejnoÅ›Ä‡ dziaÅ‚aÅ„. NajwaÅ¼niejsza obserwacja jest taka, Å¼e najwiÄ™ksze wartoÅ›ci mi dotyczÄ… juÅ¼ gÅ‚Ã³wnie czÄ™Å›ci pomiarowej (Å‚adunki krzyÅ¼owe), a nie czÄ™Å›ci strukturalnej. To zwykle oznacza, Å¼e najwiÄ™ksze niedopasowania nie wynikajÄ… juÅ¼ z braku jednej kowariancji miÄ™dzy zmiennymi latentnymi, tylko z tego, Å¼e wskaÅºniki nie sÄ… â€czystymiâ€ miernikami swoich czynnikÃ³w albo istnieje dodatkowy efekt metody wÅ›rÃ³d zadaÅ„.\nW pierwszej kolejnoÅ›ci dodamy x7 ~~ x8 (mi = 19.271). Jest to modyfikacja o relatywnie najniÅ¼szym koszcie interpretacyjnym, bo nie zmienia definicji Å¼adnego czynnika latentnego, a jedynie dopuszcza wspÃ³Å‚zmiennoÅ›Ä‡ reszt dwÃ³ch wskaÅºnikÃ³w, co czÄ™sto da siÄ™ uzasadniÄ‡ podobieÅ„stwem treÅ›ci, formatem zadania lub wspÃ³lnym komponentem metody. W praktyce taki krok czÄ™sto poprawia TLI i RMSEA bez â€przestawianiaâ€ konstruktu.\n\nKodmodel_sem_refined &lt;- paste0(model_sem_mod1, '\n  # Dodatkowa korelacja reszt wskaÅºnikÃ³w Speed\n  x7 ~~ x8\n')\n\nfit_sem_refined &lt;- sem(model_sem_refined, data = hs,\n                      estimator = \"MLR\",\n                      meanstructure = TRUE)\nmodel_performance(fit_sem_refined, c(\"Chi2\",\"Chi2_df\",\"p_Chi2\",\"CFI\",\"NNFI\",\"RMSEA\",\"RMSEA_CI_low\",\"RMSEA_CI_high\",\"SRMR\")) %&gt;%\n  print_html()\n\n\n\n\n\nChi2\nChi2_df\np_Chi2\nCFI\nNNFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nSRMR\n\n\n83.38\n35\n7.96e-06\n0.95\n0.92\n0.07\n0.05\n0.09\n0.05\n\n\n\n\n\nW ujÄ™ciu decyzyjnym dopasowanie jest juÅ¼ na poziomie, przy ktÃ³rym sensowne jest zatrzymanie siÄ™, o ile kolejne modyfikacje wymagaÅ‚yby Å‚adunkÃ³w krzyÅ¼owych lub zmian strukturalnych bez mocnego uzasadnienia treÅ›ciowego.\n\nKodmodel_parameters(fit_sem_refined, standardized = TRUE) \n\n# Loading\n\nLink          | Coefficient |   SE |       95% CI |     z |      p\n------------------------------------------------------------------\nVisual =~ x1  |        1.00 | 0.00 | [1.00, 1.00] |       | &lt; .001\nVisual =~ x2  |        0.61 | 0.12 | [0.37, 0.85] |  4.97 | &lt; .001\nVisual =~ x3  |        0.81 | 0.13 | [0.55, 1.07] |  6.06 | &lt; .001\nTextual =~ x4 |        1.00 | 0.00 | [1.00, 1.00] |       | &lt; .001\nTextual =~ x5 |        1.12 | 0.07 | [0.99, 1.25] | 16.61 | &lt; .001\nTextual =~ x6 |        0.92 | 0.06 | [0.80, 1.04] | 15.12 | &lt; .001\nSpeed =~ x7   |        1.00 | 0.00 | [1.00, 1.00] |       | &lt; .001\nSpeed =~ x8   |        1.31 | 0.21 | [0.89, 1.72] |  6.16 | &lt; .001\nSpeed =~ x9   |        1.94 | 0.42 | [1.12, 2.77] |  4.60 | &lt; .001\n\n# Regression\n\nLink                 | Coefficient |   SE |         95% CI |     z |      p\n---------------------------------------------------------------------------\nSpeed ~ Visual (b1)  |        0.30 | 0.07 | [ 0.17,  0.43] |  4.59 | &lt; .001\nSpeed ~ Textual (b2) |        0.04 | 0.04 | [-0.05,  0.12] |  0.84 | 0.402 \nSpeed ~ age (d1)     |        0.10 | 0.05 | [ 0.01,  0.20] |  2.12 | 0.034 \nSpeed ~ sex01 (d2)   |       -0.16 | 0.06 | [-0.29, -0.04] | -2.54 | 0.011 \nVisual ~ age (a1)    |       -0.04 | 0.06 | [-0.16,  0.09] | -0.60 | 0.547 \nVisual ~ sex01 (a2)  |        0.33 | 0.12 | [ 0.09,  0.57] |  2.70 | 0.007 \nTextual ~ age (a3)   |       -0.24 | 0.06 | [-0.35, -0.12] | -4.14 | &lt; .001\nTextual ~ sex01 (a4) |       -0.08 | 0.12 | [-0.31,  0.16] | -0.62 | 0.537 \n\n# Correlation\n\nLink              | Coefficient |   SE |       95% CI |    z |      p\n---------------------------------------------------------------------\nVisual ~~ Textual |        0.38 | 0.09 | [0.20, 0.56] | 4.15 | &lt; .001\nx7 ~~ x8          |        0.30 | 0.07 | [0.17, 0.43] | 4.47 | &lt; .001\nage ~~ sex01      |        0.08 | 0.00 | [0.08, 0.08] |      | &lt; .001\n\n# Defined\n\nTo        | Coefficient |   SE |         95% CI |     z |     p\n---------------------------------------------------------------\n(ind_age) |       -0.02 | 0.02 | [-0.06,  0.02] | -0.92 | 0.358\n(ind_sex) |        0.10 | 0.05 | [ 0.01,  0.19] |  2.13 | 0.033\n(dir_age) |        0.10 | 0.05 | [ 0.01,  0.20] |  2.12 | 0.034\n(dir_sex) |       -0.16 | 0.06 | [-0.29, -0.04] | -2.54 | 0.011\n(tot_age) |        0.08 | 0.04 | [ 0.00,  0.17] |  1.94 | 0.052\n(tot_sex) |       -0.07 | 0.06 | [-0.18,  0.05] | -1.08 | 0.279\n\n\nStruktura pomiarowa jest spÃ³jna z zaÅ‚oÅ¼eniem trzech czynnikÃ³w. Wszystkie Å‚adunki czynnikowe sÄ… dodatnie i istotne, a przedziaÅ‚y ufnoÅ›ci sÄ… dalekie od zera, co oznacza, Å¼e wskaÅºniki x1â€“x3 dobrze operacjonalizujÄ… Visual, x4â€“x6 dobrze operacjonalizujÄ… Textual, a x7â€“x9 dobrze operacjonalizujÄ… Speed. JednoczeÅ›nie istotna korelacja resztowa x7 ~~ x8 sugeruje, Å¼e te dwa wskaÅºniki Speed wspÃ³Å‚dzielÄ… dodatkowÄ… wariancjÄ™ niewyjaÅ›nionÄ… przez czynnik (np. podobieÅ„stwo formatu zadania lub efekt metody), ale nie podwaÅ¼a to samej trÃ³jczynnikowej struktury.\nW hipotezie dotyczÄ…cej zaleÅ¼noÅ›ci Speed od Visual i Textual uzyskuje siÄ™ czÄ™Å›ciowe potwierdzenie. Speed zaleÅ¼y istotnie od Visual (b1 = 0.30, p &lt; 0.001), co jest zgodne z zaÅ‚oÅ¼eniem, Å¼e wyÅ¼sza zdolnoÅ›Ä‡ wizualna wiÄ…Å¼e siÄ™ z wyÅ¼szym wynikiem na czynniku Speed. Natomiast wpÅ‚yw Textual na Speed nie jest istotny (b2 = 0.04, p = 0.402), a przedziaÅ‚ ufnoÅ›ci obejmuje 0, wiÄ™c w tych danych nie ma podstaw do tezy, Å¼e zdolnoÅ›Ä‡ tekstowa wnosi niezaleÅ¼ny wkÅ‚ad do Speed po uwzglÄ™dnieniu Visual oraz predyktorÃ³w bezpoÅ›rednich.\nHipoteza o wieku (â€wiek dodatnio wpÅ‚ywa na Visual i Textual oraz poÅ›rednio na Speedâ€) nie znajduje potwierdzenia w proponowanym kierunku. Wiek nie wpÅ‚ywa istotnie na Visual (a1 = âˆ’0.04, p = 0.547), a na Textual wpÅ‚ywa istotnie, ale ujemnie (a3 = âˆ’0.24, p &lt; 0.001). Oznacza to, Å¼e wraz ze wzrostem wieku (przy takim kodowaniu age, jakie mamy w danych) obserwuje siÄ™ spadek czynnika Textual, a nie wzrost. Konsekwentnie, Å‚Ä…czny efekt poÅ›redni wieku na Speed nie jest istotny (ind_age = âˆ’0.02, p = 0.358). JednoczeÅ›nie pojawia siÄ™ istotny dodatni wpÅ‚yw bezpoÅ›redni wieku na Speed (dir_age = 0.10, p = 0.034), czyli po kontrolowaniu Visual i Textual wiek nadal wnosi dodatni wkÅ‚ad do Speed. W efekcie caÅ‚kowity wpÅ‚yw wieku na Speed jest na granicy istotnoÅ›ci (tot_age = 0.08, p = 0.052), co sugeruje, Å¼e wypadkowy efekt jest sÅ‚aby i czÄ™Å›ciowo znoszony przez ujemnÄ… Å›cieÅ¼kÄ™ age â†’ Textual przy nieistotnym Textual â†’ Speed.\nHipoteza o pÅ‚ci (â€kod 1 = chÅ‚opiec: dodatni wpÅ‚yw na Visual, sÅ‚abszy lub ujemny na Textual, a wpÅ‚yw na Speed poÅ›rednio przez Visual i Textualâ€) jest potwierdzona tylko czÄ™Å›ciowo i w waÅ¼nym fragmencie inaczej niÅ¼ zakÅ‚adano. Zgodnie z hipotezÄ…, pÅ‚eÄ‡ ma dodatni i istotny wpÅ‚yw na Visual (a2 = 0.33, p = 0.007), co oznacza, Å¼e chÅ‚opcy osiÄ…gajÄ… wyÅ¼szy poziom czynnika Visual. WpÅ‚yw pÅ‚ci na Textual jest nieistotny (a4 = âˆ’0.08, p = 0.537), wiÄ™c nie ma podstaw, aby mÃ³wiÄ‡ o systematycznym â€sÅ‚abszymâ€ profilu tekstowym w tej prÃ³bie. Co do Speed, model wskazuje istotny dodatni efekt poÅ›redni pÅ‚ci (ind_sex = 0.10, p = 0.033), ktÃ³ry w praktyce pochodzi gÅ‚Ã³wnie z drogi sex01 â†’ Visual (dodatniej) oraz Visual â†’ Speed (dodatniej), przy braku wkÅ‚adu drogi przez Textual. JednoczeÅ›nie wystÄ™puje istotny ujemny efekt bezpoÅ›redni pÅ‚ci na Speed (dir_sex = âˆ’0.16, p = 0.011), czyli po uwzglÄ™dnieniu Visual i Textual chÅ‚opcy majÄ… niÅ¼szy poziom Speed. Te dwa efekty dziaÅ‚ajÄ… w przeciwnych kierunkach, przez co efekt caÅ‚kowity pÅ‚ci na Speed nie jest istotny (tot_sex = âˆ’0.07, p = 0.279).\nIstotna dodatnia kowariancja Visual ~~ Textual (0.38, p &lt; 0.001) potwierdza zaÅ‚oÅ¼enie, Å¼e zdolnoÅ›ci wizualne i tekstowe wspÃ³Å‚wystÄ™pujÄ…, nawet po uwzglÄ™dnieniu wieku i pÅ‚ci. To wspiera narracjÄ™ o â€zaleÅ¼noÅ›ci miÄ™dzy zdolnoÅ›ciamiâ€, ale w formie korelacji (wspÃ³Å‚dzielonej wariancji), a nie relacji kierunkowej.\nPodsumowujÄ…c, model wspiera tezÄ™ o trÃ³jczynnikowej strukturze pomiarowej oraz o istotnym zwiÄ…zku Speed z Visual, ale nie wspiera tezy o niezaleÅ¼nym wpÅ‚ywie Textual na Speed. ZaÅ‚oÅ¼enie o dodatnim wpÅ‚ywie wieku na zdolnoÅ›ci nie znajduje potwierdzenia; wiek dziaÅ‚a bezpoÅ›rednio dodatnio na Speed, a na Textual dziaÅ‚a ujemnie. Dla pÅ‚ci uzyskuje siÄ™ profil mieszany: dodatni wpÅ‚yw na Visual, brak istotnego wpÅ‚ywu na Textual, dodatni wpÅ‚yw poÅ›redni na Speed przez Visual oraz jednoczeÅ›nie ujemny wpÅ‚yw bezpoÅ›redni na Speed, co wypadkowo daje nieistotny efekt caÅ‚kowity.\n\nKod# Wizualizacja modelu SEM\nsemPaths(fit_sem_refined,\n         what = \"std\", \n         whatLabels = \"std\",\n         layout = \"tree2\",\n         style = \"lisrel\",\n         residuals = T, intercepts = F,\n         nCharNodes = 0, sizeMan = 5,\n         groups = \"latent\",\n         color = brewer.pal(3, \"Pastel2\"),\n         edge.color = \"grey60\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#cb-sem-vs.-pls-sem",
    "href": "sem.html#cb-sem-vs.-pls-sem",
    "title": "Modele strukturalne",
    "section": "CB-SEM vs.Â PLS-SEM",
    "text": "CB-SEM vs.Â PLS-SEM\nAnaliza rÃ³wnaÅ„ strukturalnych (SEM) rozwija siÄ™ w dwÃ³ch gÅ‚Ã³wnych nurtach: CB-SEM (ang. Covariance Based SEM) oraz PLS-SEM (ang. Partial Least Squares SEM). RÃ³Å¼nica pomiÄ™dzy CB-SEM a PLS-SEM sprowadza siÄ™ przede wszystkim do podejÅ›cia badawczego i celu analizy. CB-SEM koncentruje siÄ™ na globalnym dopasowaniu caÅ‚ego modelu do danych i jest metodÄ… konfirmacyjnÄ… â€“ sÅ‚uÅ¼y do testowania hipotez wyprowadzonych z teorii. Wymaga dobrze zdefiniowanego modelu, duÅ¼ych prÃ³b i speÅ‚nienia klasycznych zaÅ‚oÅ¼eÅ„ statystycznych, a w zamian dostarcza bogaty zestaw wskaÅºnikÃ³w dopasowania i rzetelnych testÃ³w statystycznych. Z kolei PLS-SEM opiera siÄ™ na minimalizacji reszt na poziomie zaleÅ¼noÅ›ci miÄ™dzy zmiennymi i traktuje model bardziej jako narzÄ™dzie predykcyjne niÅ¼ potwierdzajÄ…ce (Latan i Noonan 2017). Jest elastyczniejszy, lepiej sprawdza siÄ™ przy maÅ‚ych prÃ³bach i nienormalnych danych, ale oferuje mniej rozwiniÄ™te kryteria dopasowania i bywa obciÄ…Å¼ony w sensie statystycznym. W praktyce CB-SEM wybiera siÄ™ do badaÅ„ potwierdzajÄ…cych teoriÄ™, a PLS-SEM â€“ do badaÅ„ eksploracyjnych i predykcyjnych.\nCB-SEM stosuje siÄ™ gÅ‚Ã³wnie w sytuacjach, gdy badacz chce przetestowaÄ‡ ugruntowany model teoretyczny, wymagajÄ…cy duÅ¼ych prÃ³b i danych o normalnym rozkÅ‚adzie. PLS-SEM natomiast okazuje siÄ™ przydatny w badaniach eksploracyjnych, przy mniejszych prÃ³bach i danych odchylajÄ…cych siÄ™ od normalnoÅ›ci. Trzeba jednak pamiÄ™taÄ‡, Å¼e wyniki PLS-SEM mogÄ… byÄ‡ bardziej obciÄ…Å¼one, a sama metoda wciÄ…Å¼ jest rozwijana. Najnowsze podejÅ›cia, takie jak PLSc-SEM, starajÄ… siÄ™ poÅ‚Ä…czyÄ‡ zalety obu nurtÃ³w.\n\n\n\n\n\n\n\nKryterium\nCB-SEM\nPLS-SEM\n\n\n\nCel analizy\nPotwierdzanie caÅ‚oÅ›ciowego modelu i dobrze zdefiniowanej teorii\nEksploracja i predykcja, rozwÃ³j teorii w poczÄ…tkowej fazie\n\n\nMetoda estymacji\nNajczÄ™Å›ciej najwiÄ™ksza wiarygodnoÅ›Ä‡ (ML), wymagajÄ…ca normalnoÅ›ci\nMetoda najmniejszych kwadratÃ³w (Partial Least Squares), odporna na nienormalnoÅ›Ä‡\n\n\nZmienne latentne\nModele czynnikowe (factor-based), akcent na konstrukty latentne\nModele kompozytowe (composite-based), akcent na wskaÅºniki i prognozowanie\n\n\nDopasowanie modelu\nBogaty zestaw wskaÅºnikÃ³w globalnego dopasowania (Ï‡Â², RMSEA, CFI, GFI)\nOgraniczony zestaw wskaÅºnikÃ³w â€“ gÅ‚Ã³wnie RÂ², AVE, Î± Cronbacha\n\n\nElastycznoÅ›Ä‡ modelu\nMniej elastyczny, restrykcyjny, wymaga dokÅ‚adnego okreÅ›lenia teorii\nBardziej elastyczny, radzi sobie z maÅ‚ymi prÃ³bami i brakiem normalnoÅ›ci\n\n\nWymagania co do prÃ³by\nZwykle duÅ¼a prÃ³ba (â‰¥200), dane normalne\nMoÅ¼e byÄ‡ stosowany dla mniejszych prÃ³b, brak wymogu normalnoÅ›ci\n\n\nTyp badaÅ„\nPotwierdzajÄ…ce, weryfikacja teorii\nEksploracyjne, poszukujÄ…ce nowych zaleÅ¼noÅ›ci\n\n\nRozwÃ³j metody\nStabilna, ugruntowana tradycja\nWciÄ…Å¼ rozwijana (np. PLSc-SEM), wyniki mogÄ… byÄ‡ obciÄ…Å¼one",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#tworzenie-i-adaptacja-narzÄ™dzi-pomiarowych-w-sem",
    "href": "sem.html#tworzenie-i-adaptacja-narzÄ™dzi-pomiarowych-w-sem",
    "title": "Modele strukturalne",
    "section": "Tworzenie i adaptacja narzÄ™dzi pomiarowych w SEM",
    "text": "Tworzenie i adaptacja narzÄ™dzi pomiarowych w SEM\nPoniÅ¼ej zostanie przedstawiona peÅ‚na, uporzÄ…dkowana Å›cieÅ¼ka tworzenia oraz adaptacji narzÄ™dzia pomiarowego (np. psychometrycznego) â€“ od konceptualizacji do finalnego podrÄ™cznika â€“ wraz z kluczowymi statystykami, ich wzorami, sposobem liczenia i interpretacjÄ…. CaÅ‚oÅ›Ä‡ formuÅ‚ujemy w duchu klasycznej teorii testÃ³w, uzupeÅ‚niajÄ…c o elementy analizy czynnikowej oraz SEM.\n\nKonceptualizacja konstruktu i specyfikacja treÅ›ci\n\nRozpoczynamy od precyzyjnego zdefiniowania konstruktu (dziedzina, zakres, wymiary) na podstawie literatury. Tworzymy mapÄ™ treÅ›ci (tzw. blueprint), ktÃ³ra Å‚Ä…czy wymiary teoretyczne z planowanymi obszarami itemÃ³w i formatem odpowiedzi. JuÅ¼ na tym etapie okreÅ›lamy typ modelu pomiarowego (refleksyjny czy formatywny)4, bo determinuje to dalszÄ… metodologiÄ™.\n4Â Model refleksyjny zakÅ‚ada, Å¼e konstrukt latentny wywoÅ‚uje pewien poziom odpowiedzi na pozycje: zmienne obserwowalne sÄ… efektami wspÃ³lnej przyczyny, ich bÅ‚Ä™dy sÄ… specyficzne i niepowiÄ…zane, a wysoka wspÃ³Å‚zaleÅ¼noÅ›Ä‡ pozycji jest oczekiwana. Model formatywny zakÅ‚ada przeciwny kierunek przyczynowy: wskaÅºniki â€tworzÄ…â€ konstrukt (kompozyt), wiÄ™c itemy nie muszÄ… byÄ‡ skorelowane, a miary spÃ³jnoÅ›ci wewnÄ™trznej nie majÄ… zastosowania.\n\nGenerowanie puli pozycji i weryfikacja treÅ›ci\n\nBudujemy szerokÄ… pulÄ™ itemÃ³w o zrÃ³Å¼nicowanej trudnoÅ›ci/poziomie (w testach osiÄ…gniÄ™Ä‡) lub â€natÄ™Å¼eniuâ€ treÅ›ci (w skalach postaw). ZapewniaÄ‡ jednoznacznoÅ›Ä‡ jÄ™zykowÄ…, unikaÄ‡ sformuÅ‚owaÅ„ double-barreled (â€Czy jest Pan zadowolony z obsÅ‚ugi i ceny usÅ‚ugi?â€ - pytanie jest jednoczeÅ›nie o obsÅ‚ugÄ™ i cenÄ™) i niepotrzebnych zaprzeczeÅ„ (â€Nie zgadzam siÄ™ z tym, Å¼e nie powinno siÄ™ zabraniaÄ‡ palenia w restauracjachâ€).\nTrafnoÅ›Ä‡ treÅ›ciowa sÅ‚uÅ¼y do oceny, czy zestaw pozycji (itemÃ³w) faktycznie reprezentuje zakres treÅ›ci, ktÃ³ry ma byÄ‡ mierzony, czyli czy narzÄ™dzie â€pokrywaâ€ istotne aspekty konstruktu i nie zawiera pytaÅ„ przypadkowych lub marginalnych. PoniewaÅ¼ jest to wÅ‚asnoÅ›Ä‡ merytoryczna, a nie czysto statystyczna, weryfikowaÄ‡ jÄ… naleÅ¼y na etapie projektowania narzÄ™dzia poprzez ocenÄ™ ekspertÃ³w dziedzinowych, ktÃ³rzy przypisujÄ… kaÅ¼dej pozycji oceny adekwatnoÅ›ci (np. na skali porzÄ…dkowej 1â€“4). Do iloÅ›ciowego podsumowania zgodnoÅ›ci tych ocen stosuje siÄ™ wspÃ³Å‚czynnik Aikena \\(V\\), ktÃ³ry normalizuje oceny do przedziaÅ‚u [0,1]: \\[\nV \\;=\\; \\frac{\\sum_{i=1}^{N} (s_i - s_{\\min})}{N\\,(s_{\\max}-s_{\\min})},\n\\] gdzie \\(s_i\\) oznacza ocenÄ™ \\(i\\)-tego eksperta, \\(N\\) liczbÄ™ ekspertÃ³w, a \\(s_{\\min}\\) i \\(s_{\\max}\\) odpowiednio minimalnÄ… i maksymalnÄ… wartoÅ›Ä‡ skali ocen. WartoÅ›Ä‡ \\(V=0\\) odpowiada sytuacji, w ktÃ³rej wszyscy eksperci przyznajÄ… oceny minimalne, natomiast \\(V=1\\) oznacza peÅ‚nÄ… zgodnoÅ›Ä‡ na poziomie maksymalnym; im wyÅ¼sze \\(V\\), tym silniejsza jest zgodnoÅ›Ä‡, Å¼e dana pozycja jest trafna treÅ›ciowo. W praktyce czÄ™sto przyjmuje siÄ™, Å¼e wysokie \\(V\\) (np. \\(\\ge 0{,}70\\)) wskazuje na dobrÄ… trafnoÅ›Ä‡ treÅ›ciowÄ… pozycji, a niskie wartoÅ›ci stanowiÄ… przesÅ‚ankÄ™ do jej przeformuÅ‚owania lub odrzucenia, przy czym interpretacjÄ™ zawsze naleÅ¼y powiÄ…zaÄ‡ z liczbÄ… ekspertÃ³w i kryteriami oceny (np. adekwatnoÅ›Ä‡, jasnoÅ›Ä‡, reprezentatywnoÅ›Ä‡).\n\nAdaptacja jÄ™zykowo-kulturowa\n\nPrzy przenoszeniu narzÄ™dzia miÄ™dzy jÄ™zykami stosowaÄ‡ forward translation (2 niezaleÅ¼ne tÅ‚umaczenia), back-translation, konsensus zespoÅ‚u i decentering (ew. korekta ÅºrÃ³dÅ‚a). Wykonuje siÄ™ cognitive interviews (parafrazy - powtarzanie pytaÅ„ wÅ‚asnymi sÅ‚owami, think-aloud - respondent mÃ³wi na gÅ‚os, jak rozumie pytanie i dlaczego wybiera danÄ… odpowiedÅº) w grupie docelowej, aby sprawdziÄ‡ proces odpowiedzi. WersjÄ™ pilotaÅ¼owÄ… poprzedza siÄ™ audytem jÄ™zykowym i kulturowym przykÅ‚adÃ³w/skal.\n\nPilotaÅ¼ i analiza pozycji\n\nW badaniu pilotaÅ¼owym szacuje siÄ™ wÅ‚asnoÅ›ci pozycji. W klasycznej teorii testÃ³w kluczowe sÄ…:\n\nkorelacja pozycjaâ€“wynik caÅ‚kowity (skorygowana o danÄ… pozycjÄ™); wartoÅ›ci \\(\\ge 0.30\\) wskazuje satysfakcjonujÄ…cÄ… dyskryminacjÄ™;\ntrudnoÅ›Ä‡ pozycji5 (w testach osiÄ…gniÄ™Ä‡) jako Å›redni wynik lub odsetek poprawnych odpowiedzi \\(p\\in[0,1]\\); poÅ¼Ä…dany rozkÅ‚ad trudnoÅ›ci dla zakresu zdolnoÅ›ci badanych;\nwpÅ‚yw usuniÄ™cia pozycji na rzetelnoÅ›Ä‡ (alpha if item deleted).\n\n5Â TrudnoÅ›Ä‡ pozycji dotyczy gÅ‚Ã³wnie testÃ³w osiÄ…gniÄ™Ä‡, gdzie istnieje odpowiedÅº poprawna/niepoprawna. NajproÅ›ciej definiuje siÄ™ jÄ… jako odsetek poprawnych odpowiedzi \\(p \\in [0,1]\\) albo jako Å›redni wynik w danej pozycji. Wysokie \\(p\\) oznacza pozycjÄ™ Å‚atwÄ…, niskie \\(p\\) oznacza pozycjÄ™ trudnÄ…. PoÅ¼Ä…dany nie jest jeden â€idealnyâ€ poziom trudnoÅ›ci, tylko taki rozkÅ‚ad trudnoÅ›ci w caÅ‚ym teÅ›cie, ktÃ³ry pokrywa zakres umiejÄ™tnoÅ›ci badanych: czÄ™Å›Ä‡ zadaÅ„ ma byÄ‡ Å‚atwa, czÄ™Å›Ä‡ umiarkowana, czÄ™Å›Ä‡ trudna. DziÄ™ki temu test potrafi odrÃ³Å¼niaÄ‡ osoby zarÃ³wno sÅ‚absze, jak i bardzo dobre; gdy wszystkie pozycje sÄ… zbyt Å‚atwe lub zbyt trudne, pojawia siÄ™ efekt sufitu lub podÅ‚ogi i pomiar traci rozdzielczoÅ›Ä‡.\nWstÄ™pna struktura czynnikowa (EFA)\n\nNa oddzielnej prÃ³bie wykonuje siÄ™ eksploracyjnÄ… analizÄ™ czynnikowÄ… (EFA). Ustala siÄ™ liczbÄ™ czynnikÃ³w stosujÄ…c parallel analysis i kryterium MAP Velicera. NastÄ™pnie stosuje siÄ™ estymatjcÄ™ modelu za pomocÄ… PAF lub ML, z rotacjÄ… ortogonalnÄ… (np. varimax) lub ukoÅ›nÄ… (np. oblimin), zaleÅ¼nie od oczekiwanej korelacji czynnikÃ³w. WartoÅ›ci Å‚adunkÃ³w \\(|\\lambda| \\ge 0.40\\) zwykle uznaje siÄ™ za uÅ¼yteczne; diagnozuje siÄ™ ewentualne Å‚adunki krzyÅ¼owe i jeÅ›li takie wystÄ…piÄ… starami siÄ™ je eliminowaÄ‡.\n\nKonfirmacja analiza czynniowa (CFA) i model pomiarowy\n\nUstalamy model \\[\n\\mathbf{x} \\;=\\; \\Lambda \\mathbf{f} \\;+\\; \\boldsymbol{\\epsilon},\n\\qquad \\mathrm{Cov}(\\mathbf{f})=\\Phi,\\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon})=\\Psi,\n\\] co implikuje macierz kowariancji \\[\n\\Sigma(\\theta) \\;=\\; \\Lambda \\,\\Phi\\, \\Lambda^\\top \\;+\\; \\Psi.\n\\] Estymujemy parametry metodÄ… ML lub odpornÄ… (np. MLR), dla danych porzÄ…dkowych â€“ DWLS. Ocena globalna dopasowania opiera siÄ™ na:\n\nstatystyce \\(\\chi^2\\) rozbieÅ¼noÅ›ci;\nRMSEA z 90% PU;\nCFI i TLI (przyrostowe w stosunku do modelu niezaleÅ¼nego):\n\n\\(\\mathrm{CFI} = 1 - \\frac{\\max(\\chi^2_{\\text{model}}-df_{\\text{model}},\\,0)}{\\max(\\chi^2_{\\text{baseline}}-df_{\\text{baseline}},\\,0)},\\)\n\\(\\mathrm{TLI} = \\frac{\\chi^2_{\\text{baseline}}/df_{\\text{baseline}} - \\chi^2_{\\text{model}}/df_{\\text{model}}}{\\chi^2_{\\text{baseline}}/df_{\\text{baseline}} - 1};\\)\n\n\nSRMR jako Å›redni moduÅ‚ reszt standaryzowanych.\n\n\nRzetelnoÅ›Ä‡ skali\n\nW klasycznej teorii testÃ³w przyjmuje siÄ™, Å¼e wynik obserwowany \\(X\\) skÅ‚ada siÄ™ z dwÃ³ch skÅ‚adnikÃ³w: wyniku prawdziwego \\(T\\) oraz bÅ‚Ä™du pomiaru \\(E\\), czyli \\(X = T + E\\). Wynik prawdziwy naleÅ¼y rozumieÄ‡ jako stabilny, â€rzeczywistyâ€ poziom mierzonej cechy u osoby, natomiast bÅ‚Ä…d pomiaru obejmuje wszystkie losowe czynniki, ktÃ³re powodujÄ…, Å¼e pomiar w danym momencie nie jest idealnie powtarzalny (np. przypadkowe wahania uwagi, niejednoznacznoÅ›Ä‡ itemÃ³w, chwilowe warunki badania). W tym ujÄ™ciu rzetelnoÅ›Ä‡ skali \\(\\rho_{XXâ€™}\\) opisuje, na ile zmiennoÅ›Ä‡ wynikÃ³w obserwowanych w populacji wynika ze zmiennoÅ›ci wyniku prawdziwego, a na ile jest tylko â€szumemâ€ pomiarowym.\nWzÃ³r \\[\n\\rho_{XXâ€™} \\;=\\; \\frac{\\sigma_{T}^2}{\\sigma_{X}^2}\n\\] mÃ³wi, Å¼e rzetelnoÅ›Ä‡ jest ilorazem wariancji wyniku prawdziwego \\(\\sigma_T^2\\) do caÅ‚kowitej wariancji wyniku obserwowanego \\(\\sigma_X^2\\). JeÅ¼eli wiÄ™kszoÅ›Ä‡ wariancji obserwowanej pochodzi z rÃ³Å¼nic w \\(T\\), rzetelnoÅ›Ä‡ jest wysoka, co oznacza, Å¼e skala stabilnie rÃ³Å¼nicuje osoby ze wzglÄ™du na mierzonÄ… cechÄ™. JeÅ¼eli natomiast duÅ¼a czÄ™Å›Ä‡ wariancji obserwowanej pochodzi z bÅ‚Ä™du, rzetelnoÅ›Ä‡ spada, a wyniki sÄ… mniej powtarzalne i mniej precyzyjne.\nZ tego wynika teÅ¼ intuicyjna interpretacja wartoÅ›ci liczbowych: \\(\\rho_{XXâ€™}=1\\) oznaczaÅ‚oby pomiar idealny (brak bÅ‚Ä™du), \\(\\rho_{XXâ€™}=0\\) oznaczaÅ‚oby, Å¼e obserwowane rÃ³Å¼nice sÄ… czystym bÅ‚Ä™dem (brak informacji o prawdziwych rÃ³Å¼nicach). W praktyce wartoÅ›Ä‡ rzetelnoÅ›ci interpretuje siÄ™ jako â€odsetekâ€ wariancji obserwowanej, ktÃ³ry jest sygnaÅ‚em, a nie szumem. PrzykÅ‚adowo \\(\\rho_{XXâ€™}=0{,}80\\) sugeruje, Å¼e okoÅ‚o 80% wariancji wynikÃ³w wynika z rÃ³Å¼nic prawdziwych, a okoÅ‚o 20% stanowi wariancja bÅ‚Ä™du.\nWarto dodaÄ‡, Å¼e \\(\\sigma_T^2\\) nie jest bezpoÅ›rednio obserwowalna, wiÄ™c w praktyce rzetelnoÅ›Ä‡ szacuje siÄ™ przy pomocy procedur poÅ›rednich, takich jak zgodnoÅ›Ä‡ wewnÄ™trzna (np. alfa Cronbacha, omega), stabilnoÅ›Ä‡ czasowa (testâ€“retest) lub rzetelnoÅ›Ä‡ poÅ‚Ã³wkowa. Wszystkie te metody prÃ³bujÄ… oszacowaÄ‡, jaka czÄ™Å›Ä‡ wariancji \\(X\\) jest powtarzalna i zwiÄ…zana z \\(T\\), a jaka wynika z losowych fluktuacji.\nNajczÄ™Å›ciej uÅ¼ywane miary do oceny rzetelnoÅ›ci to:\n\nalfa Cronbacha (spÃ³jnoÅ›Ä‡ wewnÄ™trzna), dla \\(k\\) pozycji \\[\n\\alpha \\;=\\; \\frac{k}{k-1}\\left(1 - \\frac{\\sum_{i=1}^{k}\\sigma_i^2}{\\sigma_X^2}\\right)\\!,\n\\] gdzie \\(\\sigma_i^2\\) to wariancja pozycji, a \\(\\sigma_X^2\\) wariancja sumy skali. \\(\\alpha \\ge 0.70\\) czÄ™sto uznawane jest za akceptowalne (zaleÅ¼nie od celu).\nomega McDonalda \\[\n\\omega \\;=\\; \\frac{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2}{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2 + \\sum_{i=1}^{k}\\psi_{ii}},\n\\] gdzie \\(\\lambda_i\\) to Å‚adunki czynnika ogÃ³lnego, a \\(\\psi_{ii}\\) wariancje unikalne. Dla rozwiÄ…zaÅ„ hierarchicznych uÅ¼ywamy \\(\\omega_h\\) (udziaÅ‚ czynnika ogÃ³lnego).\n\n\nTrafnoÅ›Ä‡\n\nW analizie CFA/SEM ocenia siÄ™ nie tylko dopasowanie caÅ‚ego modelu, ale takÅ¼e jakoÅ›Ä‡ pomiaru poszczegÃ³lnych konstruktÃ³w latentnych, w szczegÃ³lnoÅ›ci trafnoÅ›Ä‡ zbieÅ¼nÄ… i rozbieÅ¼nÄ…. TrafnoÅ›Ä‡ zbieÅ¼na oznacza, Å¼e wskaÅºniki przypisane do jednego czynnika rzeczywiÅ›cie â€zbiegajÄ… siÄ™â€ i mierzÄ… ten sam konstrukt (czyli majÄ… wysokie Å‚adunki i relatywnie maÅ‚e bÅ‚Ä™dy). TrafnoÅ›Ä‡ rozbieÅ¼na oznacza natomiast, Å¼e rÃ³Å¼ne czynniki sÄ… od siebie empirycznie odrÃ³Å¼nialne (czyli korelacje miÄ™dzy konstruktami nie sÄ… tak wysokie, aby sugerowaÄ‡, Å¼e mierzÄ… to samo). W tym celu wykorzystuje siÄ™ zestaw indeksÃ³w opartych na Å‚adunkach czynnikowych i wariancjach bÅ‚Ä™du.\nPodstawÄ… obliczeÅ„ sÄ… Å‚adunki czynnikowe \\(\\lambda_i\\) (czyli siÅ‚a zwiÄ…zku wskaÅºnika i z czynnikiem) oraz wariancje bÅ‚Ä™du \\(\\theta_i\\) (czyli wariancje reszt/unikalnoÅ›ci wskaÅºnikÃ³w, bÄ™dÄ…ce czÄ™Å›ciÄ… niewyjaÅ›nionÄ… przez czynnik). Zwykle zakÅ‚ada siÄ™, Å¼e korzysta siÄ™ z Å‚adunkÃ³w wystandaryzowanych, bo wtedy interpretacja jest bezpoÅ›rednia: \\(\\lambda_i^2\\) moÅ¼na traktowaÄ‡ jako czÄ™Å›Ä‡ wariancji wskaÅºnika wyjaÅ›nianÄ… przez czynnik, a \\(\\theta_i\\) jako czÄ™Å›Ä‡ â€bÅ‚Ä™duâ€ lub wariancji specyficznej.\nPierwszym indeksem jest composite reliability (CR), ktÃ³re opisuje spÃ³jnoÅ›Ä‡ pomiaru czynnika podobnie jak rzetelnoÅ›Ä‡, ale w logice CFA (uwzglÄ™dnia nierÃ³wne Å‚adunki). Definiuje siÄ™ je jako \\[\n\\mathrm{CR} \\;=\\; \\frac{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2}{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2 + \\sum_{i=1}^{k} \\theta_i},\n\\] gdzie \\(k\\) oznacza liczbÄ™ wskaÅºnikÃ³w czynnika, \\(\\lambda_i\\) sÄ… Å‚adunkami, a \\(\\theta_i\\) wariancjami bÅ‚Ä™du. W liczniku znajduje siÄ™ â€siÅ‚a sygnaÅ‚uâ€ (Å‚Ä…czny wkÅ‚ad wskaÅºnikÃ³w w pomiar czynnika), a w mianowniku sygnaÅ‚ jest zestawiony z sumarycznym bÅ‚Ä™dem pomiaru. Im wiÄ™ksze Å‚adunki i mniejsze \\(\\theta_i\\), tym wyÅ¼sze CR. WartoÅ›ci rzÄ™du \\(\\ge 0{,}70\\) zwykle interpretuje siÄ™ jako satysfakcjonujÄ…cÄ… spÃ³jnoÅ›Ä‡ pomiaru; wartoÅ›ci duÅ¼o niÅ¼sze sugerujÄ…, Å¼e wskaÅºniki sÄ… sÅ‚abo zwiÄ…zane z czynnikiem lub majÄ… duÅ¼e bÅ‚Ä™dy.\nDrugim indeksem jest average variance extracted (AVE), ktÃ³ry mierzy, ile Å›rednio wariancji wskaÅºnikÃ³w jest â€wyciÄ…ganeâ€ przez czynnik, czyli jak duÅ¼a czÄ™Å›Ä‡ wariancji wskaÅºnikÃ³w jest wyjaÅ›niana przez konstrukt latentny. AVE definiuje siÄ™ jako \\[\n\\mathrm{AVE} \\;=\\; \\frac{\\sum_{i=1}^{k} \\lambda_i^2}{\\sum_{i=1}^{k} \\lambda_i^2 + \\sum_{i=1}^{k} \\theta_i}.\n\\] W liczniku pojawiajÄ… siÄ™ kwadraty Å‚adunkÃ³w \\(\\lambda_i^2\\), czyli wkÅ‚ad czynnika do wariancji kaÅ¼dego wskaÅºnika. Interpretacja jest nastÄ™pujÄ…ca: \\(\\mathrm{AVE}=0{,}50\\) oznacza, Å¼e czynnik wyjaÅ›nia przeciÄ™tnie okoÅ‚o 50% wariancji swoich wskaÅºnikÃ³w, a pozostaÅ‚a czÄ™Å›Ä‡ to bÅ‚Ä…d i wariancja specyficzna. Przyjmuje siÄ™, Å¼e \\(\\mathrm{AVE} \\ge 0{,}50\\) stanowi przesÅ‚ankÄ™ trafnoÅ›ci zbieÅ¼nej, poniewaÅ¼ sygnaÅ‚ konstruktu jest co najmniej tak samo silny jak szum pomiarowy w obrÄ™bie jego wskaÅºnikÃ³w. Niska AVE sugeruje, Å¼e wskaÅºniki nie â€zbiegajÄ… siÄ™â€ dostatecznie w pomiarze jednego konstruktu.\nTrafnoÅ›Ä‡ rozbieÅ¼nÄ… czÄ™sto ocenia siÄ™ kryterium Fornellaâ€“Larckera. W tym podejÅ›ciu porÃ³wnuje siÄ™ pierwiastek z AVE danego czynnika z jego korelacjami z innymi czynnikami. Warunek ma postaÄ‡ \\[\n\\sqrt{\\mathrm{AVE}_A} \\;&gt;\\; |\\phi_{A,B}|\\quad \\text{dla kaÅ¼dego } B \\neq A,\n\\] gdzie \\(\\phi_{A,B}\\) oznacza korelacjÄ™ miÄ™dzy czynnikami \\(A\\) i \\(B\\). Intuicja jest taka, Å¼e \\(\\sqrt{\\mathrm{AVE}_A}\\) moÅ¼na czytaÄ‡ jako miarÄ™ â€typowejâ€ siÅ‚y zwiÄ…zku czynnika z wÅ‚asnymi wskaÅºnikami, natomiast \\(|\\phi_{A,B}|\\) opisuje, jak mocno czynnik \\(A\\) jest powiÄ…zany z innym czynnikiem. JeÅ¼eli korelacja miÄ™dzy dwoma czynnikami przekracza \\(\\sqrt{\\mathrm{AVE}}\\), to znaczy, Å¼e czynnik jest bardziej podobny do innego konstruktu niÅ¼ do wÅ‚asnych wskaÅºnikÃ³w, co osÅ‚abia argument o rozrÃ³Å¼nialnoÅ›ci konstruktÃ³w.\nAlternatywnÄ… i czÄ™sto bardziej czuÅ‚Ä… miarÄ… trafnoÅ›ci rozbieÅ¼nej jest HTMT (heterotraitâ€“monotrait ratio). Idea polega na porÃ³wnaniu przeciÄ™tnej korelacji miÄ™dzy wskaÅºnikami rÃ³Å¼nych konstruktÃ³w (heterotrait) z przeciÄ™tnÄ… korelacjÄ… wskaÅºnikÃ³w w obrÄ™bie tego samego konstruktu (monotrait). W uproszczonej postaci moÅ¼na to zapisaÄ‡ jako \\[\n\\mathrm{HTMT} \\;=\\; \\frac{\\text{Å›rednia korelacja miÄ™dzy wskaÅºnikami z rÃ³Å¼nych konstruktÃ³w}}{\\text{Å›rednia korelacja miÄ™dzy wskaÅºnikami w obrÄ™bie tych konstruktÃ³w}}.\n\\] JeÅ¼eli konstrukty \\(A\\) i \\(B\\) sÄ… rozrÃ³Å¼nialne, to korelacje â€pomiÄ™dzyâ€ konstruktami powinny byÄ‡ istotnie mniejsze niÅ¼ korelacje â€wewnÄ…trzâ€ konstruktÃ³w, wiÄ™c HTMT powinno byÄ‡ wyraÅºnie poniÅ¼ej 1. W praktyce wartoÅ›ci mniejsze niÅ¼ okoÅ‚o \\(0{,}85\\) (kryterium bardziej restrykcyjne) lub \\(0{,}90\\) (kryterium bardziej liberalne) interpretuje siÄ™ jako przesÅ‚ankÄ™ dobrej rozrÃ³Å¼nialnoÅ›ci, natomiast HTMT bliskie 1 sugeruje, Å¼e dwa konstrukty empirycznie nakÅ‚adajÄ… siÄ™ na siebie.\n\nSkalowanie, punktacja i normy\n\nDecydujemy o sposobie punktowania: suma/Å›rednia pozycji (po ewentualnym odwrÃ³ceniu kodowania) czy punktacja czynnikowa (regresyjna/ Bartlettâ€™a6). Ustalamy rÃ³wnieÅ¼ normy na podstawie np. siatki stenowej (niski, przeciÄ™tny i wysoki poziom skali).\n6Â Bartlett scores sÄ… nieobciÄ…Å¼onymi estymatorami czynnikÃ³w latentnych, ale mogÄ… byÄ‡ mniej stabilne w maÅ‚ych prÃ³bach i przy sÅ‚abych Å‚adunkach.\nPrzykÅ‚ad 4.4 Dla ilustracji adaptacji narzÄ™dzia pomiarowego, wykorzystamy oszacowanÄ… juÅ¼ strukturÄ™ czynnikowÄ… z PrzykÅ‚adÂ 8.1. MieliÅ›my tam 13 pozycji opisujÄ…cych trzy strategie: zapamiÄ™tywania, opracowywania i kontroli dla osÃ³b z Wielkiej Brytanii. ZaÅ‚Ã³Å¼my, Å¼e tÄ… samÄ… strukturÄ™ chcemy przenieÅ›Ä‡ na rynek HiszpaÅ„ski. W tym celu sprawdzimy dopasowanie modelu konfirmacyjnego na danych hiszpaÅ„skich.\n\nKodpisaspa &lt;- PISA09[PISA09$cnt == \"ESP\", c(alitems, \"sex\")]\npisaspa &lt;- pisaspa[complete.cases(pisaspa[, c(mitems, \n  eitems, citems)]), ]\n\n# Definicja modelu CFA\nmodel_cfa &lt;- '\n  # Definicja czynnikÃ³w\n  ZapamiÄ™tywanie =~ st27q01 + st27q03 + st27q05 + st27q07\n  Opracowywanie =~ st27q04 + st27q08 + st27q10 + st27q12\n  Kontrola =~ st27q02 + st27q06 + st27q09 + st27q11 + st27q13\n'\n\n# Estymacja modelu CFA\nfit_cfa_spa &lt;- cfa(model_cfa, data = pisaspa, auto.var = TRUE, auto.cov.lv.x = TRUE, std.lv = TRUE)\n\n\n\nKodcompare_performance(fit_cfa, fit_cfa_spa, metrics = c(\"p_Chi2\", \"GFI\", \"AGFI\", \"NFI\", \"NNFI\", \"CFI\", \"RMSEA\", \"RMR\", \"SRMR\", \"RFI\")) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nName\nModel\np_Chi2\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMR\nSRMR\nRFI\n\n\n\nfit_cfa\nlavaan\n0.000\n0.936\n0.907\n0.881\n0.856\n0.885\n0.081\n0.042\n0.057\n0.850\n\n\nfit_cfa_spa\nlavaan\n0.000\n0.940\n0.912\n0.882\n0.854\n0.884\n0.079\n0.053\n0.059\n0.851\n\n\n\n\n\n\nOcena jakoÅ›ci dopasowania modelu pokazuje, Å¼e przyjÄ™ta struktura dla Wielkiej Brytanii sprawdza siÄ™ rÃ³wnieÅ¼ dla Hiszpanii. To dopiero pierwszy (oczywiÅ›cie pominÄ…wszy wszystkie wczeÅ›niejsze kroki jak tÅ‚umaczenie, czy analiza eksploracyjna) krok do adaptacji narzÄ™dzia do nowych warunkÃ³w.\nTeraz ocenimy rzetelnoÅ›Ä‡ skali.\n\nKodtab_itemscale(df = pisaspa, factor.groups = c(rep(\"ZapamiÄ™tywanie\", 4), rep(\"Opracowanie\", 4), rep(\"Kontrola\", 5)), factor.groups.titles = c(\"Kontrola\", \"Opracowanie\", \"ZapamiÄ™tywanie\")) \n\n\nKontrola\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nst27q02\n0.00 %\n2.93\n0.91\n-0.44\n0.73\n0.46\n0.72\n\n\n\nst27q06\n0.00 %\n3.07\n0.92\n-0.6\n0.77\n0.57\n0.68\n\n\n\nst27q09\n0.00 %\n2.71\n0.89\n-0.14\n0.68\n0.56\n0.68\n\n\n\nst27q11\n0.00 %\n3.17\n0.87\n-0.78\n0.79\n0.54\n0.69\n\n\n\nst27q13\n0.00 %\n2.45\n1.02\n0.12\n0.61\n0.43\n0.73\n\n\n\nMean inter-item-correlation=0.371 Â· Cronbach's Î±=0.744\n\n\n\nÂ \n\nOpracowanie\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nst27q04\n0.00 %\n2.49\n1.01\n0.05\n0.62\n0.47\n0.72\n\n\n\nst27q08\n0.00 %\n2.01\n0.94\n0.62\n0.50\n0.53\n0.68\n\n\n\nst27q10\n0.00 %\n2.33\n0.95\n0.21\n0.58\n0.56\n0.66\n\n\n\nst27q12\n0.00 %\n2.17\n0.93\n0.39\n0.54\n0.57\n0.66\n\n\n\nMean inter-item-correlation=0.416 Â· Cronbach's Î±=0.738\n\n\n\nÂ \n\nZapamiÄ™tywanie\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nst27q01\n0.00 %\n2.6\n0.98\n0.02\n0.65\n0.54\n0.64\n\n\n\nst27q03\n0.00 %\n2.85\n0.95\n-0.33\n0.71\n0.52\n0.65\n\n\n\nst27q05\n0.00 %\n2.35\n1.03\n0.21\n0.59\n0.54\n0.64\n\n\n\nst27q07\n0.00 %\n2.92\n0.96\n-0.43\n0.73\n0.44\n0.70\n\n\n\nMean inter-item-correlation=0.390 Â· Cronbach's Î±=0.719\n\n\n\nÂ \n\n\n\n\n\n\n\n\n\nÂ \nComponent 1\nComponent 2\nComponent 3\n\n\nComponent 1\nÎ±=0.744\nÂ \nÂ \n\n\nComponent 2\n0.521(&lt;.001)\n\nÎ±=0.738\nÂ \n\n\nComponent 3\n0.376(&lt;.001)\n\n0.222(&lt;.001)\n\nÎ±=0.719\n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nSpÃ³jnoÅ›Ä‡ wewnÄ™trzna poszczegÃ³lnych komponentÃ³w\nKaÅ¼da z trzech skal (komponentÃ³w) osiÄ…ga akceptowalny poziom rzetelnoÅ›ci wewnÄ™trznej. WartoÅ›ci wspÃ³Å‚czynnika Cronbacha \\(\\alpha\\) wynoszÄ… odpowiednio 0.738 dla strategii opracowywania, 0.744 dla kontroli oraz 0.719 dla zapamiÄ™tywania. SÄ… to wartoÅ›ci przekraczajÄ…ce prÃ³g 0.70, co w badaniach psychometrycznych uznaje siÄ™ za wystarczajÄ…ce dla narzÄ™dzi we wczesnym etapie walidacji. Wskazuje to, Å¼e pozycje w kaÅ¼dej ze skal mierzÄ… spÃ³jny konstrukt.\nWartoÅ›ci korelacji miÄ™dzy pozycjami (mean inter-item correlation) mieszczÄ… siÄ™ w zakresie 0.37â€“0.42, co uznaje siÄ™ za optymalne (wartoÅ›ci zbyt niskie &lt;0.20 sugerujÄ… brak spÃ³jnoÅ›ci, natomiast zbyt wysokie &gt;0.70 nadmiarowoÅ›Ä‡). Oznacza to, Å¼e pozycje sÄ… ze sobÄ… skorelowane w stopniu umiarkowanym, zachowujÄ…c jednoczeÅ›nie rÃ³Å¼norodnoÅ›Ä‡ treÅ›ciowÄ….\nAnaliza jakoÅ›ci pozycji\nWszystkie pozycje majÄ… trudnoÅ›Ä‡ (item difficulty) w przedziale 0.50â€“0.79, co oznacza, Å¼e Å›rednie odpowiedzi respondentÃ³w oscylujÄ… wokÃ³Å‚ Å›rodka skali, bez efektÃ³w podÅ‚ogowych czy sufitowych. Pozycje majÄ… takÅ¼e umiarkowane lub wysokie wartoÅ›ci dyskryminacji (item discrimination w przedziale 0.43â€“0.57), wskazujÄ…ce, Å¼e dobrze rÃ³Å¼nicujÄ… osoby z wyÅ¼szymi i niÅ¼szymi wynikami ogÃ³lnymi. Å»adna z pozycji nie obniÅ¼a znaczÄ…co rzetelnoÅ›ci caÅ‚ej skali (wszystkie wartoÅ›ci â€Î± if deletedâ€ pozostajÄ… na poziomie podobnym lub niÅ¼szym niÅ¼ peÅ‚ne Î±).\nZwiÄ…zki miÄ™dzy komponentami\nKorelacje pomiÄ™dzy skalami sÄ… wszystkie istotne statystycznie, ale majÄ… zrÃ³Å¼nicowanÄ… siÅ‚Ä™. Najsilniejszy zwiÄ…zek wystÄ™puje pomiÄ™dzy KontrolÄ… a Opracowaniem (r = 0.521, p &lt; .001). MoÅ¼na to interpretowaÄ‡ tak, Å¼e osoby, ktÃ³re dbajÄ… o planowanie i monitorowanie swojego uczenia siÄ™, czÄ™Å›ciej stosujÄ… takÅ¼e strategie gÅ‚Ä™bszego opracowywania materiaÅ‚u. Skala ZapamiÄ™tywanie koreluje umiarkowanie z Opracowaniem (r = 0.376, p &lt; .001), co jest zgodne z intuicjÄ…: aby skutecznie zapamiÄ™taÄ‡, czÄ™sto trzeba wczeÅ›niej przetworzyÄ‡ materiaÅ‚. NajsÅ‚abszy, choÄ‡ istotny zwiÄ…zek obserwujemy miÄ™dzy KontrolÄ… a ZapamiÄ™tywaniem (r = 0.222, p &lt; .001), co sugeruje, Å¼e te dwie strategie sÄ… bardziej odrÄ™bne, a ich powiÄ…zanie jest ograniczone.\nPodsumowanie\nOtrzymane wyniki sugerujÄ…, Å¼e narzÄ™dzie jest psychometrycznie poprawne: ma akceptowalnÄ… spÃ³jnoÅ›Ä‡ wewnÄ™trznÄ…, zrÃ³wnowaÅ¼ony poziom trudnoÅ›ci pozycji, dobre wskaÅºniki dyskryminacji oraz pozwala rozrÃ³Å¼niaÄ‡ trzy powiÄ…zane, ale odrÄ™bne strategie uczenia siÄ™. Komponent 2 wydaje siÄ™ peÅ‚niÄ‡ rolÄ™ centralnÄ…, poniewaÅ¼ jest najwyraÅºniej powiÄ…zany zarÃ³wno z komponentem 1, jak i 3. To moÅ¼e sugerowaÄ‡ jego bardziej ogÃ³lny charakter lub funkcjÄ™ â€pomostuâ€ miÄ™dzy dwoma innymi wymiarami.\nPo wykonaniu analizy CFA i ocenie rzetelnoÅ›ci kolejnym etapem adaptacji narzÄ™dzia jest przeprowadzenie rozszerzonych analiz stabilnoÅ›ci w czasie, takich jak testâ€“retest czy wspÃ³Å‚czynnik ICC (ang. intraclass correlation coefficient), ktÃ³re pozwalajÄ… oceniÄ‡ powtarzalnoÅ›Ä‡ wynikÃ³w (tych nie wykonamy, poniewaÅ¼ do tego potrzeba przeprowadzenia ankiety w dwÃ³ch momentach czasowych). NaleÅ¼y takÅ¼e zweryfikowaÄ‡ trafnoÅ›Ä‡ â€“ konwergencyjnÄ…, dyskryminacyjnÄ… i kryterialnÄ… â€“ aby upewniÄ‡ siÄ™, Å¼e narzÄ™dzie mierzy to, co zakÅ‚adano teoretycznie.\n\nKodlibrary(semTools)\nAVE(fit_cfa_spa) \n\nZapamiÄ™tywanie  Opracowywanie       Kontrola \n         0.395          0.418          0.373 \n\nKodcompRelSEM(fit_cfa_spa)\n\nZapamiÄ™tywanie  Opracowywanie       Kontrola \n         0.722          0.744          0.752 \n\nKodhtmt(model_cfa, data = pisaspa)\n\n               ZpmÄ™ty Oprcwy Kontrl\nZapamiÄ™tywanie  1.000              \nOpracowywanie   0.288  1.000       \nKontrola        0.482  0.680  1.000\n\n\nWyniki moÅ¼na interpretowaÄ‡ na trzech poziomach: rzetelnoÅ›ci wewnÄ™trznej (CR), trafnoÅ›ci konwergencyjnej (AVE) oraz trafnoÅ›ci dyskryminacyjnej (HTMT).\n\nWspÃ³Å‚czynniki Composite Reliability (CR) mieszczÄ… siÄ™ w przedziale od 0.72 do 0.75. SÄ… to wartoÅ›ci powyÅ¼ej progu 0.70, co sugeruje akceptowalnÄ… trafnoÅ›Ä‡ wewnÄ™trznÄ… kaÅ¼dej ze skal. Oznacza to, Å¼e wskaÅºniki w ramach czynnika zapamiÄ™tywania, opracowywania i kontroli dostarczajÄ… relatywnie stabilnej informacji o zmiennej latentnej.\nÅšrednia wyjaÅ›niona wariancja (AVE) dla wszystkich trzech czynnikÃ³w jest niska: 0.395, 0.418 i 0.373. Kryterium akceptowalne to zwykle AVE â‰¥ 0.50, co oznacza, Å¼e czynnik powinien wyjaÅ›niaÄ‡ przynajmniej poÅ‚owÄ™ wariancji swoich wskaÅºnikÃ³w. Tutaj wartoÅ›ci poniÅ¼ej 0.5 wskazujÄ…, Å¼e wyjaÅ›niona czÄ™Å›Ä‡ wariancji jest mniejsza niÅ¼ ta przypisana bÅ‚Ä™dowi. MoÅ¼e to oznaczaÄ‡, Å¼e wskaÅºniki sÄ… doÅ›Ä‡ zrÃ³Å¼nicowane, a ich wspÃ³lna treÅ›Ä‡ (latentna) nie jest wystarczajÄ…co silnie uchwycona. W praktyce oznacza to ograniczonÄ… trafnoÅ›Ä‡ konwergencyjnÄ… â€“ czyli wskaÅºniki nie â€zbiegajÄ… siÄ™â€ wystarczajÄ…co na wspÃ³lny konstrukt.\nMacierz HTMT wskazuje na poziom rozrÃ³Å¼nialnoÅ›ci czynnikÃ³w (trafnoÅ›ci dyskryminacyjnej). Przyjmuje siÄ™, Å¼e wartoÅ›ci HTMT &lt; 0.85 (lub bardziej liberalnie &lt; 0.90) oznaczajÄ… satysfakcjonujÄ…cÄ… rozrÃ³Å¼nialnoÅ›Ä‡. W tym przypadku:\n\nZapamiÄ™tywanieâ€“Opracowywanie = 0.288 â€“ bardzo niski wspÃ³Å‚czynnik, dobra rozrÃ³Å¼nialnoÅ›Ä‡,\nZapamiÄ™tywanieâ€“Kontrolne = 0.482 â€“ umiarkowany, nadal bezpieczny,\nOpracowywanieâ€“Kontrolne = 0.680 â€“ wyÅ¼szy, ale poniÅ¼ej progu 0.85, wiÄ™c rozrÃ³Å¼nialnoÅ›Ä‡ jest zachowana.\n\n\n\nPodsumowujÄ…c, model charakteryzuje siÄ™ akceptowalnÄ… rzetelnoÅ›ciÄ… i zadowalajÄ…cÄ… trafnoÅ›ciÄ… dyskryminacyjnÄ…, ale ograniczonÄ… trafnoÅ›ciÄ… konwergencyjnÄ…. W praktyce oznacza to, Å¼e choÄ‡ skale mierzÄ… rÃ³Å¼ne konstrukty i sÄ… spÃ³jne wewnÄ™trznie, to konstrukty te nie sÄ… jeszcze w peÅ‚ni â€czystoâ€ uchwycone przez zestaw wskaÅºnikÃ³w â€“ byÄ‡ moÅ¼e potrzebna byÅ‚aby rewizja niektÃ³rych pozycji, ich dodanie lub modyfikacja.\nIstotnym krokiem jest rÃ³wnieÅ¼ badanie rÃ³wnowaÅ¼noÅ›ci pomiaru (measurement invariance) przy uÅ¼yciu CFA wielogrupowej, co umoÅ¼liwia porÃ³wnywanie wynikÃ³w miÄ™dzy grupami, np. ze wzglÄ™du na pÅ‚eÄ‡ czy wiek. My wykonamy analizÄ™ w podziale ze wzglÄ™du na pÅ‚eÄ‡, aby dowiedzieÄ‡ siÄ™ czy narzÄ™dzie mierzy badane konstrukty podobnie w obu grupach.\nRÃ³wnowaÅ¼noÅ›Ä‡ pomiaru, oznacza sprawdzenie, czy ten sam model pomiarowy (CFA) dziaÅ‚a w ten sam sposÃ³b w rÃ³Å¼nych grupach. W praktyce chodzi o odpowiedÅº na pytanie, czy porÃ³wnywanie wynikÃ³w miÄ™dzy grupami (np. kobietami i mÄ™Å¼czyznami) jest uczciwe metodologicznie: czy rÃ³Å¼nice w wynikach odzwierciedlajÄ… rzeczywiste rÃ³Å¼nice w konstrukcie latentnym, a nie to, Å¼e pozycje sÄ… inaczej rozumiane, majÄ… innÄ… â€siÅ‚Ä™â€ pomiaru lub inny poziom bazowy w danej grupie.\nW CFA wielogrupowej estymuje siÄ™ ten sam model w kilku grupach jednoczeÅ›nie (tu: group = \"sex\"), a nastÄ™pnie narzuca siÄ™ coraz silniejsze ograniczenia rÃ³wnoÅ›ci parametrÃ³w pomiarowych miÄ™dzy grupami. KaÅ¼dy kolejny krok odpowiada wyÅ¼szemu poziomowi rÃ³wnowaÅ¼noÅ›ci i pozwala na coraz â€mocniejszeâ€ porÃ³wnania miÄ™dzy grupami.\nRÃ³wnowaÅ¼noÅ›Ä‡ konfiguracyjna (configural invariance) polega na tym, Å¼e w obu grupach obowiÄ…zuje ta sama struktura czynnikowa: te same pozycje Å‚adujÄ… na te same czynniki, ale wartoÅ›ci Å‚adunkÃ³w, przeciÄ™Ä‡ i bÅ‚Ä™dÃ³w mogÄ… siÄ™ rÃ³Å¼niÄ‡. JeÅ¼eli ten model ma akceptowalne dopasowanie, oznacza to, Å¼e w obu grupach da siÄ™ opisaÄ‡ dane tÄ… samÄ… â€mapÄ…â€ konstruktÃ³w, co jest warunkiem bazowym dalszych krokÃ³w.\nRÃ³wnowaÅ¼noÅ›Ä‡ metryczna (metric invariance, nazywana teÅ¼ â€sÅ‚abÄ…â€) dodaje ograniczenie rÃ³wnoÅ›ci Å‚adunkÃ³w czynnikowych miÄ™dzy grupami: group.equal = \"loadings\". Oznacza to, Å¼e relacja miÄ™dzy czynnikiem a wskaÅºnikiem ma tÄ™ samÄ… siÅ‚Ä™ w obu grupach. JeÅ¼eli metrycznoÅ›Ä‡ siÄ™ utrzymuje, moÅ¼na porÃ³wnywaÄ‡ zaleÅ¼noÅ›ci strukturalne miÄ™dzy konstruktami w grupach, na przykÅ‚ad porÃ³wnywaÄ‡ regresje (Å›cieÅ¼ki) lub korelacje miÄ™dzy czynnikami, bo skala czynnika jest w obu grupach â€taka samaâ€ w sensie jednostki miary.\nRÃ³wnowaÅ¼noÅ›Ä‡ skalowa (scalar invariance, â€silnaâ€) narzuca rÃ³wnoÅ›Ä‡ zarÃ³wno Å‚adunkÃ³w, jak i przeciÄ™Ä‡ (intercepts): group.equal = c(\"loadings\",\"intercepts\"). PrzeciÄ™cie jest poziomem oczekiwanej odpowiedzi na pozycjÄ™, gdy czynnik latentny ma wartoÅ›Ä‡ 0. JeÅ¼eli przeciÄ™cia sÄ… rÃ³wne, oznacza to, Å¼e przy tym samym poziomie cechy latentnej osoby z obu grup majÄ… tÄ™ samÄ… â€bazowÄ…â€ tendencjÄ™ do udzielania odpowiedzi. Ten poziom rÃ³wnowaÅ¼noÅ›ci jest kluczowy, aby porÃ³wnywaÄ‡ Å›rednie czynnikÃ³w latentnych miÄ™dzy grupami. Bez rÃ³wnowaÅ¼noÅ›ci skalowej rÃ³Å¼nice Å›rednich mogÅ‚yby wynikaÄ‡ z systematycznego przesuniÄ™cia pozycji, a nie z realnych rÃ³Å¼nic w konstrukcie.\nRÃ³wnowaÅ¼noÅ›Ä‡ Å›cisÅ‚a (strict invariance) dodatkowo wymusza rÃ³wnoÅ›Ä‡ wariancji resztowych (bÅ‚Ä™dÃ³w pomiaru) wskaÅºnikÃ³w: group.equal = c(\"loadings\",\"intercepts\",\"residuals\"). Oznacza to, Å¼e precyzja pomiaru pozycji jest taka sama w obu grupach. Jest to najsilniejsze i najtrudniejsze do speÅ‚nienia zaÅ‚oÅ¼enie; bywa wymagane, jeÅ›li chce siÄ™ porÃ³wnywaÄ‡ wyniki obserwowalne (sumy/Å›rednie) wprost miÄ™dzy grupami lub gdy chce siÄ™ wykazaÄ‡ peÅ‚nÄ… porÃ³wnywalnoÅ›Ä‡ pomiaru, ale w praktyce wiele badaÅ„ zatrzymuje siÄ™ na poziomie skalowym.\n\nKod# rÃ³wnowaÅ¼noÅ›Ä‡ konfiguracyjna \nfit_config &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", std.lv = TRUE)\n\n# rÃ³wnowaÅ¼noÅ›Ä‡ metryczna (rÃ³wne Å‚adunki czynnikowe)\nfit_metric &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = \"loadings\", std.lv = TRUE)\n\n# rÃ³wnowaÅ¼noÅ›Ä‡ skalowa (rÃ³wne Å‚adunki i przeciÄ™cia)\nfit_scalar &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = c(\"loadings\", \"intercepts\"), std.lv = TRUE)\n\n# rÃ³wnowaÅ¼noÅ›Ä‡ Å›cisÅ‚a (Å‚adunki, przeciÄ™cia i bÅ‚Ä™dy pomiarowe)\nfit_strict &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = c(\"loadings\", \"intercepts\", \"residuals\"), std.lv = TRUE)\n\n# PorÃ³wnania dopasowania\nanova(fit_config, fit_metric, fit_scalar, fit_strict)\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nfit_config 124 246657 247240 3035.8                                           \nfit_metric 134 246700 247214 3099.0     63.216 0.037427      10  8.880e-10 ***\nfit_scalar 144 246922 247366 3340.7    241.727 0.078100      10  &lt; 2.2e-16 ***\nfit_strict 157 246967 247321 3411.6     70.901 0.034240      13  5.477e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nKodfitMeasures(fit_config, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.886 0.079 0.054 \n\nKodfitMeasures(fit_metric, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.884 0.076 0.056 \n\nKodfitMeasures(fit_scalar, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.875 0.076 0.058 \n\nKodfitMeasures(fit_strict, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.873 0.074 0.059 \n\n\nPowyÅ¼sze wyniki uwidaczniajÄ… siÄ™ dwie rzeczy: (1) testy rÃ³Å¼nicowe \\(\\chi^2\\) wskazujÄ… na â€istotneâ€ pogorszenie dopasowania po kaÅ¼dym zaostrzeniu ograniczeÅ„, oraz (2) zmiany wskaÅºnikÃ³w przybliÅ¼onego dopasowania (CFI/RMSEA/SRMR) sÄ… maÅ‚e. W praktyce, przy duÅ¼ych prÃ³bach, testy \\(\\chi^2\\) sÄ… bardzo czuÅ‚e i czÄ™sto â€wychodzÄ… istotneâ€ nawet wtedy, gdy pogorszenie dopasowania jest znikome z punktu widzenia zastosowaÅ„. Dlatego w analizie rÃ³wnowaÅ¼noÅ›ci pomiaru zwykle interpretuje siÄ™ oba typy informacji, ale wiÄ™kszÄ… wagÄ™ przykÅ‚ada siÄ™ do zmian CFI, RMSEA i SRMR.\nNajpierw warto oceniÄ‡ dopasowanie modelu bazowego (konfiguracyjnego), bo ono wyznacza â€punkt startowyâ€ caÅ‚ej procedury. U nas fit_config ma CFI = 0.886, RMSEA = 0.079, SRMR = 0.054. To oznacza dopasowanie umiarkowane: SRMR jest akceptowalne, natomiast CFI jest wyraÅºnie poniÅ¼ej 0.90, a RMSEA jest na granicy wartoÅ›ci, ktÃ³re czÄ™sto uznaje siÄ™ za â€przeciÄ™tneâ€ (okoÅ‚o 0.08). Wniosek jest taki, Å¼e sama struktura pomiarowa w obu grupach dziaÅ‚a podobnie (bo model siÄ™ estymuje i ma umiarkowane dopasowanie), ale model nie opisuje danych idealnie juÅ¼ na poziomie bazowym. To nie przekreÅ›la testu rÃ³wnowaÅ¼noÅ›ci, ale kaÅ¼e ostroÅ¼nie formuÅ‚owaÄ‡ wnioski: rÃ³wnowaÅ¼noÅ›Ä‡ moÅ¼na uznaÄ‡ za wspieranÄ…, natomiast jakoÅ›Ä‡ samego modelu pomiarowego nadal moÅ¼e wymagaÄ‡ dopracowania.\nPrzejÅ›cie z rÃ³wnowaÅ¼noÅ›ci konfiguracyjnej do metrycznej (rÃ³wne Å‚adunki) daje: CFI 0.886 â†’ 0.884 (spadek \\(\\Delta CFI = âˆ’0.002\\)), RMSEA 0.079 â†’ 0.076 (\\(\\Delta RMSEA = âˆ’0.003\\)), SRMR 0.054 â†’ 0.056 (\\(\\Delta SRMR = 0.002\\)). To sÄ… zmiany bardzo maÅ‚e, czyli Å‚adunki czynnikowe moÅ¼na traktowaÄ‡ jako zbliÅ¼one w obu grupach, a wiÄ™c jednostka miary czynnika jest porÃ³wnywalna. Istotny test \\(\\chi^2\\) rÃ³Å¼nic (\\(p \\ll 0.001\\)) prawdopodobnie wynika tu gÅ‚Ã³wnie z czuÅ‚oÅ›ci testu, bo wskaÅºniki przybliÅ¼onego dopasowania prawie siÄ™ nie zmieniajÄ….\nPrzejÅ›cie z metrycznej do skalowej (rÃ³wne Å‚adunki i przeciÄ™cia) daje: CFI 0.884 â†’ 0.875 (\\(\\Delta CFI = âˆ’0.009\\)), RMSEA 0.076 â†’ 0.076 (\\(\\Delta RMSEA \\approx 0.000\\)), SRMR 0.056 â†’ 0.058 (\\(\\Delta SRMR = 0.002\\)). To jest najbardziej â€kosztownyâ€ krok, co jest typowe, bo przeciÄ™cia czÄ™sto rÃ³Å¼niÄ… siÄ™ miÄ™dzy grupami. JednoczeÅ›nie spadek CFI o 0.009 nadal jest niewielki i zwykle mieÅ›ci siÄ™ w praktycznych kryteriach akceptacji. W efekcie moÅ¼na powiedzieÄ‡, Å¼e rÃ³wnowaÅ¼noÅ›Ä‡ skalowa jest w przybliÅ¼eniu speÅ‚niona, czyli porÃ³wnywanie Å›rednich latentnych miÄ™dzy pÅ‚ciami ma metodologiczne uzasadnienie (z zastrzeÅ¼eniem, Å¼e model bazowy nie jest jakoÅ› Å›wietnie dopasowany).\nPrzejÅ›cie do rÃ³wnowaÅ¼noÅ›ci Å›cisÅ‚ej (dodatkowo rÃ³wne reszty) daje: CFI 0.875 â†’ 0.873 (\\(\\Delta CFI = âˆ’0.002\\)), RMSEA 0.076 â†’ 0.074 (\\(\\Delta RMSEA = âˆ’0.002\\)), SRMR 0.058 â†’ 0.059 (\\(\\Delta SRMR = 0.001\\)). Zmiany sÄ… minimalne, wiÄ™c takÅ¼e Å›cisÅ‚oÅ›Ä‡ wyglÄ…da na akceptowalnÄ… w sensie praktycznym. Oznacza to, Å¼e precyzja pomiaru wskaÅºnikÃ³w (wariancje reszt) jest podobna w obu grupach, co wzmacnia argument o porÃ³wnywalnoÅ›ci narzÄ™dzia.\nAIC i BIC rosnÄ… wraz z narzucaniem ograniczeÅ„, co jest normalne, bo modele stajÄ… siÄ™ bardziej restrykcyjne; w testach rÃ³wnowaÅ¼noÅ›ci i tak kluczowe sÄ… zmiany dopasowania oraz interpretacja merytoryczna, a nie wybÃ³r â€najlepszegoâ€ modelu wedÅ‚ug AIC/BIC. Wynik anova mÃ³wi tu przede wszystkim, Å¼e kaÅ¼de dodatkowe ograniczenie ma wykrywalny koszt w \\(\\chi^2\\), ale wskaÅºniki przybliÅ¼onego dopasowania sugerujÄ…, Å¼e koszt jest maÅ‚y.\nPodsumowujÄ…c, mamy przesÅ‚anki do przyjÄ™cia rÃ³wnowaÅ¼noÅ›ci metrycznej, a skalowÄ… i Å›cisÅ‚Ä… moÅ¼esz uznaÄ‡ za speÅ‚nione w sensie przybliÅ¼onym (praktycznym). JednoczeÅ›nie warto zaznaczyÄ‡ w raporcie, Å¼e dopasowanie modelu konfiguracyjnego jest umiarkowane (CFI &lt; 0.90), wiÄ™c wnioski o rÃ³wnowaÅ¼noÅ›ci sÄ… tak dobre, jak dobry jest sam model pomiarowy. JeÅ¼eli chcesz wzmocniÄ‡ czÄ™Å›Ä‡ pomiarowÄ…, sensownie jest przejrzeÄ‡ reszty i wskaÅºniki modyfikacji osobno w grupach, a w razie potrzeby zastosowaÄ‡ rÃ³wnowaÅ¼noÅ›Ä‡ czÄ™Å›ciowÄ…, czyli zwolniÄ‡ pojedyncze intercepty lub Å‚adunki, ktÃ³re najbardziej â€psujÄ…â€ krok skalowy, zamiast odrzucaÄ‡ porÃ³wnywalnoÅ›Ä‡ caÅ‚ego narzÄ™dzia.\nNa koniec utworzymy skale, ktÃ³re moÅ¼na Å‚atwo intepretowaÄ‡ przez osoby, ktÃ³re nie znajÄ… narzÄ™dzia i jego konstrukcji dobrze.\n\nKod# --- KLUCZE SKAL (dokÅ‚adnie jak w modelu) ---\nzap_items &lt;- c(\"st27q01\",\"st27q03\",\"st27q05\",\"st27q07\")                 # ZapamiÄ™tywanie\nopr_items &lt;- c(\"st27q04\",\"st27q08\",\"st27q10\",\"st27q12\")                 # Opracowywanie\nkon_items &lt;- c(\"st27q02\",\"st27q06\",\"st27q09\",\"st27q11\",\"st27q13\")       # Kontrola\n\n# Uwaga: jeÅ›li jakieÅ› pozycje wymagajÄ… odwrÃ³cenia, zastosowaÄ‡ recoding przed agregacjÄ….\n\nscores_simple &lt;- pisaspa %&gt;%\n  mutate(\n    Zapam_mean = rowMeans(across(all_of(zap_items)), na.rm = TRUE),\n    Oprac_mean = rowMeans(across(all_of(opr_items)), na.rm = TRUE),\n    Kontr_mean = rowMeans(across(all_of(kon_items)), na.rm = TRUE),\n    Zapam_sum  = rowSums(across(all_of(zap_items)), na.rm = TRUE),\n    Oprac_sum  = rowSums(across(all_of(opr_items)), na.rm = TRUE),\n    Kontr_sum  = rowSums(across(all_of(kon_items)), na.rm = TRUE)\n  )\n\n# Wyniki czynnikowe â€“ metoda Bartletta (bardziej \"czyste\" wobec bÅ‚Ä™dÃ³w specyficznych)\nfs_bartlett &lt;- lavPredict(fit_cfa_spa, method = \"Bartlett\")\n\n# ZÅ‚oÅ¼yÄ‡ do wspÃ³lnej ramki (zachowujÄ…c ewentualne zmienne grupujÄ…ce)\nscores_latent &lt;- cbind(\n  pisaspa %&gt;% select(any_of(c(\"sex\",\"age\"))),\n  as.data.frame(fs_bartlett)\n)\n\n\n\nKodto_sten &lt;- function(z){\n  # transformacja przybliÅ¼ona; wyniki przyciÄ™te do 1..10\n  s &lt;- round(2*z + 5.5)\n  pmin(10, pmax(1, s))\n}\n\n# Percentyle z ECDF\nperc_ecdf &lt;- function(x) round(ecdf(x)(x)*100, 1)\n\n# --- NORMY GLOBALNE dla wynikÃ³w latentnych Bartletta ---\nlatent_names &lt;- c(\"ZapamiÄ™tywanie\",\"Opracowywanie\",\"Kontrola\")\n\nnorms_global &lt;- scores_latent %&gt;%\n  mutate(\n    across(all_of(latent_names), scale, .names = \"{.col}_z\") %&gt;% as.data.frame()\n  )\n\n# Dla wygody wylicz stens, percentyle dla kaÅ¼dej skali latentnej\nfor(lat in latent_names){\n  zcol &lt;- paste0(lat, \"_z\")\n  norms_global[[paste0(lat,\"_sten\")]]    &lt;- to_sten(norms_global[[zcol]])\n  norms_global[[paste0(lat,\"_pct\")]]     &lt;- perc_ecdf(scores_latent[[lat]])\n}\n\n# PodglÄ…d wybranych kolumn\nhead(norms_global %&gt;% select(any_of(c(\"sex\",\"age\")),\n                             ends_with(\"_z\"),\n                             ends_with(\"_sten\"),\n                             ends_with(\"_pct\")), n = 20) %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 2) %&gt;% \n  tab_options(\n    table.font.size = px(10), \n  )\n\n\n\n\n\nsex\nZapamiÄ™tywanie_z\nOpracowywanie_z\nKontrola_z\nZapamiÄ™tywanie_sten\nOpracowywanie_sten\nKontrola_sten\nZapamiÄ™tywanie_pct\nOpracowywanie_pct\nKontrola_pct\n\n\n\nf\n0.86\nâˆ’0.65\n0.20\n7.00\n4.00\n6.00\n78.90\n27.10\n55.00\n\n\nm\nâˆ’0.51\nâˆ’0.38\nâˆ’0.69\n4.00\n5.00\n4.00\n32.40\n34.80\n22.80\n\n\nm\n0.24\n0.74\n0.26\n6.00\n7.00\n6.00\n58.90\n77.30\n57.70\n\n\nm\nâˆ’1.23\n2.44\n1.69\n3.00\n10.00\n9.00\n11.50\n98.30\n96.50\n\n\nm\nâˆ’0.18\n0.35\n1.09\n5.00\n6.00\n8.00\n43.60\n65.10\n87.10\n\n\nm\nâˆ’0.25\nâˆ’0.34\n0.30\n5.00\n5.00\n6.00\n40.70\n37.50\n58.40\n\n\nf\nâˆ’0.70\nâˆ’1.04\n0.55\n4.00\n3.00\n7.00\n24.80\n17.30\n67.70\n\n\nf\n0.24\n1.07\nâˆ’0.93\n6.00\n8.00\n4.00\n58.90\n86.10\n17.30\n\n\nf\n0.31\n0.63\n1.69\n6.00\n7.00\n9.00\n61.60\n72.10\n97.20\n\n\nf\n0.70\n1.74\n0.16\n7.00\n9.00\n6.00\n73.80\n95.10\n54.30\n\n\nf\nâˆ’1.06\nâˆ’1.73\nâˆ’1.36\n3.00\n2.00\n3.00\n14.50\n0.90\n10.30\n\n\nm\n0.12\n2.16\n1.69\n6.00\n10.00\n9.00\n54.10\n98.20\n98.80\n\n\nf\n0.96\n0.99\n1.69\n7.00\n7.00\n9.00\n80.90\n81.80\n96.50\n\n\nm\n0.03\n1.32\n0.95\n6.00\n8.00\n7.00\n51.20\n89.00\n81.90\n\n\nm\nâˆ’0.25\n0.04\n0.11\n5.00\n6.00\n6.00\n41.20\n53.30\n50.10\n\n\nm\nâˆ’1.32\nâˆ’0.03\nâˆ’1.37\n3.00\n5.00\n3.00\n9.50\n51.80\n8.40\n\n\nm\nâˆ’1.74\nâˆ’0.72\nâˆ’0.88\n2.00\n4.00\n4.00\n4.60\n25.20\n18.30\n\n\nm\nâˆ’0.94\nâˆ’0.34\nâˆ’1.37\n4.00\n5.00\n3.00\n19.00\n40.00\n9.70\n\n\nf\nâˆ’1.79\nâˆ’0.76\nâˆ’0.24\n2.00\n4.00\n5.00\n4.30\n23.30\n37.20\n\n\nf\nâˆ’0.56\n0.46\nâˆ’0.09\n4.00\n6.00\n5.00\n28.90\n68.90\n43.70\n\n\n\n\n\nKod# --- NORMY WEDÅUG PÅCI (grupowe) ---\nnorms_by_sex &lt;- scores_latent %&gt;%\n  group_by(sex) %&gt;%\n  mutate(\n    # z-score wewnÄ…trz pÅ‚ci\n    across(all_of(latent_names),\n           ~ as.numeric(scale(.x)), .names = \"{.col}_z_g\"),\n    # stens wewnÄ…trz pÅ‚ci\n    across(ends_with(\"_z_g\"),\n           ~ to_sten(.x), .names = \"{.col}_sten\"),\n  ) %&gt;%\n  # percentyle z ECDF wewnÄ…trz pÅ‚ci\n  mutate(\n    across(all_of(latent_names),\n           ~ perc_ecdf(.x), .names = \"{.col}_pct_g\")\n  ) %&gt;%\n  ungroup()\n\n# PodglÄ…d\nhead(norms_by_sex %&gt;%\n       select(sex, starts_with(\"ZapamiÄ™tywanie_\"),\n                    starts_with(\"Opracowywanie_\"),\n                    starts_with(\"Kontrola_\")), n = 20)%&gt;% \n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 2) %&gt;% \n  tab_options(\n    table.font.size = px(10), \n  )\n\n\n\n\n\nsex\nZapamiÄ™tywanie_z_g\nZapamiÄ™tywanie_z_g_sten\nZapamiÄ™tywanie_pct_g\nOpracowywanie_z_g\nOpracowywanie_z_g_sten\nOpracowywanie_pct_g\nKontrola_z_g\nKontrola_z_g_sten\nKontrola_pct_g\n\n\n\nf\n0.82\n7.00\n77.70\nâˆ’0.61\n4.00\n29.20\n0.06\n6.00\n49.30\n\n\nm\nâˆ’0.42\n5.00\n35.50\nâˆ’0.42\n5.00\n32.10\nâˆ’0.53\n4.00\n27.40\n\n\nm\n0.31\n6.00\n61.70\n0.69\n7.00\n76.30\n0.38\n6.00\n63.20\n\n\nm\nâˆ’1.12\n3.00\n13.50\n2.39\n10.00\n97.90\n1.76\n9.00\n97.10\n\n\nm\nâˆ’0.10\n5.00\n46.00\n0.31\n6.00\n63.20\n1.19\n8.00\n89.50\n\n\nm\nâˆ’0.17\n5.00\n43.30\nâˆ’0.38\n5.00\n34.70\n0.43\n6.00\n63.80\n\n\nf\nâˆ’0.80\n4.00\n21.50\nâˆ’1.00\n3.00\n18.60\n0.43\n6.00\n63.20\n\n\nf\n0.18\n6.00\n56.00\n1.12\n8.00\n86.40\nâˆ’1.15\n3.00\n13.40\n\n\nf\n0.25\n6.00\n59.00\n0.67\n7.00\n73.40\n1.64\n9.00\n97.00\n\n\nf\n0.65\n7.00\n71.90\n1.79\n9.00\n95.40\n0.02\n6.00\n48.50\n\n\nf\nâˆ’1.18\n3.00\n12.20\nâˆ’1.70\n2.00\n1.10\nâˆ’1.61\n2.00\n7.00\n\n\nm\n0.19\n6.00\n56.80\n2.11\n10.00\n97.90\n1.76\n9.00\n98.90\n\n\nf\n0.92\n7.00\n79.90\n1.03\n8.00\n82.50\n1.64\n9.00\n95.90\n\n\nm\n0.10\n6.00\n53.60\n1.28\n8.00\n88.60\n1.05\n8.00\n85.40\n\n\nm\nâˆ’0.17\n5.00\n43.80\n0.00\n5.00\n51.50\n0.24\n6.00\n55.40\n\n\nm\nâˆ’1.21\n3.00\n11.30\nâˆ’0.07\n5.00\n49.90\nâˆ’1.19\n3.00\n10.60\n\n\nm\nâˆ’1.62\n2.00\n6.20\nâˆ’0.76\n4.00\n23.40\nâˆ’0.71\n4.00\n22.10\n\n\nm\nâˆ’0.84\n4.00\n21.80\nâˆ’0.38\n5.00\n38.00\nâˆ’1.19\n3.00\n12.70\n\n\nf\nâˆ’1.94\n2.00\n2.80\nâˆ’0.72\n4.00\n25.20\nâˆ’0.41\n5.00\n31.70\n\n\nf\nâˆ’0.66\n4.00\n25.70\n0.50\n7.00\n70.40\nâˆ’0.25\n5.00\n38.10\n\n\n\n\n\n\n\n\n\n\n\nBollen, Kenneth A. 1989. â€Structural Equation Models with Observed Variablesâ€. Structural Equations with Latent Variables, kwiecieÅ„, 80â€“150. https://doi.org/10.1002/9781118619179.ch4.\n\n\nHuang, Yafei, i Peter M. Bentler. 2015. â€Behavior of Asymptotically Distribution Free Test Statistics in Covariance Versus Correlation Structure Analysisâ€. Structural Equation Modeling: A Multidisciplinary Journal 22 (4): 489â€“503. https://doi.org/10.1080/10705511.2014.954078.\n\n\nKILIÃ‡, Abdullah, Ä°brahim UYSAL, i Burcu ATAR. 2020. â€Comparison of Confirmatory Factor Analysis Estimation Methods on Binary Dataâ€. International Journal of Assessment Tools in Education 7 (3): 451â€“87. https://doi.org/10.21449/ijate.660353.\n\n\nKyriazos, Theodoros, i Mary Poga-Kyriazou. 2023. â€Applied Psychometrics: Estimator Considerations in Commonly Encountered Conditions in CFA, SEM, and EFA Practiceâ€. Psychology 14 (05): 799â€“828. https://doi.org/10.4236/psych.2023.145043.\n\n\nLatan, Hengky, i Richard Noonan, red. 2017. Partial Least Squares Path Modeling. Springer International Publishing. https://doi.org/10.1007/978-3-319-64069-3.\n\n\nLi, Cheng-Hsien. 2015. â€Confirmatory Factor Analysis with Ordinal Data: Comparing Robust Maximum Likelihood and Diagonally Weighted Least Squaresâ€. Behavior Research Methods 48 (3): 936â€“49. https://doi.org/10.3758/s13428-015-0619-7.\n\n\nâ€”â€”â€”. 2021. â€Statistical Estimation of Structural Equation Models with a Mixture of Continuous and Categorical Observed Variablesâ€. Behavior Research Methods 53 (5): 2191â€“2213. https://doi.org/10.3758/s13428-021-01547-z.\n\n\nSchweizer, Karl, i Christine DiStefano, red. 2016. Principles and Methods of Test Construction. Hogrefe Publishing. https://doi.org/10.1027/00449-000.\n\n\nSpearman, C. 1961. â€\"General Intelligence\" Objectively Determined and Measured.â€ W, 59â€“73. Appleton-Century-Crofts. https://doi.org/10.1037/11491-006.\n\n\nâ€Supplemental Material for The Performance of ML, DWLS, and ULS Estimation With Robust Corrections in Structural Equation Models With Ordinal Variablesâ€. 2016. Psychological Methods. https://doi.org/10.1037/met0000093.supp.\n\n\nTarka, Piotr. 2017. â€An Overview of Structural Equation Modeling: Its Beginnings, Historical Development, Usefulness and Controversies in the Social Sciencesâ€. Quality & Quantity 52 (1): 313â€“54. https://doi.org/10.1007/s11135-017-0469-8.\n\n\nThurstone, L. L. 1931. â€Multiple Factor Analysis.â€ Psychological Review 38 (5): 406â€“27. https://doi.org/10.1037/h0069792.\n\n\nTurney, A. H. 1939. â€Factor Analysis Makes ProgressA Study in Factor Analysis: The Stability of a Bi-Factor Solution. Karl J. Holzinger , Frances Swinefordâ€. The School Review 47 (9): 709â€“11. https://doi.org/10.1086/440440.\n\n\nWright, Sewall. 1934. â€The Method of Path Coefficientsâ€. The Annals of Mathematical Statistics 5 (3): 161â€“215. https://doi.org/10.1214/aoms/1177732676.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Metody redukcji wymiarowoÅ›ci",
    "section": "",
    "text": "PCA (Pearson 1901)\nHistoria metod redukcji wymiarowoÅ›ci jest Å›ciÅ›le zwiÄ…zana z rozwojem statystyki, psychometrii, a nastÄ™pnie uczenia maszynowego i eksploracji danych. JuÅ¼ na poczÄ…tku XX wieku zaczÄ™to poszukiwaÄ‡ narzÄ™dzi pozwalajÄ…cych na uproszczenie zÅ‚oÅ¼onych zbiorÃ³w danych, w ktÃ³rych liczba zmiennych byÅ‚a zbyt duÅ¼a, aby daÅ‚o siÄ™ je analizowaÄ‡ bezpoÅ›rednio. GÅ‚Ã³wnym celem byÅ‚o uchwycenie istotnych wzorcÃ³w i zaleÅ¼noÅ›ci przy zachowaniu moÅ¼liwie duÅ¼ej iloÅ›ci informacji.\nJednym z pierwszych i do dziÅ› najczÄ™Å›ciej stosowanych podejÅ›Ä‡ jest analiza gÅ‚Ã³wnych skÅ‚adowych (Principal Component Analysis, PCA). Jej poczÄ…tki siÄ™gajÄ… pracy Karla Pearsona z 1901 roku, ktÃ³ry zaproponowaÅ‚ metodÄ™ znajdowania â€linii najlepszego dopasowaniaâ€ w przestrzeni wielowymiarowej. ZostaÅ‚a ona nastÄ™pnie rozwiniÄ™ta przez Harolda Hotellinga w latach 30. XX wieku, ktÃ³ry sformalizowaÅ‚ PCA jako metodÄ™ przeksztaÅ‚cania skorelowanych zmiennych w nowy zbiÃ³r nieskorelowanych skÅ‚adowych, uporzÄ…dkowanych wedÅ‚ug wariancji. PCA szybko znalazÅ‚a zastosowanie w psychometrii i naukach spoÅ‚ecznych, a nastÄ™pnie w genetyce, obrazowaniu i ekonomii.\nW latach powojennych, wraz z rozwojem psychologii eksperymentalnej i neuronauk, pojawiÅ‚a siÄ™ potrzeba metod lepiej uchwytujÄ…cych niezaleÅ¼ne ÅºrÃ³dÅ‚a sygnaÅ‚u. DoprowadziÅ‚o to do opracowania analizy niezaleÅ¼nych skÅ‚adowych (Independent Component Analysis, ICA). ChoÄ‡ koncepcje matematyczne stojÄ…ce za ICA siÄ™gajÄ… teorii informacji z poÅ‚owy XX wieku, to metoda zostaÅ‚a sformalizowana dopiero w latach 80. i 90. XX wieku, m.in. dziÄ™ki pracom Jeana-FranÃ§oisa Cardoso czy Aapa HyvÃ¤rinena. ICA staÅ‚a siÄ™ niezwykle uÅ¼yteczna w problemach takich jak separacja ÅºrÃ³deÅ‚ w sygnaÅ‚ach biomedycznych (np. EEG, fMRI), odszumianie danych czy analiza obrazÃ³w.\nRÃ³wnolegle rozwijaÅ‚y siÄ™ metody oparte na odlegÅ‚oÅ›ciach i podobieÅ„stwach, takie jak skalowanie wielowymiarowe (Multidimensional Scaling, MDS). Pierwsze idee pojawiÅ‚y siÄ™ w psychometrii w latach 50., a szczegÃ³lnie w pracach Torgersona i Kruskala. Celem MDS byÅ‚o odwzorowanie obiektÃ³w opisanych macierzÄ… podobieÅ„stw lub odlegÅ‚oÅ›ci w przestrzeni niskowymiarowej w taki sposÃ³b, aby zachowaÄ‡ relacje strukturalne. Metoda ta znalazÅ‚a szerokie zastosowanie w badaniach percepcji, marketingu, biologii oraz w analizie sieci spoÅ‚ecznych.\nOd koÅ„ca XX wieku rozwÃ³j metod redukcji wymiarowoÅ›ci przyspieszyÅ‚, co byÅ‚o zwiÄ…zane z eksplozjÄ… danych wysokowymiarowych w biologii molekularnej, informatyce czy analizie obrazÃ³w. OprÃ³cz klasycznych metod liniowych zaczÄ™to rozwijaÄ‡ techniki nieliniowe, takie jak t-SNE (2008, Laurens van der Maaten i Geoffrey Hinton) czy UMAP (2018, McInnes, Healy i Melville), ktÃ³re pozwalajÄ… zachowaÄ‡ lokalne struktury danych w niskowymiarowej przestrzeni wizualizacji. Metody te zrewolucjonizowaÅ‚y analizÄ™ danych w uczeniu maszynowym i biologii obliczeniowej, np. w analizie danych pojedynczych komÃ³rek.\nDziÅ› redukcja wymiarowoÅ›ci jest nie tylko technikÄ… wspomagajÄ…cÄ… wizualizacjÄ™ danych, lecz takÅ¼e kluczowym elementem przetwarzania wstÄ™pnego w wielu modelach uczenia maszynowego. Od klasycznych metod PCA i MDS po nowoczesne techniki oparte na sieciach neuronowych, takie jak autoenkodery, rozwÃ³j tego obszaru odzwierciedla rosnÄ…cÄ… potrzebÄ™ uproszczenia i interpretacji zÅ‚oÅ¼onoÅ›ci wspÃ³Å‚czesnych danych.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Metody redukcji wymiarowoÅ›ci</span>"
    ]
  },
  {
    "objectID": "pca.html#pca-pearson1901",
    "href": "pca.html#pca-pearson1901",
    "title": "Metody redukcji wymiarowoÅ›ci",
    "section": "",
    "text": "Matematyczna definicja modelu\nPunktem wyjÅ›cia analizy gÅ‚Ã³wnych skÅ‚adowych jest problem odwzorowania wielowymiarowego zbioru danych w przestrzeni o mniejszej liczbie wymiarÃ³w przy moÅ¼liwie minimalnej stracie informacji. W praktyce dÄ…Å¼y siÄ™ do kompresji i odszumiania sygnaÅ‚u, usuwania wspÃ³Å‚liniowoÅ›ci, stabilizacji dalszych modeli (np. regresji), a takÅ¼e do wizualizacji struktur klasowych i gradientÃ³w zmiennoÅ›ci. PrzykÅ‚adowo, dla dwÃ³ch silnie skorelowanych cech pierwsza skÅ‚adowa gÅ‚Ã³wna jest skierowana wzdÅ‚uÅ¼ linii najwiÄ™kszego rozrzutu (blisko prostej \\(y \\approx x\\)), a redukcja do jednego wymiaru zachowuje wiÄ™kszÄ… czÄ™Å›Ä‡ wariancji niÅ¼ dowolna inna projekcja.\nMatematyczna definicja poprzez maksymalizacjÄ™ wariancji i dekompozycjÄ™ spektralnÄ… polega na transformacji scentralizowanej macierzy danych \\(X \\in \\mathbb{R}^{n\\times p}\\) (kaÅ¼dÄ… kolumnÄ™ odjÄ…Ä‡ o jej Å›redniÄ…). Niech \\(\\Sigma=\\frac{1}{n-1}X^\\top X\\) oznacza empirycznÄ… macierz kowariancji. PierwszÄ… skÅ‚adowÄ… wyznaczamy jako kierunek \\(w\\in\\mathbb{R}^{p}\\) rozwiÄ…zujÄ…cy zadanie maksymalizacji wariancji projekcji, czyli maksymalizacji \\(\\mathrm{Var}(Xw)=w^\\top\\Sigma w\\) przy ograniczeniu \\(\\|w\\|_{2}=1\\). Zastosowanie mnoÅ¼nikÃ³w Lagrangeâ€™a prowadzi do warunku stacjonarnoÅ›ci \\(\\Sigma w=\\lambda w\\), a wiÄ™c \\(w\\) jest wektorem wÅ‚asnym \\(\\Sigma\\), zaÅ› \\(\\lambda\\) jest odpowiadajÄ…cÄ… mu wartoÅ›ciÄ… wÅ‚asnÄ…. Wybieramy najwiÄ™kszÄ… wartoÅ›Ä‡ wÅ‚asnÄ… \\(\\lambda_{1}\\) i jej wektor \\(w_{1}\\), wÃ³wczas wariancja pierwszych wynikÃ³w projekcji \\(z_{1}=Xw_{1}\\) rÃ³wna siÄ™ \\(\\lambda_{1}\\). Kolejne skÅ‚adowe otrzymujemy analogicznie jako rozwiÄ…zania tego samego problemu z dodatkowymi ograniczeniami ortogonalnoÅ›ci \\(w_{j}^\\top w_{k}=0\\) dla \\(k&lt;j\\), co ustawia kolejne wektory wÅ‚asne \\(\\Sigma\\) w porzÄ…dku malejÄ…cych wartoÅ›ci wÅ‚asnych \\(\\lambda_{1}\\ge \\lambda_{2}\\ge \\dots \\ge \\lambda_{p}\\). Wektor wynikÃ³w projekcji \\(z_{j}\\) nazywamy w praktyce scores, a \\(w_{j}\\) â€” wektorem Å‚adunkÃ³w (loadings). Kumulatywny udziaÅ‚ wariancji wyjaÅ›nianej przez pierwsze \\(k\\) skÅ‚adowych wynosi wÃ³wczas \\(\\sum_{j=1}^{k}\\lambda_{j}\\big/\\sum_{j=1}^{p}\\lambda_{j}\\) i sÅ‚uÅ¼y na czÄ™sto do doboru \\(k\\).\nRÃ³wnowaÅ¼ne wyprowadzenie modelu przez rozkÅ‚ad na wartoÅ›ci osobliwe, czyli SVD (ang. Singular Value Decomposition), opiera siÄ™ na faktoryzacji \\(X=UDV^\\top\\), gdzie \\(U\\in\\mathbb{R}^{n\\times r}\\) i \\(V\\in\\mathbb{R}^{p\\times r}\\) majÄ… ortonormalne kolumny, \\(D=\\mathrm{diag}(d_{1},\\dots,d_{r})\\) zawiera uporzÄ…dkowane wartoÅ›ci osobliwe \\(d_{1}\\ge \\dots \\ge d_{r}&gt;0\\), a \\(r=\\mathrm{rank}(X)\\). WÃ³wczas kolumny \\(V\\) pokrywajÄ… siÄ™ (co do znaku) z wektorami Å‚adunkÃ³w \\(w_{j}\\), zaÅ› macierz wynikÃ³w projekcji \\(T=XV\\) rÃ³wna siÄ™ \\(UD\\). ZwiÄ…zek miÄ™dzy oboma podejÅ›ciami jest Å›cisÅ‚y: \\(\\lambda_{j}=d_{j}^{2}/(n-1)\\), a wiÄ™c wariancje skÅ‚adowych odwzorowuje siÄ™ przez kwadraty wartoÅ›ci osobliwych przeskalowane czynnikiem \\(1/(n-1)\\). Projekcja do \\(k\\) wymiarÃ³w przyjmuje wÃ³wczas postaÄ‡ \\(X\\mapsto T_{k}=UD_{k}\\), a rekonstrukcja rzÄ™du \\(k\\) ma postaÄ‡ \\[\nX_{k}=T_{k}V_{k}^\\top=U_{k}D_{k}V_{k}^\\top.\n\\] Z twierdzenia Eckartaâ€“Youngaâ€“Mirskyâ€™ego wynika, Å¼e \\(X_{k}\\) minimalizuje bÅ‚Ä…d Frobeniusa \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\) o rzÄ…dzie co najwyÅ¼ej \\(k\\), czyli PCA daje najlepszÄ… aproksymacjÄ™ niskorangowÄ… w sensie Å›redniokwadratowym (jest to tzw. obciÄ™te SVD). Ta rÃ³wnowaÅ¼noÅ›Ä‡ Å‚Ä…czyÄ‡ dwie intuicje: maksymalizacja przechwyconej wariancji i minimalizacja bÅ‚Ä™du rekonstrukcji.\n\nTwierdzenie 5.1 (Twierdzenie Eckartaâ€“Youngaâ€“Mirsky) Niech \\(X\\in\\mathbb{R}^{n\\times p}\\) i niech \\(r=\\mathrm{rank}(X)\\). Dla \\(k&lt;r\\) niech \\(X_{k}=U_{k}D_{k}V_{k}^\\top\\) bÄ™dzie obciÄ™tym rozkÅ‚adem SVD rzÄ™du \\(k\\). WÃ³wczas \\(X_{k}\\) jest jedynÄ… macierzÄ… o \\(\\mathrm{rank}(X_{k})=k\\), ktÃ³ra minimalizuje bÅ‚Ä…d Frobeniusa1 \\(\\|X-Y\\|_{F}\\) w klasie macierzy \\(Y\\in\\mathbb{R}^{n\\times p}\\) o \\(\\mathrm{rank}(Y)\\le k\\). Ponadto zachodzi rÃ³wnoÅ›Ä‡ \\(\\|X-X_{k}\\|_{F}^{2}=\\sum_{j=k+1}^{r}d_{j}^{2}\\).\n1Â BÅ‚Ä…d Frobeniusa \\(\\|A\\|_{F}\\) macierzy \\(A\\) definiujemy jako \\(\\|A\\|_{F}=\\sqrt{\\sum_{i,j}a_{ij}^{2}}=\\sqrt{\\mathrm{tr}(A^\\top A)}.\\)\nZadania optymalizacyjne wyraÅ¼a siÄ™ zarÃ³wno w wersji wektorowej, jak i macierzowej. Dla pierwszej skÅ‚adowej rozwiÄ…zujemy problem maksymalizacji \\(w^\\top\\Sigma\\) w przy \\(\\|w\\|_{2}=1\\), co prowadzi do najwiÄ™kszej wartoÅ›ci wÅ‚asnej. Dla \\(k\\) skÅ‚adowych poszukujemy macierzy \\(W\\in\\mathbb{R}^{p\\times k}\\) o kolumnach ortonormalnych, ktÃ³ra maksymalizuje \\(\\mathrm{tr}(W^\\top\\Sigma W)\\), skÄ…d wynika wybÃ³r \\(k\\) wektorÃ³w wÅ‚asnych \\(\\Sigma\\). RÃ³wnowaÅ¼nie, szukamy projekcji \\(P=WW^\\top\\) minimalizujÄ…cej bÅ‚Ä…d rekonstrukcji \\(\\|X-XWW^\\top\\|_{F}^{2}\\). W notacji SVD rozwiÄ…zanie ma postaÄ‡ \\(W=V_{k}\\), a wiÄ™c projekcja dziaÅ‚a przez mnoÅ¼enie przez \\(V_{k}V_{k}^\\top\\).\nGdy cechy mierzymy w rÃ³Å¼nych jednostkach i skalach, zaleca siÄ™ stosowaÄ‡ macierz korelacji zamiast kowariancji, co jest rÃ³wnowaÅ¼ne standaryzacji kolumn \\(X\\) do wariancji 1. Wiele implementacji (np. w R funkcja prcomp) wykorzystuje SVD na scentralizowanych i ewentualnie standaryzowanych danych, co zapewnia numerycznÄ… stabilnoÅ›Ä‡, zwÅ‚aszcza gdy \\(p\\gg n\\). W sytuacji \\(p\\gg n\\) korzystniejsze bywa liczenie mniejszych rozkÅ‚adÃ³w: albo dual PCA2 na macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\), albo bezpoÅ›rednio obciÄ™tego SVD. Dla danych zaburzonych wartoÅ›ciami odstajÄ…cymi rozwaÅ¼a siÄ™ wersje odporne, np. zastÄ™puje siÄ™ \\(\\Sigma\\) estymatorem odpornym (ang. Minimum Covariance Determinant, MCD)3 lub stosuje siÄ™ robust PCA i dekompozycje oparte na normie jÄ…dra i normie \\(L_{1}\\)4.\n2Â WÃ³wczas wektory wÅ‚asne \\(\\tilde{w}_{j}\\) macierzy \\(XX^\\top\\in\\mathbb{R}^{n\\times n}\\) (ktÃ³ra jest niÅ¼szego wymiaru niÅ¼ \\(X^\\top X\\) a co za tym idzie lepiej siÄ™ zachowuje numerycznie) przeksztaÅ‚ca siÄ™ w wektory wÅ‚asne \\(\\Sigma\\) przez \\(w_{j}=X^\\top \\tilde{w}_{j}/\\sqrt{(n-1)\\tilde{\\lambda}_{j}}\\), gdzie \\(\\tilde{\\lambda}_{j}\\) jest odpowiadajÄ…cÄ… wartoÅ›ciÄ… wÅ‚asnÄ….3Â Estymator MCD polega na znalezieniu podzbioru \\(h\\) obserwacji (zwykle \\(h \\approx 0.75 n\\)) o najmniejszym wyznaczniku macierzy kowariancji, a nastÄ™pnie obliczeniu Å›redniej i kowariancji na tym podzbiorze. Jest odporny na wartoÅ›ci odstajÄ…ce, poniewaÅ¼ ignoruje obserwacje, ktÃ³re znacznie zwiÄ™kszajÄ… wyznacznik.4Â Metoda ta zakÅ‚ada, Å¼e macierz danych \\(X\\) ma postaÄ‡ \\(X=L+S+E\\), gdzie \\(L\\) jest macierzÄ… niskorangowÄ… (sygnaÅ‚), \\(S\\) jest macierzÄ… rzadkÄ… (wartoÅ›ci odstajÄ…ce), a \\(E\\) jest szumem o maÅ‚ej wariancji. Celem jest odzyskanie \\(L\\) poprzez minimalizacjÄ™ funkcji celu \\(\\|L\\|_{*}+\\lambda\\|S\\|_{1}\\) przy ograniczeniu \\(X=L+S\\), gdzie \\(\\|L\\|_{*}\\) jest normÄ… jÄ…dra (suma wartoÅ›ci osobliwych \\(L\\)), a \\(\\|S\\|_{1}\\) jest normÄ… \\(L_{1}\\) macierzy \\(S\\).PodsumowujÄ…c, PCA moÅ¼na sformuÅ‚owaÄ‡ trojako: jako maksymalizacjÄ™ wariancji projekcji przy ograniczeniach ortogonalnoÅ›ci, jako dekompozycjÄ™ spektralnÄ… macierzy kowariancji oraz jako obciÄ™te SVD zapewniajÄ…ce najlepszÄ… aproksymacjÄ™ niskorangowÄ….\nZaÅ‚oÅ¼enia modelu\nZaÅ‚oÅ¼enia dotyczÄ…ce danych wejÅ›ciowych do analizy gÅ‚Ã³wnych skÅ‚adowych (PCA) sÄ… stosunkowo sÅ‚abe, ale majÄ… istotny wpÅ‚yw na jakoÅ›Ä‡ wynikÃ³w i interpretacjÄ™. MoÅ¼na je podzieliÄ‡ na kilka grup:\n\nStruktura danych\n\nLiniowoÅ›Ä‡ â€“ PCA zakÅ‚ada, Å¼e gÅ‚Ã³wne wzorce zmiennoÅ›ci w danych moÅ¼na uchwyciÄ‡ przez liniowe kombinacje zmiennych wejÅ›ciowych. JeÅ›li zaleÅ¼noÅ›ci sÄ… silnie nieliniowe (np. dane leÅ¼Ä… na zakrzywionej rozmaitoÅ›ci), PCA nie odwzoruje ich poprawnie â€“ lepiej wtedy stosowaÄ‡ kernel PCA albo metody sÄ…siedztwa (np. t-SNE, UMAP).\nWspÃ³Å‚zaleÅ¼noÅ›Ä‡ zmiennych â€“ metoda ma sens tylko wtedy, gdy miÄ™dzy cechami istniejÄ… korelacje. JeÅ›li wszystkie zmienne sÄ… niezaleÅ¼ne, PCA nie zredukuje wymiarÃ³w i kaÅ¼da skÅ‚adowa odpowiadaÄ‡ bÄ™dzie jednej zmiennej.\n\n\nJednostki i skale pomiarowe\n\nPCA jest wraÅ¼liwa na skalÄ™ zmiennych, poniewaÅ¼ opiera siÄ™ na wariancji.\nJeÅ›li cechy mierzone sÄ… w rÃ³Å¼nych jednostkach (np. temperatura w Â°C i masa w kg), naleÅ¼y je standaryzowaÄ‡ (np. do Å›redniej 0 i wariancji 1).\nGdy wszystkie cechy sÄ… w tej samej skali, moÅ¼na pracowaÄ‡ na macierzy kowariancji; w przeciwnym razie lepiej korzystaÄ‡ z macierzy korelacji.\n\n\nRozkÅ‚ad danych\n\nNormalnoÅ›Ä‡ wielowymiarowa nie jest wymagana, ale jeÅ¼eli dane majÄ… rozkÅ‚ad wielowymiarowo normalny, to skÅ‚adowe gÅ‚Ã³wne sÄ… niezaleÅ¼ne (nie tylko nieskorelowane), co upraszcza interpretacjÄ™. Naruszenie zaÅ‚oÅ¼enia o normalnoÅ›ci nie sprawia, Å¼e PCA nie dziaÅ‚a, lecz niezaleÅ¼noÅ›Ä‡ skÅ‚adowych nie jest zagwarantowana.\nBrak wartoÅ›ci odstajÄ…cych â€“ PCA jest bardzo wraÅ¼liwa na outliery, ktÃ³re mogÄ… wpÅ‚ynÄ…Ä‡ na kierunki gÅ‚Ã³wnych skÅ‚adowych, bo opiera siÄ™ na kowariancji. Dlatego dane powinny byÄ‡ oczyszczone lub naleÅ¼y stosowaÄ‡ wersje metody odporne (robust PCA).\n\n\nLiczebnoÅ›Ä‡ prÃ³by\n\nAby oszacowaÄ‡ macierz kowariancji, liczba obserwacji \\(n\\) powinna byÄ‡ odpowiednio duÅ¼a wzglÄ™dem liczby zmiennych \\(p\\).\nGdy \\(p \\gg n\\), klasyczna PCA bywa niestabilna i stosuje siÄ™ wtedy dual PCA albo obciÄ™te SVD.\n\n\nBraki danych - PCA wymaga peÅ‚nej macierzy danych (bez brakÃ³w). W przypadku brakÃ³w stosuje siÄ™ najczÄ™Å›ciej imputacjÄ™ (np. metodÄ… Å›rednich czy metody oparte na modelach).\nInterpretacja graficzna i praktyczna\n\nKod# Pakiety\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(scales)\n\nset.seed(44)\n\n# 1) Dane 2D o eliptycznym rozkÅ‚adzie (silna wspÃ³Å‚zmiennoÅ›Ä‡)\nn  &lt;- 300\nmu &lt;- c(0, 0)\nsd1 &lt;- 2\nsd2 &lt;- 1\nrho &lt;- 0.8\nSigma &lt;- matrix(c(sd1^2, rho*sd1*sd2,\n                  rho*sd1*sd2, sd2^2), nrow = 2, byrow = TRUE)\n\nX &lt;- MASS::mvrnorm(n, mu = mu, Sigma = Sigma) %&gt;%\n  as_tibble(.name_repair = ~c(\"x1\",\"x2\"))\n\n# 2) PCA na danych scentralizowanych (bez standaryzacji)\npca &lt;- prcomp(X, center = TRUE, scale. = FALSE)\n\n# WartoÅ›ci wÅ‚asne i wektory (Å‚adunki)\nlambda &lt;- pca$sdev^2\nV &lt;- pca$rotation    # kolumny: PC1, PC2\ncenter &lt;- colMeans(X)\n\n# 3) Punkty koÅ„cowe wektorÃ³w PC1 i PC2 (skalowaÄ‡ dÅ‚ugoÅ›ciÄ… ~ odchylenie wzdÅ‚uÅ¼ skÅ‚adowej)\n# Skala wektora: k * sd wzdÅ‚uÅ¼ danej skÅ‚adowej (tu k = 2 dla czytelnoÅ›ci)\nk &lt;- 2\npc1_end &lt;- center + k * pca$sdev[1] * V[,1]\npc2_end &lt;- center + k * pca$sdev[2] * V[,2]\n\n# 4) Ramy wykresu i linie osi oryginalnego ukÅ‚adu\nxr &lt;- range(X$x1); yr &lt;- range(X$x2)\n\n# 5) Dane pomocnicze do geometrii\narrows_df &lt;- tribble(\n  ~x,          ~y,          ~xend,        ~yend,     ~label,\n  center[1],   center[2],   pc1_end[1],   pc1_end[2], \"PC1\",\n  center[1],   center[2],   pc2_end[1],   pc2_end[2], \"PC2\"\n)\n\n# Opisy udziaÅ‚u wariancji\nexpl &lt;- percent(lambda / sum(lambda), accuracy = 0.1)\n\n# 6) Wykres\nggplot(X, aes(x = x1, y = x2)) +\n  # chmura punktÃ³w\n  geom_point(alpha = 0.5, size = 1.6) +\n  # elipsa rozrzutu (1 odchylenie standardowe ~ poziom 0.68)\n  stat_ellipse(type = \"norm\", level = 0.68, linewidth = 0.8) +\n  # oryginalne osie ukÅ‚adu wspÃ³Å‚rzÄ™dnych (przez (0,0))\n  geom_hline(yintercept = 0, linetype = 3, linewidth = 0.5) +\n  geom_vline(xintercept = 0, linetype = 3, linewidth = 0.5) +\n  # wektory skÅ‚adowych gÅ‚Ã³wnych (wychodzÄ…ce ze Å›rodka danych)\n  geom_segment(data = arrows_df,\n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")),\n               linewidth = 1) +\n  # etykiety PC z udziaÅ‚em wariancji\n  geom_text(data = arrows_df %&gt;%\n              mutate(txt = ifelse(label==\"PC1\",\n                                  paste0(\"PC1 (\", expl[1], \")\"),\n                                  paste0(\"PC2 (\", expl[2], \")\"))),\n            aes(x = xend, y = yend, label = txt),\n            nudge_x = 0.05, nudge_y = 0.05, hjust = 0, vjust = 0,\n            size = 3.5) +\n  # punkt Å›rodka\n  geom_point(aes(x = center[1], y = center[2]), color = \"black\", size = 2) +\n  coord_fixed() +\n  labs(x = \"x1 (oÅ› oryginalna)\",\n       y = \"x2 (oÅ› oryginalna)\",\n       title = \"Oryginalne osie, dane oraz dwie skÅ‚adowe gÅ‚Ã³wne (2D)\",\n       subtitle = paste0(\"UdziaÅ‚ wariancji: PC1 = \", expl[1], \", PC2 = \", expl[2])) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"plain\"),\n        plot.subtitle = element_text(face = \"plain\"))\n\n\n\n\n\n\n\nDla dwÃ³ch wymiarÃ³w elipsa rozrzutu danych ma osie ustawione dokÅ‚adnie wzdÅ‚uÅ¼ \\(w_{1}\\) i \\(w_{2}\\), a ich dÅ‚ugoÅ›ci proporcjonalne do \\(\\sqrt{\\lambda_{1}}\\) i \\(\\sqrt{\\lambda_{2}}\\). Transformacja do przestrzeni skÅ‚adowych odpowiada obrotowi ukÅ‚adu wspÃ³Å‚rzÄ™dnych tak, by oÅ› \\(X_{1}'\\) leÅ¼aÅ‚a w kierunku najwiÄ™kszego rozrzutu, a \\(X_{2}'\\) â€” w kierunku pozostaÅ‚ej zmiennoÅ›ci. Projekcja do \\(k&lt;p\\) wymiarÃ³w dziaÅ‚a jak rzut ortogonalny na podprzestrzeÅ„ rozpiÄ™tÄ… przez pierwsze \\(k\\) osi i â€spÅ‚aszczenieâ€ w pominiÄ™tych kierunkach, co minimalizuje bÅ‚Ä…d rekonstrukcji w sensie Å›redniokwadratowym. Wykresy scores prezentujÄ… obiekty w przestrzeni skÅ‚adowych gÅ‚Ã³wnych i czÄ™sto ujawniajÄ… skupiska lub obserwacje odstajÄ…ce. Wektory loadings sÄ… przedstawiane na tzw. kole korelacji, gdzie koÅ„ce strzaÅ‚ek leÅ¼Ä… na okrÄ™gu jednostkowym, a ich dÅ‚ugoÅ›ci i kÄ…ty odzwierciedlajÄ… korelacje zmiennych oryginalnych ze skÅ‚adowymi. Zmienne wskazujÄ…ce podobne kierunki tworzÄ… grupy, co pomaga rozumieÄ‡ wspÃ³Å‚zmiennoÅ›Ä‡. Wykres biplot Å‚Ä…czy obie perspektywy: punkty obiektÃ³w i kierunki zmiennych w tej samej pÅ‚aszczyÅºnie, dziÄ™ki czemu moÅ¼na podejrzeÄ‡ jednoczeÅ›nie relacje miÄ™dzy obiektami i kontrybucje cech. Dodatkowo wykres udziaÅ‚u wariancji, czyli scree plot, porzÄ…dkuje \\(\\lambda_{j}\\) i pomagaÄ‡ wyznaczyÄ‡ \\(k\\) przez identyfikacjÄ™ â€Å‚okciaâ€ krzywej lub przez osiÄ…gniÄ™cie zaÅ‚oÅ¼onego poziomu wariancji kumulatywnej.\nÅadunek \\(w_{jk}\\) to wspÃ³Å‚czynnik liniowej kombinacji \\(j\\)-tej skÅ‚adowej dla \\(k\\)-tej zmiennej; jego znak i wartoÅ›Ä‡ bezwzglÄ™dna informujÄ… o kierunku i sile zwiÄ…zku. KorelacjÄ™ zmiennej z \\(j\\)-tÄ… skÅ‚adowÄ… szacujemy jako cosinus kÄ…ta miÄ™dzy wektorem zmiennej a osiÄ… skÅ‚adowej na kole korelacji; duÅ¼e wartoÅ›ci sugerujÄ™ duÅ¼Ä… kontrybucjÄ™ tej cechy do skÅ‚adowej. Rekonstrukcja obiektu \\(i\\)-tego z \\(k\\) skÅ‚adowych ma postaÄ‡ \\(\\hat{x}_{i}=\\sum_{j=1}^{k} t_{ij} \\, w_{j}^\\top\\), gdzie \\(t_{ij} = x_i^\\top w_j\\), co pozwala na analizÄ™ bÅ‚Ä™dÃ³w rekonstrukcji i odszumianie przez odciÄ™cie skÅ‚adowych o maÅ‚ych \\(d_{j}\\). W regresji, gdy predyktory sÄ… wspÃ³Å‚liniowe, stosuje siÄ™ regresjÄ™ na skÅ‚adowych gÅ‚Ã³wnych albo regresjÄ™ grzbietowÄ… w przestrzeni scores, co poprawia stronÄ™ obliczeniowÄ… i zmniejszaÄ‡ wariancjÄ™ estymatorÃ³w.\nKryteria doboru liczby skÅ‚adowych gÅ‚Ã³wnych\nDobÃ³r liczby skÅ‚adowych gÅ‚Ã³wnych (\\(k\\)) jest jednym z kluczowych etapÃ³w analizy PCA, poniewaÅ¼ decyduje o tym, ile informacji (wariancji) zostanie zachowane przy redukcji wymiarowoÅ›ci. Zbyt maÅ‚a liczba skÅ‚adowych prowadzi do utraty istotnych informacji, a zbyt duÅ¼a â€“ do utrzymania szumu i nadmiarowej redundancji. W praktyce stosuje siÄ™ zestaw kryteriÃ³w iloÅ›ciowych i jakoÅ›ciowych, ktÃ³re moÅ¼na podzieliÄ‡ na kilka grup.\nKryteria oparte na wariancji wyjaÅ›nianej\nNajbardziej klasyczne podejÅ›cie polega na analizie udziaÅ‚u wariancji przechwyconej przez pierwsze \\(k\\) skÅ‚adowych. Dla kaÅ¼dej skÅ‚adowej liczy siÄ™ wartoÅ›Ä‡ wÅ‚asnÄ… \\(\\lambda_j\\) macierzy kowariancji, a udziaÅ‚ wariancji wyjaÅ›nianej przez pierwsze \\(k\\) skÅ‚adowych to \\[\n\\eta(k) = \\frac{\\sum_{j=1}^k \\lambda_j}{\\sum_{j=1}^p \\lambda_j}.\n\\] Stosowane reguÅ‚y:\n\nReguÅ‚a progu wariancji - wybiera siÄ™ najmniejsze \\(k\\), dla ktÃ³rego \\(\\eta(k)\\) przekracza ustalony prÃ³g, np. 80%, 90% lub 95% (w literaturze nie ma jednego progu). Gdy dane silnie skorelowane â€“ wystarczÄ… 2â€“3 skÅ‚adowe, a gdy dane sÄ… bardziej zÅ‚oÅ¼one â€“ potrzeba wiÄ™cej (5â€“10 i wiÄ™cej).\nWykres osypiska (scree plot) â€“ wykres wartoÅ›ci wÅ‚asnych \\(\\lambda_j\\) uporzÄ…dkowanych malejÄ…co. Wybiera siÄ™ punkt, w ktÃ³rym tempo spadku gwaÅ‚townie maleje (â€Å‚okieÄ‡ krzywejâ€).\nWskaÅºnik udziaÅ‚u marginalnego - \\(\\Delta \\eta_j = \\eta(j) - \\eta(j-1)\\). Gdy przyrost staje siÄ™ znikomy, dalsze skÅ‚adowe nie wnoszÄ… istotnej informacji.\nKryteria algebraiczne\nKryterium wartoÅ›ci wÅ‚asnej (Kaiseraâ€“Guttmana) oparte jest na macierzy korelacji, ktÃ³re mÃ³wi, Å¼e zachowuje siÄ™ tylko te skÅ‚adowe, ktÃ³rych wartoÅ›ci wÅ‚asne \\(\\lambda_j &gt; 1\\). Oznacza to, Å¼e dana skÅ‚adowa wyjaÅ›nia wiÄ™cej wariancji niÅ¼ pojedyncza standaryzowana zmienna. ReguÅ‚a ta jest prosta, ale czÄ™sto zbyt konserwatywna (tendencja do wyboru zbyt wielu skÅ‚adowych).\nKryteria statystyczne i walidacyjne\n\nAnaliza rÃ³wnolegÅ‚a (Parallel Analysis) - polega na porÃ³wnaniu wartoÅ›ci wÅ‚asnych uzyskanych z danych rzeczywistych z wartoÅ›ciami wÅ‚asnymi uzyskanymi z wielu symulowanych zestawÃ³w danych o tych samych wymiarach, ale z losowym szumem. Zachowuje siÄ™ tylko te skÅ‚adowe, ktÃ³rych wartoÅ›ci wÅ‚asne przekraczajÄ… Å›redniÄ… (lub kwantyl) z rozkÅ‚adu symulowanego. Ta metoda ogranicza ryzyko wyboru skÅ‚adowych wynikajÄ…cych z przypadku.\nWalidacja krzyÅ¼owa (Cross-Validation) - gdy PCA wykorzystuje siÄ™ w kontekÅ›cie modelowania predykcyjnego (np. PCA regression). Wybiera siÄ™ takie \\(k\\), ktÃ³re minimalizuje bÅ‚Ä…d predykcji (np. RMSE) obliczany metodÄ… walidacji krzyÅ¼owej.\n\nBartlettâ€™s Test of Sphericity sprawdza, czy korelacje sÄ… wystarczajÄ…co silne, by PCA miaÅ‚a sens.\n\nBroken Stick Model porÃ³wnuje udziaÅ‚ wariancji kaÅ¼dej skÅ‚adowej z oczekiwanÄ… wartoÅ›ciÄ… przy losowym rozkÅ‚adzie wariancji â€“ zachowuje siÄ™ tylko te skÅ‚adowe, ktÃ³re przekraczajÄ… tÄ™ wartoÅ›Ä‡ (\\(E_k=\\frac{1}{p}\\sum_{j=k}^{p}\\frac{1}{j}\\)).\nKryteria interpretacyjne i dziedzinowe\nCzasami najwaÅ¼niejszy jest nie wynik numeryczny, lecz uÅ¼ytecznoÅ›Ä‡ interpretacyjna:\n\nWybiera siÄ™ tyle skÅ‚adowych, ile da siÄ™ sensownie zinterpretowaÄ‡ (np. odpowiadajÄ…cych znanym procesom fizycznym, ekonomicznym, biologicznym).\nW analizie wizualnej (np. w eksploracji danych) czÄ™sto wybiera siÄ™ 2 lub 3 pierwsze skÅ‚adowe, ktÃ³re umoÅ¼liwiajÄ… wykresy 2D lub 3D.\n\n\n\n\n\n\n\n\n\nKryterium\nOpis\nZalety\nOgraniczenia\n\n\n\nUdziaÅ‚ wariancji (np. â‰¥90%)\nZachowaj tyle skÅ‚adowych, by wyjaÅ›niÄ‡ okreÅ›lony procent caÅ‚kowitej wariancji\nProste i intuicyjne\nWybÃ³r progu bywa arbitralny\n\n\nWykres osypiska (scree plot)\nWybÃ³r punktu â€kolanaâ€ na krzywej wartoÅ›ci wÅ‚asnych\nWizualnie czytelne\nSubiektywne, zaleÅ¼y od interpretacji obserwatora\n\n\nWartoÅ›Ä‡ wÅ‚asna &gt; 1 (Kaiserâ€“Guttman)\nZachowaj skÅ‚adowe, ktÃ³rych wartoÅ›ci wÅ‚asne przekraczajÄ… 1 (dla macierzy korelacji)\nÅatwe obliczeniowo\nCzÄ™sto zbyt liberalne â€“ wybiera zbyt wiele skÅ‚adowych\n\n\nAnaliza rÃ³wnolegÅ‚a (Parallel Analysis)\nPorÃ³wnanie wartoÅ›ci wÅ‚asnych z rozkÅ‚adem uzyskanym z danych losowych\nStatystycznie uzasadnione, ogranicza wybÃ³r przypadkowych komponentÃ³w\nWymaga symulacji lub dedykowanego oprogramowania\n\n\nWalidacja krzyÅ¼owa (Cross-Validation)\nWybÃ³r liczby skÅ‚adowych minimalizujÄ…cej bÅ‚Ä…d predykcji (np. RMSE)\nNajlepsza w kontekÅ›cie modeli predykcyjnych\nKosztowna obliczeniowo, wymaga podziaÅ‚u danych\n\n\nModel Broken Stick\nPorÃ³wnanie udziaÅ‚u wariancji skÅ‚adowych z oczekiwanym rozkÅ‚adem losowym\nUzasadnione teoretycznie, ogranicza przeuczenie\nMniej intuicyjne, rzadziej uÅ¼ywane\n\n\nKryterium interpretacyjne\nWybÃ³r liczby skÅ‚adowych moÅ¼liwych do sensownej interpretacji\nPraktyczne i kontekstowe\nSubiektywne i zaleÅ¼ne od wiedzy dziedzinowej\n\n\n\n\nPrzykÅ‚ad 5.1 (PCA na danych irysÃ³w) Â \n\nKodlibrary(factoextra)\nlibrary(easystats)\nlibrary(gt)\n\npca_iris &lt;- prcomp(iris[,-5], center = TRUE, scale. = TRUE) \n# albo\npca &lt;- principal_components(iris, n = 4, rotate = \"none\") # domyÅ›lnie standaryzuje zmienne\n\n# Wykres osypiska\nfviz_eig(pca_iris, addlabels = TRUE)\n\n\n\n\n\n\n\nJak widaÄ‡ z powyÅ¼szego wykresu osypiska pierwsza skÅ‚adowa wyjaÅ›nia okoÅ‚o 73% caÅ‚kowitej wariancji, a druga 23%. JeÅ›li chcieÄ‡ opieraÄ‡ wybÃ³r liczby skÅ‚adowych gÅ‚Ã³wnych na kryteriach (rÃ³wnieÅ¼ takich, ktÃ³re nie byÅ‚y prezentowane powyÅ¼ej), to moÅ¼na uÅ¼yÄ‡ funkcji n_components() pakietu parameters w ekosystemie easystats.\n\nKodk &lt;- n_components(iris[,-5])\nas.data.frame(k)\n\n   n_Factors              Method       Family\n1          0          Scree (R2)     Scree_SE\n2          1             Bentler      Bentler\n3          1 Optimal coordinates        Scree\n4          1 Acceleration factor        Scree\n5          1   Parallel analysis        Scree\n6          1    Kaiser criterion        Scree\n7          1       Velicer's MAP Velicers_MAP\n8          2          Scree (SE)     Scree_SE\n9          2    VSS complexity 1          VSS\n10         2    VSS complexity 2          VSS\n11         3            Bartlett      Barlett\n12         3            Anderson      Barlett\n13         3              Lawley      Barlett\n\nKodplot(k)\n\n\n\n\n\n\n\nChoÄ‡ wiÄ™kszoÅ›Ä‡ kryteriÃ³w wskazuje na 1 skÅ‚adowÄ…, to na potrzeby przykÅ‚adu wykorzystamy dwie skÅ‚adowe. WyjaÅ›niajÄ… one blisko 96% caÅ‚kowitej wariancji (patrz poniÅ¼ej). MoÅ¼emy teraz przejrzeÄ‡ wyniki PCA, czyli macierz Å‚adunkÃ³w (wektorÃ³w wÅ‚asnych) i macierz wynikÃ³w projekcji (scores).\n\nKod# UdziaÅ‚ wariancji\nsummary(pca_iris)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\nKod# Åadunki (wektory wÅ‚asne)\npca_iris$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\nKod# scores\nhead(pca_iris$x)\n\n           PC1        PC2         PC3          PC4\n[1,] -2.257141 -0.4784238  0.12727962  0.024087508\n[2,] -2.074013  0.6718827  0.23382552  0.102662845\n[3,] -2.356335  0.3407664 -0.04405390  0.028282305\n[4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n[5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n[6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\nPierwsza skÅ‚adowa gÅ‚Ã³wna (PC1) jest kombinacjÄ… liniowÄ… wszystkich czterech zmiennych, przy czym trzy z nich â€” Sepal.Length, Petal.Length oraz Petal.Width â€” majÄ… dodatnie Å‚adunki, natomiast Sepal.Width ma Å‚adunek ujemny. Oznacza to, Å¼e skÅ‚adowa ta roÅ›nie, gdy dÅ‚ugoÅ›Ä‡ dziaÅ‚ki kielicha oraz dÅ‚ugoÅ›Ä‡ i szerokoÅ›Ä‡ pÅ‚atkÃ³w sÄ… duÅ¼e, a maleje, gdy szerokoÅ›Ä‡ dziaÅ‚ki jest duÅ¼a. MoÅ¼na zatem interpretowaÄ‡ PC1 jako wymiar opisujÄ…cy ogÃ³lny rozmiar kwiatu: kwiaty o wiÄ™kszych pÅ‚atkach i wÄ™Å¼szych dziaÅ‚kach uzyskujÄ… wyÅ¼sze wartoÅ›ci tej skÅ‚adowej. W zbiorze iris PC1 bardzo dobrze rozdziela gatunki â€“ setosa charakteryzuje siÄ™ niskimi wartoÅ›ciami tej skÅ‚adowej (krÃ³tkie pÅ‚atki, szerokie dziaÅ‚ki), natomiast versicolor i virginica majÄ… wartoÅ›ci wysokie, co odpowiada wiÄ™kszym rozmiarom kwiatÃ³w.\nDruga skÅ‚adowa gÅ‚Ã³wna (PC2) ma zupeÅ‚nie innÄ… strukturÄ™ Å‚adunkÃ³w. Zdominowana jest przez bardzo silny ujemny wspÃ³Å‚czynnik dla Sepal.Width oraz mniejszy, rÃ³wnieÅ¼ ujemny, dla Sepal.Length. WpÅ‚yw pÅ‚atkÃ³w na tÄ™ skÅ‚adowÄ… jest niewielki. PC2 odzwierciedla zatem zmiennoÅ›Ä‡ w obrÄ™bie ksztaÅ‚tu dziaÅ‚ki kielicha, a zwÅ‚aszcza jej proporcji dÅ‚ugoÅ›ci do szerokoÅ›ci. Kwiaty o wÄ™Å¼szych dziaÅ‚kach majÄ… wyÅ¼sze wartoÅ›ci PC2, natomiast te o szerszych â€“ niÅ¼sze.\nInterpretujÄ…c wspÃ³lnie obie skÅ‚adowe, moÅ¼na stwierdziÄ‡, Å¼e PC1 opisuje rozmiar kwiatu, natomiast PC2 â€“ proporcje i ksztaÅ‚t dziaÅ‚ki. W przestrzeni PC1â€“PC2 dane tworzÄ… ukÅ‚ad, w ktÃ³rym setosa jest wyraÅºnie oddzielona od pozostaÅ‚ych gatunkÃ³w poprzez niskie wartoÅ›ci PC1 i wysokie PC2, a versicolor i virginica rÃ³Å¼niÄ… siÄ™ miÄ™dzy sobÄ… gÅ‚Ã³wnie wzdÅ‚uÅ¼ drugiej osi. W rezultacie te dwie skÅ‚adowe pozwalajÄ… na niemal peÅ‚ne odwzorowanie i wizualne rozdzielenie gatunkÃ³w, przy czym PC1 odpowiada za wymiar wielkoÅ›ciowy, a PC2 â€“ za wymiar ksztaÅ‚towy. Na potrzeby wizualizacji moÅ¼emy narysowaÄ‡ wykres biplot Å‚Ä…czÄ…cy obiekty i zmienne w przestrzeni dwÃ³ch pierwszych skÅ‚adowych.\n\nKodfviz_pca_biplot(pca_iris, repel = TRUE,\n                col.var = \"blue\", # kolor zmiennych\n                col.ind = iris$Species) + # kolor obiektÃ³w wg gatunku\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Metody redukcji wymiarowoÅ›ci</span>"
    ]
  },
  {
    "objectID": "pca.html#ica-comon1994",
    "href": "pca.html#ica-comon1994",
    "title": "Metody redukcji wymiarowoÅ›ci",
    "section": "ICA (Comon 1994)\n",
    "text": "ICA (Comon 1994)\n\nPodstawowy model ICA (ang. Independent Component Analysis) zakÅ‚ada, Å¼e wektor obserwacji \\(X \\in \\mathbb{R}^p\\) powstaje poprzez liniowe i natychmiastowe wymieszanie wektora ukrytych ÅºrÃ³deÅ‚ \\(s \\in \\mathbb{R}^m\\) o statystycznie niezaleÅ¼nych skÅ‚adowych \\[\nX = A s,\\qquad A \\in \\mathbb{R}^{p\\times m},\n\\] przy czym \\(m \\le \\min(p,n)\\) oraz macierz mieszajÄ…ca \\(A\\) ma peÅ‚ny rzÄ…d. Celem jest oszacowanie macierzy demiksujÄ…cej \\(W \\in \\mathbb{R}^{m\\times p}\\) tak, aby \\(y = W X\\) aproksymowaÄ‡ \\(s\\) skÅ‚adowymi moÅ¼liwie niezaleÅ¼nymi w sensie probabilistycznym. Z istoty problemu rozwiÄ…zanie identyfikowalne jest jedynie do permutacji i skalowania - kolejnoÅ›Ä‡ oraz skale (a wiÄ™c i znaki) skÅ‚adowych nie sÄ… odzyskiwalne.\nWyprowadzenie algorytmÃ³w ICA rozpoczynamy od scentralizowania danych i ich whiteningu. Niech \\(\\Sigma_X = \\tfrac{1}{n}\\sum_i (X_i-\\bar X)(X_i-\\bar X)^\\top\\) oraz niech \\(V\\) oznacza macierz whitening takÄ…, Å¼e \\(Z=V(X-\\bar X)\\) speÅ‚nia \\(\\operatorname{Cov}(Z)=I_p\\). W praktyce przyjmujemy \\(V=\\Lambda^{-1/2}U^\\top\\) z dekompozycji \\(\\Sigma_X=U\\Lambda U^\\top\\). W przestrzeni whitened model przyjmuje postaÄ‡ \\[\nZ = V A s \\equiv R\\, s,\n\\] gdzie \\(R\\) jest macierzÄ… ortogonalnÄ… (dla przypadku \\(m=p\\)). Poszukujemy wiÄ™c wektorÃ³w o jednostkowej normie, dla ktÃ³rych skalarna projekcja \\(y=w^\\top Z\\) jest moÅ¼liwie â€nienormalnaâ€ (niesymetryczna lub ciÄ™Å¼koogonowa), co stanowi praktyczne kryterium niezaleÅ¼noÅ›ci.\n\nKod# Pakiety\nlibrary(fastICA)\nlibrary(patchwork)\n\nset.seed(44)\n\n# 1) Generowanie dwÃ³ch niezaleÅ¼nych ÅºrÃ³deÅ‚ (niegaussowskich)\nn &lt;- 3000\ns1 &lt;- rexp(n, rate = 1) - 1  # Jednostronnie ciÄ™Å¼szy ogon (Eksponencjalny przesuniÄ™ty do zera Å›redniej)\ns2 &lt;- runif(n, -2, 2)        # RÃ³wnomierny (pÅ‚askie ogony)\nS  &lt;- cbind(s1, s2)\nS  &lt;- scale(S, center = TRUE, scale = FALSE) # zero-mean dla wygody\n\n# 2) Mieszanie ÅºrÃ³deÅ‚ macierzÄ… A -&gt; obserwacje X\nA &lt;- matrix(c(1, 2,\n              2, 1), nrow = 2, byrow = TRUE)\nX &lt;- S %*% t(A)                        # model X = S A^T\nX &lt;- scale(X, center = TRUE, scale = FALSE)  # centrowanie (odjÄ™cie Å›redniej kolumn)\n\n# 3) Whitening (sferyzacja): Z = V X, gdzie V = Î›^{-1/2} U^T z dekompozycji Î£_X\nSigmaX &lt;- cov(X)\ne &lt;- eigen(SigmaX)\nU &lt;- e$vectors\nLambda &lt;- diag(e$values)\nV &lt;- solve(sqrt(Lambda)) %*% t(U)     # Î›^{-1/2} U^T\nZ &lt;- t(V %*% t(X))                    # Z = V X (w wierszach obserwacje)\n\n# Kontrola: kowariancja Z ~ I\nround(cov(Z), 3)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKod# 4) ICA (FastICA) na danych whitened (moÅ¼na teÅ¼ na X bo fastICA automatycznie wykonuje whitening)\nica &lt;- fastICA(Z, n.comp = 2, method = \"C\")  # odzyskane ÅºrÃ³dÅ‚a Y i mieszanie A_ICA\nY &lt;- ica$S                                   # szacowane ÅºrÃ³dÅ‚a niezaleÅ¼ne (kolumny ~ komponenty)\nW &lt;- ica$K %*% ica$W                         # Å‚Ä…czne \"demiksowanie\" wzglÄ™dem X (tu pracowaliÅ›my na Z)\n# Uwaga: fastICA zwraca teÅ¼ K (whitening) i W (unmixing), skÅ‚adnia zaleÅ¼y od wejÅ›cia\n\n# 5) Przygotowanie danych do wykresÃ³w\nto_df &lt;- function(M, name){\n  as.data.frame(M) |&gt;\n    setNames(c(\"c1\",\"c2\")) |&gt;\n    mutate(stage = name)\n}\ndf_X &lt;- to_df(X, \"X: dane zmieszane\")\ndf_Z &lt;- to_df(Z, \"Z: po whitening\")\ndf_Y &lt;- to_df(Y, \"Y: po ICA (ÅºrÃ³dÅ‚a)\")\n\ndf_all &lt;- bind_rows(df_X, df_Z, df_Y)\n\n# 6) OÅ› ukÅ‚adu i wektory bazowe (do wizualizacji obrotÃ³w)\n#    W przestrzeni Z osie sÄ… juÅ¼ sferyczne (I), wiÄ™c ICA to â€tylkoâ€ obrÃ³t.\naxes_df &lt;- function(scale_len = 2){\n  data.frame(x = c(0,0), y = c(0,0),\n             xend = c(scale_len,0), yend = c(0,scale_len),\n             label = c(\"e1\",\"e2\"))\n}\naxesZ &lt;- axes_df()\n\n# 7) Wykresy: chmury punktÃ³w w 2D (X, Z, Y)\npX &lt;- ggplot(df_X, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8) +\n  coord_equal() +\n  labs(title = \"Przed whiteningiem (X)\",\n       x = \"X[,1]\", y = \"X[,2]\") +\n  theme_minimal()\n\npZ &lt;- ggplot(df_Z, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#2E86DE\") +\n  geom_segment(data = axesZ, aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.18,\"cm\")), linewidth = 0.8) +\n  geom_text(data = axesZ, aes(x = xend, y = yend, label = label),\n            nudge_x = 0.05, nudge_y = 0.05, size = 3) +\n  coord_equal() +\n  labs(title = \"Po whitening (Z): Cov â‰ˆ I\",\n       x = \"Z[,1]\", y = \"Z[,2]\") +\n  theme_minimal()\n\npY &lt;- ggplot(df_Y, aes(c1, c2)) +\n  geom_point(alpha = 0.25, size = 0.8, color = \"#16A085\") +\n  coord_equal() +\n  labs(title = \"Po ICA (Y): odzyskane ÅºrÃ³dÅ‚a niezaleÅ¼ne\",\n       x = \"Y[,1]\", y = \"Y[,2]\") +\n  theme_minimal()\n\n(pX | pZ | pY)\n\n\n\n\n\n\nKod# 8) Dodatkowo: marginesowe histogramy pokazujÄ…ce â€nienormalnoÅ›Ä‡â€\nhX &lt;- df_X |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"X\")\nhZ &lt;- df_Z |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Z\")\nhY &lt;- df_Y |&gt;\n  pivot_longer(c(\"c1\",\"c2\"), names_to = \"col\", values_to = \"val\") |&gt;\n  filter(col %in% c(\"c1\",\"c2\")) |&gt;\n  mutate(stage = \"Y\")\n\nh_all &lt;- bind_rows(hX,hZ,hY) |&gt;\n  mutate(stage = factor(stage, levels = c(\"X\",\"Z\",\"Y\")))\n\nggplot(h_all, aes(val)) +\n  geom_histogram(bins = 60, fill = \"grey70\", color = \"white\") +\n  facet_grid(stage ~ col, scales = \"free_y\") +\n  labs(title = \"Marginalne rozkÅ‚ady: przed whiteningiem, po whitening, po ICA\",\n       x = \"wartoÅ›Ä‡\", y = \"licznoÅ›Ä‡\") +\n  theme_minimal()\n\n\n\n\n\n\nKod# 9) KrÃ³tka kontrola: kowariancje i korelacje\ncat(\"\\nKowariancja X:\\n\"); print(round(cov(X),3))\n\n\nKowariancja X:\n\n\n      [,1]  [,2]\n[1,] 6.295 4.620\n[2,] 4.620 5.123\n\nKodcat(\"\\nKowariancja Z (powinna byÄ‡ bliska I):\\n\"); print(round(cov(Z),3))\n\n\nKowariancja Z (powinna byÄ‡ bliska I):\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nKodcat(\"\\nKorelacje pomiÄ™dzy kolumnami Y (powinny byÄ‡ bliskie 0; niezaleÅ¼noÅ›Ä‡ jest mocniejsza niÅ¼ brak korelacji):\\n\")\n\n\nKorelacje pomiÄ™dzy kolumnami Y (powinny byÄ‡ bliskie 0; niezaleÅ¼noÅ›Ä‡ jest mocniejsza niÅ¼ brak korelacji):\n\nKodprint(round(cor(Y),3))\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nPodejÅ›cie maksymalizujÄ…ce nienormalnoÅ›Ä‡ opiera siÄ™ na kurtozie lub na przybliÅ¼onej negatywnej entropii (ang. negentropy). Dla \\(\\operatorname{Var}(y)=1\\) kurtoza \\(\\kappa(y)=\\mathbb{E}\\{y^4\\}-3\\) przyjmuje wartoÅ›ci 0 dla rozkÅ‚adu normalnego i wartoÅ›ci odlegÅ‚e od zera dla rozkÅ‚adÃ³w nienormalnych. Maksymalizacja \\(|\\kappa(w^\\top Z)|\\) prowadzi do skÅ‚adowych niezaleÅ¼nych. Stabilniejsze i bardziej ogÃ³lne kryterium stanowi negentropy \\(J(y)=H(y_{\\text{gauss}})-H(y)\\), gdzie \\(H\\) oznacza entropiÄ™. W praktyce stosuje siÄ™ aproksymacje postaci \\[\nJ(y)\\approx \\Big(\\mathbb{E}\\,G(y)-\\mathbb{E}\\,G(v)\\Big)^2,\n\\] z dobranÄ… nieliniowoÅ›ciÄ… \\(G\\) oraz \\(v\\sim \\mathcal N(0,1)\\). Maksymalizacja \\(J\\) przy ograniczeniu \\(\\|w\\|=1\\) zapewnia poszukiwanie najbardziej nienormalnych kierunkÃ³w. CzÄ™ste wybory \\(G\\), to\n\n\n\\(G(u)=\\frac{1}{a_1}\\log\\cosh(a_1 u)\\) (np. \\(a_1=1\\)) - uniwersalna,\n\n\\(G(u)=-\\exp(-u^2/2)\\) - dla rozkÅ‚adÃ³w o ciÄ™Å¼kich ogonach,\n\n\\(G(u)=\\frac{1}{4}u^4\\) - dla rozkÅ‚adÃ³w o lekkich ogonach.\n\nZ kryteriÃ³w tych wynika algorytm FastICA jako iteracyjne poszukiwanie staÅ‚ego punktu. Dla jednego komponentu w przestrzeni whitened stosujemy aktualizacjÄ™ \\[\nw \\leftarrow \\mathbb{E}\\{Z\\,g(w^\\top Z)\\}-\\mathbb{E}\\{gâ€™(w^\\top Z)\\}\\, w,\\qquad \\text{nastÄ™pnie } w\\leftarrow \\frac{w}{\\|w\\|},\n\\] gdzie \\(g=Gâ€™\\) jest score function (np. \\(g(u)=\\tanh(u)\\), \\(g(u)=u^3\\) lub \\(g(u)=u\\exp(-u^2/2)\\)). Pierwszy skÅ‚adnik \\(\\mathbb{E}\\{Z\\,g(w^\\top Z)\\}\\) jest â€uogÃ³lnionÄ…â€ wersjÄ… gradientu kryterium niegaussowskoÅ›ci: wzmacnia te kierunki \\(w\\), w ktÃ³rych projekcja \\(w^\\top Z\\) daje rozkÅ‚ad o duÅ¼ej wartoÅ›ci wybranej miary niegaussowskoÅ›ci. Drugi skÅ‚adnik \\(-\\mathbb{E}\\{gâ€™(w^\\top Z)\\}\\,\\) w peÅ‚ni rolÄ™ korekty wynikajÄ…cej z ograniczenia normy oraz z faktu, Å¼e pracuje siÄ™ w przestrzeni whitened. DziÄ™ki temu iteracja nie â€puchnieâ€ wzdÅ‚uÅ¼ tego samego kierunku i ma poprawnÄ… geometriÄ™ z punktu widzenia warunku \\(\\|w\\|=1\\). W praktyce oba wyraÅ¼enia oblicza siÄ™ jako Å›rednie z prÃ³by: wartoÅ›ci oczekiwane \\(\\mathbb{E}[\\cdot]\\) zastÄ™puje siÄ™ Å›rednimi po obserwacjach \\(Z_i\\).\nDla wielu skÅ‚adowych stosujemy rÃ³wnolegÅ‚e aktualizacje i ortogonalizacjÄ™ w kolejnych krokach, np. metodÄ… rzutÃ³w Gramaâ€“Schmidta lub przez dekompozycjÄ™ symetrycznÄ… \\(W\\leftarrow (WW^\\top)^{-1/2}W\\), co zachowuje wzajemnÄ… ortogonalnoÅ›Ä‡ wektorÃ³w w przestrzeni whitened i zapobiega zbieÅ¼noÅ›ci do tej samej skÅ‚adowej.\nAlternatywne wyprowadzenie pochodzi z maksymalizacji funkcji wiarygodnoÅ›ci (maximum likelihood). ZakÅ‚adajÄ…c niezaleÅ¼noÅ›Ä‡ ÅºrÃ³deÅ‚ z gÄ™stoÅ›ciami \\(p_{s_i}\\) i (dla prostoty) brak szumu, otrzymujemy logarytm funkcji wiarygodnoÅ›ci \\[\n\\mathcal L(W)=\\sum_{t=1}^n\\Bigg(\\sum_{i=1}^m \\log p_{s_i}\\big((W X_t)_i\\big)\\Bigg) + n\\log|\\det W|.\n\\] Jej gradient prowadzi do zasady Infomax, ktÃ³ra brzmi: dobraÄ‡ \\(W\\) tak, aby wyjÅ›cia miaÅ‚y jak najwiÄ™kszÄ… sumÄ™ entropii (co przy zachowaniu \\(\\log|\\det W|\\) jest rÃ³wnowaÅ¼ne maksymalizacji wspÃ³lnej niezaleÅ¼noÅ›ci). W praktyce wybÃ³r rodziny \\(p_{s_i}\\) implikuje odpowiednie nieliniowoÅ›ci w regule uczenia, formalnie zbieÅ¼ne z powyÅ¼szymi kontrastami na negentropy.\nW obecnoÅ›ci szumu addytywnego \\(X = A s + \\varepsilon\\) z \\(\\varepsilon\\sim \\mathcal N(0,\\sigma^2 I)\\) problem staje siÄ™ trudniejszy. Stosuje siÄ™ wÃ³wczas rozszerzone modele ICA z estymacjÄ… rzÄ™du i skÅ‚adowej szumowej, warianty bayesowskie, lub metody wykorzystujÄ…ce dodatkowe wÅ‚asnoÅ›ci ÅºrÃ³deÅ‚ (np. niezaleÅ¼noÅ›Ä‡ czasowÄ… wyÅ¼szych rzÄ™dÃ³w, jak w SOBI wykorzystujÄ…cym autokowariancje).\nZaÅ‚oÅ¼enia identyfikowalnoÅ›ci obejmujÄ… liniowoÅ›Ä‡ i natychmiastowoÅ›Ä‡ mieszania, niezaleÅ¼noÅ›Ä‡ skÅ‚adowych ÅºrÃ³dÅ‚owych, co najwyÅ¼ej jednÄ… skÅ‚adowÄ… o rozkÅ‚adzie normalnym (inaczej problem staje siÄ™ nierozwiÄ…zywalny z powodu nieodrÃ³Å¼nialnoÅ›ci kierunkÃ³w gaussowskich), peÅ‚ny rzÄ…d macierzy \\(A\\) oraz wystarczajÄ…cÄ… nienormalnoÅ›Ä‡ ÅºrÃ³deÅ‚, aby kontrasty informacyjne miaÅ‚y sens. Zwyczajowo zakÅ‚ada siÄ™ rÃ³wnieÅ¼ stacjonarnoÅ›Ä‡ w czasie, o ile wykorzystujemy momenty lub autokorelacje do estymacji.\nDobÃ³r liczby skÅ‚adowych w ICA nie opiera siÄ™ na udziale wariancji, jak w PCA, poniewaÅ¼ ICA nie porzÄ…dkuje komponentÃ³w wedÅ‚ug wariancji. W praktyce najpierw wybiera siÄ™ wymiar whiteningu \\(m\\) (efektywny rzÄ…d sygnaÅ‚u), a nastÄ™pnie ekstrahuje \\(m\\) skÅ‚adowych niezaleÅ¼nych. Kryteria wyboru \\(m\\) obejmujÄ… informacyjne miary rzÄ™du macierzy kowariancji, takie jak MDL/BIC dopasowane do modelu skÅ‚adowej szumowej i niezerowych wartoÅ›ci wÅ‚asnych, testy istotnoÅ›ci dla wartoÅ›ci wÅ‚asnych po whiteningu (warianty analizy rÃ³wnolegÅ‚ej, permutacyjne testy mierzÄ…ce losowoÅ›Ä‡), walidacjÄ™ na podstawie wiarygodnoÅ›ci w modelu ML-ICA z rÃ³Å¼nymi \\(m\\) oraz kryteria stabilnoÅ›ci. Kryteria stabilnoÅ›ci polegajÄ… na wielokrotnym uruchomieniu algorytmu z rÃ³Å¼nymi inicjalizacjami i grupowaniu uzyskanych komponentÃ³w. Liczba dobrze replikujÄ…cych siÄ™ grup daje oszacowanie na \\(m\\). Dodatkowo stosuje siÄ™ testy resztowej zaleÅ¼noÅ›ci miÄ™dzy oszacowanymi ÅºrÃ³dÅ‚ami (np. testy niezaleÅ¼noÅ›ci na bazie informacji wzajemnej). JeÅ›li po dodaniu kolejnej skÅ‚adowej informacja wzajemna miÄ™dzy â€ÅºrÃ³dÅ‚amiâ€ nie maleje, zwiÄ™kszanie \\(m\\) nie przynosi korzyÅ›ci. W zastosowaniach z szumem wybieramy \\(m\\) tak, by oddzielaÄ‡ podprzestrzeÅ„ sygnaÅ‚owÄ… od szumowej, co praktycznie sprowadza siÄ™ do analizy spektrum wartoÅ›ci wÅ‚asnych i modelowania ogona jako biaÅ‚ego szumu.\nInterpretacja wynikÃ³w ICA rÃ³Å¼niÄ‡ siÄ™ od PCA. SkÅ‚adowe ICA \\(y_i\\) stanowiÄ… oceny ÅºrÃ³deÅ‚ o maksymalnej niezaleÅ¼noÅ›ci, a wiersze \\(W\\) definiujÄ… filtry demiksujÄ…ce, podczas gdy kolumny \\(A\\) (przyjmujÄ…c \\(A\\approx W^{-1}\\)) reprezentujÄ… wzorce mieszania, czyli â€mapy obciÄ…Å¼eniaâ€ ÅºrÃ³deÅ‚ na czujniki/cechy. Skale i znaki skÅ‚adowych sÄ… arbitralne, co wymaga interpretowaÄ‡ je wzglÄ™dnie: znormalizowaÄ‡ wariancjÄ™ lub maksymalnÄ… wartoÅ›Ä‡, a znak dobraÄ‡ tak, by uÅ‚atwiÄ‡ opis dziedzinowy5. W przeciwieÅ„stwie do PCA, skÅ‚adowe ICA nie muszÄ… byÄ‡ ortogonalne, a ich wariancje nie sÄ… uporzÄ…dkowane6.\n5Â WyobraÅºmy sobie, Å¼e ICA rozdziela dwa ÅºrÃ³dÅ‚a dÅºwiÄ™ku â€” skrzypce i fortepian. JeÅ›li algorytm zwrÃ³ci sygnaÅ‚, ktÃ³ry jest odwrÃ³cony w fazie (czyli pomnoÅ¼ony przez -1), to dÅºwiÄ™k fortepianu jest ten sam fizycznie, tylko wszystkie amplitudy majÄ… odwrotny znak. Dlatego znak (i skala) nie majÄ… znaczenia dla jakoÅ›ci separacji â€” sÄ… arbitralne.6Â W sygnaÅ‚ach biologicznych, takich jak EEG, ICA moÅ¼e oddzieliÄ‡ artefakty ruchowe od sygnaÅ‚Ã³w mÃ³zgowych. Artefakty te mogÄ… byÄ‡ silnie nienormalne i niezaleÅ¼ne od sygnaÅ‚Ã³w mÃ³zgowych, co czyni ICA skutecznÄ… metodÄ… ich identyfikacji i usuniÄ™cia.\nPrzykÅ‚ad 5.2 (ICA na mieszance sygnaÅ‚Ã³w) Â \n\nKoddata(\"EuStockMarkets\")\nP &lt;- as.data.frame(EuStockMarkets)              # poziomy indeksÃ³w: DAX, SMI, CAC, FTSE\nR &lt;- as.data.frame(apply(P, 2, function(x) diff(log(x))))  # dzienne log-zwroty\ncolnames(R) &lt;- colnames(P)\n\n# ICA na dziennych zwrotach\nset.seed(123)\nica_res &lt;- fastICA(R, n.comp = 4, method = \"C\")\nS_est &lt;- as.data.frame(ica_res$S)               # odzyskane ÅºrÃ³d\ncolnames(S_est) &lt;- paste0(\"IC\", 1:4)\nA_est &lt;- ica_res$A                              # macierz mieszajÄ…ca\nW_est &lt;- ica_res$K %*% ica_res$W                # macierz demiksujÄ…ca\n\n\nMacierz A_est, czyli macierz mieszania, opisuje sposÃ³b, w jaki oryginalne zmienne obserwowalne â€” w tym przypadku cztery indeksy gieÅ‚dowe: DAX, SMI, CAC i FTSE â€” powstajÄ… jako liniowe kombinacje ukrytych, niezaleÅ¼nych czynnikÃ³w. KaÅ¼dy wiersz tej macierzy odpowiada jednemu indeksowi, a kaÅ¼da kolumna jednej skÅ‚adowej niezaleÅ¼nej. WartoÅ›ci liczbowe oznaczajÄ… wspÃ³Å‚czynniki liniowych kombinacji, czyli wpÅ‚yw danej skÅ‚adowej na dany indeks. WartoÅ›Ä‡ dodatnia wskazuje, Å¼e wzrost komponentu powoduje wzrost indeksu, wartoÅ›Ä‡ ujemna â€” Å¼e ruch komponentu przekÅ‚ada siÄ™ na spadek indeksu, a wartoÅ›Ä‡ bliska zeru oznacza brak istotnego zwiÄ…zku.\n\nKodround(A_est, 3)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,] -0.001 -0.001 -0.002 -0.006\n[2,]  0.003 -0.004  0.002  0.000\n[3,] -0.010 -0.008 -0.008 -0.005\n[4,]  0.000  0.001  0.008  0.001\n\n\n\nPierwszy komponent (IC1) najsilniej Å‚aduje siÄ™ na indeks CAC, a w mniejszym stopniu na SMI. Znak ujemny dla CAC i dodatni dla SMI sugeruje, Å¼e komponent ten uchwyca rÃ³Å¼nicÄ™ pomiÄ™dzy rynkami strefy euro a rynkiem szwajcarskim, czyli czynnik kontrastujÄ…cy. W praktyce oznacza to, Å¼e wzrost aktywnoÅ›ci na rynkach kontynentalnych wiÄ…zaÄ‡ siÄ™ moÅ¼e z relatywnym osÅ‚abieniem rynku SMI lub odwrotnie.\nDrugi komponent (IC2) rÃ³wnieÅ¼ oddziaÅ‚uje na CAC i SMI w kierunku ujemnym, co moÅ¼e Å›wiadczyÄ‡ o uchwyceniu wspÃ³lnego czynnika kontynentalnego o mniejszej amplitudzie. Dodatnie, choÄ‡ niewielkie wartoÅ›ci dla FTSE wskazujÄ…, Å¼e komponent ten czÄ™Å›ciowo kontrastuje rynki kontynentalne z brytyjskim.\nTrzeci komponent (IC3) ma wyraÅºnie odmiennÄ… strukturÄ™. Dla FTSE wspÃ³Å‚czynnik jest dodatni i najwiÄ™kszy, natomiast dla pozostaÅ‚ych indeksÃ³w ujemny. Komponent ten rozdziela zatem rynek brytyjski od reszty Europy i moÅ¼na go interpretowaÄ‡ jako czynnik geograficzny lub walutowy, zwiÄ…zany z odmiennym otoczeniem gospodarczym Wielkiej Brytanii.\nCzwarty komponent (IC4) ma wspÃ³Å‚czynniki bardzo maÅ‚e, rzÄ™du 10-3â€“10-2, co sugeruje, Å¼e jego wpÅ‚yw na strukturÄ™ indeksÃ³w jest marginalny. Prawdopodobnie odpowiada on za szum lub krÃ³tkotrwaÅ‚e, lokalne fluktuacje, ktÃ³re nie majÄ… znaczenia ekonomicznego.\n\n\nKodround(W_est, 3)\n\n         [,1]     [,2]    [,3]    [,4]\n[1,]   43.737  112.245 -74.794 -85.258\n[2,]   34.702 -146.908 -47.294  -7.649\n[3,]   15.975   14.330   8.569 141.650\n[4,] -173.263   -5.797  10.661 -35.834\n\n\nMacierz W_est, czyli macierz demiksujÄ…ca, zawiera wspÃ³Å‚czynniki liniowych kombinacji oryginalnych zmiennych (indeksÃ³w gieÅ‚dowych), ktÃ³re pozwalajÄ… uzyskaÄ‡ poszczegÃ³lne niezaleÅ¼ne komponenty. KaÅ¼dy wiersz tej macierzy odpowiada jednemu komponentowi ICA (IC1â€“IC4), a kaÅ¼da kolumna â€” jednej zmiennej obserwowalnej (DAX, SMI, CAC, FTSE). WartoÅ›ci w tej macierzy moÅ¼na zatem interpretowaÄ‡ jako wagi, z jakimi poszczegÃ³lne indeksy uczestniczÄ… w tworzeniu danego odzyskanego ÅºrÃ³dÅ‚a.\n\nPierwszy komponent (IC1) ma duÅ¼e dodatnie wagi dla indeksÃ³w DAX i SMI oraz silnie ujemnÄ… wagÄ™ dla FTSE. Oznacza to, Å¼e IC1 odzwierciedla kontrast pomiÄ™dzy rynkami kontynentalnymi (Niemcy, Szwajcaria) a rynkiem brytyjskim. Wzrost wartoÅ›ci IC1 odpowiada sytuacji, w ktÃ³rej indeksy kontynentalne zachowujÄ… siÄ™ silniej niÅ¼ FTSE â€” moÅ¼na wiÄ™c interpretowaÄ‡ ten czynnik jako rÃ³Å¼nicowy, typu â€Europa kontynentalna kontra Wielka Brytaniaâ€.\nDrugi komponent (IC2) pokazuje odwrotny schemat: dodatni wpÅ‚yw DAX, silnie ujemny SMI, a sÅ‚aby wpÅ‚yw pozostaÅ‚ych indeksÃ³w. MoÅ¼na go interpretowaÄ‡ jako czynnik rozrÃ³Å¼niajÄ…cy zachowanie rynku niemieckiego i szwajcarskiego, ktÃ³ry w ICA czÄ™sto ujawnia siÄ™ jako efekt odmiennych warunkÃ³w walutowych i struktury gospodarczej.\nTrzeci komponent (IC3) ma wszystkie wagi ujemne, z wyjÄ…tkiem niewielkich dodatnich dla CAC i FTSE. Oznacza to, Å¼e IC3 reprezentuje wspÃ³lny kierunek zmian wiÄ™kszoÅ›ci indeksÃ³w (ruch globalny), ale w konstrukcji demiksujÄ…cej wystÄ™puje ze znakiem ujemnym. W praktyce odpowiada to czynnikowi rynkowemu o charakterze ogÃ³lnym â€” globalnemu impulsowi, ktÃ³ry oddziaÅ‚uje w podobny sposÃ³b na wiÄ™kszoÅ›Ä‡ rynkÃ³w.\nCzwarty komponent (IC4) ma wysokÄ… dodatniÄ… wagÄ™ dla CAC oraz ujemne dla pozostaÅ‚ych indeksÃ³w, co sugeruje, Å¼e moÅ¼e on odzwierciedlaÄ‡ czynnik specyficzny dla rynku francuskiego â€” reakcje lokalne lub sektorowe, ktÃ³re nie sÄ… wspÃ³lne dla innych gieÅ‚d..\n\nZnaki wspÃ³Å‚czynnikÃ³w w ICA sÄ… arbitralne (zmiana wszystkich znakÃ³w w jednym wierszu nie zmienia modelu), dlatego przy interpretacji naleÅ¼y zwracaÄ‡ uwagÄ™ na wzglÄ™dne zaleÅ¼noÅ›ci miÄ™dzy indeksami, a nie na samÄ… polaryzacjÄ™ znakÃ³w. WartoÅ›ci bezwzglÄ™dne wag pokazujÄ… natomiast, ktÃ³re indeksy majÄ… najwiÄ™kszy udziaÅ‚ w ksztaÅ‚towaniu danego czynnika.\n\nKodkurt &lt;- apply(S_est, 2, function(x) mean(x^4) - 3)  # kurtozy odzyskanych ÅºrÃ³deÅ‚\nprint(round(kurt, 3))                           # kurtozy (nienormalnoÅ›Ä‡)\n\n  IC1   IC2   IC3   IC4 \n5.601 1.985 8.206 2.274 \n\n\nWartoÅ›ci kurtozy stanowiÄ… miarÄ™ niegaussowskoÅ›ci rozkÅ‚adu â€” czyli tego, jak bardzo dany sygnaÅ‚ odbiega od ksztaÅ‚tu rozkÅ‚adu normalnego. WartoÅ›ci dodatnie oznaczajÄ… rozkÅ‚ady o â€ciÄ™Å¼szych ogonachâ€ i bardziej spiczastym ksztaÅ‚cie (tzw. leptokurtyczne), co jest typowe dla sygnaÅ‚Ã³w rzadkich, zawierajÄ…cych wyraÅºne piki i okresy stabilnoÅ›ci. W kontekÅ›cie ICA wysoka kurtoza jest poÅ¼Ä…dana, poniewaÅ¼ algorytm poszukuje wÅ‚aÅ›nie takich komponentÃ³w â€” maksymalnie odmiennych od normalnych, a wiÄ™c potencjalnie niezaleÅ¼nych ÅºrÃ³deÅ‚.\n\n\nIC1 (5.601) ma bardzo wysokÄ… kurtozÄ™, co wskazuje na silnÄ… niegaussowskoÅ›Ä‡. Komponent ten prawdopodobnie reprezentuje gÅ‚Ã³wny, â€rzadkiâ€ czynnik ekonomiczny, ktÃ³ry reaguje gwaÅ‚townie w momentach istotnych zmian rynkowych. MoÅ¼e to byÄ‡ globalny impuls rynkowy lub okresowe szoki finansowe.\n\nIC2 (1.985) ma umiarkowanie dodatniÄ… kurtozÄ™, sugerujÄ…cÄ… rozkÅ‚ad jedynie lekko leptokurtyczny. Oznacza to, Å¼e komponent jest bliÅ¼szy rozkÅ‚adowi normalnemu, a zatem mniej â€niezaleÅ¼nyâ€ w sensie ICA. MoÅ¼e reprezentowaÄ‡ Å‚agodniejszy czynnik wspÃ³lny, np. codziennÄ… zmiennoÅ›Ä‡ lub trend regionalny.\n\nIC3 (8.206) ma najwyÅ¼szÄ… kurtozÄ™ spoÅ›rÃ³d wszystkich komponentÃ³w. Jest to bardzo silny sygnaÅ‚ niegaussowski, typowy dla ÅºrÃ³dÅ‚a zawierajÄ…cego rzadkie, intensywne zdarzenia â€” w kontekÅ›cie finansowym mogÄ… to byÄ‡ momenty skokowych zmian cen lub kryzysÃ³w, wpÅ‚ywajÄ…ce selektywnie na czÄ™Å›Ä‡ indeksÃ³w. Ten komponent moÅ¼na traktowaÄ‡ jako najbardziej â€czysteâ€ ÅºrÃ³dÅ‚o w sensie ICA.\n\nIC4 (2.274) wykazuje umiarkowanÄ… kurtozÄ™, zbliÅ¼onÄ… do IC2. MoÅ¼na go interpretowaÄ‡ jako dodatkowy, mniej wyraÅºny czynnik poboczny, ktÃ³ry w pewnym stopniu odbiega od normalnoÅ›ci, ale nie ma charakteru dominujÄ…cego.\n\n\nKod# Sprawdzenie korelacji miÄ™dzy oryginalnymi a odzyskanymi sygnaÅ‚ami\ncor_matrix &lt;- cor(R, S_est)\nprint(round(cor_matrix, 3))\n\n        IC1    IC2    IC3   IC4\nDAX  -0.076  0.287 -0.954 0.037\nSMI  -0.060 -0.475 -0.871 0.108\nCAC  -0.189  0.140 -0.686 0.689\nFTSE -0.788  0.001 -0.602 0.125\n\n\nMacierz korelacji miÄ™dzy oryginalnymi indeksami gieÅ‚dowymi a odzyskanymi komponentami niezaleÅ¼nymi (cor_matrix) pokazuje, jak silnie i w jakim kierunku (znak dodatni lub ujemny) kaÅ¼dy z indeksÃ³w jest powiÄ…zany z danym ÅºrÃ³dÅ‚em ICA. Wysokie wartoÅ›ci bezwzglÄ™dne wskazujÄ…, Å¼e dany komponent w duÅ¼ym stopniu tÅ‚umaczy zmiennoÅ›Ä‡ danego indeksu, natomiast wartoÅ›ci bliskie zera oznaczajÄ… sÅ‚aby zwiÄ…zek.\n\nNajsilniejsze korelacje obserwuje siÄ™ dla komponentu IC3, ktÃ³ry ma wartoÅ›ci ujemne i bardzo wysokie w module: DAX (âˆ’0.954), SMI (âˆ’0.871) i CAC (âˆ’0.686). Oznacza to, Å¼e IC3 stanowi wspÃ³lny czynnik dominujÄ…cy dla trzech kontynentalnych indeksÃ³w europejskich. Wszystkie trzy reagujÄ… w tym samym kierunku (ujemny znak jest konwencjonalny, jego odwrÃ³cenie nie zmienia interpretacji). MoÅ¼na zatem uznaÄ‡, Å¼e IC3 reprezentuje globalny czynnik rynkowy, wspÃ³lny dla gÅ‚Ã³wnych gieÅ‚d kontynentalnych, a jego wysoka kurtoza (8.206) wskazuje, Å¼e czynnik ten cechuje siÄ™ silnymi, epizodycznymi wahaniami â€” typowymi dla okresÃ³w zawirowaÅ„ finansowych.\nKomponent IC1 wykazuje wyraÅºnÄ… ujemnÄ… korelacjÄ™ z FTSE (âˆ’0.788), przy braku silnych zaleÅ¼noÅ›ci z pozostaÅ‚ymi indeksami. Oznacza to, Å¼e IC1 moÅ¼na interpretowaÄ‡ jako czynnik specyficzny dla rynku brytyjskiego, niezaleÅ¼ny od ruchÃ³w kontynentalnych. Wysoka wartoÅ›Ä‡ bezwzglÄ™dna korelacji sugeruje, Å¼e ten komponent odpowiada za znacznÄ… czÄ™Å›Ä‡ zmiennoÅ›ci FTSE, co dobrze wspÃ³Å‚gra z interpretacjÄ… wczeÅ›niejszej macierzy mieszania â€” IC1 oddzielaÅ‚ WielkÄ… BrytaniÄ™ od reszty Europy.\nKomponent IC2 wykazuje umiarkowane korelacje o rÃ³Å¼nych znakach: dodatniÄ… z DAX (0.287) i ujemnÄ… ze SMI (âˆ’0.475). MoÅ¼na go zatem interpretowaÄ‡ jako czynnik rÃ³Å¼nicowy pomiÄ™dzy rynkami Niemiec i Szwajcarii. W praktyce moÅ¼e on odzwierciedla odmiennÄ… reakcjÄ™ tych rynkÃ³w na czynniki lokalne, np. rÃ³Å¼nice w strukturze sektorowej lub polityce monetarnej.\nKomponent IC4 ma umiarkowanÄ… dodatniÄ… korelacjÄ™ z CAC (0.689), a pozostaÅ‚e indeksy reagujÄ… na niego sÅ‚abo. Oznacza to, Å¼e IC4 moÅ¼e byÄ‡ czynnikiem czÄ™Å›ciowo specyficznym dla rynku francuskiego, prawdopodobnie o charakterze lokalnym lub szumowym.\n\nNa koniec wizualizacja ICA.\n\nKod# odzyskane komponenty \nS_est_long &lt;- S_est |&gt;\n  mutate(Time = 1:nrow(S_est)) |&gt;\n  pivot_longer(cols = starts_with(\"IC\"),\n                      names_to = \"Component\", values_to = \"Value\")\n\np1 &lt;- ggplot(S_est_long, aes(x = Time, y = Value, color = Component)) +\n  geom_line() +\n  facet_wrap(~ Component, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Odzyskane niezaleÅ¼ne komponenty (ICA)\", x = \"Czas\", y = \"WartoÅ›Ä‡\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nR_long &lt;- R |&gt;\n  mutate(Time = 1:nrow(R)) |&gt;\n  pivot_longer(cols = -Time,    \n                      names_to = \"Index\", values_to = \"Return\")\n\np2 &lt;- ggplot(R_long, aes(x = Time, y = Return, color = Index)) +\n  geom_line() +\n  facet_wrap(~ Index, ncol = 1, scales = \"free_y\") +\n  labs(title = \"Oryginalne dzienne log-zwroty indeksÃ³w\", x = \"Czas\", y = \"Log-zwrot\") +\n  scale_color_flat_d() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Metody redukcji wymiarowoÅ›ci</span>"
    ]
  },
  {
    "objectID": "pca.html#mds",
    "href": "pca.html#mds",
    "title": "Metody redukcji wymiarowoÅ›ci",
    "section": "MDS",
    "text": "MDS\nMetoda Multidimensional Scaling (MDS), czyli skalowanie wielowymiarowe, stanowi rodzinÄ™ technik sÅ‚uÅ¼Ä…cych do odwzorowania danych opisanych za pomocÄ… macierzy odlegÅ‚oÅ›ci (lub podobieÅ„stw) w przestrzeÅ„ o niskim wymiarze â€” najczÄ™Å›ciej dwuwymiarowÄ… lub trÃ³jwymiarowÄ… â€” w taki sposÃ³b, aby relacje miÄ™dzy obiektami zostaÅ‚y zachowane moÅ¼liwie wiernie. Celem jest wiÄ™c konstrukcja wektorowej reprezentacji obiektÃ³w, ktÃ³ra odtwarza zadane odlegÅ‚oÅ›ci.\nIstniejÄ… dwie podstawowe wersje MDS: metryczna i niemetryczna, rÃ³Å¼niÄ…ce siÄ™ sposobem odwzorowania wartoÅ›ci wejÅ›ciowych i kryterium dopasowania.\nWersja metryczna (klasyczna MDS) (Torgerson 1952)\n\nZaÅ‚oÅ¼enia\nDane wejÅ›ciowe stanowi ma macierz odlegÅ‚oÅ›ci euklidesowych \\[\n\\Delta = [\\delta_{ij}]_{n\\times n}, \\quad \\delta_{ij} \\ge 0, \\quad \\delta_{ii}=0, \\quad \\delta_{ij}=\\delta_{ji}.\n\\] Celem jest znalezienie konfiguracji punktÃ³w \\(X \\in \\mathbb{R}^{n\\times p}\\), takiej Å¼e odlegÅ‚oÅ›ci miÄ™dzy punktami \\(d_{ij}(X) = \\|x_i - x_j\\|\\) sÄ… jak najbardziej zbliÅ¼one do odlegÅ‚oÅ›ci zadanych \\(\\delta_{ij}.\\)\nWyprowadzenie modelu\nZ klasycznej geometrii euklidesowej wynika, Å¼e iloczyn skalarny miÄ™dzy wektorami moÅ¼na zapisaÄ‡ przez odlegÅ‚oÅ›ci \\[\nx_i^\\top x_j = \\frac{1}{2}\\bigl(\\|x_i\\|^2 + \\|x_j\\|^2 - \\|x_i - x_j\\|^2 \\bigr).\n\\] JeÅ¼eli dane sÄ… wyraÅ¼one przez odlegÅ‚oÅ›ci \\(\\delta_{ij}\\), to moÅ¼na odtworzyÄ‡ tzw. macierz iloczynÃ³w skalarnych \\(B = XX^\\top\\), ktÃ³ra okreÅ›la wspÃ³Å‚rzÄ™dne punktÃ³w po odpowiednim scentralizowaniu ukÅ‚adu wspÃ³Å‚rzÄ™dnych. Operacja ta nazywa siÄ™ podwÃ³jnym centrowaniem (double centering) \\[\nB = -\\frac{1}{2} J \\Delta^{(2)} J,\n\\] gdzie \\(\\Delta^{(2)} = [\\delta_{ij}^2]\\) to macierz kwadratÃ³w odlegÅ‚oÅ›ci, a \\(J = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top\\) to macierz centrowania (usuwa Å›rodek ciÄ™Å¼koÅ›ci ukÅ‚adu).\nMacierz \\(B\\) powinna byÄ‡ dodatnio pÃ³Å‚okreÅ›lona (w przypadku euklidesowym). NastÄ™pnie wykonuje siÄ™ jej rozkÅ‚ad spektralny \\[\nB = V \\Lambda V^\\top,\n\\] gdzie \\(\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_n)\\) zawiera wartoÅ›ci wÅ‚asne uporzÄ…dkowane malejÄ…co, a \\(V\\) to odpowiadajÄ…ce im wektory wÅ‚asne. WspÃ³Å‚rzÄ™dne punktÃ³w w przestrzeni o wymiarze \\(p\\) wyznacza siÄ™ przez \\[\nX_p = V_p \\Lambda_p^{1/2},\n\\] gdzie \\(V_p\\) zawiera \\(p\\) pierwszych wektorÃ³w wÅ‚asnych, a \\(\\Lambda_p\\) odpowiadajÄ…ce im najwiÄ™ksze wartoÅ›ci wÅ‚asne. Otrzymana konfiguracja \\(X_p\\) odwzorowuje relacje odlegÅ‚oÅ›ci w sposÃ³b minimalizujÄ…cy bÅ‚Ä…d w sensie Å›redniokwadratowym.\nKryterium dopasowania\nW wersji metrycznej minimalizuje siÄ™ bÅ‚Ä…d rekonstrukcji odlegÅ‚oÅ›ci \\[\n\\min_{X} \\sum_{i&lt;j} \\bigl( d_{ij}(X) - \\delta_{ij} \\bigr)^2.\n\\] RozwiÄ…zanie klasycznej wersji powyÅ¼szego problemu daje wprost powyÅ¼sza dekompozycja macierzy \\(B\\), dlatego czÄ™sto okreÅ›la siÄ™ jÄ… jako Classical Scaling lub Principal Coordinates Analysis (PCoA). Metoda ta jest w peÅ‚ni analityczna i odpowiada PCA zastosowanej do macierzy odlegÅ‚oÅ›ci.\nWersja niemetryczna (Non-metric MDS) (Kruskal 1964)\n\nZaÅ‚oÅ¼enia\nW wersji niemetrycznej nie wymaga siÄ™, aby wartoÅ›ci \\(\\delta_{ij}\\) byÅ‚y dokÅ‚adnymi odlegÅ‚oÅ›ciami â€” mogÄ… byÄ‡ dowolnymi miarami niepodobieÅ„stwa, niekoniecznie metrycznymi (np. o charakterze porzÄ…dkowym). Celem jest znalezienie konfiguracji \\(X\\), dla ktÃ³rej rangi odlegÅ‚oÅ›ci \\(d_{ij}(X)\\) sÄ… moÅ¼liwie zgodne z rangami danych \\(\\delta_{ij}\\).\nModel i funkcja stresu\nPoniewaÅ¼ nie zakÅ‚ada siÄ™ liniowego zwiÄ…zku miÄ™dzy \\(\\delta_{ij}\\) a \\(d_{ij}(X)\\), wprowadza siÄ™ monotonicznÄ… funkcjÄ™ przeksztaÅ‚cenia \\(f(\\cdot)\\), ktÃ³ra dopasowuje skalÄ™ \\[\n\\hat{\\delta}_{ij} = f(d_{ij}(X)),\n\\] przy czym \\(f\\) zachowuje monotonicznoÅ›Ä‡ rangowÄ… (jeÅ¼eli \\(\\delta_{ij} &gt; \\delta_{kl}\\), to \\(f(\\delta_{ij}) &gt; f(\\delta_{kl})\\)).\nOptymalizuje siÄ™ wtedy miarÄ™ stresu Kruskala (STRESS - STandardized REsidual Sum of Squares) \\[\nS(X) = \\sqrt{\\frac{\\sum_{i&lt;j} \\bigl(f(\\delta_{ij}) - d_{ij}(X)\\bigr)^2}{\\sum_{i&lt;j} d_{ij}(X)^2}}.\n\\] Minimalizacja stresu odbywa siÄ™ iteracyjnie â€” zmienia siÄ™ poÅ‚oÅ¼enie punktÃ³w \\(x_i\\), aby zmniejszyÄ‡ rÃ³Å¼nicÄ™ miÄ™dzy rangami obserwowanych i odwzorowanych odlegÅ‚oÅ›ci.\n\n\nWartoÅ›Ä‡ STRESS\nOcena dopasowania\n\n\n\n&lt; 0.05\nDoskonaÅ‚e\n\n\n0.05â€“0.10\nBardzo dobre\n\n\n0.10â€“0.20\nUmiarkowane\n\n\n0.20â€“0.30\nSÅ‚abe\n\n\n&gt; 0.30\nBardzo sÅ‚abe (odwzorowanie nieadekwatne)\n\n\nInterpretacja\nW niemetrycznym MDS nie dÄ…Å¼y siÄ™ do zachowania dokÅ‚adnych odlegÅ‚oÅ›ci, lecz do zachowania porzÄ…dku relacji niepodobieÅ„stwa, obiekty podobne majÄ… byÄ‡ blisko siebie, a niepodobne â€” daleko. DziÄ™ki temu metoda jest odporna na nieliniowe znieksztaÅ‚cenia skali w danych wejÅ›ciowych.\nPorÃ³wnanie metod MDS\n\n\n\n\n\n\n\nCecha\nMetryczny MDS\nNiemetryczny MDS\n\n\n\nTyp danych wejÅ›ciowych\nOdlegÅ‚oÅ›ci euklidesowe\nDowolne niepodobieÅ„stwa, takÅ¼e porzÄ…dkowe\n\n\nZaleÅ¼noÅ›Ä‡ miÄ™dzy danymi a odlegÅ‚oÅ›ciami\nLiniowa\nMonotoniczna (dowolna funkcja porzÄ…dkowa)\n\n\nAlgorytm\nDekompozycja wÅ‚asna macierzy centrowanej\nIteracyjna optymalizacja stresu\n\n\nKryterium dopasowania\nMinimalizacja bÅ‚Ä™du kwadratowego odlegÅ‚oÅ›ci\nMinimalizacja stresu Kruskala\n\n\nInterpretacja\nOdtwarza dokÅ‚adne relacje geometryczne\nOdtwarza relacje rangowe (porzÄ…dek podobieÅ„stw)\n\n\n\nW praktyce metryczny MDS jest szybszy, prostszy i rÃ³wnowaÅ¼ny klasycznemu podejÅ›ciu PCA, gdy dane majÄ… charakter metryczny. Niemetryczny MDS natomiast jest bardziej elastyczny â€” pozwala odwzorowaÄ‡ struktury nieliniowe, zachowujÄ…c tylko relacje porzÄ…dkowe miÄ™dzy obiektami, co czyni go odpowiednim do danych percepcyjnych, preferencyjnych lub ankietowych.\n\nPrzykÅ‚ad 5.3 (MDS na danych o odlegÅ‚oÅ›ciach miÄ™dzy miastami) Â \n\nKodlibrary(MASS)        # isoMDS, UScitiesD\nlibrary(maps)        # zarysy map\nlibrary(ggrepel)\n\n\n# Dane: odlegÅ‚oÅ›ci drogowe miÄ™dzy miastami w USA (w milach)\ndata(\"UScitiesD\")  # obiekt klasy 'dist' z pakietu MASS\n\n# 1) Metryczny MDS\nmds_metric &lt;- cmdscale(UScitiesD, k = 2)\nmds_metric\n\n                    [,1]       [,2]\nAtlanta        -718.7594  142.99427\nChicago        -382.0558 -340.83962\nDenver          481.6023  -25.28504\nHouston        -161.4663  572.76991\nLosAngeles     1203.7380  390.10029\nMiami         -1133.5271  581.90731\nNewYork       -1072.2357 -519.02423\nSanFrancisco   1420.6033  112.58920\nSeattle        1341.7225 -579.73928\nWashington.DC  -979.6220 -335.47281\n\nKod# 2) Niemetryczny MDS\nmds_nonmetric &lt;- isoMDS(UScitiesD, k = 2)$points\n\ninitial  value 0.049975 \niter   5 value 0.049265\niter  10 value 0.048377\niter  15 value 0.047490\niter  20 value 0.046603\niter  25 value 0.045715\niter  30 value 0.044828\niter  35 value 0.043941\niter  40 value 0.043053\niter  45 value 0.042166\niter  50 value 0.041278\nfinal  value 0.041278 \nstopped after 50 iterations\n\nKodmds_nonmetric\n\n                    [,1]       [,2]\nAtlanta        -718.7595  142.99430\nChicago        -382.0558 -340.83969\nDenver          481.6024  -25.28505\nHouston        -161.1399  572.76696\nLosAngeles     1203.7842  389.77723\nMiami         -1133.8537  581.91049\nNewYork       -1072.2359 -519.02433\nSanFrancisco   1420.6036  112.58922\nSeattle        1341.6768 -579.41626\nWashington.DC  -979.6222 -335.47288\n\nKod# 3) Ramy danych do wykresu MDS\nmds_df &lt;- data.frame(\n  City = rownames(mds_metric),\n  Metric_X = mds_metric[,1],\n  Metric_Y = mds_metric[,2],\n  Nonmetric_X = mds_nonmetric[,1],\n  Nonmetric_Y = mds_nonmetric[,2]\n)\n\n# 4) Wykresy MDS (po symetrii wzglÄ™dem obu osi dla lepszej czytelnoÅ›ci)\np1 &lt;- ggplot(mds_df, aes(x = -Metric_X, y = -Metric_Y, label = City)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_text_repel(size = 3) +\n  labs(title = \"Metryczny MDS na danych o odlegÅ‚oÅ›ciach miÄ™dzy miastami\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal()\n\np2 &lt;- ggplot(mds_df, aes(x = -Nonmetric_X, y = -Nonmetric_Y, label = City)) +\n  geom_point(color = \"red\", size = 2) +\n  geom_text_repel(size = 3) +\n  labs(title = \"Niemetryczny MDS na danych o odlegÅ‚oÅ›ciach miÄ™dzy miastami\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal()\n\n# 5) Faktyczne poÅ‚oÅ¼enia miast (long/lat) â€“ nazwy muszÄ… pokrywaÄ‡ siÄ™ z UScitiesD\ncity_coords &lt;- tribble(\n  ~City,           ~lon,      ~lat,\n  \"Atlanta\",       -84.39,     33.75,\n  \"Chicago\",       -87.63,     41.88,\n  \"Denver\",       -104.99,     39.74,\n  \"Houston\",       -95.37,     29.76,\n  \"LosAngeles\",   -118.24,     34.05,\n  \"Miami\",         -80.19,     25.77,\n  \"NewYork\",       -74.01,     40.71,\n  \"SanFrancisco\", -122.42,     37.77,\n  \"Seattle\",      -122.33,     47.61,\n  \"Washington\",    -77.04,     38.90\n)\n\n# 6) Zarys mapy USA\nusa_map &lt;- map_data(\"state\")\n\n# 7) Wykres p3: faktyczna mapa z punktami miast\np3 &lt;- ggplot() +\n  geom_polygon(data = usa_map,\n               aes(x = long, y = lat, group = group),\n               fill = \"grey95\", color = \"grey70\", linewidth = 0.3) +\n  geom_point(data = city_coords,\n             aes(x = lon, y = lat),\n             size = 2, color = \"black\") +\n  geom_text_repel(data = city_coords,\n                  aes(x = lon, y = lat, label = City),\n                  size = 3) +\n  labs(title = \"Faktyczne poÅ‚oÅ¼enie miast na mapie USA\",\n       x = \"DÅ‚ugoÅ›Ä‡ geograficzna\", y = \"SzerokoÅ›Ä‡ geograficzna\") +\n  theme_minimal()\n\n# 8) Prezentacja obok siebie (patchwork)\np1/ p2 / p3",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Metody redukcji wymiarowoÅ›ci</span>"
    ]
  },
  {
    "objectID": "pca.html#t-sne-vandermaaten08a",
    "href": "pca.html#t-sne-vandermaaten08a",
    "title": "Metody redukcji wymiarowoÅ›ci",
    "section": "t-SNE (Maaten i Hinton 2008)\n",
    "text": "t-SNE (Maaten i Hinton 2008)\n\nMetoda t-distributed Stochastic Neighbor Embedding (t-SNE) zostaÅ‚a opracowana przez Laurensa van der Maatena i Geoffreya Hintona w 2008 roku jako nieliniowa technika redukcji wymiarowoÅ›ci, ktÃ³rej celem jest odwzorowanie lokalnej struktury danych wysokowymiarowych w przestrzeni o mniejszej liczbie wymiarÃ³w, zwykle dwuwymiarowej lub trÃ³jwymiarowej. W przeciwieÅ„stwie do metod liniowych, takich jak PCA, t-SNE nie dÄ…Å¼y do maksymalizacji wariancji, lecz do zachowania sÄ…siedztw pomiÄ™dzy punktami â€“ obserwacje, ktÃ³re w przestrzeni oryginalnej sÄ… blisko siebie, powinny rÃ³wnieÅ¼ pozostawaÄ‡ blisko w przestrzeni odwzorowania.\nNiech dane wejÅ›ciowe tworzÄ… macierz \\(X = [x_1, x_2, \\dots, x_n]^\\top\\), gdzie kaÅ¼dy wektor \\(x_i \\in \\mathbb{R}^p\\) reprezentuje jednÄ… obserwacjÄ™ w przestrzeni o wymiarze \\(p\\). Pierwszym krokiem jest przeksztaÅ‚cenie danych wysokowymiarowych w macierz podobieÅ„stw, ktÃ³ra opisuje, jak bardzo punkty sÄ… â€bliskieâ€ wzglÄ™dem siebie. Dla kaÅ¼dego punktu \\(x_i\\) definiuje siÄ™ rozkÅ‚ad warunkowy \\[\np_{j|i} = \\frac{\\exp\\!\\left(-\\frac{|x_i - x_j|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{|x_i - x_k|^2}{2\\sigma_i^2}\\right)}, \\quad p_{i|i} = 0,\n\\] gdzie parametr \\(\\sigma_i\\) (odpowiednik bandwidth) dobiera siÄ™ tak, aby entropia rozkÅ‚adu \\(P_i = (p_{j|i})_j = (p_{1|i}, p_{2|i}, \\dots, p_{n|i})\\) odpowiadaÅ‚a zadanej perplexity, czyli efektywnej liczbie sÄ…siadÃ³w. Perplexity jest hiperparametrem kontrolujÄ…cym zakres lokalnoÅ›ci analizowanych relacji.\nNastÄ™pnie konstruuje siÄ™ symetrycznÄ… macierz podobieÅ„stw \\[\np_{ij} = \\frac{p_{i|j} + p_{j|i}}{2n},\n\\] ktÃ³ra reprezentuje prawdopodobieÅ„stwo, Å¼e punkty \\(x_i\\) i \\(x_j\\) sÄ… bliskimi sÄ…siadami w przestrzeni oryginalnej.\nKolejnym krokiem jest utworzenie analogicznego rozkÅ‚adu w przestrzeni odwzorowania \\(Y = [y_1, y_2, \\dots, y_n]^\\top\\), gdzie \\(y_i \\in \\mathbb{R}^q\\) i zwykle \\(q = 2\\) lub 3. Dla tych punktÃ³w definiuje siÄ™ rozkÅ‚ad podobieÅ„stw oparty na rozkÅ‚adzie t-Studenta z jednym stopniem swobody \\[\nq_{ij} = \\frac{(1 + |y_i - y_j|^2)^{-1}}{\\sum_{k \\neq l} (1 + |y_k - y_l|^2)^{-1}}, \\quad q_{ii} = 0.\n\\] RozkÅ‚ad t-Studenta ma grube ogony, co umoÅ¼liwia bardziej realistyczne odwzorowanie relacji miÄ™dzy punktami odlegÅ‚ymi od siebie i redukuje problem crowding, czyli nadmiernego Å›ciskania punktÃ³w w centrum przestrzeni odwzorowania.\nCelem t-SNE jest minimalizacja dywergencji Kullbackaâ€“Leiblera miÄ™dzy rozkÅ‚adami \\(P\\) i \\(Q\\) \\[\nC = \\operatorname{KL}(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n\\] Optymalizacja tej funkcji, zwykle za pomocÄ… spadku gradientowego, prowadzi do znalezienia takich wspÃ³Å‚rzÄ™dnych \\(Y\\), ktÃ³re zachowujÄ… lokalne relacje miÄ™dzy punktami w jak najwiÄ™kszym stopniu. Gradient funkcji celu wzglÄ™dem wspÃ³Å‚rzÄ™dnych \\(y_i\\) ma postaÄ‡ \\[\n\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij}) (y_i - y_j) (1 + \\|y_i - y_j\\|^2)^{-1},\n\\] a wspÃ³Å‚czynnik 4 peÅ‚ni rolÄ™ skalujÄ…cÄ…. W praktyce stosuje siÄ™ dodatkowe techniki stabilizujÄ…ce proces uczenia, takie jak momentum, etap early exaggeration zwiÄ™kszajÄ…cy kontrast lokalnych podobieÅ„stw, oraz wczeÅ›niejszÄ… redukcjÄ™ wymiarowoÅ›ci metodÄ… PCA w celu ograniczenia szumu.\nMetoda t-SNE nie zakÅ‚ada liniowoÅ›ci ani rozkÅ‚adu normalnego danych, lecz wymaga, aby dane byÅ‚y znormalizowane w przypadku rÃ³Å¼nych jednostek pomiarowych, poniewaÅ¼ odlegÅ‚oÅ›ci euklidesowe sÄ… wraÅ¼liwe na skalÄ™. Wskazane jest wczeÅ›niejsze zastosowanie PCA w celu usuniÄ™cia szumu i zmniejszenia zÅ‚oÅ¼onoÅ›ci obliczeniowej. Dane nie powinny zawieraÄ‡ duÅ¼ej liczby wartoÅ›ci odstajÄ…cych ani duplikatÃ³w, ktÃ³re mogÅ‚yby zaburzyÄ‡ lokalne struktury. Kluczowy hiperparametr perplexity powinien byÄ‡ dostosowany do liczby obserwacji â€” zbyt maÅ‚a wartoÅ›Ä‡ prowadzi do przeuczenia lokalnego, a zbyt duÅ¼a powoduje zatarcie drobnych struktur.\nWyniki t-SNE przedstawione w przestrzeni dwuwymiarowej lub trÃ³jwymiarowej nie majÄ… interpretacji metrycznej. Oznacza to, Å¼e odlegÅ‚oÅ›ci miÄ™dzy klastrami nie sÄ… bezpoÅ›rednio interpretowalne iloÅ›ciowo. Interpretacja opiera siÄ™ gÅ‚Ã³wnie na analizie sÄ…siedztwa: punkty znajdujÄ…ce siÄ™ blisko siebie w przestrzeni t-SNE odpowiadajÄ… obserwacjom podobnym w oryginalnych cechach, natomiast wyraÅºne skupiska punktÃ³w mogÄ… wskazywaÄ‡ na istnienie klas lub podgrup. OÅ› pierwsza i druga nie majÄ… znaczenia merytorycznego â€“ sÄ… jedynie wspÃ³Å‚rzÄ™dnymi w przestrzeni odwzorowania, ktÃ³re zachowuje lokalnÄ… strukturÄ™, a nie globalnÄ… geometriÄ™ danych.\n\n\n\n\n\n\nAdnotacjaWaÅ¼ne kroki przy stosowaniu t-SNE\n\n\n\n\nStandaryzacja danych â€” kaÅ¼da zmienna powinna byÄ‡ przeskalowana do Å›redniej 0 i wariancji 1, aby uniknÄ…Ä‡ dominacji jednej cechy w metryce euklidesowej.\nWybÃ³r zakresu perplexity â€” zwykle testuje siÄ™ kilka wartoÅ›ci (np. 5, 15, 30, 50) i ocenia stabilnoÅ›Ä‡ struktur (czy klastry sÄ… rozdzielne, czy stabilne wzglÄ™dem permutacji danych).\nUstawienie learning rate â€” zaczyna siÄ™ od wartoÅ›ci domyÅ›lnej (200) i w razie potrzeby zwiÄ™ksza do 500â€“1000, jeÅ›li klastry sÄ… zbyt zwarte.\nUstalenie liczby iteracji â€” co najmniej 500; w przypadku duÅ¼ych zbiorÃ³w moÅ¼na zwiÄ™kszyÄ‡ do 1000â€“2000, jeÅ›li rozkÅ‚ad nadal siÄ™ zmienia.\nPorÃ³wnanie z PCA lub UMAP â€” warto sprawdziÄ‡, czy t-SNE nie generuje artefaktÃ³w (np. sztucznych przerw miÄ™dzy klastrami), ktÃ³rych nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykÅ‚ad 5.4 (t-SNE na danych iris) Â \n\nKodlibrary(Rtsne)      # t-SNE\n\nset.seed(44)\n\n# Przygotowanie danych (jak wczeÅ›niej)\niris_data &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  as.matrix() %&gt;%\n  scale()\n\n# t-SNE\ntsne_result &lt;- Rtsne(\n  iris_data,\n  dims = 2, perplexity = 30, verbose = TRUE,\n  max_iter = 500, check_duplicates = FALSE\n)\n\nPerforming PCA\nRead the 150 x 4 data matrix successfully!\nUsing no_dims = 2, perplexity = 30.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.00 seconds (sparsity = 0.711156)!\nLearning embedding...\nIteration 50: error is 45.359967 (50 iterations in 0.01 seconds)\nIteration 100: error is 46.332059 (50 iterations in 0.01 seconds)\nIteration 150: error is 47.179047 (50 iterations in 0.01 seconds)\nIteration 200: error is 45.111348 (50 iterations in 0.01 seconds)\nIteration 250: error is 46.009835 (50 iterations in 0.01 seconds)\nIteration 300: error is 0.394921 (50 iterations in 0.01 seconds)\nIteration 350: error is 0.167269 (50 iterations in 0.01 seconds)\nIteration 400: error is 0.147481 (50 iterations in 0.01 seconds)\nIteration 450: error is 0.147297 (50 iterations in 0.01 seconds)\nIteration 500: error is 0.145512 (50 iterations in 0.01 seconds)\nFitting performed in 0.07 seconds.\n\nKodtsne_df &lt;- data.frame(\n  Dim1 = tsne_result$Y[,1],\n  Dim2 = tsne_result$Y[,2],\n  Species = iris$Species\n)\n\n# PCA na tych samych danych\npca_fit &lt;- prcomp(iris_data, center = FALSE, scale. = FALSE)\npca_df &lt;- data.frame(\n  PC1 = pca_fit$x[,1],\n  PC2 = pca_fit$x[,2],\n  Species = iris$Species\n)\n\n# Wykres t-SNE\np1 &lt;- ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"t-SNE\",\n       x = \"Wymiar 1\", y = \"Wymiar 2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# Wykres PCA (pierwsze dwie skÅ‚adowe)\np2 &lt;- ggplot(pca_df, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np1 | p2",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Metody redukcji wymiarowoÅ›ci</span>"
    ]
  },
  {
    "objectID": "pca.html#umap-konopka2018",
    "href": "pca.html#umap-konopka2018",
    "title": "Metody redukcji wymiarowoÅ›ci",
    "section": "UMAP (Konopka 2018)\n",
    "text": "UMAP (Konopka 2018)\n\nMetoda Uniform Manifold Approximation and Projection (UMAP) jest nieliniowÄ… technikÄ… redukcji wymiarowoÅ›ci opracowanÄ… przez McInnesa i Healyâ€™ego w 2018 roku. Jej celem jest odwzorowanie danych z przestrzeni wysokowymiarowej w przestrzeÅ„ o mniejszej liczbie wymiarÃ³w przy zachowaniu struktury geometrycznej â€” zarÃ³wno lokalnej, jak i globalnej â€” poprzez modelowanie danych jako rozkÅ‚adu na rozmaitoÅ›ci (manifold). W odrÃ³Å¼nieniu od metody t-SNE, UMAP opiera siÄ™ na teorii rozmaitoÅ›ci Riemanna oraz na pojÄ™ciach pochodzÄ…cych z teorii zbiorÃ³w rozmytych i topologii algebraicznej.\nNiech dane wejÅ›ciowe stanowiÄ… zbiÃ³r punktÃ³w \\[\nX = \\{x_1, x_2, \\dots, x_n\\}, \\quad x_i \\in \\mathbb{R}^p.\n\\] ZakÅ‚ada siÄ™, Å¼e punkty te leÅ¼Ä… na rozmaitoÅ›ci \\(\\mathcal{M} \\subset \\mathbb{R}^p\\) o niÅ¼szym wymiarze rzeczywistym \\(d &lt; p\\), zanurzonej w przestrzeni obserwowalnej. Metoda UMAP tworzy dwie probabilistyczne reprezentacje tej rozmaitoÅ›ci: po pierwsze, graf sÄ…siedztwa w przestrzeni wysokowymiarowej (fuzzy simplicial set), ktÃ³ry opisuje lokalne zaleÅ¼noÅ›ci miÄ™dzy punktami, oraz po drugie, graf w przestrzeni niskowymiarowej, ktÃ³rego struktura ma jak najlepiej odwzorowywaÄ‡ pierwszy.\nW celu konstrukcji grafu w przestrzeni wejÅ›ciowej dla kaÅ¼dego punktu \\(x_i\\) okreÅ›la siÄ™ odlegÅ‚oÅ›ci do jego \\(k\\)-najbliÅ¼szych sÄ…siadÃ³w. NastÄ™pnie wyznacza siÄ™ dwa parametry lokalne \\[\n\\rho_i = \\min_{j: d(x_i,x_j) &gt; 0} d(x_i, x_j),\n\\] czyli najmniejszÄ… dodatniÄ… odlegÅ‚oÅ›Ä‡ (umoÅ¼liwiajÄ…cÄ… niezerowÄ… gÄ™stoÅ›Ä‡), oraz \\(\\sigma_i &gt; 0,\\) skalÄ™ lokalnÄ… dobranÄ… tak, aby speÅ‚niony byÅ‚ warunek normalizacji entropii \\[\n\\sum_{j} \\exp\\!\\left(-\\frac{\\max(0, d(x_i,x_j) - \\rho_i)}{\\sigma_i}\\right) = \\log_2(k).\n\\] Na tej podstawie definiuje siÄ™ rozmyte prawdopodobieÅ„stwa sÄ…siedztwa \\[\np_{j|i} = \\exp\\!\\left(-\\frac{\\max(0, d(x_i, x_j) - \\rho_i)}{\\sigma_i}\\right).\n\\] PoniewaÅ¼ macierz tych wartoÅ›ci nie jest symetryczna, Å‚Ä…czy siÄ™ oba kierunki zgodnie z zasadami teorii zbiorÃ³w rozmytych \\[\np_{ij} = p_{i|j} + p_{j|i} - p_{i|j}\\,p_{j|i}.\n\\] Tak powstaÅ‚y rozmyty graf sÄ…siedztwa zawiera wagi \\(p_{ij}\\), ktÃ³re odzwierciedlajÄ… siÅ‚Ä™ poÅ‚Ä…czeÅ„ miÄ™dzy punktami.\nNastÄ™pnie w przestrzeni wynikowej \\(Y = \\{y_1, y_2, \\dots, y_n\\} \\subset \\mathbb{R}^q\\), gdzie zwykle \\(q = 2\\) lub 3, definiuje siÄ™ analogiczny rozmyty graf \\(q_{ij}\\), ktÃ³rego wagi opisuje funkcja jÄ…dra typu heavy-tailed \\[\nq_{ij} = \\frac{1}{1 + a\\,\\|y_i - y_j\\|^{2b}},\n\\] gdzie \\(a\\) i \\(b\\) sÄ… parametrami dopasowanymi empirycznie (standardowo \\(a \\approx 1.929,\\ b \\approx 0.7915\\)).\nZasadniczym celem UMAP jest znalezienie takiej konfiguracji punktÃ³w \\(Y\\), aby rozmyty graf \\(q_{ij}\\) jak najlepiej przybliÅ¼aÅ‚ graf \\(p_{ij}\\). Kryterium optymalizacji ma postaÄ‡ minimalizacji rozbieÅ¼noÅ›ci krzyÅ¼owej (ang. cross-entropy) miÄ™dzy dwoma rozkÅ‚adami sÄ…siedztwa \\[\nC = \\sum_{i &lt; j} \\left[ -p_{ij}\\log(q_{ij}) - (1 - p_{ij})\\log(1 - q_{ij}) \\right].\n\\] Minimalizacja tej funkcji jest realizowana metodami gradientowymi, zazwyczaj z wykorzystaniem stochastic gradient descent (SGD). W wyniku optymalizacji punkty \\(y_i\\) sÄ… przesuwane tak, aby utrzymaÄ‡ bliskie relacje w miejscach, gdzie \\(p_{ij}\\) jest duÅ¼e i rozdzielaÄ‡ punkty, gdzie \\(p_{ij}\\) jest maÅ‚e.\nZaÅ‚oÅ¼enia metody UMAP sÄ… stosunkowo niewielkie, lecz istotne. ZakÅ‚ada siÄ™, Å¼e dane leÅ¼Ä… na rozmaitoÅ›ci o niskim wymiarze, a wiÄ™c moÅ¼na je opisaÄ‡ poprzez ciÄ…gÅ‚Ä… strukturÄ™ geometrycznÄ…. Przyjmuje siÄ™ rÃ³wnieÅ¼, Å¼e uÅ¼yta miara odlegÅ‚oÅ›ci (zwykle euklidesowa) odzwierciedla faktyczne podobieÅ„stwo obserwacji oraz Å¼e rozkÅ‚ad punktÃ³w jest gÅ‚adki, czyli w maÅ‚ych sÄ…siedztwach struktura jest dobrze przybliÅ¼ana liniowo.\nInterpretacja wynikÃ³w UMAP nie odnosi siÄ™ do bezwzglÄ™dnych wartoÅ›ci wspÃ³Å‚rzÄ™dnych, lecz do relacji miÄ™dzy punktami. Punkty poÅ‚oÅ¼one blisko siebie w przestrzeni wynikowej sÄ… podobne w przestrzeni oryginalnej, a wiÄ™ksze odlegÅ‚oÅ›ci odpowiadajÄ… mniejszemu podobieÅ„stwu. W przeciwieÅ„stwie do t-SNE metoda ta lepiej zachowuje nie tylko lokalne klastry, ale rÃ³wnieÅ¼ czÄ™Å›ciowo strukturÄ™ globalnÄ…, co umoÅ¼liwia analizÄ™ gradientÃ³w i ciÄ…gÅ‚ych przejÅ›Ä‡ miÄ™dzy grupami obserwacji.\n\n\n\n\n\n\nAdnotacjaWaÅ¼ne kroki przy stosowaniu UMAP\n\n\n\n\nStandaryzacja danych â€” kaÅ¼da zmienna powinna byÄ‡ przeskalowana do Å›redniej 0 i wariancji 1, aby uniknÄ…Ä‡ dominacji jednej cechy w metryce euklidesowej.\nWybÃ³r liczby sÄ…siadÃ³w (n_neighbors) â€” kontroluje lokalnoÅ›Ä‡ odwzorowania; mniejsze wartoÅ›ci (5â€“15) podkreÅ›lajÄ… lokalne struktury, wiÄ™ksze (30â€“50) zachowujÄ… wiÄ™cej globalnych relacji.\nUstawienie wymiaru wynikowego (n_components) â€” zwykle 2 lub 3, w zaleÅ¼noÅ›ci od potrzeb wizualizacji.\nWybÃ³r metryki odlegÅ‚oÅ›ci â€” domyÅ›lnie euklidesowa, ale moÅ¼na uÅ¼yÄ‡ innych (np. Manhattan, cosine) w zaleÅ¼noÅ›ci od charakteru danych.\nPorÃ³wnanie z PCA lub t-SNE â€” warto sprawdziÄ‡, czy UMAP nie generuje artefaktÃ³w (np. sztucznych przerw miÄ™dzy klastrami), ktÃ³rych nie ma w prostszych odwzorowaniach.\n\n\n\n\nPrzykÅ‚ad 5.5 (UMAP na danych iris) Â \n\nKodlibrary(uwot)     # UMAP\n\nset.seed(44)\n\n# Przygotowanie danych: oddzieliÄ‡ etykiety klas i standaryzowaÄ‡ cechy\nX &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  scale() %&gt;%\n  as.matrix()\n\ny &lt;- iris$Species\n\n# UMAP 2D: podstawowe parametry\n# n_neighbors = \"skala lokalnoÅ›ci\", min_dist = \"zwartoÅ›Ä‡ klastrÃ³w\", metric = metryka odlegÅ‚oÅ›ci\nemb_umap &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist    = 0.1,\n  metric      = \"euclidean\",\n  n_components = 2,\n  verbose = TRUE\n)\n\n# Ramka wynikowa do wykresu\ndf_umap &lt;- data.frame(\n  UMAP1 = emb_umap[, 1],\n  UMAP2 = emb_umap[, 2],\n  Species = y\n)\n\n# Dla porÃ³wnania: PCA 2D (opcjonalnie)\npca &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  PC1 = pca$x[, 1],\n  PC2 = pca$x[, 2],\n  Species = y\n)\n\n# Wykresy\np_umap &lt;- ggplot(df_umap, aes(x = UMAP1, y = UMAP2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"UMAP\",\n       x = \"UMAP1\", y = \"UMAP2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\np_pca &lt;- ggplot(df_pca, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(title = \"PCA\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n# WyÅ›wietlenie obok siebie\np_umap | p_pca\n\n\n\n\n\n\n\n\nPoniÅ¼ej prezentujÄ™ zbiorcze porÃ³wnanie wszystkich omÃ³wionych metod redukcji wymiarowoÅ›ci na tym samym zbiorze danych iris. WykorzystujÄ™ PCA, ICA, MDS, t-SNE oraz UMAP, aby zobaczyÄ‡, jak rÃ³Å¼ne techniki odwzorowujÄ… strukturÄ™ danych.\n\nKodset.seed(44)\n\n# UsuÅ„ duplikaty (t-SNE i MDS niemetryczny sÄ… na to wraÅ¼liwe)\niris_unique &lt;- iris[!duplicated(iris[, -5]), ]\nX &lt;- as.matrix(scale(iris_unique[, -5]))\ny &lt;- iris_unique$Species\n\n# PCA (2 pierwsze skÅ‚adowe)\npca_fit &lt;- prcomp(X, center = FALSE, scale. = FALSE)\ndf_pca &lt;- data.frame(\n  Dim1 = pca_fit$x[, 1],\n  Dim2 = pca_fit$x[, 2],\n  Method = \"PCA\",\n  Species = y\n)\n\n# ICA (2 komponenty niezaleÅ¼ne)\nica_fit &lt;- fastICA(X, n.comp = 2, method = \"C\")\ndf_ica &lt;- data.frame(\n  Dim1 = ica_fit$S[, 1],\n  Dim2 = ica_fit$S[, 2],\n  Method = \"ICA\",\n  Species = y\n)\n\n# MDS metryczny (klasyczny)\nmds_metric &lt;- cmdscale(dist(X), k = 2)\ndf_mds_metric &lt;- data.frame(\n  Dim1 = mds_metric[, 1],\n  Dim2 = mds_metric[, 2],\n  Method = \"MDS (metryczny)\",\n  Species = y\n)\n\n# MDS niemetryczny (isoMDS)\nmds_nonmetric &lt;- isoMDS(dist(X), k = 2)$points\n\ninitial  value 4.818373 \nfinal  value 4.818117 \nconverged\n\nKoddf_mds_nonmetric &lt;- data.frame(\n  Dim1 = mds_nonmetric[, 1],\n  Dim2 = mds_nonmetric[, 2],\n  Method = \"MDS (niemet.)\",\n  Species = y\n)\n\n# t-SNE\ntsne_fit &lt;- Rtsne(\n  X, dims = 2, perplexity = 30,\n  max_iter = 750, check_duplicates = FALSE, verbose = FALSE\n)\ndf_tsne &lt;- data.frame(\n  Dim1 = tsne_fit$Y[, 1],\n  Dim2 = tsne_fit$Y[, 2],\n  Method = \"t-SNE\",\n  Species = y\n)\n\n# UMAP\numap_emb &lt;- umap(\n  X,\n  n_neighbors = 15,\n  min_dist = 0.1,\n  metric = \"euclidean\",\n  n_components = 2,\n  verbose = FALSE\n)\ndf_umap &lt;- data.frame(\n  Dim1 = umap_emb[, 1],\n  Dim2 = umap_emb[, 2],\n  Method = \"UMAP\",\n  Species = y\n)\n\n# PoÅ‚Ä…czenie wszystkich metod\ndf_all &lt;- bind_rows(\n  df_pca,\n  df_ica,\n  df_mds_metric,\n  df_mds_nonmetric,\n  df_tsne,\n  df_umap\n)\n\n# Wykres porÃ³wnawczy\nggplot(df_all, aes(Dim1, Dim2, color = Species)) +\n  geom_point(size = 2, alpha = 0.8) +\n  facet_wrap(~ Method, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"PorÃ³wnanie metod redukcji wymiarowoÅ›ci na zbiorze iris\",\n    x = \"Wymiar 1\", y = \"Wymiar 2\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nComon, Pierre. 1994. â€Independent Component Analysis, A New Concept?â€ Signal Processing 36 (3): 287â€“314. https://doi.org/10.1016/0165-1684(94)90029-9.\n\n\nKonopka, Tomasz. 2018. â€umap: Uniform Manifold Approximation and Projectionâ€. The R Foundation. https://doi.org/10.32614/cran.package.umap.\n\n\nKruskal, J. B. 1964. â€Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesisâ€. Psychometrika 29 (1): 1â€“27. https://doi.org/10.1007/bf02289565.\n\n\nMaaten, Laurens van der, i Geoffrey Hinton. 2008. â€Visualizing Data using t-SNEâ€. Journal of Machine Learning Research 9 (86): 2579â€“2605. http://jmlr.org/papers/v9/vandermaaten08a.html.\n\n\nPearson, Karl. 1901. â€LIII. On Lines and Planes of Closest Fit to Systems of Points in Spaceâ€. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559â€“72. https://doi.org/10.1080/14786440109462720.\n\n\nTorgerson, Warren S. 1952. â€Multidimensional Scaling: I. Theory and Methodâ€. Psychometrika 17 (4): 401â€“19. https://doi.org/10.1007/bf02288916.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Metody redukcji wymiarowoÅ›ci</span>"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "Analiza skupieÅ„",
    "section": "",
    "text": "Rys historyczny\nAnaliza skupieÅ„, znana rÃ³wnieÅ¼ jako cluster analysis, ma swoje korzenie w poÅ‚owie XX wieku, choÄ‡ jej podstawy koncepcyjne pojawiÅ‚y siÄ™ znacznie wczeÅ›niej w statystyce i biologii systematycznej. Jej rozwÃ³j przebiegaÅ‚ rÃ³wnolegle w kilku dziedzinach, w tym w psychologii, biologii, socjologii i informatyce, a z czasem staÅ‚a siÄ™ jednym z fundamentalnych narzÄ™dzi eksploracyjnej analizy danych. Pierwsze idee grupowania obiektÃ³w o podobnych cechach moÅ¼na odnaleÅºÄ‡ juÅ¼ w XVIII i XIX wieku w klasyfikacji biologicznej. Carl Linneusz wprowadziÅ‚ system binominalny oparty na cechach morfologicznych organizmÃ³w, co stanowiÅ‚o wczesny przykÅ‚ad klasyfikacji hierarchicznej. WspÃ³Å‚czesne podejÅ›cie matematyczne do analizy skupieÅ„ zaczÄ™Å‚o siÄ™ jednak ksztaÅ‚towaÄ‡ dopiero w XX wieku wraz z rozwojem metod statystycznych i koncepcji odlegÅ‚oÅ›ci w przestrzeni wielowymiarowej. Za wÅ‚aÅ›ciwy poczÄ…tek analizy skupieÅ„ w sensie statystycznym uznaje siÄ™ lata 30. i 40. XX wieku. W 1939 roku Tryon wprowadziÅ‚ pojÄ™cie analizy grupowej (cluster analysis) w psychologii, stosujÄ…c jÄ… do klasyfikacji zmiennych i jednostek na podstawie macierzy podobieÅ„stw. W latach 50. i 60. intensywny rozwÃ³j metod klasyfikacji hierarchicznej byÅ‚ zwiÄ…zany z rozwojem biologii numerycznej (numerical taxonomy), gÅ‚Ã³wnie dziÄ™ki pracom Sokalâ€™a i Sneathâ€™a, ktÃ³rzy w latach 60. zaproponowali formalne podstawy taksonomii numerycznej opartej na macierzach podobieÅ„stw miÄ™dzy organizmami. Lata 60. i 70. XX wieku przyniosÅ‚y znaczÄ…cy rozwÃ³j metod niehierarchicznych, w tym przede wszystkim metody k-means, zaproponowanej przez MacQueena w 1967 roku. Algorytm ten staÅ‚ siÄ™ jednym z najczÄ™Å›ciej stosowanych narzÄ™dzi w analizie skupieÅ„ dziÄ™ki swojej prostocie, interpretowalnoÅ›ci i efektywnoÅ›ci obliczeniowej. W tym samym okresie rozwijano rÃ³wnieÅ¼ metody oparte na gÄ™stoÅ›ci (np. pÃ³Åºniejszy DBSCAN), metody probabilistyczne (modele mieszanek Gaussa) oraz techniki optymalizacyjne pozwalajÄ…ce na automatyczne wyznaczanie liczby skupieÅ„. W latach 80. i 90. wraz z rozwojem informatyki oraz eksploracji danych (data mining), analiza skupieÅ„ zaczÄ™Å‚a byÄ‡ szeroko stosowana w zastosowaniach praktycznych â€“ od segmentacji rynku, przez rozpoznawanie obrazÃ³w, po bioinformatykÄ™. PojawiÅ‚y siÄ™ rÃ³wnieÅ¼ metody adaptacyjne i oparte na uczeniu nienadzorowanym, w tym sieci neuronowe typu self-organizing maps (SOM) opracowane przez Kohonena. W XXI wieku analiza skupieÅ„ staÅ‚a siÄ™ kluczowym elementem nauki o danych (data science). WspÃ³Å‚czesne metody integrujÄ… klasyczne podejÅ›cia statystyczne z algorytmami uczenia maszynowego. Opracowano wiele nowych technik, takich jak metody oparte na gÄ™stoÅ›ci (DBSCAN, OPTICS), metody spektralne wykorzystujÄ…ce wartoÅ›ci wÅ‚asne macierzy podobieÅ„stwa, czy algorytmy gÅ‚Ä™bokiego grupowania (deep clustering) bazujÄ…ce na sieciach neuronowych. RÃ³wnoczeÅ›nie rozwiniÄ™to teoretyczne podstawy walidacji skupieÅ„, takie jak wspÃ³Å‚czynniki silhouette, indeks Calinskiego-Harabasza czy Davies-Bouldin, umoÅ¼liwiajÄ…ce obiektywnÄ… ocenÄ™ jakoÅ›ci grupowania.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Analiza skupieÅ„</span>"
    ]
  },
  {
    "objectID": "cluster.html#podziaÅ‚-metod-analizy-skupieÅ„",
    "href": "cluster.html#podziaÅ‚-metod-analizy-skupieÅ„",
    "title": "Analiza skupieÅ„",
    "section": "PodziaÅ‚ metod analizy skupieÅ„",
    "text": "PodziaÅ‚ metod analizy skupieÅ„\nMetody analizy skupieÅ„ moÅ¼na klasyfikowaÄ‡ wedÅ‚ug rÃ³Å¼nych kryteriÃ³w, takich jak sposÃ³b tworzenia skupieÅ„, zaÅ‚oÅ¼enia o strukturze danych, rodzaj miary podobieÅ„stwa czy sposÃ³b reprezentacji wynikÃ³w. NajczÄ™Å›ciej przyjmuje siÄ™ podziaÅ‚ taksonomiczny oparty na sposobie grupowania obiektÃ³w, ktÃ³ry pozwala wyrÃ³Å¼niÄ‡ cztery gÅ‚Ã³wne klasy metod: hierarchiczne, niehierarchiczne, oparte na gÄ™stoÅ›ci i oparte na modelach probabilistycznych.\nPierwszÄ… i jednÄ… z najstarszych kategorii sÄ… metody hierarchiczne. Ich istotÄ… jest budowa dendrogramu odzwierciedlajÄ…cego stopniowe Å‚Ä…czenie (lub rozdzielanie) obiektÃ³w w skupienia. WyrÃ³Å¼nia siÄ™ dwa podejÅ›cia: aglomeracyjne, ktÃ³re rozpoczynajÄ… od traktowania kaÅ¼dego obiektu jako odrÄ™bnego skupienia i nastÄ™pnie Å‚Ä…czÄ… je zgodnie z okreÅ›lonÄ… miarÄ… odlegÅ‚oÅ›ci (np. metoda pojedynczego, peÅ‚nego lub Å›redniego wiÄ…zania), oraz dzielÄ…ce, ktÃ³re zaczynajÄ… od jednego skupienia zawierajÄ…cego wszystkie obiekty i w kolejnych krokach dokonujÄ… jego podziaÅ‚u. Metody hierarchiczne majÄ… tÄ™ zaletÄ™, Å¼e nie wymagajÄ… wczeÅ›niejszego okreÅ›lenia liczby skupieÅ„, lecz ich wadÄ… jest wysoka zÅ‚oÅ¼onoÅ›Ä‡ obliczeniowa i wraÅ¼liwoÅ›Ä‡ na szumy.\nDrugÄ… grupÄ™ stanowiÄ… metody niehierarchiczne, wÅ›rÃ³d ktÃ³rych najbardziej znane sÄ… algorytmy typu k-means oraz k-medoids. Ich celem jest bezpoÅ›rednie przypisanie kaÅ¼dego obiektu do jednego z ustalonej liczby skupieÅ„ na podstawie minimalizacji sumy kwadratÃ³w odlegÅ‚oÅ›ci wewnÄ…trzgrupowych. Metoda k-means jest szybka i skuteczna przy danych o wyraÅºnie kulistych skupieniach, natomiast k-medoids (np. algorytm PAM) jest bardziej odporna na wartoÅ›ci odstajÄ…ce. Do tej kategorii naleÅ¼Ä… rÃ³wnieÅ¼ algorytmy optymalizacyjne, takie jak k-means++ czy mini-batch k-means, dostosowane do duÅ¼ych zbiorÃ³w danych.\nTrzeciÄ… kategoriÄ™ tworzÄ… metody oparte na gÄ™stoÅ›ci, w ktÃ³rych skupienia definiuje siÄ™ jako obszary przestrzeni danych o wysokim zagÄ™szczeniu punktÃ³w oddzielone obszarami o niskiej gÄ™stoÅ›ci. Klasycznym przykÅ‚adem jest algorytm DBSCAN, ktÃ³ry wykrywa skupienia dowolnego ksztaÅ‚tu i pozwala automatycznie identyfikowaÄ‡ punkty szumu. UdoskonalonÄ… wersjÄ… tej metody jest OPTICS, umoÅ¼liwiajÄ…ca hierarchiczne przedstawienie struktur gÄ™stoÅ›ciowych. Metody tego typu sÄ… szczegÃ³lnie uÅ¼yteczne przy analizie danych przestrzennych oraz w sytuacjach, gdy skupienia nie majÄ… regularnego ksztaÅ‚tu.\nCzwartÄ… grupÄ… sÄ… metody oparte na modelach probabilistycznych. ZakÅ‚adajÄ… one, Å¼e dane pochodzÄ… z mieszaniny rozkÅ‚adÃ³w (najczÄ™Å›ciej wielowymiarowych normalnych), a zadaniem algorytmu jest estymacja parametrÃ³w tych rozkÅ‚adÃ³w oraz przypisanie obiektÃ³w do skupieÅ„ na podstawie maksymalnego prawdopodobieÅ„stwa. Do tej kategorii naleÅ¼Ä… modele mieszanek Gaussa (GMM) estymowane metodÄ… EM (Expectationâ€“Maximization), ktÃ³re umoÅ¼liwiajÄ… probabilistyczne przypisanie obiektÃ³w do wielu skupieÅ„ z rÃ³Å¼nym stopniem przynaleÅ¼noÅ›ci.\nPoza gÅ‚Ã³wnymi czterema klasami wyrÃ³Å¼nia siÄ™ rÃ³wnieÅ¼ metody hybrydowe i wspÃ³Å‚czesne podejÅ›cia uczenia nienadzorowanego. PrzykÅ‚adem sÄ… metody spektralne, ktÃ³re wykorzystujÄ… analizÄ™ wartoÅ›ci wÅ‚asnych macierzy podobieÅ„stwa, oraz metody gÅ‚Ä™bokiego grupowania (deep clustering), integrujÄ…ce sieci neuronowe autoenkoderowe z klasycznymi procedurami klastrowania1.\n1Â Ten rodzaj klastrowania nie bÄ™dzie przedmiotem tego rozdziaÅ‚u poniewaÅ¼ wykracza poza klasyczne podejÅ›cie statystyczne i wymaga wiedzy na temat sieci neuronowych, ktÃ³ra pojawia siÄ™ na pÃ³Åºniejszych semestrach.\n\nPodziaÅ‚ metod grupowania",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Analiza skupieÅ„</span>"
    ]
  },
  {
    "objectID": "cluster.html#metody-hierarchiczne",
    "href": "cluster.html#metody-hierarchiczne",
    "title": "Analiza skupieÅ„",
    "section": "Metody hierarchiczne",
    "text": "Metody hierarchiczne\nMetody hierarchiczne w analizie skupieÅ„ opierajÄ… siÄ™ na iteracyjnym Å‚Ä…czeniu lub dzieleniu obiektÃ³w w sposÃ³b odzwierciedlajÄ…cy ich podobieÅ„stwo, prowadzÄ…c do utworzenia struktury drzewiastej (dendrogramu). Struktura ta ukazuje hierarchiczne relacje miÄ™dzy obiektami â€“ od indywidualnych elementÃ³w aÅ¼ po jednÄ… nadrzÄ™dnÄ… grupÄ™ lub odwrotnie. WyrÃ³Å¼nia siÄ™ dwa gÅ‚Ã³wne podejÅ›cia: metody aglomeracyjne oraz deglomeracyjne (dzielÄ…ce).\nMetody aglomeracyjne\nW podejÅ›ciu aglomeracyjnym proces rozpoczyna siÄ™ od traktowania kaÅ¼dego obiektu jako odrÄ™bnego skupienia jednoelementowego. NastÄ™pnie w kolejnych krokach Å‚Ä…czy siÄ™ dwa najbardziej podobne skupienia, aÅ¼ do momentu uzyskania jednego skupienia zawierajÄ…cego wszystkie obiekty. Proces ten moÅ¼na formalnie zapisaÄ‡ nastÄ™pujÄ…co\n\nNiech zbiÃ³r danych skÅ‚ada siÄ™ z \\(n\\) obiektÃ³w \\[\nX = \\{x_1, x_2, \\ldots, x_n\\},\n\\] gdzie kaÅ¼dy obiekt \\(x_i \\in \\mathbb{R}^p.\\)\n\nPoczÄ…tkowo kaÅ¼dy obiekt stanowi odrÄ™bne skupienie \\[\nC_i^{(0)} = \\{x_i\\} \\quad \\text{dla}\\quad i = 1, \\ldots, n.\n\\]\n\nDefiniuje siÄ™ macierz odlegÅ‚oÅ›ci \\(D = [d(x_i, x_j)]\\), gdzie funkcja \\(d(\\cdot, \\cdot)\\) okreÅ›la miarÄ™ odlegÅ‚oÅ›ci (np. euklidesowÄ…, Mahalanobisa, Manhattan).\nNa kaÅ¼dym kroku \\(t\\) wyszukuje siÄ™ dwa skupienia \\(C_p^{(t)}\\) i \\(C_q^{(t)}\\), ktÃ³re sÄ… najbliÅ¼sze wzglÄ™dem przyjÄ™tej miary odlegÅ‚oÅ›ci miÄ™dzy skupieniami \\(D(C_p, C_q)\\). NastÄ™pnie Å‚Ä…czy siÄ™ je w jedno nowe skupienie \\[\nC_{pq}^{(t+1)} = C_p^{(t)} \\cup C_q^{(t)}.\n\\]\n\nOdlegÅ‚oÅ›ci miÄ™dzy nowo utworzonym skupieniem a pozostaÅ‚ymi aktualizuje siÄ™ zgodnie z przyjÄ™tÄ… reguÅ‚Ä… wiÄ…zania (linkage criterion). Niech \\(D(C_i,C_j)\\) oznacza odlegÅ‚oÅ›Ä‡ klasterâ€“klaster, \\(d(x,y)\\) bazowÄ… odlegÅ‚oÅ›Ä‡ punktâ€“punkt, \\(|C_i|=n_i\\) licznoÅ›Ä‡ klastra \\(C_i\\), \\(\\bar x_i\\) centroid \\(C_i\\). ReguÅ‚a aglomeracji w postaci rekurencji Lanceâ€™aâ€“Williamsa przyjmuje wÃ³wczas postaÄ‡ \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\alpha_i D(C_i,C_k)+\\alpha_j D(C_j,C_k)+\\beta D(C_i,C_j)+\\gamma\\,\\big|D(C_i,C_k)-D(C_j,C_k)\\big|,\n\\] z wspÃ³Å‚czynnikami (\\(\\alpha_i,\\alpha_j,\\beta,\\gamma\\)) zaleÅ¼nymi od wybranego sposobu Å‚Ä…czenia. Dla metod centroidowych i Warda inicjalizujemy macierz odlegÅ‚oÅ›ci kwadratami odlegÅ‚oÅ›ci euklidesowych i interpretujemy wyniki jako wartoÅ›ci kwadratowe. MoÅ¼emy wÃ³wczas wyrÃ³Å¼niÄ‡ nastÄ™pujÄ…ce metody aglomeracji:\n\nMetoda pojedynczego wiÄ…zania (single linkage) \\[\nD(C_i,C_j)=\\min_{x\\in C_i,\\,y\\in C_j} d(x,y).\n\\] Zbiory Å‚Ä…czymy reguÅ‚Ä… \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\min\\!\\big(D(C_i,C_k),\\,D(C_j,C_k)\\big),\n\\] co odpowiada \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=0,\\ \\gamma=-\\tfrac12\\) przy inicjalizacji \\(D(\\{x\\},\\{y\\})=d(x,y)\\).\nMetoda peÅ‚nego wiÄ…zania (complete linkage) \\[\nD(C_i,C_j)=\\max_{x\\in C_i,\\,y\\in C_j} d(x,y).\n\\] Zbiory Å‚Ä…czymy reguÅ‚Ä… \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\max\\!\\big(D(C_i,C_k),\\,D(C_j,C_k)\\big),\n\\] czyli \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=0,\\ \\gamma=+\\tfrac12\\), z inicjalizacjÄ… \\(d(x,y)\\).\nMetoda Å›redniego wiÄ…zania (average linkage, UPGMA - Unweighted Pair Group Method using Arithmetic Averages) \\[\nD(C_i,C_j)=\\frac{1}{n_i n_j}\\sum_{x\\in C_i}\\sum_{y\\in C_j} d(x,y).\n\\] Zbiory Å‚Ä…czymy reguÅ‚Ä… \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\frac{n_i\\,D(C_i,C_k)+n_j\\,D(C_j,C_k)}{n_i+n_j},\n\\] co daje \\(\\alpha_i=\\tfrac{n_i}{n_i+n_j},\\ \\alpha_j=\\tfrac{n_j}{n_i+n_j},\\ \\beta=\\gamma=0\\).\nMetoda waÅ¼onego Å›redniego wiÄ…zania (weighted average linkage, WPGMA - Weighted Pair Group Method using Arithmetic Averages, McQuitty) - zbiory Å‚Ä…czymy reguÅ‚Ä… \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\tfrac12\\big(D(C_i,C_k)+D(C_j,C_k)\\big),\n\\] tj. \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=\\gamma=0\\), przy inicjalizacji \\(d(x,y)\\).\nMetoda centroidÃ³w (centroid linkage, UPGMC - Unweighted Pair Group Method using Centroids)2 - definicja przez centroidy (wymaga kwadratÃ³w odlegÅ‚oÅ›ci euklidesowych) \\[\nD(C_i,C_j)=\\|\\bar x_i-\\bar x_j\\|^2,\\qquad \\bar x_i=\\frac{1}{n_i}\\sum_{x\\in C_i}x.\n\\] Zbiory Å‚Ä…czymy reguÅ‚Ä… \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\frac{n_i}{n_i+n_j}D(C_i,C_k)+\\frac{n_j}{n_i+n_j}D(C_j,C_k)-\\frac{n_i n_j}{(n_i+n_j)^2}D(C_i,C_j),\n\\] co odpowiada \\(\\alpha_i=\\tfrac{n_i}{n_i+n_j},\\ \\alpha_j=\\tfrac{n_j}{n_i+n_j},\\ \\beta=-\\tfrac{n_i n_j}{(n_i+n_j)^2},\\ \\gamma=0\\), inicjalizujemy \\(D(\\{x\\},\\{y\\})=\\|x-y\\|^2\\).\nMetoda mediany (median linkage, WPGMC - Weighted Pair Group Method using Centroids) - centra klastrÃ³w aktualizujemy przez punkt Å›rodkowy median \\(m_{i\\cup j}=\\tfrac12(m_i+m_j).\\) Zbiory Å‚Ä…czymy reguÅ‚Ä… \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\tfrac12\\big(D(C_i,C_k)+D(C_j,C_k)\\big)-\\tfrac14\\,D(C_i,C_j),\n\\] czyli \\(\\alpha_i=\\alpha_j=\\tfrac12,\\ \\beta=-\\tfrac14,\\ \\gamma=0\\), z inicjalizacjÄ… \\(D(\\{x\\},\\{y\\})=\\|x-y\\|^2\\).\nMetoda Warda (Wardâ€™s linkage) \\[\nD(C_i,C_j)=\\frac{2\\,n_i n_j}{n_i+n_j}\\,\\|\\bar x_i-\\bar x_j\\|^2,\n\\] Zbiory Å‚Ä…czymy reguÅ‚Ä… \\[\nD\\big((C_i\\!\\cup\\!C_j),C_k\\big)=\\frac{n_i+n_k}{n_i+n_j+n_k}D(C_i,C_k)+\\frac{n_j+n_k}{n_i+n_j+n_k}D(C_j,C_k)-\\frac{n_k}{n_i+n_j+n_k}D(C_i,C_j),\n\\] przy inicjalizacji \\(D(\\{x\\},\\{y\\})=\\|x-y\\|^2\\).\n\n\nProces powtarza siÄ™ do momentu, gdy wszystkie obiekty znajdÄ… siÄ™ w jednym skupieniu, tworzÄ…c hierarchiczny ukÅ‚ad poÅ‚Ä…czeÅ„.\n\n2Â W metodach centroidowych, medianowej i Warda podkreÅ›la siÄ™ koniecznoÅ›Ä‡ pracy na kwadratach odlegÅ‚oÅ›ci euklidesowych.ZaletÄ… metod aglomeracyjnych jest to, Å¼e nie wymagajÄ… a priori okreÅ›lenia liczby skupieÅ„. WadÄ… jest natomiast ich nieodwracalnoÅ›Ä‡ â€“ raz poÅ‚Ä…czone skupienia nie mogÄ… zostaÄ‡ rozdzielone, a wynik koÅ„cowy jest wraÅ¼liwy na wybÃ³r miary odlegÅ‚oÅ›ci i kryterium wiÄ…zania.\n\nPrzykÅ‚ad 6.1 Â \n\nKodlibrary(factoextra)\nlibrary(ggpubr)\n\n# 1. Przygotowanie danych: standaryzacja czterech cech numerycznych\nX &lt;- scale(iris[, 1:4])\n\n# 2. Macierz odlegÅ‚oÅ›ci euklidesowych\nd &lt;- dist(X, method = \"euclidean\")\n\n# 3. Budowa dendrogramÃ³w dla rÃ³Å¼nych metod Å‚Ä…czenia\nhc_single &lt;- hclust(d, method = \"single\")\nhc_complete &lt;- hclust(d, method = \"complete\")\nhc_average &lt;- hclust(d, method = \"average\") # UPGMA\nhc_ward &lt;- hclust(d, method = \"ward.D2\") # Ward (zalecany wariant .D2)\n\n# 4. Wizualizacja dendrogramÃ³w z liniÄ… ciÄ™cia na K=3\np_d_single &lt;- fviz_dend(\n  hc_single,\n  k = 3,\n  cex = 0.6,\n  main = \"single linkage (K=3)\"\n)\np_d_complete &lt;- fviz_dend(\n  hc_complete,\n  k = 3,\n  cex = 0.6,\n  main = \"complete linkage (K=3)\"\n)\np_d_average &lt;- fviz_dend(\n  hc_average,\n  k = 3,\n  cex = 0.6,\n  main = \"average linkage (K=3)\"\n)\np_d_ward &lt;- fviz_dend(hc_ward, k = 3, cex = 0.6, main = \"Ward.D2 (K=3)\")\n\nggarrange(p_d_single, p_d_complete, p_d_average, p_d_ward, ncol = 2, nrow = 2)\n\n\n\n\n\n\nKod# 5. Uzyskanie etykiet klastrÃ³w dla K=3 i rzutowanie grup w przestrzeÅ„ PCA\ncl_single &lt;- cutree(hc_single, k = 3)\ncl_complete &lt;- cutree(hc_complete, k = 3)\ncl_average &lt;- cutree(hc_average, k = 3)\ncl_ward &lt;- cutree(hc_ward, k = 3)\n\np_c_single &lt;- fviz_cluster(\n  list(data = X, cluster = cl_single),\n  geom = \"point\",\n  ellipse.type = \"norm\",\n  main = \"single linkage\"\n)\np_c_complete &lt;- fviz_cluster(\n  list(data = X, cluster = cl_complete),\n  geom = \"point\",\n  ellipse.type = \"norm\",\n  main = \"complete linkage\"\n)\np_c_average &lt;- fviz_cluster(\n  list(data = X, cluster = cl_average),\n  geom = \"point\",\n  ellipse.type = \"norm\",\n  main = \"average linkage\"\n)\np_c_ward &lt;- fviz_cluster(\n  list(data = X, cluster = cl_ward),\n  geom = \"point\",\n  ellipse.type = \"norm\",\n  main = \"Ward.D2\"\n)\n\nggarrange(p_c_single, p_c_complete, p_c_average, p_c_ward, ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\nMetody deglomeracyjne\nMetody deglomeracyjne (dzielÄ…ce) stanowiÄ… odwrotnoÅ›Ä‡ podejÅ›cia aglomeracyjnego. Zaczyna siÄ™ od jednego skupienia zawierajÄ…cego wszystkie obiekty, ktÃ³re nastÄ™pnie sÄ… iteracyjnie dzielone na mniejsze podzbiory, aÅ¼ do osiÄ…gniÄ™cia oczekiwanej liczby skupieÅ„ lub speÅ‚nienia kryterium zatrzymania.\nFormalnie proces moÅ¼na przedstawiÄ‡ w postaci\n\nPoczÄ…tkowo przyjmuje siÄ™ jedno skupienie \\[C^{(0)} = X.\\]\n\nW kaÅ¼dym kroku wybiera siÄ™ skupienie \\(C_i^{(t)}\\), ktÃ³re zostanie podzielone. WybÃ³r ten moÅ¼e wynikaÄ‡ z maksymalnej wariancji wewnÄ…trzgrupowej, liczby elementÃ³w lub innych kryteriÃ³w jakoÅ›ci skupieÅ„.\nDokonuje siÄ™ podziaÅ‚u wybranego skupienia na dwa mniejsze, minimalizujÄ…c bÅ‚Ä…d wewnÄ…trzgrupowegy lub maksymalizujÄ…c rÃ³Å¼nice miÄ™dzygrupowe. NajczÄ™Å›ciej stosuje siÄ™ algorytm analogiczny do bisecting k-means \\[\nC_i^{(t)} \\rightarrow \\{C_{i1}^{(t+1)}, C_{i2}^{(t+1)}\\},\n\\] przy czym podziaÅ‚ realizuje siÄ™ poprzez iteracyjne zastosowanie k-means z \\(k = 2\\).\nProces dzielenia jest powtarzany do momentu uzyskania Å¼Ä…danej liczby skupieÅ„ lub gdy dalszy podziaÅ‚ nie prowadzi do istotnej poprawy jakoÅ›ci.\n\nMetody deglomeracyjne sÄ… mniej popularne z powodu wyÅ¼szego kosztu obliczeniowego i koniecznoÅ›ci przyjÄ™cia dodatkowych kryteriÃ³w decyzyjnych dotyczÄ…cych wyboru skupienia do podziaÅ‚u. Jednak w duÅ¼ych zbiorach danych mogÄ… byÄ‡ efektywniejsze niÅ¼ aglomeracyjne, szczegÃ³lnie gdy implementuje siÄ™ je z wykorzystaniem metod heurystycznych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Analiza skupieÅ„</span>"
    ]
  },
  {
    "objectID": "cluster.html#metody-niehierarchiczne",
    "href": "cluster.html#metody-niehierarchiczne",
    "title": "Analiza skupieÅ„",
    "section": "Metody niehierarchiczne",
    "text": "Metody niehierarchiczne\nMetody niehierarchiczne w analizie skupieÅ„ koncentrujÄ… siÄ™ na bezpoÅ›rednim przypisaniu obiektÃ³w do okreÅ›lonej liczby skupieÅ„ bez tworzenia struktury hierarchicznej. Najbardziej znane i szeroko stosowane sÄ… algorytmy typu k-means oraz k-medoids. PodstawÄ… matematycznÄ… metod niehierarchicznych jest minimalizacja pewnej funkcji celu, najczÄ™Å›ciej sumy kwadratÃ³w odchyleÅ„ punktÃ³w od Å›rodkÃ³w grup, zwanych centroidami. W przeciwieÅ„stwie do podejÅ›cia hierarchicznego, proces ten ma charakter iteracyjny i wymaga wczeÅ›niejszego okreÅ›lenia liczby klastrÃ³w. W efekcie powstaje partycja przestrzeni danych, w ktÃ³rej kaÅ¼dy obiekt zostaje przypisany do jednego lub kilku klastrÃ³w w zaleÅ¼noÅ›ci od przyjÄ™tej koncepcji przynaleÅ¼noÅ›ci (podziaÅ‚ pÅ‚aski3). W tym kontekÅ›cie wyrÃ³Å¼nia siÄ™ dwa gÅ‚Ã³wne typy podziaÅ‚Ã³w: podziaÅ‚ twardy i podziaÅ‚ rozmyty.\n3Â podziaÅ‚ pÅ‚aski oznacza, Å¼e kaÅ¼dy obiekt naleÅ¼y do dokÅ‚adnie jednej grupy i nie istnieje hierarchia miÄ™dzy grupamiPodziaÅ‚ twardy\nPodziaÅ‚ twardy opiera siÄ™ na jednoznacznym przypisaniu kaÅ¼dego obiektu do dokÅ‚adnie jednego klastra. W ujÄ™ciu matematycznym przyjmuje siÄ™, Å¼e dla zbioru obserwacji \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\) oraz ustalonej liczby klastrÃ³w \\(K\\), istnieje macierz przynaleÅ¼noÅ›ci \\(U = [u_{ik}]\\), w ktÃ³rej kaÅ¼dy element przyjmuje wartoÅ›Ä‡ 0 lub 1. WartoÅ›Ä‡ \\(u_{ik} = 1\\) oznacza, Å¼e obiekt \\(x_i\\) naleÅ¼y do klastra \\(k\\), natomiast \\(u_{ik} = 0\\) â€“ Å¼e do niego nie naleÅ¼y.\nPierwszym warunkiem podziaÅ‚u twardego jest to, Å¼e kaÅ¼dy obiekt musi naleÅ¼eÄ‡ dokÅ‚adnie do jednej grupy. Oznacza to, Å¼e dla kaÅ¼dego obiektu suma przynaleÅ¼noÅ›ci po wszystkich klastrach rÃ³wna siÄ™ jeden \\[\n\\sum_{k=1}^{K} u_{ik} = 1, \\quad \\forall i \\in \\{1, 2, \\ldots, n\\}.\n\\] Z kolei kaÅ¼dy klaster powinien zawieraÄ‡ przynajmniej jeden element, co moÅ¼na zapisaÄ‡ jako \\[\n1 \\leq \\sum_{i=1}^{n} u_{ik}, \\quad \\forall k \\in \\{1, 2, \\ldots, K\\}.\n\\] Wynika z tego, Å¼e nie dopuszcza siÄ™ powstawania pustych grup. Kolejnym warunkiem jest binarnoÅ›Ä‡ przypisaÅ„, czyli \\[\nu_{ik} \\in \\{0, 1\\}, \\quad \\forall i, k.\n\\] PrzynaleÅ¼noÅ›Ä‡ obiektu do klastra jest zatem caÅ‚kowita i nie dopuszcza stanÃ³w poÅ›rednich. Wreszcie, formalnym celem podziaÅ‚u twardego jest minimalizacja funkcji bÅ‚Ä™du, ktÃ³ra okreÅ›la sumÄ™ kwadratÃ³w odchyleÅ„ poszczegÃ³lnych obiektÃ³w od centroidÃ³w klastrÃ³w, do ktÃ³rych zostaÅ‚y przypisane \\[\nJ = \\sum_{k=1}^{K} \\sum_{i=1}^{n} u_{ik} \\, \\|x_i - \\mu_k\\|^2,\n\\] gdzie \\(\\mu_k\\) oznacza Å›rodek klastra \\(k\\), a \\(\\|\\cdot\\|\\) jest najczÄ™Å›ciej normÄ… euklidesowÄ…. KaÅ¼dy obiekt powinien zostaÄ‡ przypisany do tego klastra, ktÃ³rego centroid jest najbliÅ¼szy, czyli \\[\nu_{ik} =\n\\begin{cases}\n1, & \\text{jeÅ›li } k = \\arg \\min_{j} \\|x_i - \\mu_j\\|, \\\\\n0, & \\text{w przeciwnym razie.}\n\\end{cases}\n\\]\nPodziaÅ‚ rozmyty\nPodziaÅ‚ rozmyty (ang. fuzzy clustering) stanowi uogÃ³lnienie klasycznego, twardego podejÅ›cia do grupowania, w ktÃ³rym dopuszcza siÄ™ moÅ¼liwoÅ›Ä‡ czÄ™Å›ciowej przynaleÅ¼noÅ›ci obiektu do wiÄ™cej niÅ¼ jednego klastra. Zamiast przypisywaÄ‡ kaÅ¼dy element jednoznacznie do jednej grupy, wprowadza siÄ™ pojÄ™cie stopnia przynaleÅ¼noÅ›ci, ktÃ³ry przyjmuje wartoÅ›ci z przedziaÅ‚u \\([0,1]\\). W ten sposÃ³b odzwierciedla siÄ™ niepewnoÅ›Ä‡ lub pÅ‚ynnoÅ›Ä‡ granic miÄ™dzy grupami, co czyni tÄ™ metodÄ™ bardziej elastycznÄ… i lepiej dostosowanÄ… do danych o niejednoznacznej strukturze.\nMatematycznie, dla zbioru obserwacji \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\) oraz ustalonej liczby klastrÃ³w \\(K\\), definiuje siÄ™ macierz przynaleÅ¼noÅ›ci \\(U = [u_{ik}]\\), gdzie kaÅ¼dy element \\(u_{ik}\\) oznacza stopieÅ„, w jakim obiekt \\(x_i\\) naleÅ¼y do klastra \\(k\\). W odrÃ³Å¼nieniu od podziaÅ‚u twardego, tutaj \\(u_{ik} \\in [0,1]\\), a nie tylko \\(\\{0,1\\}\\). Zachowany zostaje jednak warunek, Å¼e suma stopni przynaleÅ¼noÅ›ci danego obiektu do wszystkich klastrÃ³w musi byÄ‡ rÃ³wna jeden \\[\n\\sum_{k=1}^{K} u_{ik} = 1, \\quad \\forall i \\in \\{1, 2, \\ldots, n\\}.\n\\] Warunek ten oznacza, Å¼e przynaleÅ¼noÅ›ci majÄ… charakter wzglÄ™dny â€“ im silniejszy zwiÄ…zek obiektu z jednym klastrem, tym sÅ‚abszy z innymi.\nPodziaÅ‚ rozmyty opiera siÄ™ na minimalizacji rozmytej funkcji celu, znanej z algorytmu Fuzzy c-means \\[\nJ_m = \\sum_{k=1}^{K} \\sum_{i=1}^{n} (u_{ik})^m \\, \\|x_i - \\mu_k\\|^2,\n\\] gdzie \\(\\mu_k\\) oznacza centroid klastra \\(k\\), a parametr \\(m &gt; 1\\) kontroluje poziom rozmycia. Im wiÄ™ksza wartoÅ›Ä‡ \\(m\\), tym bardziej rozmyty staje siÄ™ podziaÅ‚, poniewaÅ¼ rÃ³Å¼nice pomiÄ™dzy wartoÅ›ciami przynaleÅ¼noÅ›ci poszczegÃ³lnych obiektÃ³w do klastrÃ³w ulegajÄ… spÅ‚aszczeniu. W praktyce najczÄ™Å›ciej przyjmuje siÄ™ \\(m = 2\\). Optymalizacja funkcji celu prowadzi do nastÄ™pujÄ…cych warunkÃ³w aktualizacji. Stopnie przynaleÅ¼noÅ›ci obliczane sÄ… wedÅ‚ug wzoru \\[\nu_{ik} = \\frac{1}{\\sum_{j=1}^{K} \\left( \\frac{\\|x_i - \\mu_k\\|}{\\|x_i - \\mu_j\\|} \\right)^{\\frac{2}{m-1}}},\n\\] natomiast nowe poÅ‚oÅ¼enie centroidÃ³w wyznacza siÄ™ jako waÅ¼onÄ… Å›redniÄ… punktÃ³w, gdzie wagi stanowiÄ… stopnie przynaleÅ¼noÅ›ci podniesione do potÄ™gi \\(m\\) \\[\n\\mu_k = \\frac{\\sum_{i=1}^{n} (u_{ik})^m x_i}{\\sum_{i=1}^{n} (u_{ik})^m}.\n\\] Proces ten przebiega iteracyjnie â€“ w kaÅ¼dej iteracji obliczane sÄ… nowe wartoÅ›ci \\(u_{ik}\\) i \\(\\mu_k\\), aÅ¼ do osiÄ…gniÄ™cia zbieÅ¼noÅ›ci funkcji celu \\(J_m\\).\nMetoda k-Å›rednich (k-means)\nAlgorytm k-means wynika z problemu minimalizacji sumy kwadratÃ³w odchyleÅ„ punktÃ³w od reprezentantÃ³w grup w metryce euklidesowej. Niech dany bÄ™dzie zbiÃ³r obserwacji \\(X=\\{x_1,\\dots,x_n\\}\\subset\\mathbb{R}^p\\) oraz liczba klastrÃ³w \\(K\\). Celem jest znalezienie partycji danych i wektorÃ³w \\(\\mu_1,\\dots,\\mu_K\\in\\mathbb{R}^p\\) minimalizujÄ…cych funkcjÄ™ celu \\[\nJ(U,\\mu)=\\sum_{k=1}^K\\sum_{i=1}^n u_{ik}\\,\\|x_i-\\mu_k\\|^2,\n\\] gdzie \\(U=[u_{ik}]\\) jest macierzÄ… przypisaÅ„ speÅ‚niajÄ…cÄ… warunki podziaÅ‚u twardego \\(u_{ik}\\in\\{0,1\\}\\), \\(\\sum_{k=1}^K u_{ik}=1\\) dla kaÅ¼dego \\(i\\), a \\(\\sum_{i=1}^n u_{ik}\\ge 1\\) dla kaÅ¼dego \\(k\\).\nWyprowadzenie algorytmu polega na zastosowaniu naprzemiennej minimalizacji wzglÄ™dem \\(U\\) i \\(\\mu\\), poniewaÅ¼ jednoczesna minimalizacja jest problemem kombinatorycznym trudnym obliczeniowo. RozwaÅ¼my najpierw minimalizacjÄ™ wzglÄ™dem centroidÃ³w przy ustalonych przypisaniach. Dla danego \\(k\\) rozwaÅ¼my funkcjÄ™ \\[\nJ_k(\\mu_k)=\\sum_{i=1}^n u_{ik}\\,\\|x_i-\\mu_k\\|^2.\n\\] Jest to funkcja kwadratowa Å›ciÅ›le wypukÅ‚a w \\(\\mu_k\\). Obliczamy gradient \\[\n\\nabla_{\\mu_k}J_k(\\mu_k)=2\\sum_{i=1}^n u_{ik}\\,(\\mu_k-x_i)=2\\left(\\Big(\\sum_{i}u_{ik}\\Big)\\mu_k-\\sum_{i}u_{ik}x_i\\right).\n\\] Warunek \\(\\nabla_{\\mu_k}J_k(\\mu_k)=0\\) daje \\[\n\\mu_k^\\star=\\frac{\\sum_{i=1}^n u_{ik}x_i}{\\sum_{i=1}^n u_{ik}},\n\\] czyli optymalny centroid jest Å›redniÄ… arytmetycznÄ… punktÃ³w przypisanych do klastra. WypukÅ‚oÅ›Ä‡ zapewnia, Å¼e jest to minimum globalne wzglÄ™dem \\(\\mu_k\\). Zatem przy ustalonych \\(U\\) krok aktualizacji centroidÃ³w ma postaÄ‡ Å›redniej waÅ¼onej ze wskaÅºnikami \\(u_{ik}\\).\nNastÄ™pnie dokonujemy minimalizacji wzglÄ™dem przypisaÅ„ przy ustalonych centroidach. Dla kaÅ¼dego obiektu \\(x_i\\) problem redukuje siÄ™ do \\[\n\\min_{u_{i1},\\dots,u_{iK}} \\sum_{k=1}^K u_{ik}\\,\\|x_i-\\mu_k\\|^2\\quad \\text{przy}\\quad u_{ik}\\in\\{0,1\\},\\ \\sum_k u_{ik}=1.\n\\] PoniewaÅ¼ wyraÅ¼enie jest liniowe w \\(u_{ik}\\), optimum osiÄ…ga siÄ™, wybierajÄ…c \\(u_{ik}=1\\) dla indeksu \\(k\\) minimalizujÄ…cego odlegÅ‚oÅ›Ä‡ euklidesowÄ… \\[\nu_{ik}=\\mathbf{1}_\\left\\{k=\\operatorname{argmin}_{j\\in\\{1,\\dots,K\\}}\\|x_i-\\mu_j\\|^2\\right\\}.\n\\] Wynika stÄ…d reguÅ‚a â€przypisz do najbliÅ¼szego centroiduâ€, co geometrycznie odpowiada podziaÅ‚owi przestrzeni na komÃ³rki Woronoja wyznaczone przez \\(\\{\\mu_k\\}\\).\n\nZÅ‚oÅ¼enie obu krokÃ³w prowadzi do procedury znanej jako Lloydâ€™s algorithm:\n\nStartujemy od wstÄ™pnych centroidÃ³w \\(\\mu^{(0)}\\).\nNaprzemiennie wykonujemy przypisanie do najbliÅ¼szego centroidu.\nPrzeprowadzamy aktualizacjÄ™ centroidÃ³w jako Å›rednich.\nWykonujemy kroki 2-3 aÅ¼ do osiÄ…gniÄ™cia zbieÅ¼noÅ›ci (punkty nie zmieniajÄ… swoich skupieÅ„).\n\nKaÅ¼dy z krokÃ³w 2-3 nie zwiÄ™ksza funkcji celu, bo przy ustalonych centroidach wybÃ³r najbliÅ¼szego centroidu minimalizuje skÅ‚adnik \\(\\|x_i-\\mu_k\\|^2\\) dla kaÅ¼dego \\(i\\), wiÄ™c \\(J\\) maleje lub pozostaje staÅ‚a, a przy ustalonych przypisaniach do klastrÃ³w wybÃ³r Å›redniej minimalizuje sumÄ™ kwadratÃ³w, wiÄ™c \\(J\\) rÃ³wnieÅ¼ maleje lub pozostaje staÅ‚a. PoniewaÅ¼ istnieje skoÅ„czona liczba moÅ¼liwych partycji i \\(J\\ge 0\\), monotonicznie niemalejÄ…ca sekwencja wartoÅ›ci funkcji celu musi zatrzymaÄ‡ siÄ™ w skoÅ„czonej liczbie krokÃ³w na punkcie stacjonarnym, czyli minimum lokalnym problemu z ograniczeniami twardych przypisaÅ„.\nWarto zauwaÅ¼yÄ‡, Å¼e rÃ³wnowaÅ¼nie moÅ¼na interpretowaÄ‡ cel jako minimalizacjÄ™ wariancji wewnÄ…trzklastrowej. W klasycznej dekompozycji SST \\[\n\\underbrace{\\sum_{i=1}^n \\|x_i-\\bar{x}\\|^2}_{\\text{SST}}=\\underbrace{\\sum_{k=1}^K\\sum_{i=1}^n u_{ik}\\|x_i-\\mu_k\\|^2}_{\\text{WSS}}+\\underbrace{\\sum_{k=1}^K n_k\\|\\mu_k-\\bar{x}\\|^2}_{\\text{BSS}},\n\\] gdzie \\(\\bar{x}\\) jest Å›redniÄ… globalnÄ…, a \\(n_k=\\sum_i u_{ik}\\). Minimalizacja WSS (ang. within-cluster sum of squares) przy zadanym \\(K\\) jest rÃ³wnowaÅ¼na maksymalizacji BSS, czyli maksymalizacji separacji centroidÃ³w wzglÄ™dem Å›redniej globalnej, co formalnie uzasadnia intuicjÄ™ â€maksymalizowaÄ‡ jednorodnoÅ›Ä‡ wewnÄ…trz klastrÃ³w i rÃ³Å¼nice miÄ™dzy klastramiâ€.\n\n\n\n\n\n\nAdnotacjak-means++\n\n\n\nZastosowanie inicjalizacji k-means++ polega na losowaniu poczÄ…tkÃ³w z uprzywilejowaniem punktÃ³w odlegÅ‚ych od juÅ¼ wybranych centroidÃ³w, co w sensie teoretycznym daje gwarancje aproksymacyjne rzÄ™du \\(O(\\log K)\\) wzglÄ™dem optimum oczekiwanego, a w praktyce istotnie poprawia jakoÅ›Ä‡ minimum lokalnego.\n\n\n\n\n\n\n\n\nOstrzeÅ¼enieWarunki stacjonarnoÅ›ci otrzymanego rozwiÄ…zania\n\n\n\nPara \\((U^\\star,\\mu^\\star)\\) jest punktem staÅ‚ym algorytmu wtedy i tylko wtedy, gdy speÅ‚nia jednoczeÅ›nie dwa warunki: po pierwsze \\(\\mu_k^\\star\\) sÄ… Å›rednimi swoich klastrÃ³w, po drugie przypisania \\(U^\\star\\) sÄ… zgodne z najbliÅ¼szymi centroidami \\(\\mu^\\star.\\) Takie rozwiÄ…zanie speÅ‚nia warunki optymalnoÅ›ci pierwszego rzÄ™du wzglÄ™dem naprzemiennych blokÃ³w zmiennych i stanowi minimum lokalne funkcji \\(J\\) na zbiorze dopuszczalnych rozwiÄ…zaÅ„ wyznaczonych z ograniczeniami twardych przypisaÅ„.\n\n\nMetoda k-medoidÃ³w (k-medoids)\nMetoda k-medoid (ang. k-medoids) stanowi bliski odpowiednik klasycznej metody k-means, lecz wprowadza zasadniczÄ… zmianÄ™ w sposobie definiowania reprezentanta klastra. Zamiast centroidu obliczanego jako Å›rednia arytmetyczna wszystkich punktÃ³w w danym klastrze, metoda k-medoid wykorzystuje medoid, czyli rzeczywisty punkt ze zbioru danych, ktÃ³ry minimalizuje sumÄ™ odlegÅ‚oÅ›ci do pozostaÅ‚ych elementÃ³w tego samego klastra. DziÄ™ki temu metoda ta jest bardziej odporna na obserwacje odstajÄ…ce oraz umoÅ¼liwia zastosowanie dowolnej miary odlegÅ‚oÅ›ci, nie tylko euklidesowej.\nNiech dany bÄ™dzie zbiÃ³r obserwacji \\(X = \\{x_1, x_2, \\ldots, x_n\\} \\subset \\mathbb{R}^p\\) oraz liczba klastrÃ³w \\(K\\). Celem jest podziaÅ‚ zbioru \\(X\\) na \\(K\\) grup w taki sposÃ³b, aby suma odlegÅ‚oÅ›ci pomiÄ™dzy punktami a reprezentantami ich klastrÃ³w byÅ‚a minimalna. FunkcjÄ™ celu moÅ¼na zapisaÄ‡ jako \\[\nJ(M, U) = \\sum_{k=1}^{K} \\sum_{i=1}^{n} u_{ik} \\, d(x_i, m_k),\n\\] gdzie \\(M = \\{m_1, m_2, \\ldots, m_K\\} \\subset X\\) to zbiÃ³r medoidÃ³w, \\(U = [u_{ik}]\\) to macierz przypisaÅ„ punktÃ³w do klastrÃ³w, a \\(d(x_i, m_k)\\) oznacza wybranÄ… miarÄ™ odlegÅ‚oÅ›ci. Dla kaÅ¼dego obiektu zachodzi warunek \\(u_{ik} \\in \\{0,1\\}\\) oraz \\(\\sum_{k=1}^{K} u_{ik} = 1\\), co oznacza, Å¼e kaÅ¼dy punkt naleÅ¼y dokÅ‚adnie do jednego klastra. Medoid klastra definiuje siÄ™ jako punkt \\(m_k \\in X\\), ktÃ³ry minimalizuje sumÄ™ odlegÅ‚oÅ›ci do wszystkich pozostaÅ‚ych punktÃ³w tego klastra \\[\nm_k = \\operatorname{argmin}_{x_j \\in X_k} \\sum_{x_i \\in X_k} d(x_i, x_j),\n\\] gdzie \\(X_k = \\{x_i : u_{ik} = 1\\}\\).\nW praktyce metoda realizowana jest iteracyjnie, analogicznie do k-means, ale z innym sposobem aktualizacji reprezentantÃ³w. Najbardziej znanym algorytmem implementujÄ…cym tÄ™ ideÄ™ jest PAM (Partitioning Around Medoids). Procedura ta obejmuje nastÄ™pujÄ…ce kroki. Po pierwsze, inicjalizuje siÄ™ losowo \\(K\\) punktÃ³w jako poczÄ…tkowe medoidy. NastÄ™pnie kaÅ¼dy obiekt przypisywany jest do najbliÅ¼szego medoidu, zgodnie z reguÅ‚Ä… \\[\nu_{ik} = \\mathbf{1}_\\left\\{\\,k = \\operatorname{argmin}_{j} d(x_i, m_j)\\right\\}.\n\\] W ten sposÃ³b powstaje podziaÅ‚ przestrzeni na obszary przypominajÄ…ce komÃ³rki Woronoja. W kolejnym kroku, dla kaÅ¼dego klastra wybiera siÄ™ nowy medoid, czyli punkt, ktÃ³ry minimalizuje sumÄ™ odlegÅ‚oÅ›ci do pozostaÅ‚ych punktÃ³w w tym klastrze. Algorytm powtarza naprzemienne kroki przypisania i aktualizacji aÅ¼ do momentu, gdy zestaw medoidÃ³w przestaje siÄ™ zmieniaÄ‡ lub wartoÅ›Ä‡ funkcji celu stabilizuje siÄ™.\nMetoda k-medoid jest blisko spokrewniona z metodÄ… k-means, ktÃ³ra minimalizuje sumÄ™ kwadratÃ³w odlegÅ‚oÅ›ci euklidesowych \\[\nJ_{\\text{k-means}} = \\sum_{k=1}^{K}\\sum_{i=1}^{n} u_{ik}\\,\\|x_i - \\mu_k\\|^2,\n\\] gdzie \\(\\mu_k\\) oznacza centroid klastra. W k-medoid zamiast Å›redniej stosuje siÄ™ rzeczywisty punkt danych, a w funkcji celu pojawia siÄ™ bezpoÅ›rednia odlegÅ‚oÅ›Ä‡, nie jej kwadrat. W konsekwencji metoda k-means jest szybsza, lecz wraÅ¼liwa na wartoÅ›ci odstajÄ…ce i ograniczona do przestrzeni euklidesowych, natomiast k-medoid jest bardziej odporna i umoÅ¼liwia pracÄ™ z dowolnymi macierzami odlegÅ‚oÅ›ci, takÅ¼e nieliczbowymi.\n\n\n\n\n\n\nOstrzeÅ¼enie\n\n\n\nWarto odrÃ³Å¼niÄ‡ metodÄ™ k-medoid od metody k-median. W k-medoid reprezentantem klastra jest rzeczywisty punkt ze zbioru danych, natomiast w k-median mediana klastra moÅ¼e znajdowaÄ‡ siÄ™ w dowolnym miejscu przestrzeni. Funkcja celu w k-median minimalizuje sumÄ™ odlegÅ‚oÅ›ci w sensie L1 (Manhattan) \\[\nJ_{\\text{k-median}} = \\sum_{k=1}^{K}\\sum_{i=1}^{n} u_{ik} \\, \\|x_i - m_k\\|_1.\n\\] Zatem k-median stanowi ciÄ…gÅ‚y odpowiednik metody k-medoid, podobnie jak k-means jest wersjÄ… ciÄ…gÅ‚Ä… dla odlegÅ‚oÅ›ci euklidesowych w kwadracie.\n\n\n\n\n\n\n\n\n\n\nCechy\nk-means\nk-median\nk-medoid\n\n\n\nReprezentant\nÅ›rednia arytmetyczna (punkt w â„áµ–)\nmediana geometryczna (punkt w â„áµ–)\nrzeczywisty punkt danych\n\n\nMiara odlegÅ‚oÅ›ci\nkwadrat euklidesowej\nManhattan (L1)\ndowolna miara\n\n\nOdpornoÅ›Ä‡ na odstajÄ…ce\nniska\nÅ›rednia\nwysoka\n\n\nTyp zmiennych\nciÄ…gÅ‚e\nciÄ…gÅ‚e\ndowolne (takÅ¼e kategoryczne)\n\n\nCLARA i CLARANS\nAlgorytmy CLARA i CLARANS stanowiÄ… rozwiniÄ™cia metody k-medoid, opracowane w celu rozwiÄ…zania problemu wysokiej zÅ‚oÅ¼onoÅ›ci obliczeniowej klasycznego algorytmu PAM. Oba podejÅ›cia zachowujÄ… tÄ™ samÄ… ideÄ™ â€” minimalizacjÄ™ sumy odlegÅ‚oÅ›ci punktÃ³w do reprezentantÃ³w (medoidÃ³w) â€” lecz rÃ³Å¼niÄ… siÄ™ strategiÄ… poszukiwania najlepszego zbioru medoidÃ³w w duÅ¼ych zbiorach danych.\nAlgorytm CLARA (Clustering LARge Applications)\nAlgorytm CLARA zostaÅ‚ zaproponowany przez Kaufmana i Rousseeuwa (1990) jako metoda przybliÅ¼ona dla k-medoids, umoÅ¼liwiajÄ…ca efektywne dziaÅ‚anie przy duÅ¼ej liczbie obserwacji. KluczowÄ… ideÄ… CLARA jest ograniczenie peÅ‚nych obliczeÅ„ do prÃ³bek danych, zamiast caÅ‚ego zbioru.\nProcedura przebiega w kilku etapach:\n\nLosowanie prÃ³bki - z caÅ‚ego zbioru danych \\(X\\) losuje siÄ™ podzbiÃ³r \\(S \\subset X\\) o umiarkowanej licznoÅ›ci (np. 5â€“10% wszystkich obserwacji).\nZastosowanie algorytmu PAM - na wylosowanej prÃ³bce \\(S\\) przeprowadza siÄ™ peÅ‚nÄ… procedurÄ™ PAM w celu wyznaczenia \\(K\\) medoidÃ³w \\(M_S = \\{m_1, \\ldots, m_K\\}\\).\nOcena jakoÅ›ci podziaÅ‚u - uzyskane medoidy testuje siÄ™ na caÅ‚ym zbiorze danych, obliczajÄ…c wartoÅ›Ä‡ funkcji kosztu \\[\nJ(M_S) = \\sum_{i=1}^{n} \\min_{m_k \\in M_S} d(x_i, m_k),\n\\] czyli sumÄ™ odlegÅ‚oÅ›ci kaÅ¼dego punktu do najbliÅ¼szego medoidu.\nPowtÃ³rzenia i wybÃ³r najlepszego rozwiÄ…zania - proces losowania prÃ³bki i przeprowadzania PAM powtarza siÄ™ kilka razy (np. 5â€“10), a koÅ„cowy wynik wybiera siÄ™ na podstawie minimalnej wartoÅ›ci funkcji celu \\(J(M_S)\\).\n\nZaletÄ… CLARA jest znaczne obniÅ¼enie kosztÃ³w obliczeniowych w porÃ³wnaniu z PAM, ktÃ³rego zÅ‚oÅ¼onoÅ›Ä‡ wynosi \\(O(k(n-k)^2)\\). W CLARA zÅ‚oÅ¼onoÅ›Ä‡ zaleÅ¼y od rozmiaru prÃ³bki, a nie caÅ‚ego zbioru, co umoÅ¼liwia stosowanie metody na duÅ¼ych danych. WadÄ… jest jednak moÅ¼liwoÅ›Ä‡ utraty jakoÅ›ci rozwiÄ…zania, jeÅ›li prÃ³bka nie jest reprezentatywna â€” w szczegÃ³lnoÅ›ci, jeÅ›li pomija mniejsze skupienia obecne w zbiorze danych.\nAlgorytm CLARANS (Clustering Large Applications based on RANdomized Search)\nAlgorytm CLARANS, opracowany przez Ng i Hana (1994), stanowi dalsze rozwiniÄ™cie CLARA i PAM, oparte na losowym przeszukiwaniu przestrzeni moÅ¼liwych zbiorÃ³w medoidÃ³w. Jego dziaÅ‚anie inspirowane jest technikami heurystycznymi, takimi jak local search lub simulated annealing. CLARANS traktuje przestrzeÅ„ wszystkich moÅ¼liwych zestawÃ³w medoidÃ³w jako graf, w ktÃ³rym kaÅ¼dy wierzchoÅ‚ek odpowiada pewnemu zestawowi \\(K\\) medoidÃ³w, a krawÄ™dzie Å‚Ä…czÄ… wierzchoÅ‚ki rÃ³Å¼niÄ…ce siÄ™ jednym medoidem. Poszukiwanie najlepszego rozwiÄ…zania odbywa siÄ™ przez losowe przechodzenie po tym grafie, przy czym zmiany medoidÃ³w dokonuje siÄ™ tylko wtedy, gdy poprawiajÄ… funkcjÄ™ celu.\nSchemat dziaÅ‚ania moÅ¼na opisaÄ‡ nastÄ™pujÄ…co:\n\nInicjalizacja - losowo wybraÄ‡ zestaw \\(K\\) medoidÃ³w \\(M\\).\nLosowa eksploracja sÄ…siedztwa - spoÅ›rÃ³d wszystkich moÅ¼liwych â€zamianâ€ jednego medoidu \\(m \\in M\\) na punkt niebÄ™dÄ…cy medoidem \\(x \\in X \\setminus M\\), losowo wybraÄ‡ kilka par kandydatÃ³w (tzw. neighbors).\nOcena sÄ…siadÃ³w - dla kaÅ¼dego kandydata obliczyÄ‡ zmianÄ™ funkcji celu \\[\n\\Delta J = J(Mâ€™) - J(M),\n\\] gdzie \\(Mâ€™\\) to nowy zestaw medoidÃ³w po zamianie.\nKrok optymalizacyjny - jeÅ›li znajdzie siÄ™ sÄ…siad z mniejszÄ… wartoÅ›ciÄ… funkcji kosztu, przyjÄ…Ä‡ go jako nowy zestaw medoidÃ³w \\(M \\leftarrow Mâ€™\\).\nKontynuacja - powtarzaÄ‡ losowe przeszukiwanie do osiÄ…gniÄ™cia lokalnego minimum (brak poprawiajÄ…cych siÄ™ sÄ…siadÃ³w) lub do wyczerpania limitu iteracji.\nPowtÃ³rzenia - dla zwiÄ™kszenia szansy znalezienia rozwiÄ…zania globalnego, procedurÄ™ powtarza siÄ™ kilka razy z rÃ³Å¼nymi poczÄ…tkowymi zestawami medoidÃ³w.\n\nCLARANS jest wiÄ™c algorytmem probabilistycznym, ktÃ³ry w kaÅ¼dym kroku dokonuje losowej eksploracji przestrzeni moÅ¼liwych rozwiÄ…zaÅ„. W przeciwieÅ„stwie do CLARA nie ogranicza siÄ™ do jednej prÃ³bki danych, lecz do ograniczonej liczby losowo sprawdzanych sÄ…siadÃ³w, co pozwala zachowaÄ‡ kompromis miÄ™dzy dokÅ‚adnoÅ›ciÄ… a szybkoÅ›ciÄ….\n\nPrzykÅ‚ad 6.2 Â \n\nKodlibrary(cluster)\n\nset.seed(44) \nX &lt;- scale(iris[, 1:4]) \nd &lt;- dist(X)\n\n# 1. k-means\n\nkmeans_res &lt;- kmeans(X, centers = 3, nstart = 25) \np_kmeans &lt;- fviz_cluster(kmeans_res, data = X, geom = \"point\", ellipse.type =\"norm\", main = \"k-means\")\n\n# 2. k-medoids (PAM) \npam_res &lt;- pam(X, k = 3) \np_pam &lt;- fviz_cluster(pam_res, geom = \"point\", ellipse.type =\"norm\", main = \"PAM\n(k-medoids)\")\n\n# 3. CLARA \nclara_res &lt;- clara(X, k = 3, samples = 5, pamLike=TRUE) \np_clara &lt;- fviz_cluster(clara_res, geom = \"point\", ellipse.type =\n\"norm\", main = \"CLARA\")\n\n# 4. CLARANS \nlibrary(fastkmedoids) \nclarans_res &lt;- fastclarans(d, k = 3, n = nrow(X)) \np_clarans &lt;- fviz_cluster(list(data = X, cluster = clara_res$clustering), geom = \"point\", ellipse.type = \"norm\", main =\n\"CLARANS\") \nggarrange(p_kmeans, p_pam, p_clara, p_clarans, ncol = 2, nrow = 2)",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Analiza skupieÅ„</span>"
    ]
  },
  {
    "objectID": "cluster.html#metody-oparte-na-gÄ™stoÅ›ci",
    "href": "cluster.html#metody-oparte-na-gÄ™stoÅ›ci",
    "title": "Analiza skupieÅ„",
    "section": "Metody oparte na gÄ™stoÅ›ci",
    "text": "Metody oparte na gÄ™stoÅ›ci\nMetody oparte na gÄ™stoÅ›ci traktujÄ… klaster jako obszar przestrzeni cech, w ktÃ³rym punkty wystÄ™pujÄ… gÄ™Å›ciej niÅ¼ w otoczeniu. Zamiast narzucaÄ‡ kuliste ksztaÅ‚ty lub minimalizowaÄ‡ wariancjÄ™, jak w metodach centroidowych, identyfikuje siÄ™ spÃ³jne â€wyspyâ€ wysokiej gÄ™stoÅ›ci oddzielone obszarami niskiej gÄ™stoÅ›ci. KluczowÄ… konsekwencjÄ… jest naturalna obsÅ‚uga szumu: punkty w rzadkich rejonach pozostajÄ… nieprzypisane, co sprzyja detekcji anomalii. Klasyczny przedstawiciel, czyli DBSCAN (ang. Density-Based Spatial Clustering of Applications with Noise), definiuje gÄ™stoÅ›Ä‡ lokalnie przez promieÅ„ \\(\\varepsilon\\) oraz prÃ³g licznoÅ›ci MinPts. Punkt rdzeniowy (ang. core point) to taki, ktÃ³ry ma co najmniej MinPts sÄ…siadÃ³w w kuli o promieniu \\(\\varepsilon\\). Punkty w zasiÄ™gu rdzeniowych tworzÄ… Å‚aÅ„cuchy dostÄ™pnoÅ›ci gÄ™stoÅ›ciowej, a maksymalne zbiory tak poÅ‚Ä…czone stanowiÄ… klastry. Punkty w zasiÄ™gu \\(\\varepsilon\\), ktÃ³re same nie sÄ… rdzeniowe, traktujemy jako brzegowe i doÅ‚Ä…czamy do pobliskich klastrÃ³w; pozostaÅ‚e uznajemy za szum. ZaletÄ… jest moÅ¼liwoÅ›Ä‡ wykrywania klastrÃ³w o dowolnym ksztaÅ‚cie, odpornoÅ›Ä‡ na pojedyncze wartoÅ›ci odstajÄ…ce i brak koniecznoÅ›ci z gÃ³ry podawania liczby klastrÃ³w.\nGÅ‚Ã³wnym ograniczeniem DBSCAN jest koniecznoÅ›Ä‡ ustalenia globalnego parametr \\(\\varepsilon\\): przy zrÃ³Å¼nicowanej gÄ™stoÅ›ci danych jedno ustawienie nie odzwierciedla wszystkich struktur (dla maÅ‚ego \\(\\varepsilon\\) drobne, gÄ™ste klastry sÄ… poprawne, ale rzadkie sÄ… traktowane rozÅ‚Ä…cznie; dla duÅ¼ego \\(\\varepsilon\\) rzadkie siÄ™ Å‚Ä…czÄ…, a gÄ™ste zlewajÄ…). Problem moÅ¼e Å‚agodziÄ‡ metoda OPTICS (ang. Ordering Points To Identify the Clustering Structure). W metodzie OPTICS pojÄ™cia core-distance i reachability-distance sÅ‚uÅ¼Ä… do opisania lokalnej gÄ™stoÅ›ci punktÃ³w w sposÃ³b ciÄ…gÅ‚y, bez potrzeby ustalania z gÃ³ry jednego progu \\(\\varepsilon\\), jak w klasycznym DBSCAN.\nAlgorytm HDBSCAN (ang. Hierarchical Density-Based Spatial Clustering of Applications with Noise) jest uogÃ³lnieniem metody DBSCAN do jej hierarchicznego rozwiniÄ™cia. GÅ‚Ã³wna idea polega na tym, by nie wybieraÄ‡ z gÃ³ry jednego progu gÄ™stoÅ›ci (czyli parametru \\(\\varepsilon\\)), lecz analizowaÄ‡ strukturÄ™ skupieÅ„ w szerokim zakresie poziomÃ³w gÄ™stoÅ›ci i budowaÄ‡ ich hierarchiÄ™. W klasycznym DBSCAN wynik zaleÅ¼y od jednego promienia \\(\\varepsilon\\): jeÅ›li dane zawierajÄ… obszary o rÃ³Å¼nej gÄ™stoÅ›ci, trudno dobraÄ‡ jednÄ… wartoÅ›Ä‡ odpowiedniÄ… dla wszystkich. HDBSCAN eliminuje tÄ™ sÅ‚aboÅ›Ä‡, zastÄ™pujÄ…c staÅ‚e \\(\\varepsilon\\) ciÄ…gÅ‚ym parametrem opisujÄ…cym zmiennoÅ›Ä‡ gÄ™stoÅ›ci i tworzÄ…c dendrogram gÄ™stoÅ›ci â€“ czyli hierarchiczne drzewo pokazujÄ…ce, jak klastry pojawiajÄ… siÄ™ i Å‚Ä…czÄ… przy stopniowym â€rozluÅºnianiuâ€ kryterium gÄ™stoÅ›ci.\nAlgorytm DBSCAN\nNiech \\(X = \\{x_1, x_2, \\dots, x_n\\} \\subset \\mathbb{R}^p\\) oznaczaÄ‡ zbiÃ³r obserwacji, a \\(d(\\cdot,\\cdot)\\) dowolnÄ… metrykÄ™ w tej przestrzeni (np. euklidesowÄ…). Algorytm DBSCAN opiera siÄ™ na dwÃ³ch parametrach:\n\npromieniu sÄ…siedztwa \\(\\varepsilon &gt; 0\\),\nminimalnej liczbie punktÃ³w w sÄ…siedztwie \\(\\text{MinPts} \\in \\mathbb{N}\\).\n\nNa tej podstawie definiuje siÄ™ nastÄ™pujÄ…ce pojÄ™cia:\n\n\n\\(\\varepsilon\\)-sÄ…siedztwo punktu\n\nDla kaÅ¼dego punktu \\(x \\in X\\) definiuje siÄ™ jego sÄ…siedztwo w promieniu \\(\\varepsilon\\) \\[\n\\mathcal{N}_\\varepsilon(x) = \\{ y \\in X : d(x,y) \\le \\varepsilon \\}.\n\\] LicznoÅ›Ä‡ tego zbioru, oznaczona jako \\(|\\mathcal{N}_\\varepsilon(x)| = \\rho_\\varepsilon(x)\\), stanowi miarÄ™ lokalnej gÄ™stoÅ›ci wokÃ³Å‚ punktu \\(x\\).\n\nPunkt rdzeniowy (core point)\n\nPunkt \\(x \\in X\\) nazywamy rdzeniowym, jeÅ›li liczba punktÃ³w w jego sÄ…siedztwie jest co najmniej rÃ³wna progowi gÄ™stoÅ›ci \\[\nx \\ \\text{jest rdzeniowy} \\quad \\Longleftrightarrow \\quad |\\mathcal{N}_\\varepsilon(x)| \\ge \\text{MinPts}.\n\\] ZbiÃ³r wszystkich punktÃ³w rdzeniowych oznaczamy jako \\[\nC_{\\text{core}} = \\{x \\in X : |\\mathcal{N}_\\varepsilon(x)| \\ge \\text{MinPts}\\}.\n\\]\n\nOsiÄ…galnoÅ›Ä‡ gÄ™stoÅ›ciowa (density reachability)\n\nPunkt \\(y \\in X\\) jest bezpoÅ›rednio osiÄ…galny gÄ™stoÅ›ciowo z punktu rdzeniowego \\(x\\), jeÅ›li \\[\ny \\in \\mathcal{N}_\\varepsilon(x) \\quad \\text{i} \\quad x \\in C_{\\text{core}}.\n\\] Oznaczamy tÄ™ relacjÄ™ jako \\(y \\leftarrow x\\).\nPunkt \\(y\\) jest osiÄ…galny gÄ™stoÅ›ciowo z \\(x\\), jeÅ›li istnieje ciÄ…g punktÃ³w \\((x_1, x_2, \\dots, x_m) \\subset X\\) taki, Å¼e \\[\nx_1 = x, \\quad x_m = y, \\quad \\text{oraz} \\quad x_{i+1} \\leftarrow x_i \\ \\text{dla kaÅ¼dego } i=1,\\dots,m-1.\n\\] Zapisujemy wtedy \\(y \\overset{*}{\\leftarrow} x\\).\nRelacja osiÄ…galnoÅ›ci gÄ™stoÅ›ciowej jest przechodnia, lecz nie jest symetryczna, poniewaÅ¼ ostatni punkt Å‚aÅ„cucha nie musi byÄ‡ rdzeniowy.\n\nPoÅ‚Ä…czenie gÄ™stoÅ›ciowe (density connectivity)\n\nDwa punkty \\(x, y \\in X\\) sÄ… poÅ‚Ä…czone gÄ™stoÅ›ciowo, jeÅ›li istnieje punkt rdzeniowy \\(o \\in X\\) taki, Å¼e oba sÄ… osiÄ…galne gÄ™stoÅ›ciowo od \\(o\\) \\[\nx \\leftrightarrow y \\quad \\Longleftrightarrow \\quad \\exists_o \\in C_{\\text{core}}: \\ x \\overset{}{\\leftarrow} o \\ \\text{i} \\ y \\overset{}{\\leftarrow} o.\n\\] Relacja ta jest symetryczna i sÅ‚uÅ¼y do definiowania klastrÃ³w.\n\n\nKlaster DBSCAN\n\nKlaster jest maksymalnym wzglÄ™dem inkluzji zbiorem punktÃ³w, w ktÃ³rym wszystkie punkty sÄ… poÅ‚Ä…czone gÄ™stoÅ›ciowo \\[\nC = \\{ x \\in X : \\exists o \\in C_{\\text{core}} \\ \\text{takie, Å¼e} \\ x \\overset{*}{\\leftarrow} o \\}.\n\\] Zbiory \\(C_1, C_2, \\dots, C_K\\) tworzÄ… nieprzecinajÄ…cy siÄ™ podziaÅ‚ punktÃ³w rdzeniowych i brzegowych (tych osiÄ…galnych od rdzeniowych), a pozostaÅ‚e punkty \\[\nN = X \\setminus \\bigcup_{k=1}^K C_k\n\\] traktujemy jako szum lub punkty odosobnione (noise).\n\n\nPrzykÅ‚ad 6.3 Â \n\nKod# --- Pakiety ---\nlibrary(dbscan)\nlibrary(factoextra)\nlibrary(ggplot2)\n\n# --- Dane iris (4D) i standaryzacja ---\nX4 &lt;- scale(iris[, 1:4])\n\n# --- 1. DobÃ³r parametrÃ³w eps i minPts ---\n# eps okreÅ›la promieÅ„ sÄ…siedztwa; minPts â€“ minimalnÄ… liczbÄ™ punktÃ³w w klastrze.\n# Dobrym sposobem jest analiza wykresu kNN-dist (tzw. \"elbow method\")\n\nk &lt;- 5  # zazwyczaj minPts = liczba wymiarÃ³w + 1\nkNNdistplot(X4, k = k)\nabline(h = 0.79, col = \"red\", lty = 2)  # przykÅ‚adowy prÃ³g eps\ntitle(main = \"Wykres kNN-dist (pomoc przy wyborze eps)\")\n\n\n\n\n\n\nKod# --- 2. Klasteryzacja DBSCAN w przestrzeni 4D ---\neps_val &lt;- 0.79\ndb &lt;- dbscan(X4, eps = eps_val, minPts = k)\n\ncat(\"Liczba klastrÃ³w (bez szumu):\", max(db$cluster), \"\\n\")\n\nLiczba klastrÃ³w (bez szumu): 2 \n\nKodcat(\"Liczba punktÃ³w zaklasyfikowanych jako szum:\", sum(db$cluster == 0), \"\\n\")\n\nLiczba punktÃ³w zaklasyfikowanych jako szum: 4 \n\nKod# --- 3. PCA tylko do wizualizacji (2D) ---\npca &lt;- prcomp(X4, center = TRUE, scale. = FALSE)\nscores &lt;- as.data.frame(pca$x[, 1:2])\ncolnames(scores) &lt;- c(\"PC1\", \"PC2\")\nscores$cluster &lt;- factor(ifelse(db$cluster == 0, \"noise\", paste0(\"C\", db$cluster)))\nscores$species &lt;- iris$Species\n\n# --- 4. Wykres klastrÃ³w DBSCAN w PCA (2D) ---\nfviz_cluster(\n  list(data = pca$x[, 1:2], cluster = as.integer(as.factor(ifelse(db$cluster == 0, NA, db$cluster)))),\n  geom = \"point\", ellipse = FALSE, show.clust.cent = FALSE\n) +\n  geom_point(data = subset(scores, cluster == \"noise\"),\n             aes(PC1, PC2), inherit.aes = FALSE,\n             shape = 4, size = 2, color = \"grey40\") +\n  labs(\n    title = \"Klasteryzacja DBSCAN na iris\",\n    subtitle = paste(\"eps =\", eps_val, \", minPts =\", k)\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAlgorytm OPTICS\nNiech \\(X = \\{x_1, x_2, \\dots, x_n\\} \\subset \\mathbb{R}^p\\) oznaczaÄ‡ zbiÃ³r obserwacji, a \\(d(\\cdot,\\cdot)\\) â€” dowolnÄ… metrykÄ™ w tej przestrzeni (np. euklidesowÄ…). Algorytm OPTICS rozszerza koncepcjÄ™ DBSCAN, eliminujÄ…c koniecznoÅ›Ä‡ ustalania jednej wartoÅ›ci promienia sÄ…siedztwa. Zamiast tego wprowadza siÄ™ dwa parametry:\n\nmaksymalny promieÅ„ sÄ…siedztwa \\(\\varepsilon_{\\max} &gt; 0\\),\nminimalnÄ… liczbÄ™ punktÃ³w w sÄ…siedztwie \\(\\text{MinPts} \\in \\mathbb{N}\\).\n\n\nMaksymalne sÄ…siedztwo punktu\n\nDla kaÅ¼dego punktu \\(x \\in X\\) definiuje siÄ™ jego sÄ…siedztwo w promieniu \\(\\varepsilon_{\\max}\\) \\[\n\\mathcal{N}_{\\varepsilon_{\\max}}(x) = \\{ y \\in X : d(x,y) \\le \\varepsilon_{\\max} \\}.\n\\] Jest to zbiÃ³r wszystkich punktÃ³w w zasiÄ™gu maksymalnym, z ktÃ³rego bÄ™dÄ… obliczane lokalne miary gÄ™stoÅ›ci.\n\nOdlegÅ‚oÅ›Ä‡ rdzeniowa (core-distance)\n\nDla punktu \\(x \\in X,\\) jeÅ›li w jego sÄ…siedztwie znajduje siÄ™ co najmniej \\(\\text{MinPts}\\) punktÃ³w, to jego odlegÅ‚oÅ›Ä‡ rdzeniowa jest zdefiniowana jako \\[\n\\text{core-dist}(x) =\n\\begin{cases}\nd(x, x{(\\text{MinPts})}), & \\text{jeÅ›li } |\\mathcal{N}_{\\varepsilon_{\\max}}(x)| \\ge \\text{MinPts}, \\\\[6pt]\n\\text{niezdefiniowana}, & \\text{w przeciwnym razie.}\n\\end{cases}\n\\] gdzie \\(x_{(\\text{MinPts})}\\) oznacza MinPts-ty najbliÅ¼szy punkt wzglÄ™dem \\(x\\) w zbiorze \\(\\mathcal{N}_{\\varepsilon_{\\max}}(x)\\). Intuicyjnie, \\(\\text{core-dist}(x)\\) opisuje minimalny promieÅ„ kuli wokÃ³Å‚ \\(x\\), ktÃ³ry zawiera co najmniej \\(\\text{MinPts}\\) punktÃ³w, a wiÄ™c jest lokalnÄ… miarÄ… gÄ™stoÅ›ci.\n\nOdlegÅ‚oÅ›Ä‡ osiÄ…galnoÅ›ci (reachability-distance)\n\nDla dwÃ³ch punktÃ³w \\(x, y \\in X\\) takich, Å¼e \\(y \\in \\mathcal{N}_{\\varepsilon_{\\max}}(x)\\), definiuje siÄ™ \\[\n\\text{reach-dist}(y \\mid x) =\n\\max\\{\\text{core-dist}(x),\\, d(x, y)\\}.\n\\] WartoÅ›Ä‡ ta mierzy minimalnÄ… odlegÅ‚oÅ›Ä‡, przy ktÃ³rej punkt \\(y\\) jest osiÄ…galny z punktu \\(x\\) przy zachowaniu zadanej gÄ™stoÅ›ci. JeÅ›li \\(\\text{core-dist}(x)\\) jest maÅ‚a, to region wokÃ³Å‚ \\(x\\) jest gÄ™sty, a punkty w jego sÄ…siedztwie majÄ… niskÄ… odlegÅ‚oÅ›Ä‡ osiÄ…galnoÅ›ci.\n\n\nKolejka priorytetowa (SeedList)\n\nPodczas przetwarzania punktÃ³w, OPTICS utrzymuje kolejkÄ™ priorytetowÄ… punktÃ³w sÄ…siednich (SeedList). KaÅ¼dy punkt \\(y\\) wstawia siÄ™ do tej kolejki z priorytetem rÃ³wnym \\(\\text{reach-dist}(y)\\). Algorytm zawsze wybiera do dalszego przetwarzania punkt o najmniejszej wartoÅ›ci \\(\\text{reach-dist}\\), co gwarantuje eksploracjÄ™ przestrzeni od regionÃ³w gÄ™stych ku rzadszym.\n\nKolejnoÅ›Ä‡ przetwarzania punktÃ³w\n\nDla kaÅ¼dego nieodwiedzonego punktu \\(x_i \\in X\\)\n\nOblicza siÄ™ \\(\\mathcal{N}_{\\varepsilon_{\\max}}(x_i)\\).\nDodaje siÄ™ \\(x_i\\) do listy OrderList (kolejnoÅ›ci odwiedzin).\nJeÅ›li \\(x_i\\) jest rdzeniowy, oblicza siÄ™ jego \\(\\text{core-dist}(x_i)\\) i aktualizuje odlegÅ‚oÅ›ci osiÄ…galnoÅ›ci wszystkich punktÃ³w \\(y \\in \\mathcal{N}_{\\varepsilon_{\\max}}(x_i)\\) \\[\n\\text{reach-dist}(y) = \\min\\big( \\text{reach-dist}(y),\\ \\max\\{\\text{core-dist}(x_i), d(x_i, y)\\} \\big).\n\\] KaÅ¼dy taki punkt dodaje siÄ™ do SeedList z priorytetem rÃ³wnym zaktualizowanej wartoÅ›ci \\(\\text{reach-dist}(y)\\).\nNastÄ™pnie wybiera siÄ™ z SeedList punkt o najmniejszej wartoÅ›ci \\(\\text{reach-dist}\\) i powtarza proces, aÅ¼ kolejka bÄ™dzie pusta.\n\nW ten sposÃ³b powstaje uporzÄ…dkowana lista punktÃ³w wraz z przypisanÄ… im wartoÅ›ciÄ… \\(\\text{reach-dist}\\).\n\nWynik algorytmu\n\nPo przetworzeniu wszystkich punktÃ³w algorytm zwraca:\n\nuporzÄ…dkowanÄ… listÄ™ punktÃ³w \\(\\text{OrderList} = (x_{i_1}, x_{i_2}, \\dots, x_{i_n})\\),\nodpowiadajÄ…ce im wartoÅ›ci odlegÅ‚oÅ›ci osiÄ…galnoÅ›ci \\[\nR(x_{i_j}) = \\text{reach-dist}(x_{i_j}).\n\\]\n\n\nNa tej podstawie tworzy siÄ™ wykres osiÄ…galnoÅ›ci (reachability plot), na ktÃ³rym oÅ› pozioma przedstawia kolejnoÅ›Ä‡ punktÃ³w z OrderList, a oÅ› pionowa â€“ wartoÅ›ci \\(R(x)\\).\nDoliny (lokalne minima \\(R(x)\\)) odpowiadajÄ… klastrom o wysokiej gÄ™stoÅ›ci, a szczyty â€“ granicom miÄ™dzy nimi lub obszarom szumu.\n\nRelacja z DBSCAN\n\nZ wynikÃ³w OPTICS moÅ¼na odtworzyÄ‡ rozwiÄ…zania DBSCAN dla dowolnego progu gÄ™stoÅ›ci \\(\\varepsilon \\le \\varepsilon_{\\max}\\) \\[\nC_k(\\varepsilon) = \\{ x \\in X : R(x) \\le \\varepsilon \\}.\n\\] W odrÃ³Å¼nieniu od DBSCAN, ktÃ³ry tworzy pojedynczy podziaÅ‚ dla jednej wartoÅ›ci , OPTICS analizuje ciÄ…gÅ‚e spektrum gÄ™stoÅ›ci i ujawnia strukturÄ™ klastrÃ³w wieloskalarowo.\n\n\nKod# --- Pakiety ---\nlibrary(dbscan)\nlibrary(factoextra)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# --- Dane iris (4D) i standaryzacja ---\nX4  &lt;- as.matrix(iris[, 1:4])\nX4s &lt;- scale(X4)\n\n# --- PCA wyÅ‚Ä…cznie do wizualizacji (2D) ---\npca &lt;- prcomp(X4s, center = TRUE, scale. = FALSE)\nscores2 &lt;- as.data.frame(pca$x[, 1:2])\ncolnames(scores2) &lt;- c(\"PC1\", \"PC2\")\n\n# --- OPTICS w 4D ---\nset.seed(42)\nminPts &lt;- 10\nopt &lt;- optics(X4s, minPts = minPts, eps = 10)\nopt\n\nOPTICS ordering/clustering for 150 objects.\nParameters: minPts = 10, eps = 10, eps_cl = NA, xi = NA\nAvailable fields: order, reachdist, coredist, predecessor, minPts, eps,\n                  eps_cl, xi\n\nKod# core distances\nopt$coredist\n\n  [1] 0.3858795 0.5216255 0.4325508 0.3624815 0.4778513 0.6595814 0.5377608\n  [8] 0.3311773 0.7294317 0.3755259 0.5405840 0.4625350 0.5037468 0.7174030\n [15] 0.9855183 1.5786111 0.5983583 0.4240501 0.7750003 0.5839000 0.5526259\n [22] 0.5187340 0.7271930 0.5552259 0.5139214 0.5846393 0.3741885 0.3795916\n [29] 0.3624815 0.4435393 0.3331252 0.5059113 0.9579891 1.1066196 0.3518639\n [36] 0.4961615 0.5466384 0.5187340 0.5846393 0.3666919 0.3731971 1.6831481\n [43] 0.6557953 0.5838299 0.7099379 0.5037468 0.5467463 0.4961615 0.5307731\n [50] 0.4435393 0.9143787 0.7269364 0.7132600 0.8758985 0.5535324 0.5258100\n [57] 0.7834236 0.9979460 0.6204957 0.7824508 1.3840715 0.5338632 0.9966110\n [64] 0.5535324 0.6175831 0.7346173 0.6175831 0.5848110 1.1564647 0.5467463\n [71] 0.7463942 0.5599681 0.7489668 0.6050408 0.5612685 0.5535324 0.7346173\n [78] 0.6237576 0.5027299 0.6221790 0.7174030 0.6845254 0.5207927 0.6097954\n [85] 0.7600350 1.0203670 0.6797003 1.0364478 0.6373124 0.6018501 0.5770279\n [92] 0.5859036 0.5679510 1.0959819 0.5286138 0.5945837 0.4785192 0.5306769\n [99] 0.9130341 0.4863634 0.8365465 0.7333838 0.6693883 0.5853250 0.5954263\n[106] 1.1087525 1.1809265 0.9266707 1.0414409 1.2995268 0.6534500 0.6627507\n[113] 0.5539087 0.9790367 0.9577993 0.5873773 0.6190683 1.8712349 1.6496314\n[120] 1.1081839 0.5920862 0.8024896 1.3678948 0.6159250 0.6732405 0.8451719\n[127] 0.5468030 0.5753843 0.6686542 0.8520646 0.9596625 2.0008804 0.7097367\n[134] 0.5770279 0.8333085 1.1009468 0.8694743 0.6657451 0.6239106 0.5621610\n[141] 0.4805930 0.6639890 0.7333838 0.6018501 0.7176060 0.6183622 0.7604949\n[148] 0.5419296 0.9421324 0.6440723\n\nKod# reachability\nopt$reachdist\n\n  [1]       Inf 0.3331252 0.3331252 0.3331252 0.3731971 0.5405840 0.4625350\n  [8] 0.3624815 0.5216255 0.3331252 0.5185405 0.3311773 0.3331252 0.5024401\n [15] 0.5983583 0.9855183 0.5405840 0.3624815 0.5405840 0.4955334 0.3624815\n [22] 0.4778513 0.5187340 0.3741885 0.3311773 0.3331252 0.3311773 0.3624815\n [29] 0.3666919 0.4435393 0.3518639 0.3624815 0.6607751 0.7485815 0.4435393\n [36] 0.4435393 0.3795916 0.3731971 0.3624815 0.3731971 0.3858795 1.3892251\n [43] 0.4435393 0.3741885 0.5187340 0.3518639 0.4878204 0.4435393 0.4744817\n [50] 0.3666919 0.6797003 0.5535324 0.5535324 0.5467463 0.5306769 0.4785192\n [57] 0.6615890 0.7591833 0.5306769 0.5286138 0.9979460 0.4785192 0.7771899\n [64] 0.5027299 0.4785192 0.5535324 0.4785192 0.4863634 0.7489668 0.5348792\n [71] 0.5753843 0.4863634 0.6097954 0.5027299 0.5306769 0.5535324 0.5535324\n [78] 0.5535324 0.4785192 0.5207927 0.5467463 0.5467463 0.4863634 0.5468030\n [85] 0.5306769 0.7269364 0.5535324 0.7489668 0.4785192 0.9130341 0.5207927\n [92] 0.5027299 0.4955334 0.9130341 0.6018501 0.4785192 0.4863634 0.5027299\n [99] 1.6831481 0.5286138 0.5873773 0.5762550 0.5539087 0.5468030 0.5419296\n[106] 0.7223803 0.7824508 0.6693883 0.6627507 0.9432249 0.5419296 0.5468030\n[113] 0.5419296 0.7333838 0.7333838 0.4805930 0.5419296 1.2995268 0.9596625\n[120] 0.7798708 0.4805930 0.6507573 0.9266707 0.5468030 0.5621610 0.6693883\n[127] 0.5429150 0.5229417 0.5419296 0.6693883 0.6693883 1.2995268 0.5853250\n[134] 0.5468030 0.6052435 0.7789081 0.5873773 0.5419296 0.5027299 0.4805930\n[141] 0.5539087 0.5539087 0.5762550 0.4805930 0.4805930 0.5419296 0.6159250\n[148] 0.5753843 0.5873773 0.5753843\n\nKod# order\nopt$order\n\n  [1]   1  41  40  50  29  32  28  21  18   8  27  25  12  38   5  44  24  37\n [19]  36  30  48  43  35  46  31  26  13  10   4   3   2  39   7  49  22  47\n [37]  20  14  11  45  23   9  19  17   6  15  33  34  16  42  99  94  90  95\n [55] 100  97  96  89  79  67  65  62  56  83  72  68  93 139  98  92  74  64\n [73]  91  80 128  60  85  75  59  55  70 127  82  81  54 134 124 112 104  84\n [91]  76  87  78  66  53  52  77 150 148 146 138 129 117 113 111 105 142 141\n[109] 145 144 140 121 116 103 125  71 143 102 133 149 137 101 135  73 147 122\n[127]  57 109 131 130 126 108  51 106  86 115 114  88  69  58  63 136 120 107\n[145] 123 110 119  61 132 118\n\nKod# --- 1) Wykres reachability (porzÄ…dek OPTICS) ---\nplot(opt,\n     main = sprintf(\"OPTICS: reachability plot (minPts = %d)\", minPts),\n     ylab = \"reachability\", xlab = \"porzÄ…dek OPTICS\")\n\n\n\n\n\n\nKod# --- 2) Wydobycie klastrÃ³w metodÄ… Xi ---\n# --- Lista wartoÅ›ci Xi ---\nxi_vals &lt;- c(0.01, 0.03, 0.05, 0.10, 0.15, 0.20)\n\n# --- Funkcja pomocnicza do generowania wykresu dla danej wartoÅ›ci xi ---\nplot_xi &lt;- function(xi_val) {\n  opt_xi &lt;- extractXi(opt, xi = xi_val)\n  # Ustaw parametry graficzne i wygeneruj wykres\n  plot(opt_xi,\n       main = sprintf(\"OPTICS + extractXi (xi = %.2f)\", xi_val))\n}\n\n# --- Rysowanie wszystkich wykresÃ³w w ukÅ‚adzie grid ---\nfor (xi_val in xi_vals) {\n  plot_xi(xi_val)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKod# --- 3) Klastery Xi na rzutowaniu PCA (2D) ---\nopt_xi &lt;- extractXi(opt, xi = 0.15)\nlab_xi &lt;- opt_xi$cluster\nscores2$cluster_xi &lt;- factor(ifelse(lab_xi == 0, \"noise\", paste0(\"c\", lab_xi)))\n\np_xi &lt;- fviz_pca_ind(\n  pca, geom = \"point\",\n  habillage = scores2$cluster_xi, addEllipses = FALSE, show.legend = TRUE\n) +\n  # wyrÃ³Å¼niÄ‡ szum krzyÅ¼ykiem\n  geom_point(data = subset(scores2, cluster_xi == \"noise\"),\n             aes(PC1, PC2), inherit.aes = FALSE,\n             shape = 4, size = 2, color = \"grey30\") +\n  labs(\n    title = \"OPTICS + Xi na iris: wizualizacja w PCA (2D)\",\n    subtitle = sprintf(\"Klasteryzacja w 4D, xi = %.2f; szum oznaczony 'x'\", xi_val)\n  ) +\n  theme_minimal()\nprint(p_xi)\n\n\n\n\n\n\nKod# --- 4) (opcjonalnie) Wydobycie klastrÃ³w DBSCAN z trajektorii OPTICS ---\n# Pozwala zasymulowaÄ‡ wynik DBSCAN dla zadanego eps bez ponownego uruchamiania DBSCAN\nkNNdistplot(X4s, k = minPts)\nabline(h = 0.76, lty=3)\n\n\n\n\n\n\nKodeps_val &lt;- 0.76 # dobraÄ‡ na podstawie kNNdistplot(X4s, k = minPts)\nopt_db &lt;- extractDBSCAN(opt, eps = eps_val)\n\nscores2$cluster_db &lt;- factor(ifelse(opt_db$cluster == 0, \"noise\", paste0(\"c\", opt_db$cluster)))\n\np_db &lt;- fviz_pca_ind(\n  pca, geom = \"point\",\n  habillage = scores2$cluster_db, addEllipses = FALSE, show.legend = TRUE\n) +\n  geom_point(data = subset(scores2, cluster_db == \"noise\"),\n             aes(PC1, PC2), inherit.aes = FALSE,\n             shape = 4, size = 2, color = \"grey40\") +\n  labs(\n    title = \"Klastry z extractDBSCAN(optics, eps) na iris\",\n    subtitle = sprintf(\"eps = %.2f, minPts = %d (klasteryzacja w 4D, rzut PCA 2D)\", eps_val, minPts)\n  ) +\n  theme_minimal()\nprint(p_db)\n\n\n\n\n\n\n\n\nAlgorytm HDBSCAN\nPrzymijmy metrykÄ™ \\(d(\\cdot, \\cdot)\\), zbiÃ³r danych \\(X=\\{x_1,\\dots,x_n\\}\\subset\\mathbb{R}^p\\) oraz parametry minimalny rozmiar klastra \\(m_c\\) i opcjonalnie minimalnÄ… wielkoÅ›Ä‡ klastra \\(m_s\\) (gdy brak â€” przyjmujemy \\(m_s=m_c\\)).\n\nDefinicje i wielkoÅ›ci pomocnicze\n\nOkreÅ›lmy sÄ…siedztwo w promieniu \\(\\varepsilon\\) \\[\n\\mathcal{N}_\\varepsilon(x)=\\{y\\in X:\\ d(x,y)\\le \\varepsilon\\}.\n\\] OdlegÅ‚oÅ›Ä‡ rdzeniowa (core-distance) dla zadanego \\(m_s\\) okreÅ›lamy jako \\[\n\\operatorname{core-dist}(x)=d\\!\\big(x,\\ x_{(m_s)}\\big),\n\\] gdzie \\(x_{(k)}\\) oznacza \\(k\\)-tego najbliÅ¼szego sÄ…siada \\(x\\). OdlegÅ‚oÅ›Ä‡ wzajemnej osiÄ…galnoÅ›ci (mutual reachability distance) okreÅ›lamy jako \\[\nd_{\\text{mreach}}(x,y)=\\max\\big\\{\\operatorname{core-dist}(x),\\ \\operatorname{core-dist}(y),\\ d(x,y)\\big\\}.\n\\] Ta metryka â€spÅ‚aszczaâ€ rÃ³Å¼nice gÄ™stoÅ›ci: dwa punkty sÄ… â€bliskieâ€ tylko jeÅ›li oba leÅ¼Ä… w porÃ³wnywalnie gÄ™stych regionach. NastÄ™pnie dokonujemy transformacji gÄ™stoÅ›ciowej \\[\n\\lambda(x,y)=\\frac{1}{d_{\\text{mreach}}(x,y)}\\quad(\\text{wiÄ™ksza } \\lambda \\Rightarrow wiÄ™ksza gÄ™stoÅ›Ä‡).\n\\] 2. Graf sÄ…siedztwa i drzewo minimalnego rozpinajÄ…cego\nBudujemy graf sÄ…siedztwa nad \\(X\\) z wagami \\(d_{\\text{mreach}}(\\cdot,\\cdot)\\). W praktyce stosujemy graf k-NN (z \\(k=m_s\\) lub wiÄ™kszym) w celu ograniczenia liczby krawÄ™dzi. NastÄ™pnie obliczamy minimum spanning tree (MST) wagi \\(d_{\\text{mreach}}\\). Drzewo MST koduje â€najtaÅ„szeâ€ (najgÄ™stsze) poÅ‚Ä…czenia miÄ™dzy punktami â€” z nim powiÄ…zana jest hierarchia klastrÃ³w.\n\nTworzenie hierarchii klastrÃ³w gÄ™stoÅ›ci\n\nNajczÄ™Å›ciej stosujÄ…c metodÄ™ single linkage, przechodzimy po krawÄ™dziach MST w kolejnoÅ›ci rosnÄ…cych wag \\(d_{\\text{mreach}}\\) (czyli malejÄ…cych \\(\\lambda\\)), Å‚Ä…czÄ…c punkty/klastry w miarÄ™ obniÅ¼ania progu gÄ™stoÅ›ci. W wyniku powstaje hierarchia klastrÃ³w gÄ™stoÅ›ci â€” drzewo, w ktÃ³rym kaÅ¼dy poziom odpowiada pewnemu progowi gÄ™stoÅ›ci \\(\\lambda\\) (lub \\(d_{\\text{mreach}}\\)), a gaÅ‚Ä™zie reprezentujÄ… klastry pojawiajÄ…ce siÄ™ i Å‚Ä…czÄ…ce w miarÄ™ zmiany tego progu.\n\nKondensacja drzewa (condensed cluster tree)\n\nPrzechodzimy po drzewie klastrÃ³w gÄ™stoÅ›ci od najwyÅ¼szego poziomu (gÄ™stoÅ›ci) w dÃ³Å‚, usuwajÄ…c gaÅ‚Ä™zie o licznoÅ›ci mniejszej niÅ¼ \\(m_c\\). W efekcie powstaje skondensowane drzewo, w ktÃ³rym kaÅ¼da gaÅ‚Ä…Åº reprezentuje klaster o co najmniej \\(m_c\\) punktach, a zmiany licznoÅ›ci sÄ… rejestrowane na poziomach gÄ™stoÅ›ci, przy ktÃ³rych klastry siÄ™ Å‚Ä…czÄ… lub rozszczepiajÄ….\n\nMiara stabilnoÅ›ci klastra i wybÃ³r rozkroju\n\nDla kaÅ¼dej gaÅ‚Ä™zi (klastra) \\(C\\) w skondensowanym drzewie definiujemy miarÄ™ stabilnoÅ›ci jako \\[\n\\mathrm{Stab}(C) = \\sum_{x_i \\in C} \\big(\\lambda_{\\text{death}}(x_i) - \\lambda_{\\text{birth}}(C)\\big),\n\\] gdzie \\(\\lambda_{\\text{birth}}(C)\\) to poziom gÄ™stoÅ›ci, przy ktÃ³rym klaster \\(C\\) siÄ™ pojawia (rodzi), a \\(\\lambda_{\\text{death}}(x_i)\\) to poziom, przy ktÃ³rym punkt \\(x_i\\) opuszcza klaster (gdy \\(C\\) siÄ™ rozszczepia lub znika). Intuicyjnie, stabilnoÅ›Ä‡ mierzy â€czas Å¼yciaâ€ punktÃ³w w klastrze waÅ¼ony przez gÄ™stoÅ›Ä‡. NastÄ™pnie wybieramy zbiÃ³r gaÅ‚Ä™zi maksymalizujÄ…cy Å‚Ä…cznÄ… stabilnoÅ›Ä‡, przy czym gaÅ‚Ä™zie nie mogÄ… siÄ™ nakÅ‚adaÄ‡ (rodzic-potomkowie). Ten zbiÃ³r stanowi ostateczny podziaÅ‚ danych na klastry\n\nPrzypisanie punktÃ³w i â€miÄ™kkieâ€ przynaleÅ¼noÅ›ci\n\nPunkty naleÅ¼Ä…ce do wybranych gaÅ‚Ä™zi otrzymujÄ… etykiety klastrÃ³w. Punkty nieprzypisane traktujemy jako szum. Opcjonalnie moÅ¼na obliczyÄ‡ â€miÄ™kkieâ€ przynaleÅ¼noÅ›ci do klastrÃ³w na podstawie stabilnoÅ›ci oraz odlegÅ‚oÅ›ci do klastrÃ³w, a takÅ¼e wskaÅºnik outlier score opisujÄ…cy stopieÅ„ bycia punktem odstajÄ…cym.\n\nParametry modelu\n\n\nminimalny rozmiar klastra \\(m_c\\) â€” okreÅ›la najmniejszÄ… liczbÄ™ punktÃ³w, aby gaÅ‚Ä…Åº byÅ‚a uznana za klaster podczas kondensacji;\nminimalny rozmiar prÃ³bki \\(m_s\\) â€” uÅ¼ywany do obliczenia odlegÅ‚oÅ›ci rdzeniowej; zwykle przyjmuje siÄ™ \\(m_s = m_c\\) lub nieco wiÄ™ksze.\n\n\n\nKod# --- Pakiety ---\nlibrary(dbscan)\nlibrary(igraph)\nlibrary(scales)\nlibrary(ggraph)\n\n# --- Dane iris (4D) i standaryzacja ---\nX4  &lt;- as.matrix(iris[, 1:4])\nX4s &lt;- scale(X4)\n\n# --- PCA wyÅ‚Ä…cznie do wizualizacji (2D) ---\npca &lt;- prcomp(X4s, center = TRUE, scale. = FALSE)\nscores2 &lt;- as.data.frame(pca$x[, 1:2])\ncolnames(scores2) &lt;- c(\"PC1\", \"PC2\")\n\n# --- HDBSCAN ---\nminPts &lt;- 5\ncl &lt;- hdbscan(X4s, minPts = minPts)\n\n# --- PeÅ‚ne drzewo hierarchii (HDBSCAN*, nieskondensowane) ---\nplot(cl$hc, main = \"nieskondensowane drzewo hierarchi gÄ™stoÅ›ci (HDBSCAN*)\")\n\n\n\n\n\n\nKod# --- Skondensowane drzewo hierarchii gÄ™stoÅ›ci ---\nplot(cl, show_flat = TRUE,\n     main = \"Skondensowane drzewo hierarchii gÄ™stoÅ›ci (HDBSCAN)\")\n\n\n\n\n\n\nKod# --- Obliczenie metryki mutual reachability ---\nk &lt;- minPts - 1\ncore_dist &lt;- as.numeric(kNNdist(X4s, k = k))\nD_eu &lt;- as.matrix(dist(X4s))\nn &lt;- nrow(X4s)\ncore_i &lt;- matrix(core_dist, n, n)\ncore_j &lt;- t(core_i)\nDmreach &lt;- pmax(core_i, core_j, D_eu)\ndiag(Dmreach) &lt;- 0\n\n# --- Graf peÅ‚ny i MST ---\ng &lt;- graph_from_adjacency_matrix(Dmreach, mode = \"undirected\", weighted = TRUE, diag = FALSE)\nmst_g &lt;- mst(g, weights = E(g)$weight)\n\n# --- Przygotowanie danych do ggraph ---\n# WspÃ³Å‚rzÄ™dne wierzchoÅ‚kÃ³w\nlayout_df &lt;- data.frame(x = scores2[,1], y = scores2[,2])\n\n# Skala kolorÃ³w wg d_mreach\nw &lt;- E(mst_g)$weight\nE(mst_g)$color &lt;- scales::col_numeric(\n  palette = c(\"blue\", \"cyan\", \"yellow\", \"red\"),\n  domain = range(w)\n)(w)\n\n# --- Wizualizacja MST w ggraph ---\nset.seed(123)\nggraph(mst_g, layout = layout_df) +\n  geom_edge_link(aes(color = weight), width = 1.2, alpha = 0.9) +\n  geom_node_point(size = 2, color = \"gray20\") +\n  scale_color_gradientn(\n    colors = c(\"blue\", \"cyan\", \"yellow\", \"red\"),\n    name = expression(d[mreach])\n  ) +\n  theme_minimal() +\n  labs(\n    title = expression(\"Minimalne drzewo rozpinajÄ…ce w metryce \" * d[mreach]),\n    x = \"PC1\", y = \"PC2\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", hjust = 0.5)\n  )\n\n\n\n\n\n\nKod# --- Wykres klastrÃ³w w 2D ---\nscores2$cluster_hdb &lt;- factor(ifelse(cl$cluster == 0, \"noise\", paste0(\"c\", cl$cluster)))\np_hdb &lt;- fviz_pca_ind(\n  pca, geom = \"point\",\n  habillage = scores2$cluster_hdb,    # kolor wg klastrÃ³w HDBSCAN\n  addEllipses = FALSE, show.legend = TRUE\n) +\n  # NakÅ‚adka: szum jako krzyÅ¼yki\n  geom_point(\n    data = subset(scores2, cluster_hdb == \"noise\"),\n    aes(PC1, PC2), inherit.aes = FALSE,\n    shape = 4, size = 2, color = \"grey40\"\n  ) +\n  labs(\n    title = \"HDBSCAN na iris: wizualizacja w PCA (2D)\",\n    subtitle = sprintf(\"Klasteryzacja w 4D, minPts = %d; 'x' = szum\", minPts),\n    x = \"PC1\", y = \"PC2\"\n  ) +\n  theme_minimal()\n\nprint(p_hdb)",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Analiza skupieÅ„</span>"
    ]
  },
  {
    "objectID": "cluster.html#metody-oparte-na-modelach-probabilistycznych",
    "href": "cluster.html#metody-oparte-na-modelach-probabilistycznych",
    "title": "Analiza skupieÅ„",
    "section": "Metody oparte na modelach probabilistycznych",
    "text": "Metody oparte na modelach probabilistycznych\nModel mieszanin Gaussowskich (GMM)\nModel mieszanek Gaussa (ang. Gaussian Mixture Model, GMM) jest jednym z klasycznych probabilistycznych modeli klasteryzacyjnych. W przeciwieÅ„stwie do metod geometrycznych, takich jak k-means czy DBSCAN, ktÃ³re dzielÄ… przestrzeÅ„ na obszary na podstawie odlegÅ‚oÅ›ci, GMM opisuje rozkÅ‚ad danych jako kombinacjÄ™ (mieszaninÄ™) wielu rozkÅ‚adÃ³w normalnych. DziÄ™ki temu umoÅ¼liwia modelowanie zÅ‚oÅ¼onych, nakÅ‚adajÄ…cych siÄ™ struktur oraz ocenÄ™ niepewnoÅ›ci przypisaÅ„ punktÃ³w do klastrÃ³w.\n\nIdea modelu mieszanek Gaussa\n\nZakÅ‚ada siÄ™, Å¼e populacja danych pochodzi z mieszaniny \\(K\\) rozkÅ‚adÃ³w normalnych, z ktÃ³rych kaÅ¼dy odpowiada jednemu klastrowi. Dla obserwacji \\(x_i \\in \\mathbb{R}^d\\) zakÅ‚ada siÄ™ \\[\np(x_i) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\n\\] gdzie\n\n\n\\(\\pi_k\\) â€“ waga (udziaÅ‚) \\(k\\)-tej skÅ‚adowej, \\(\\pi_k \\ge 0\\) i \\(\\sum_{k=1}^{K} \\pi_k = 1\\),\n\n\\(\\mu_k\\) â€“ wektor Å›rednich (centrum) \\(k\\)-tej skÅ‚adowej,\n\n\\(\\Sigma_k\\) â€“ macierz kowariancji \\(k\\)-tej skÅ‚adowej,\n\n\\(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\) â€“ gÄ™stoÅ›Ä‡ wielowymiarowego rozkÅ‚adu normalnego \\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\!\\left[-\\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)\\right].\n\\] W ten sposÃ³b model GMM stanowi probabilistycznÄ… wersjÄ™ klasteryzacji, w ktÃ³rej kaÅ¼dy punkt moÅ¼e naleÅ¼eÄ‡ do kilku klastrÃ³w z okreÅ›lonymi prawdopodobieÅ„stwami.\n\n\nZmienne ukryte i interpretacja probabilistyczna\n\nWprowadza siÄ™ zmiennÄ… ukrytÄ… \\(z_i \\in \\{1, \\dots, K\\}\\), oznaczajÄ…cÄ…, z ktÃ³rego skÅ‚adnika mieszaniny pochodzi obserwacja \\(x_i\\). Model przyjmuje wtedy postaÄ‡ \\[\nP(x_i, z_i = k) = \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k).\n\\] Dla kaÅ¼dego punktu oblicza siÄ™ posterior probability (prawdopodobieÅ„stwo przynaleÅ¼noÅ›ci do klastra) \\[\n\\gamma_{ik} = P(z_i = k \\mid x_i) =\n\\frac{\\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\, \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}.\n\\] WartoÅ›Ä‡ \\(\\gamma_{ik}\\) mieÅ›ci siÄ™ w przedziale \\([0,1]\\) i moÅ¼na jÄ… interpretowaÄ‡ jako miÄ™kkie przypisanie punktu do klastra. W przeciwieÅ„stwie do k-means, ktÃ³re wymusza jednoznaczne etykiety, GMM dopuszcza probabilistyczne przypisania.\n\nFunkcja wiarygodnoÅ›ci\n\nZadaniem estymacji parametrÃ³w jest maksymalizacja funkcji wiarygodnoÅ›ci \\[\nL(\\pi, \\mu, \\Sigma \\mid X) = \\prod_{i=1}^{n} \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k).\n\\] BezpoÅ›rednia maksymalizacja tej funkcji jest trudna, poniewaÅ¼ zawiera sumÄ™ w logarytmie. Z tego powodu stosuje siÄ™ algorytm EM (Expectationâ€“Maximization), ktÃ³ry iteracyjnie przybliÅ¼a rozwiÄ…zanie.\n\nAlgorytm EM dla GMM\n\nAlgorytm EM (Dempster, Laird i Rubin, 1977) skÅ‚ada siÄ™ z dwÃ³ch naprzemiennych etapÃ³w:\n\nKrok Expectation - obliczanie posterior probabilities (odpowiedzialnoÅ›ci) dla kaÅ¼dego punktu i komponentu \\[\n\\gamma_{ik}^{(t)} =\n\\frac{\\pi_k^{(t)} \\, \\mathcal{N}(x_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\n\\sum_{j=1}^{K} \\pi_j^{(t)} \\, \\mathcal{N}(x_i \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)}) }.\n\\]\n\nKrok Maximization - aktualizacja parametrÃ³w mieszaniny, traktujÄ…c \\(\\gamma_{ik}\\) jako wagi \\[\nN_k^{(t+1)} = \\sum_{i=1}^{n} \\gamma_{ik}^{(t)}, \\quad\n\\pi_k^{(t+1)} = \\frac{N_k^{(t+1)}}{n},\n\\] \\[\n\\mu_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^{n} \\gamma_{ik}^{(t)} x_i,\n\\] \\[\n\\Sigma_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^{n} \\gamma_{ik}^{(t)} (x_i - \\mu_k^{(t+1)})(x_i - \\mu_k^{(t+1)})^\\top.\n\\] Proces powtarza siÄ™ aÅ¼ do zbieÅ¼noÅ›ci log-wiarygodnoÅ›ci \\[\n\\ell^{(t)} = \\sum_{i=1}^{n} \\log \\left[ \\sum_{k=1}^{K} \\pi_k^{(t)} \\, \\mathcal{N}(x_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)}) \\right].\n\\]\n\n\n\nWybÃ³r liczby skÅ‚adnikÃ³w \\(K\\)\n\n\nLiczbÄ™ skÅ‚adnikÃ³w mieszaniny (liczbÄ™ klastrÃ³w) nie okreÅ›la siÄ™ z gÃ³ry, lecz dobiera na podstawie kryteriÃ³w informacyjnych, np.:\n\nBIC (Bayesian Information Criterion) - \\(\\mathrm{BIC} = -2 \\log L_{\\max} + p \\log n\\), gdzie \\(p\\) to liczba parametrÃ³w modelu, \\(n\\) liczba obserwacji. Najmniejsza wartoÅ›Ä‡ BIC wskazuje najlepszy kompromis miÄ™dzy dopasowaniem a zÅ‚oÅ¼onoÅ›ciÄ… modelu.\nAIC (Akaike Information Criterion) â€“ ktÃ³ra mniej penalizuje zÅ‚oÅ¼onoÅ›Ä‡ \\(\\mathrm{AIC} = -2 \\log L_{\\max} + 2p.\\)\n\n\n\nStruktury kowariancji w GMM\n\nKaÅ¼dy komponent posiada macierz kowariancji \\(\\Sigma_k\\), ktÃ³ra moÅ¼e mieÄ‡ rÃ³Å¼ne ograniczenia:\n\n\nspherical â€“ \\(\\Sigma_k = \\sigma^2 I\\) - klastry kuliste, identyczne wariancje,\n\ndiagonal â€“ tylko wariancje na przekÄ…tnej, brak korelacji miÄ™dzy cechami,\n\nellipsoidal â€“ peÅ‚ne macierze kowariancji (dowolne orientacje i rozciÄ…gniÄ™cia).\n\n\n\n\n\n\n\n\n\n\nPozycja\nOznaczenie\nCo kontroluje\nMoÅ¼liwe wartoÅ›ci\nZnaczenie\n\n\n\n1. litera\nV / E / I\nObjÄ™toÅ›Ä‡ (Volume)\nV = rÃ³Å¼na, E = jednakowa, I = jednostkowa\nJak duÅ¼y jest klaster â€“ rozmiar elipsoidy\n\n\n2. litera\nV / E / I\nKsztaÅ‚t (Shape)\nV = rÃ³Å¼ny, E = jednakowy, I = sferyczny\nProporcje dÅ‚ugoÅ›ci osi elipsoidy\n\n\n3. litera\nV / E / I\nOrientacja (Orientation)\nV = rÃ³Å¼na, E = wspÃ³lna, I = brak (sferyczna)\nUstawienie elipsoidy w przestrzeni\n\n\n\n\nOgraniczenia i wady\n\n\nZaÅ‚oÅ¼enie o normalnoÅ›ci â€“ kaÅ¼dy klaster ma rozkÅ‚ad Gaussa, co bywa nieadekwatne dla struktur nieregularnych.\nWraÅ¼liwoÅ›Ä‡ na inicjalizacjÄ™ â€“ EM moÅ¼e zbiec do lokalnego maksimum; czÄ™sto uÅ¼ywa siÄ™ wielu startÃ³w lub wstÄ™pnej inicjalizacji metodÄ… k-means.\nBrak odpornoÅ›ci na odstajÄ…ce obserwacje â€“ skrajne punkty mogÄ… zaburzaÄ‡ estymacjÄ™ kowariancji.\nWymÃ³g dodatnio okreÅ›lonych macierzy kowariancji â€“ bÅ‚Ä™dne dane lub wspÃ³Å‚liniowoÅ›Ä‡ mogÄ… prowadziÄ‡ do problemÃ³w numerycznych.\n\n\n\nKod# Pakiety\nlibrary(mclust)\nlibrary(factoextra)\nlibrary(ggplot2)\n\n# Dane: 4D i standaryzacja\nX4  &lt;- as.matrix(iris[, 1:4])\nX4s &lt;- scale(X4)\n\n# Mieszanki Gaussa (GMM) z wyborem liczby komponentÃ³w po BIC\n# Model domyÅ›lnie przeszukuje rÃ³Å¼ne struktury kowariancji i liczby G\nset.seed(42)\ngmm &lt;- Mclust(X4s, G = 1:9)  # moÅ¼na zmieniÄ‡ zakres G\ngmm\n\n'Mclust' model object: (VVV,2) \n\nAvailable components: \n [1] \"call\"           \"data\"           \"modelName\"      \"n\"             \n [5] \"d\"              \"G\"              \"BIC\"            \"loglik\"        \n [9] \"df\"             \"bic\"            \"icl\"            \"hypvol\"        \n[13] \"parameters\"     \"z\"              \"classification\" \"uncertainty\"   \n\nKod# Podstawowe informacje\ncat(\"Wybrana liczba komponentÃ³w (G):\", gmm$G, \"\\n\")\n\nWybrana liczba komponentÃ³w (G): 2 \n\nKodcat(\"Struktura kowariancji:\", gmm$modelName, \"\\n\")\n\nStruktura kowariancji: VVV \n\nKod# PCA wyÅ‚Ä…cznie do wizualizacji (2D)\npca &lt;- prcomp(X4s, center = TRUE, scale. = FALSE)\nscores2 &lt;- as.data.frame(pca$x[, 1:2])\ncolnames(scores2) &lt;- c(\"PC1\",\"PC2\")\n\n# Etykiety i niepewnoÅ›ci (posterior) z GMM\nscores2$cluster_gmm &lt;- factor(paste0(\"c\", gmm$classification))\nscores2$uncertainty &lt;- gmm$uncertainty                 # 1 - max posterior prob.\n\n# Wykres BIC wyboru modelu\nplot(gmm, what = \"BIC\")\n\n\n\n\n\n\nKod# Wizualizacja przypisaÅ„ w PCA (2D) z factoextra\np_gmm &lt;- fviz_pca_ind(\n  pca, geom = \"point\",\n  habillage = scores2$cluster_gmm, addEllipses = FALSE, show.legend = TRUE\n) +\n  labs(\n    title = \"Mieszanki Gaussa (GMM) na iris: wizualizacja w PCA (2D)\",\n    subtitle = sprintf(\"Klasteryzacja w 4D, BIC wybraÅ‚ G = %d, model = %s\", gmm$G, gmm$modelName),\n    x = \"PC1\", y = \"PC2\"\n  ) +\n  theme_minimal()\nprint(p_gmm)\n\n\n\n\n\n\nKod# PorÃ³wnanie z gatunkami\ntab &lt;- table(Cluster = gmm$classification, Species = iris$Species)\nprint(tab)\n\n       Species\nCluster setosa versicolor virginica\n      1     50          0         0\n      2      0         50        50",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Analiza skupieÅ„</span>"
    ]
  },
  {
    "objectID": "correspondence.html",
    "href": "correspondence.html",
    "title": "Analiza korespondencji",
    "section": "",
    "text": "Miary zwiÄ…zkÃ³w miÄ™dzy zmiennymi jakoÅ›ciowymi\nW analizie zaleÅ¼noÅ›ci miÄ™dzy zmiennymi jakoÅ›ciowymi stosuje siÄ™ zestaw wyspecjalizowanych miar, ktÃ³re rÃ³Å¼niÄ… siÄ™ w zaleÅ¼noÅ›ci od poziomu pomiaru. W przypadku zmiennych nominalnych wykorzystuje siÄ™ miary oparte na tabelach kontyngencji i odchyleniu od niezaleÅ¼noÅ›ci. Dla zmiennych porzÄ…dkowych uwzglÄ™dnia siÄ™ dodatkowo informacjÄ™ o uporzÄ…dkowaniu kategorii. PoniÅ¼ej przedstawiono katalog najczÄ™Å›ciej stosowanych miar wraz z ich definicjami, sposobem obliczania, zakresem wartoÅ›ci oraz interpretacjÄ….",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Analiza korespondencji</span>"
    ]
  },
  {
    "objectID": "correspondence.html#miary-zwiÄ…zkÃ³w-miÄ™dzy-zmiennymi-jakoÅ›ciowymi",
    "href": "correspondence.html#miary-zwiÄ…zkÃ³w-miÄ™dzy-zmiennymi-jakoÅ›ciowymi",
    "title": "Analiza korespondencji",
    "section": "",
    "text": "Miary dla zmiennych nominalnych\nV CramÃ©ra\nMiara symetryczna oparta na statystyce chi-kwadrat. \\[\nV = \\sqrt{\\frac{\\chi^{2}}{n(k-1)}},\n\\] gdzie \\(n\\) oznacza licznoÅ›Ä‡ prÃ³by, a \\(k\\) mniejszÄ… z liczby kategorii obu zmiennych. Zakres wartoÅ›ci to [0, 1]. WartoÅ›Ä‡ 0 oznacza brak zwiÄ…zku, wartoÅ›ci bliskie 1 sygnalizujÄ… silny zwiÄ…zek, ale nie okreÅ›lajÄ… kierunku.\nWspÃ³Å‚czynnik kontyngencji Pearsona\nOpiera siÄ™ na tej samej statystyce co V CramÃ©ra. \\[\nC = \\sqrt{ \\frac{\\chi^{2}}{\\chi^{2} + n} }.\n\\] Zakres wartoÅ›ci to \\([0, C_{\\max}]\\), gdzie \\(C_{\\max}&lt;1\\) i zaleÅ¼y od liczby kategorii. WartoÅ›Ä‡ 0 oznacza niezaleÅ¼noÅ›Ä‡, wyÅ¼sze wartoÅ›ci sugerujÄ… silniejszy zwiÄ…zek, lecz brak standaryzacji utrudnia porÃ³wnania.\nWspÃ³Å‚czynnik Tschuprowa T\nMiara symetryczna, poÅ›rednia miÄ™dzy CramÃ©rem a wspÃ³Å‚czynnikiem kontyngencji. \\[\nT = \\sqrt{ \\frac{\\chi^{2}}{n\\sqrt{(r-1)(c-1)}} },\n\\] gdzie \\(r\\) i \\(c\\) to liczby kategorii obu zmiennych. Zakres wartoÅ›ci to [0, 1]. Interpretacja analogiczna do V CramÃ©ra.\nWspÃ³Å‚czynnik Phi\nUÅ¼ywany dla tabel 2Ã—2. W przypadku wiÄ™kszych tabel traci interpretowalnoÅ›Ä‡. \\[\n\\phi = \\sqrt{\\frac{\\chi^{2}}{n}}.\n\\] Zakres wartoÅ›ci to [0, 1] w tabeli 2Ã—2. 0 oznacza brak zwiÄ…zku, 1 zwiÄ…zek doskonaÅ‚y. Przy wiÄ™kszych tabelach traci interpretowalnoÅ›Ä‡.\nMiary dla zmiennych porzÄ…dkowych\nDla zmiennych porzÄ…dkowych wykorzystuje siÄ™ informacjÄ™ o uporzÄ…dkowaniu kategorii. Miary te opisujÄ… monotoniczne zaleÅ¼noÅ›ci pomiÄ™dzy rangami.\n\n\\(\\tau_a\\) Kendalla\nMiara oparta na liczbie par zgodnych i niezgodnych. \\[\n\\tau_a = \\frac{N_c - N_d}{\\binom{n}{2}},\n\\] gdzie \\(N_c\\) to liczba par zgodnych (ang. concordant), a \\(N_d\\) (ang. discordant) liczba par niezgodnych 1. Zakres wartoÅ›ci to [âˆ’1, 1]. WartoÅ›Ä‡ 0 oznacza brak monotonicznej zaleÅ¼noÅ›ci, wartoÅ›ci dodatnie zwiÄ…zek rosnÄ…cy, wartoÅ›ci ujemne malejÄ…cy.\n1Â RozwaÅ¼my dwie obserwacje \\((X_i, Y_i)\\) i \\((X_j, Y_j)\\), gdzie kaÅ¼da zmienna ma uporzÄ…dkowane kategorie. Para ta jest traktowana jako jednostka porÃ³wnania. ParÄ™ uwaÅ¼amy za zgodnÄ…, jeÅ›li uporzÄ…dkowanie obserwacji w zmiennej \\(X\\) i zmiennej \\(Y\\) jest spÃ³jne. Oznacza to speÅ‚nienie warunku \\[\n(X_i - X_j)(Y_i - Y_j) &gt; 0.\n\\] JeÅ¼eli rÃ³Å¼nice majÄ… ten sam znak (obie dodatnie albo obie ujemne), relacja jest zgodna. Interpretujemy to jako sytuacjÄ™, w ktÃ³rej wyÅ¼sza wartoÅ›Ä‡ \\(X\\) towarzyszy wyÅ¼szej wartoÅ›ci \\(Y\\) albo niÅ¼szej wartoÅ›ci \\(X\\) towarzyszy niÅ¼sza wartoÅ›Ä‡ \\(Y\\). ParÄ™ uwaÅ¼amy za niezgodnÄ…, jeÅ›li uporzÄ…dkowanie obserwacji w zmiennej \\(X\\) i zmiennej \\(Y\\) nie jest spÃ³jne. Zachodzi wÃ³wczas \\[\n(X_i - X_j)(Y_i - Y_j) &lt; 0.\n\\] RÃ³Å¼nice majÄ… przeciwne znaki: wzrost jednej zmiennej towarzyszy spadkowi drugiej. Oznacza to monotonicznie przeciwstawnÄ… relacjÄ™ w tej parze. W sytuacji, gdy jedna z rÃ³Å¼nic jest rÃ³wna zero (czyli kategorie w jednej zmiennej sÄ… rÃ³wne), para nie jest klasyfikowana jako zgodna ani niezgodna. WÃ³wczas mÃ³wi siÄ™ o remisie, a liczba par remisowych w zmiennej \\(X\\) i \\(Y\\) oznaczana jest odpowiednio przez \\(T_x\\) i \\(T_y\\) (ang. ties).\n\\(\\tau_b\\) Kendalla\nMiara ta uwzglÄ™dnia remisy w obu zmiennych. \\[\n\\tau_b = \\frac{N_c - N_d}{\\sqrt{(N_c + N_d + T_x)(N_c + N_d + T_y)}},\n\\] gdzie \\(T_x\\) i \\(T_y\\) oznaczajÄ… liczby remisÃ³w w kaÅ¼dej zmiennej. Zakres wartoÅ›ci to [âˆ’1, 1]. Miara zalecana przy danych z powtarzajÄ…cymi siÄ™ kategoriami.\n\n\\(\\tau_c\\) Kendalla\nDostosowana do tabel prostokÄ…tnych (nie kwadratowych). \\[\n\\tau_c = \\frac{2m}{m-1}\\tau_a,\n\\] gdzie \\(m\\) oznacza mniejszÄ… z liczby kategorii w obu zmiennych. Zakres [âˆ’1, 1]. Interpretacja analogiczna jak dla innych wersji miar Kendalla.\nGamma Goodmanaâ€“Kruskal\nIgnoruje remisy, opiera siÄ™ tylko na parach zgodnych i niezgodnych. \\[\n\\gamma = \\frac{N_c - N_d}{N_c + N_d}.\n\\] Zakres [âˆ’1, 1]. WartoÅ›Ä‡ 0 oznacza brak monotonicznej zaleÅ¼noÅ›ci, wartoÅ›ci bliskie 1 lub âˆ’1 zwiÄ…zek silny. NaleÅ¼y pamiÄ™taÄ‡, Å¼e ignorowanie remisÃ³w moÅ¼e prowadziÄ‡ do przeszacowaÅ„ siÅ‚y zwiÄ…zku.\n\n\\(D\\) Somersa\nMiara asymetryczna, bada poprawÄ™ przewidywania jednej zmiennej na podstawie drugiej. \\[\nD_{Y|X} = \\frac{N_c - N_d}{N_c + N_d + T_y}.\n\\] Wersja \\(D_{X|Y}\\) definiowana analogicznie. Zakres wartoÅ›ci to [âˆ’1, 1]. Miara opisuje, o ile lepsze jest uporzÄ…dkowanie jednej zmiennej, gdy znana jest druga zmienna, niÅ¼ gdyby zmienna wyjaÅ›niana byÅ‚a uporzÄ…dkowana losowo. Wersja \\(D_{Y|X}\\) mÃ³wi o jakoÅ›ci przewidywania \\(Y\\) na podstawie \\(X\\). Oznacza to, Å¼e im wiÄ™ksza wartoÅ›Ä‡ wspÃ³Å‚czynnika, tym lepiej zmienna \\(X\\) porzÄ…dkuje obserwacje wzglÄ™dem \\(Y\\).\n\n\\(\\kappa\\) Cohenâ€™a\nWspÃ³Å‚czynnik \\(\\kappa\\) Cohena stosuje siÄ™ do oceny zgodnoÅ›ci dwÃ³ch klasyfikatorÃ³w lub dwÃ³ch sÄ™dziÃ³w kodujÄ…cych te same obiekty do kategorii jakoÅ›ciowych. Miara uwzglÄ™dnia, Å¼e pewien poziom zgodnoÅ›ci moÅ¼e wynikaÄ‡ z czystego przypadku, dlatego \\(\\kappa\\) porÃ³wnuje zgodnoÅ›Ä‡ obserwowanÄ… z tÄ…, ktÃ³ra byÅ‚aby oczekiwana przy losowym przypisywaniu kategorii. \\[\n\\kappa = \\frac{p_o - p_e}{1 - p_e},\n\\] gdzie \\(p_o\\) stanowi proporcjÄ™ zgodnoÅ›ci rzeczywiÅ›cie zaobserwowanej, natomiast \\(p_e=\\sum_{i=1}^k\\left(\\frac{n}{n_{i\\cdot}}\\frac{n}{n_{\\cdot j}}\\right)\\) (patrz Tab. 7.1) proporcjÄ™ zgodnoÅ›ci oczekiwanej przy losowym przypisywaniu kategorii zgodnie z rozkÅ‚adami brzegowymi. Zakres wartoÅ›ci wspÃ³Å‚czynnika wynosi od âˆ’1 do 1. WartoÅ›Ä‡ 1 oznacza peÅ‚nÄ… zgodnoÅ›Ä‡, wartoÅ›Ä‡ 0 poziom zgodnoÅ›ci zgodny z przypadkiem, wartoÅ›ci ujemne wskazujÄ… na zgodnoÅ›Ä‡ mniejszÄ… niÅ¼ oczekiwana przy losowym przypisywaniu kategorii.\n\nPrzykÅ‚ad 7.1 Â \n\nKodx &lt;- c(1, 2, 2, 3, 3)\ny &lt;- c(1, 2, 1, 3, 2)\n\ndata.frame(obs = 1:5, x, y)\n\n  obs x y\n1   1 1 1\n2   2 2 2\n3   3 2 1\n4   4 3 3\n5   5 3 2\n\nKodpairs &lt;- t(combn(1:5, 2))\ncolnames(pairs) &lt;- c(\"i\", \"j\")\npairs\n\n      i j\n [1,] 1 2\n [2,] 1 3\n [3,] 1 4\n [4,] 1 5\n [5,] 2 3\n [6,] 2 4\n [7,] 2 5\n [8,] 3 4\n [9,] 3 5\n[10,] 4 5\n\nKodclassify_pairs &lt;- function(x, y) {\n  pairs &lt;- t(combn(seq_along(x), 2))\n  colnames(pairs) &lt;- c(\"i\", \"j\")\n  \n  Nc  &lt;- 0  # liczba par zgodnych\n  Nd  &lt;- 0  # liczba par niezgodnych\n  Tx  &lt;- 0  # liczba par z remisem w X (x_i == x_j, y_i != y_j)\n  Ty  &lt;- 0  # liczba par z remisem w Y (y_i == y_j, x_i != x_j)\n  Txy &lt;- 0  # liczba par z remisem w obu zmiennych (x_i == x_j, y_i == y_j)\n  \n  details &lt;- data.frame(\n    i   = integer(0),\n    j   = integer(0),\n    xi  = numeric(0),\n    xj  = numeric(0),\n    yi  = numeric(0),\n    yj  = numeric(0),\n    dx  = numeric(0),\n    dy  = numeric(0),\n    typ = character(0)\n  )\n  \n  for (k in seq_len(nrow(pairs))) {\n    i &lt;- pairs[k, 1]\n    j &lt;- pairs[k, 2]\n    \n    dx &lt;- x[j] - x[i]\n    dy &lt;- y[j] - y[i]\n    \n    typ &lt;- \"\"\n    \n    if (dx &gt; 0 & dy &gt; 0 || dx &lt; 0 & dy &lt; 0) {\n      Nc &lt;- Nc + 1\n      typ &lt;- \"zgodne\"\n    } else if (dx &gt; 0 & dy &lt; 0 || dx &lt; 0 & dy &gt; 0) {\n      Nd &lt;- Nd + 1\n      typ &lt;- \"niezgodne\"\n    } else if (dx == 0 & dy == 0) {\n      Txy &lt;- Txy + 1\n      typ &lt;- \"remis w obu\"\n    } else if (dx == 0 & dy != 0) {\n      Tx &lt;- Tx + 1\n      typ &lt;- \"remis w x\"\n    } else if (dx != 0 & dy == 0) {\n      Ty &lt;- Ty + 1\n      typ &lt;- \"remis w y\"\n    }\n    \n    details &lt;- rbind(\n      details,\n      data.frame(i, j,\n                 xi = x[i], xj = x[j],\n                 yi = y[i], yj = y[j],\n                 dx, dy,\n                 typ)\n    )\n  }\n  \n  list(Nc = Nc, Nd = Nd, Tx = Tx, Ty = Ty, Txy = Txy, details = details)\n}\n\nres &lt;- classify_pairs(x, y)\nres$details\n\n   i j xi xj yi yj dx dy       typ\ni  1 2  1  2  1  2  1  1    zgodne\ni1 1 3  1  2  1  1  1  0 remis w y\ni2 1 4  1  3  1  3  2  2    zgodne\ni3 1 5  1  3  1  2  2  1    zgodne\ni4 2 3  2  2  2  1  0 -1 remis w x\ni5 2 4  2  3  2  3  1  1    zgodne\ni6 2 5  2  3  2  2  1  0 remis w y\ni7 3 4  2  3  1  3  1  2    zgodne\ni8 3 5  2  3  1  2  1  1    zgodne\ni9 4 5  3  3  3  2  0 -1 remis w x\n\nKodres[c(\"Nc\", \"Nd\", \"Tx\", \"Ty\", \"Txy\")]\n\n$Nc\n[1] 6\n\n$Nd\n[1] 0\n\n$Tx\n[1] 2\n\n$Ty\n[1] 2\n\n$Txy\n[1] 0\n\nKodn  &lt;- length(x)\nNc &lt;- res$Nc\nNd &lt;- res$Nd\n\ntau_a &lt;- (Nc - Nd) / choose(n, 2)\ntau_a\n\n[1] 0.6\n\nKodTx &lt;- res$Tx + res$Txy  # remisy w X (wliczamy teÅ¼ remisy w obu)\nTy &lt;- res$Ty + res$Txy  # remisy w Y (wliczamy teÅ¼ remisy w obu)\n\ntau_b &lt;- (Nc - Nd) / sqrt((Nc + Nd + Tx) * (Nc + Nd + Ty))\ntau_b\n\n[1] 0.75\n\nKod# MoÅ¼na to teÅ¼ liczyÄ‡ z wykorzystaniem pakietu DescTools\nlibrary(DescTools)\n\ntau_a &lt;- KendallTauA(x, y)\ntau_b &lt;- KendallTauB(x, y)\nprint(c(tau_a = tau_a, tau_b = tau_b))\n\ntau_a tau_b \n 0.60  0.75",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Analiza korespondencji</span>"
    ]
  },
  {
    "objectID": "correspondence.html#rys-historyczny",
    "href": "correspondence.html#rys-historyczny",
    "title": "Analiza korespondencji",
    "section": "Rys historyczny",
    "text": "Rys historyczny\nAnalizÄ™ korespondencji (ang. correspondence analysis) warto osadziÄ‡ w szerszym tle historycznym, poniewaÅ¼ metoda ta powstaÅ‚a jako odpowiedÅº na potrzebÄ™ opisywania zÅ‚oÅ¼onych zaleÅ¼noÅ›ci miÄ™dzy zmiennymi jakoÅ›ciowymi. Jej korzenie siÄ™gajÄ… pierwszej poÅ‚owy XX wieku, kiedy francuscy statystycy zaczÄ™li rozwijaÄ‡ techniki umoÅ¼liwiajÄ…ce graficzne przedstawianie relacji w duÅ¼ych tabelach kontyngencji. Jednym z kluczowych momentÃ³w byÅ‚o wprowadzenie pojÄ™cia profili wierszy i kolumn oraz koncepcji przestrzeni czynnikowej, ktÃ³ra pozwalaÅ‚a redukowaÄ‡ wielowymiarowe dane do kilku najwaÅ¼niejszych wymiarÃ³w interpretacyjnych.\nIstotnÄ… rolÄ™ odegraÅ‚a praca BenzÃ©cri (1977), uznawanego za twÃ³rcÄ™ szkoÅ‚y analizy danych we Francji. ZaproponowaÅ‚ on sformalizowanÄ… wersjÄ™ analizy korespondencji jako metody opartej na dekompozycji wartoÅ›ci osobliwych, Å‚Ä…czÄ…cej elegancjÄ™ geometrycznego podejÅ›cia z praktycznÄ… interpretowalnoÅ›ciÄ… wynikÃ³w. W kolejnych dekadach metoda zostaÅ‚a rozbudowana o warianty takie jak analiza korespondencji wieloraka czy analiza korespondencji kanonicznej, ktÃ³re umoÅ¼liwiaÅ‚y analizowanie bardziej zÅ‚oÅ¼onych struktur, w tym zbiorÃ³w wielu tabel lub relacji miÄ™dzy zestawami zmiennych.\nZastosowania analizy korespondencji zaczÄ™Å‚y pojawiaÄ‡ siÄ™ w socjologii, psychologii, biologii, marketingu i politologii, zwÅ‚aszcza tam, gdzie dominowaÅ‚y dane kategoryczne. DziÄ™ki czytelnym mapom percepcyjnym, metoda pozwalaÅ‚a identyfikowaÄ‡ ukryte wzorce, grupy podobnych kategorii oraz kierunki dominujÄ…cych zaleÅ¼noÅ›ci. Jej popularnoÅ›Ä‡ wzrosÅ‚a wraz z rozwojem komputerÃ³w, ktÃ³re umoÅ¼liwiÅ‚y wykonywanie bardziej zÅ‚oÅ¼onych obliczeÅ„ oraz wizualizacji.\nObecnie analiza korespondencji stanowi waÅ¼ne narzÄ™dzie wielowymiarowej statystyki opisowej. UmoÅ¼liwia przedstawianie danych w sposÃ³b intuicyjny, a jednoczeÅ›nie bazuje na Å›cisÅ‚ych podstawach matematycznych. Pozwala to wykorzystywaÄ‡ jÄ… zarÃ³wno jako metodÄ™ eksploracji danych, jak i wsparcie dla interpretacji zaleÅ¼noÅ›ci obserwowanych w tabelach kontyngencji, szczegÃ³lnie wtedy, gdy relacje miÄ™dzy kategoriami nie sÄ… bezpoÅ›rednio uchwytne w formie tabelarycznej.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Analiza korespondencji</span>"
    ]
  },
  {
    "objectID": "correspondence.html#cele-analizy-korespondencji",
    "href": "correspondence.html#cele-analizy-korespondencji",
    "title": "Analiza korespondencji",
    "section": "Cele analizy korespondencji",
    "text": "Cele analizy korespondencji\nCelem stosowania analizy korespondencji jest uchwycenie oraz zobrazowanie struktury zaleÅ¼noÅ›ci miÄ™dzy zmiennymi jakoÅ›ciowymi zapisanymi w tabeli kontyngencji. Metoda pozwala przedstawiaÄ‡ profile wierszy i kolumn w zredukowanej przestrzeni wymiarÃ³w, tak aby odlegÅ‚oÅ›ci miÄ™dzy punktami odzwierciedlaÅ‚y odchylenia od modelu niezaleÅ¼noÅ›ci. UmoÅ¼liwia to identyfikowanie zbliÅ¼onych kategorii, wykrywanie ukrytych wzorcÃ³w oraz interpretowanie relacji, ktÃ³re trudno dostrzec jedynie na podstawie tabeli liczebnoÅ›ci. DziÄ™ki temu analiza korespondencji sÅ‚uÅ¼y zarÃ³wno eksploracji danych, jak i wspieraniu interpretacji statystycznej zaleÅ¼noÅ›ci miÄ™dzy cechami nominalnymi.\nIstotnym elementem analizy korespondencji jest tworzenie tzw. mapy percepcji, czyli graficznego odwzorowania kategorii wierszy i kolumn w przestrzeni o dwÃ³ch lub trzech wymiarach. Taka mapa umoÅ¼liwia analizowanie relacji miÄ™dzy kategoriami w sposÃ³b intuicyjny: punkty poÅ‚oÅ¼one blisko siebie reprezentujÄ… profile o podobnej strukturze, a duÅ¼e odlegÅ‚oÅ›ci wskazujÄ… na wyraÅºne rÃ³Å¼nice wzglÄ™dem modelu niezaleÅ¼noÅ›ci. Mapa percepcji uÅ‚atwia wiÄ™c identyfikowanie grup powiÄ…zanych kategorii, obserwowanie kierunkÃ³w dominujÄ…cych zaleÅ¼noÅ›ci oraz wyciÄ…ganie wnioskÃ³w, ktÃ³re w ukÅ‚adzie tabelarycznym pozostaÅ‚yby trudne do zauwaÅ¼enia.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Analiza korespondencji</span>"
    ]
  },
  {
    "objectID": "correspondence.html#definicja-matematyczna-modelu",
    "href": "correspondence.html#definicja-matematyczna-modelu",
    "title": "Analiza korespondencji",
    "section": "Definicja matematyczna modelu",
    "text": "Definicja matematyczna modelu\nZakÅ‚ada siÄ™, Å¼e punktem wyjÅ›cia jest tabela kontyngencji \\[\nN = (n_{ij})_{i=1,\\dots,I;\\, j=1,\\dots,J},\n\\] gdzie \\(n_{ij}\\) jest liczebnoÅ›ciÄ… obserwacji naleÅ¼Ä…cych jednoczeÅ›nie do kategorii wiersza \\(i\\) i kolumny \\(j\\). \\[\n\\begin{array}{c|cccc|c}\nX/Y& j=1 & j=2 & \\cdots & j=J & \\text{Razem} \\\\\n\\hline\ni=1 & n_{11} & n_{12} & \\cdots & n_{1J} & n_{1\\cdot} \\\\\ni=2 & n_{21} & n_{22} & \\cdots & n_{2J} & n_{2\\cdot} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\ni=I & n_{I1} & n_{I2} & \\cdots & n_{IJ} & n_{I\\cdot} \\\\\n\\hline\n\\text{Razem} & n_{\\cdot 1} & n_{\\cdot 2} & \\cdots & n_{\\cdot J} & n\n\\end{array}\n\\tag{7.1}\\] gdzie \\(n_{i\\cdot} = \\sum_{j=1}^J n_{ij}\\) oznacza liczebnoÅ›Ä‡ brzegowÄ… wiersza \\(i\\), \\(n_{\\cdot j} = \\sum_{i=1}^I n_{ij}\\) oznacza liczebnoÅ›Ä‡ brzegowÄ… kolumny \\(j\\), a \\(n = \\sum_{i=1}^I \\sum_{j=1}^J n_{ij}\\) jest liczebnoÅ›ciÄ… caÅ‚kowitÄ….\nWprowadza wÃ³wczas siÄ™ macierz czÄ™stoÅ›ci wzglÄ™dnych \\(P = (p_{ij})\\) postaci\n\\[\n\\begin{array}{c|cccc|c}\nX/Y & j=1 & j=2 & \\cdots & j=J & \\text{Razem} \\\\\n\\hline\ni=1 & p_{11} & p_{12} & \\cdots & p_{1J} & r_{1} \\\\\ni=2 & p_{21} & p_{22} & \\cdots & p_{2J} & r_{2} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\ni=I & p_{I1} & p_{I2} & \\cdots & p_{IJ} & r_{I} \\\\\n\\hline\n\\text{Razem} & c_{1} & c_{2} & \\cdots & c_{J} & 1\n\\end{array}\n\\] gdzie \\(p_{ij} = n_{ij}/n\\) sÄ… czÄ™stoÅ›ciami wzglÄ™dnymi, \\(r_i = \\sum_{j=1}^J p_{ij}\\) oznacza masÄ™ wiersza (udziaÅ‚ kategorii \\(i\\)), \\(c_j = \\sum_{i=1}^I p_{ij}\\) oznacza masÄ™ kolumny, a liczba w prawym dolnym rogu rÃ³wna 1 jest sumÄ… caÅ‚ej macierzy czÄ™stoÅ›ci wzglÄ™dnych. PrzeciÄ™tny profil wierszowy definiujemy jako wektor proporcji kategorii kolumnowych w caÅ‚ej tabeli. TworzÄ… go masy kolumnowe \\((c_1, c_2, \\dots, c_J)\\). Oznacza to, Å¼e przeciÄ™tny wiersz â€zachowuje siÄ™â€ tak, jak Å›rednia struktura rozkÅ‚adu kategorii kolumnowych w caÅ‚ej prÃ³bie â€” stanowi wiÄ™c naturalny punkt referencyjny przy ocenie, czy dany wiersz nadreprezentuje lub niedoreprezentuje wybrane kolumny wzglÄ™dem tego Å›redniego rozkÅ‚adu. Analogicznie przeciÄ™tny profil kolumnowy jest wektorem \\((r_1, r_2, \\dots, r_I),\\) czyli masami wierszy. Profil ten pokazuje przeciÄ™tny udziaÅ‚ kategorii wierszowych w caÅ‚ej tabeli i sÅ‚uÅ¼y porÃ³wnaniu rzeczywistych profili kolumn ze strukturÄ… â€typowÄ…â€ wynikajÄ…cÄ… z rozkÅ‚adu marginalnego wierszy.\nPodstawowym modelem odniesienia jest model niezaleÅ¼noÅ›ci zmiennych jakoÅ›ciowych: \\[\np_{ij}^{(0)} = r_i c_j,\\quad i=1,\\dots,I,\\ j=1,\\dots,J.\n\\] Oznacza to, Å¼e przy braku zaleÅ¼noÅ›ci miÄ™dzy kategoriami wierszy i kolumn, czÄ™stoÅ›Ä‡ wspÃ³lna powinna byÄ‡ rÃ³wna iloczynowi czÄ™stoÅ›ci brzegowych. Analiza korespondencji bada odchylenia rzeczywistych czÄ™stoÅ›ci \\(p_{ij}\\) od wartoÅ›ci oczekiwanych \\(r_i\\cdot c_j\\) oraz przedstawia je w zredukowanej przestrzeni wymiarÃ³w.\nOdchylenia od modelu niezaleÅ¼noÅ›ci moÅ¼na zapisaÄ‡ jako \\[\n\\Delta_{ij} = p_{ij} - r_i c_j.\n\\] WaÅ¼one odchylenia od niezaleÅ¼noÅ›ci leÅ¼Ä… u podstaw konstrukcji statystyki chi-kwadrat oraz pojÄ™cia inercji w analizie korespondencji. Wprowadza siÄ™ macierze diagonalne z masami wierszy i kolumn: \\[\\begin{align*}\nD_r =& \\mathrm{diag}(r_1,\\dots,r_I),\\\\\nD_c =& \\mathrm{diag}(c_1,\\dots,c_J).\n\\end{align*}\\] Znormalizowane reszty definiuje siÄ™ jako \\[\nS = D_r^{-1/2}\\,(P - r c^\\top)\\,D_c^{-1/2}.\n\\] Element o indeksach \\((i,j)\\) ma postaÄ‡ \\[\ns_{ij} = \\frac{p_{ij} - r_i c_j}{\\sqrt{r_i c_j}}.\n\\] CaÅ‚kowita â€inercjaâ€ analizowanej tabeli jest rÃ³wna Å›redniej waÅ¼onej kwadratu tych reszt i jest powiÄ…zana ze statystykÄ… chi-kwadrat: \\[\n\\chi^2 = n \\sum_{i=1}^I \\sum_{j=1}^J \\frac{(p_{ij} - r_i c_j)^2}{r_i c_j},\n\\quad\nI_{\\text{caÅ‚k.}} = \\sum_{i,j} \\frac{(p_{ij} - r_i c_j)^2}{r_i c_j}\n= \\frac{\\chi^2}{n}.\n\\] Analiza korespondencji dÄ…Å¼y do takiego odwzorowania kategorii wierszy i kolumn w przestrzeni euklidesowej, aby odlegÅ‚oÅ›ci w tej przestrzeni odzwierciedlaÅ‚y wÅ‚aÅ›nie te waÅ¼one odchylenia od modelu niezaleÅ¼noÅ›ci.\nKluczowym krokiem w wyznaczeniu wspÃ³Å‚rzÄ™dnych nowego ukÅ‚adu wspÃ³Å‚rzÄ™dnych (zwanego mapÄ… percepcji) jest dekompozycja wartoÅ›ci osobliwych macierzy znormalizowanych reszt \\(S\\): \\[\nS = U\\,\\Sigma\\,V^\\top,\n\\] gdzie \\(U\\) jest macierzÄ… \\(I \\times K\\) ortonormalnych wektorÃ³w wÅ‚asnych (dla wierszy), \\(V\\) jest macierzÄ… \\(J \\times K\\) ortonormalnych wektorÃ³w wÅ‚asnych (dla kolumn), \\(\\Sigma = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_K)\\) zawiera dodatnie wartoÅ›ci osobliwe, a \\(K = \\min(I-1, J-1)\\) jest maksymalnÄ… liczbÄ… niezerowych wymiarÃ³w (po odjÄ™ciu wymiaru zwiÄ…zanego z sumÄ… do 1).\nWartoÅ›ci wÅ‚asne analizy korespondencji definiuje siÄ™ jako \\[\n\\lambda_k = \\sigma_k^2, \\quad k = 1,\\dots,K,\n\\] a suma tych wartoÅ›ci \\[\n\\sum_{k=1}^K \\lambda_k = I_{\\text{caÅ‚k.}}\n\\] odpowiada caÅ‚kowitej inercji tabeli. PoszczegÃ³lne osie czynnikowe (wymiary) sÄ… wiÄ™c uporzÄ…dkowane malejÄ…co wedÅ‚ug wyjaÅ›nianej czÄ™Å›ci inercji.\nO doborze \\(K\\) mogÄ… decydowaÄ‡ kryteria takie, jak kryterium wyjaÅ›nionej inerncji (co najmniej 60%), kryterium osypiska czy kryterium liczby cech, lecz najczÄ™Å›ciej wybiera siÄ™ 2 wymiary aby mÃ³c profile przedstawiÄ‡ w dwuwymiarowej mapie percepcji.\nWynik analizy korespondencji przedstawia siÄ™ jako wspÃ³Å‚rzÄ™dne kategorii w przestrzeni czynnikowej, przyjmujÄ…c \\[\nF = D_r^{-1/2} U \\Sigma,\n\\] oraz \\[\nG = D_c^{-1/2} V \\Sigma.\n\\] Wiersz \\(i\\) jest wiÄ™c reprezentowany przez wektor wspÃ³Å‚rzÄ™dnych \\(f_i = (f_{i1},\\dots,f_{iK})\\) â€“ \\(i\\)-ty wiersz macierzy \\(F\\), kolumna \\(j\\) przez wektor \\(g_j = (g_{j1},\\dots,g_{jK})\\) â€“ \\(j\\)-ty wiersz macierzy \\(G\\). W takim ujÄ™ciu odlegÅ‚oÅ›ci euklidesowe miÄ™dzy punktami odpowiadajÄ… odlegÅ‚oÅ›ciom chi-kwadrat miÄ™dzy profilami, przy czym:\nâ€“ profil wiersza \\(i\\) to wektor warunkowych czÄ™stoÅ›ci \\[\n(p_{i1}/r_i,\\dots,p_{iJ}/r_i),\n\\] â€“ profil kolumny \\(j\\) to wektor \\[\n(p_{1j}/c_j,\\dots,p_{Ij}/c_j).\n\\]\nAnaliza korespondencji moÅ¼e byÄ‡ takÅ¼e interpretowana jako rodzaj waÅ¼onej analizy gÅ‚Ã³wnych skÅ‚adowych zastosowanej do zbioru profilÃ³w wierszy (lub kolumn) z metrykÄ… chi-kwadrat.\nIstotnÄ… wÅ‚asnoÅ›ciÄ… modelu jest tzw. wÅ‚asnoÅ›Ä‡ barycentryczna. WspÃ³Å‚rzÄ™dne wiersza sÄ… Å›redniÄ… waÅ¼onÄ… wspÃ³Å‚rzÄ™dnych kolumn, z wagami rÃ³wnymi warunkowym prawdopodobieÅ„stwom kategorii kolumnowych w danym wierszu, i odwrotnie. DokÅ‚adniej: \\[\nf_i = \\frac{1}{r_i} \\sum_{j=1}^J p_{ij}\\, D_c^{-1}\\, g_j\n\\] oraz analogicznie dla kolumn. Intuicyjnie, punkt reprezentujÄ…cy kategoriÄ™ wiersza znajduje siÄ™ w geometrycznym Å›rodku (barycentrum) punktÃ³w kolumnowych, waÅ¼onym strukturÄ… profilu tego wiersza. To zapewnia spÃ³jnoÅ›Ä‡ geometrycznÄ… i uÅ‚atwia interpretacjÄ™ wykresÃ³w.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Analiza korespondencji</span>"
    ]
  },
  {
    "objectID": "correspondence.html#miary-jakoÅ›ci-odtworzenia-przestrzeni-czynnikowej",
    "href": "correspondence.html#miary-jakoÅ›ci-odtworzenia-przestrzeni-czynnikowej",
    "title": "Analiza korespondencji",
    "section": "Miary jakoÅ›ci odtworzenia przestrzeni czynnikowej",
    "text": "Miary jakoÅ›ci odtworzenia przestrzeni czynnikowej\nMiary jakoÅ›ci odtworzenia przestrzeni czynnikowej opisujÄ…, jak dobrze poszczegÃ³lne kategorie oraz poszczegÃ³lne wymiary (osie) oddajÄ… strukturÄ™ zaleÅ¼noÅ›ci zakodowanÄ… w tabeli kontyngencji. ObejmujÄ… trzy grupy wskaÅºnikÃ³w: korelacjÄ™ punktu z osiÄ… (tzw. \\(\\cos^2\\)), udziaÅ‚ punktu w wymiarze oraz udziaÅ‚ wymiaru w inercji. PoniÅ¼ej znajduje siÄ™ opis kaÅ¼dej z miar, ich interpretacja oraz wzory.\nKorelacja punktu z osiÄ… (\\(\\cos^2\\))\nKorelacja punktu z danÄ… osiÄ… mierzy, jaka czÄ™Å›Ä‡ inercji danego punktu (wiersza lub kolumny) jest odwzorowana w przestrzeni czynnikowej wzdÅ‚uÅ¼ tej osi. Stanowi to odpowiednik â€jakoÅ›ci reprezentacjiâ€ punktu. Dla punktu wiersza \\(i\\) w wymiarze \\(k\\) definiujemy: \\[\n\\cos^2(i,k) = \\frac{f_{ik}^2}{\\sum_{l=1}^{K} f_{il}^2},\n\\] gdzie \\(f_{ik}\\) jest wspÃ³Å‚rzÄ™dnÄ… punktu \\(i\\) na osi \\(k\\). Analogicznie dla kolumn: \\[\n\\cos^2(j,k) = \\frac{g_{jk}^2}{\\sum_{l=1}^{K} g_{jl}^2}.\n\\] Interpretacja polega na ocenie, czy punkt leÅ¼y â€blisko osiâ€ i czy dane wymiary dobrze go opisujÄ…. WartoÅ›ci bliskie 1 oznaczajÄ… dobrÄ… jakoÅ›Ä‡ odwzorowania, wartoÅ›ci maÅ‚e â€” Å¼e punkt jest sÅ‚abo reprezentowany w danym wymiarze i jego poÅ‚oÅ¼enia nie naleÅ¼y nadmiernie interpretowaÄ‡.\nUdziaÅ‚ punktu w tworzeniu wymiaru (ang. contribution)\nUdziaÅ‚ punktu w tworzeniu danego wymiaru mierzy, jak duÅ¼y wkÅ‚ad dana kategoria wnosi do inercji na danej osi. W przeciwieÅ„stwie do \\(\\cos^2\\), ktÃ³re mierzÄ… jakoÅ›Ä‡ reprezentacji punktu, contribution mierzy â€waÅ¼noÅ›Ä‡â€ punktu dla danej osi. Dla wierszy: \\[\n\\mathrm{ctr}(i,k) = \\frac{r_i f_{ik}^2}{\\lambda_k},\n\\] gdzie \\(r_i\\) jest masÄ… wiersza, a \\(\\lambda_k\\) wartoÅ›ciÄ… wÅ‚asnÄ… odpowiadajÄ…cÄ… osi \\(k\\). Dla kolumn: \\[\n\\mathrm{ctr}(j,k) = \\frac{c_j g_{jk}^2}{\\lambda_k}.\n\\] Interpretujemy to jako wzglÄ™dnÄ… czÄ™Å›Ä‡ wariancji danego wymiaru â€pochodzÄ…cÄ…â€ od danego punktu. Punkty o duÅ¼ych wartoÅ›ciach contribution dominujÄ… dany wymiar i wskazujÄ…, ktÃ³re kategorie determinujÄ… kierunek zaleÅ¼noÅ›ci uchwycony przez oÅ›.\nUdziaÅ‚ wymiaru w inercji caÅ‚kowitej\nUdziaÅ‚ wymiaru w inercji okreÅ›la, jaka czÄ™Å›Ä‡ caÅ‚kowitej zmiennoÅ›ci reszt z modelu niezaleÅ¼noÅ›ci jest odtwarzana przez danÄ… oÅ›. Wynika bezpoÅ›rednio z wartoÅ›ci wÅ‚asnych: \\[\n\\text{UdziaÅ‚ osi } k = \\frac{\\lambda_k}{\\sum_{l=1}^{K} \\lambda_l}.\n\\] WartoÅ›ci wÅ‚asne \\(\\lambda_k\\) odzwierciedlajÄ… siÅ‚Ä™ zrÃ³Å¼nicowania uchwyconÄ… w danym wymiarze. Im wiÄ™ksza wartoÅ›Ä‡, tym wiÄ™kszy wpÅ‚yw danej osi na odwzorowanie zaleÅ¼noÅ›ci miÄ™dzy kategoriami.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Analiza korespondencji</span>"
    ]
  },
  {
    "objectID": "correspondence.html#interpretacja-elementÃ³w-skÅ‚adowych-modelu",
    "href": "correspondence.html#interpretacja-elementÃ³w-skÅ‚adowych-modelu",
    "title": "Analiza korespondencji",
    "section": "Interpretacja elementÃ³w skÅ‚adowych modelu",
    "text": "Interpretacja elementÃ³w skÅ‚adowych modelu\nPoszczegÃ³lne elementy modelu majÄ… nastÄ™pujÄ…ce znaczenie interpretacyjne:\nâ€“ macierz \\(P\\) - rzeczywista struktura czÄ™stoÅ›ci wspÃ³lnych badanych kategorii; â€“ wektory \\(r\\) i \\(c\\) - â€wielkoÅ›Ä‡â€ kategorii (czÄ™stoÅ›ci brzegowe), okreÅ›lajÄ…ce masy punktÃ³w wierszy i kolumn; â€“ macierz \\(P - r c^\\top\\) - odchylenia od modelu niezaleÅ¼noÅ›ci; â€“ macierz znormalizowanych reszt \\(S\\) - odchylenia wyraÅ¼one w jednostkach odpowiadajÄ…cych testowi chi-kwadrat, co umoÅ¼liwia porÃ³wnywanie kategorii o rÃ³Å¼nych masach; â€“ wartoÅ›ci wÅ‚asne \\(\\lambda_k\\) - wyjaÅ›niana inercja wzdÅ‚uÅ¼ kolejnych osi, interpretowana jako siÅ‚a zrÃ³Å¼nicowania struktury zaleÅ¼noÅ›ci w tym wymiarze; â€“ wektory wÅ‚asne (kolumny \\(U\\) i \\(V\\)) - kierunki gÅ‚Ã³wnych inercji odpowiednio w przestrzeni wierszy i kolumn; â€“ wspÃ³Å‚rzÄ™dne \\(F\\) i \\(G\\) - poÅ‚oÅ¼enie kategorii w przestrzeni czynnikowej, wykorzystywane do wizualizacji; ich rozrzut wokÃ³Å‚ Å›rodka odzwierciedla strukturÄ™ zaleÅ¼noÅ›ci miÄ™dzy kategoriami.\nNa mapie czynnikowej kategorie wierszy i kolumn, ktÃ³re leÅ¼Ä… blisko siebie, interpretuje siÄ™ jako powiÄ…zane: wiersze â€preferujÄ…â€ te kolumny (majÄ… ponadprzeciÄ™tne czÄ™stoÅ›ci wzglÄ™dem modelu niezaleÅ¼noÅ›ci), a kategorie oddalone od siebie sÄ… rzadko Å‚Ä…czone. OdlegÅ‚oÅ›ci miÄ™dzy punktami wierszy sÄ… interpretowane jako odlegÅ‚oÅ›ci chi-kwadrat miÄ™dzy profilami wierszy, a analogicznie dla kolumn.\nW praktyce interpretacji najczÄ™Å›ciej ogranicza siÄ™ do dwÃ³ch lub trzech pierwszych wymiarÃ³w (o najwiÄ™kszych wartoÅ›ciach wÅ‚asnych) i bada poÅ‚oÅ¼enie kategorii wzglÄ™dem osi oraz wzglÄ™dem siebie nawzajem. Dodatkowo moÅ¼na analizowaÄ‡ wkÅ‚ady kategorii do inercji poszczegÃ³lnych wymiarÃ³w oraz cosinusy kwadratÃ³w kÄ…tÃ³w (miary jakoÅ›ci reprezentacji), ale sÄ… to juÅ¼ bardziej szczegÃ³Å‚owe wskaÅºniki opierajÄ…ce siÄ™ na opisanej wyÅ¼ej strukturze wÅ‚asnych wektorÃ³w i wartoÅ›ci.\n\nPrzykÅ‚ad 7.2 Â \n\nKodlibrary(FactoMineR)\nlibrary(factoextra)\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\nKod# wybieramy tylko kolory oczu i wÅ‚osÃ³w bez podziaÅ‚u na pÅ‚eÄ‡\ntab &lt;- margin.table(HairEyeColor, margin = c(1, 2))\ntab\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    68   20    15     5\n  Brown   119   84    54    29\n  Red      26   17    14    14\n  Blond     7   94    10    16\n\nKod# Analiza korespondencji\nres.ca &lt;- CA(tab, graph = FALSE)\n\n# Wyniki modelu\nsummary(res.ca)\n\n\nCall:\nCA(X = tab, graph = FALSE) \n\nThe chi square of independence between the two variables is equal to 138.2898 (p-value =  2.325287e-25 ).\n\nEigenvalues\n                       Dim.1   Dim.2   Dim.3\nVariance               0.209   0.022   0.003\n% of var.             89.373   9.515   1.112\nCumulative % of var.  89.373  98.888 100.000\n\nRows\n        Iner*1000     Dim.1     ctr    cos2     Dim.2     ctr    cos2     Dim.3\nBlack |    55.425 |   0.505  22.246   0.838 |  -0.215  37.877   0.152 |  -0.056\nBrown |    12.284 |   0.148   5.086   0.864 |   0.033   2.319   0.042 |   0.049\nRed   |    15.095 |   0.130   0.964   0.133 |   0.320  55.131   0.812 |  -0.083\nBlond |   150.793 |  -0.835  71.704   0.993 |  -0.070   4.673   0.007 |  -0.016\n          ctr    cos2  \nBlack  21.633   0.010 |\nBrown  44.284   0.094 |\nRed    31.913   0.055 |\nBlond   2.171   0.000 |\n\nColumns\n        Iner*1000     Dim.1     ctr    cos2     Dim.2     ctr    cos2     Dim.3\nBrown |    93.086 |   0.492  43.116   0.967 |  -0.088  13.042   0.031 |  -0.022\nBlue  |   111.337 |  -0.547  52.128   0.977 |  -0.083  11.244   0.022 |   0.005\nHazel |    13.089 |   0.213   3.401   0.542 |   0.167  19.804   0.336 |   0.101\nGreen |    16.085 |  -0.162   1.355   0.176 |   0.339  55.910   0.773 |  -0.088\n          ctr    cos2  \nBrown   6.680   0.002 |\nBlue    0.310   0.000 |\nHazel  61.086   0.121 |\nGreen  31.925   0.052 |\n\nKod# Podstawowe podsumowanie wynikÃ³w\nres.ca$eig          # wartoÅ›ci wÅ‚asne i udziaÅ‚ inercji\n\n       eigenvalue percentage of variance cumulative percentage of variance\ndim 1 0.208772652              89.372732                          89.37273\ndim 2 0.022226615               9.514911                          98.88764\ndim 3 0.002598439               1.112356                         100.00000\n\nKodres.ca$row$coord    # wspÃ³Å‚rzÄ™dne wierszy\n\n           Dim 1       Dim 2       Dim 3\nBlack  0.5045624 -0.21482046 -0.05550909\nBrown  0.1482527  0.03266635  0.04880414\nRed    0.1295233  0.31964240 -0.08315117\nBlond -0.8353478 -0.06957934 -0.01621471\n\nKodres.ca$col$coord    # wspÃ³Å‚rzÄ™dne kolumn\n\n           Dim 1       Dim 2        Dim 3\nBrown  0.4921577 -0.08832151 -0.021611305\nBlue  -0.5474139 -0.08295428  0.004709408\nHazel  0.2125969  0.16739109  0.100518284\nGreen -0.1617534  0.33903957 -0.087597437\n\n\n\nNa poczÄ…tku moÅ¼na stwierdziÄ‡, Å¼e statystyka chi-kwadrat jest bardzo wysoka, a wartoÅ›Ä‡ p-value praktycznie rÃ³wna zeru. Oznacza to zdecydowane odrzucenie modelu niezaleÅ¼noÅ›ci, czyli istnienie silnego zwiÄ…zku pomiÄ™dzy kolorem wÅ‚osÃ³w a kolorem oczu. ZwiÄ…zek ten nie jest przypadkowÄ… fluktuacjÄ… liczebnoÅ›ci, lecz strukturalnym odchyleniem od przeciÄ™tnych profili.\nWartoÅ›ci wÅ‚asne pokazujÄ…, Å¼e pierwszy wymiar wyjaÅ›nia okoÅ‚o dziewiÄ™Ä‡dziesiÄ…t procent caÅ‚ej inercji. Drugi wymiar wnosi niespeÅ‚na dziesiÄ™Ä‡ procent, a trzeci ma znaczenie znikome. W praktyce interpretacja zaleÅ¼noÅ›ci miÄ™dzy kategoriami ograniczaÄ‡ siÄ™ bÄ™dzie do dwÃ³ch pierwszych wymiarÃ³w, poniewaÅ¼ to one odzwierciedlajÄ… niemal caÅ‚Ä… strukturÄ™ odchyleÅ„ od modelu niezaleÅ¼noÅ›ci. Silna dominacja wymiaru pierwszego wskazuje, Å¼e gÅ‚Ã³wna oÅ› rÃ³Å¼nicowania tabeli jest jednoznaczna i przedstawia podstawowy kierunek wspÃ³Å‚wystÄ™powania kolorÃ³w wÅ‚osÃ³w i oczu.\nW kategoriach wierszy najwiÄ™kszÄ… inercjÄ… charakteryzuje siÄ™ â€Blondâ€, ktÃ³ra tworzy wyraÅºnie silny sygnaÅ‚ odrÃ³Å¼niajÄ…cy jÄ… od pozostaÅ‚ych kategorii. Wysoka wartoÅ›Ä‡ \\(\\cos^2\\) na wymiarze pierwszym oznacza, Å¼e jej poÅ‚oÅ¼enie na mapie jest niemal w peÅ‚ni opisane przez ten jeden wymiar, a zatem interpretacja tej kategorii powinna opieraÄ‡ siÄ™ przede wszystkim na pierwszej osi. Podobnie kategoriÄ™ â€Blackâ€ rÃ³wnieÅ¼ dobrze opisuje wymiar pierwszy, choÄ‡ jej pozycja czÄ™Å›ciowo opiera siÄ™ rÃ³wnieÅ¼ na wymiarze drugim. Kategoria â€Redâ€ natomiast jest silnie zwiÄ…zana z wymiarem drugim: jej \\(\\cos^2\\) w osi drugiej dominuje nad pozostaÅ‚ymi wspÃ³Å‚rzÄ™dnymi, co sugeruje, Å¼e ta kategoria wnosi zrÃ³Å¼nicowanie wzglÄ™dnie prostopadÅ‚e do podstawowej osi.\nW zakresie wkÅ‚adÃ³w (contributions) wierszy wymiar pierwszy jest w gÅ‚Ã³wnej mierze ksztaÅ‚towany przez kategoriÄ™ â€Blondâ€, ktÃ³ra odpowiada za zdecydowanÄ… wiÄ™kszoÅ›Ä‡ inercji tej osi. â€Blackâ€ daje istotny wkÅ‚ad, natomiast kategorie â€Redâ€ i â€Brownâ€ majÄ… wzglÄ™dnie niewielki wpÅ‚yw. To wskazuje, Å¼e wymiar ten opisuje przeciwstawnoÅ›Ä‡ dwÃ³ch skrajnych charakterystyk: jasnych i bardzo ciemnych kolorÃ³w wÅ‚osÃ³w. Natomiast wymiar drugi jest definiowany przede wszystkim przez kategoriÄ™ â€Redâ€, co potwierdza jej szczegÃ³lne znaczenie w drugorzÄ™dnym kierunku zaleÅ¼noÅ›ci.\nW odniesieniu do kolumn widoczna jest podobna struktura. Wymiar pierwszy jest silnie uksztaÅ‚towany przez â€Brownâ€ i â€Blueâ€, ktÃ³re leÅ¼Ä… na przeciwnych stronach osi, a ich \\(\\cos^2\\) oraz wysoki wkÅ‚ad w ten wymiar wskazujÄ…, Å¼e to te kategorie okreÅ›lajÄ… jego znaczenie. Wymiar drugi zdominowany jest przez â€Greenâ€ i â€Hazelâ€, co sugeruje, Å¼e te dwa kolory oczu odpowiadajÄ… za dodatkowe zrÃ³Å¼nicowanie, nieuchwycone przez wymiar pierwszy.\n\n\nKod# Mapa percepcji: wiersze i kolumny na jednym wykresie\nfviz_ca_biplot(\n  res.ca,\n  repel = TRUE,\n  title = \"Mapa percepcji: kolor wÅ‚osÃ³w vs kolor oczu\"\n)\n\n\n\n\n\n\n\nMapa percepcji przedstawia zaleÅ¼noÅ›ci miÄ™dzy kategoriami koloru wÅ‚osÃ³w (punkty niebieskie) i koloru oczu (punkty czerwone) w przestrzeni dwÃ³ch wymiarÃ³w analizy korespondencji. OÅ› pierwsza odpowiada za zdecydowanÄ… wiÄ™kszoÅ›Ä‡ inercji, dlatego to ona wyznacza gÅ‚Ã³wny kierunek rÃ³Å¼nicowania kategorii. Na osi poziomej widoczne jest wyraÅºne zagÄ™szczenie miÄ™dzy kategoriami â€Blondâ€ i â€Blueâ€ po stronie ujemnej oraz kategoriami â€Blackâ€ i â€Brown eyesâ€ po stronie dodatniej. Interpretuejmy to jako dominujÄ…ce wspÃ³Å‚wystÄ™powanie jasnych wÅ‚osÃ³w z niebieskimi oczami oraz ciemnych wÅ‚osÃ³w z brÄ…zowymi oczami. OÅ› ta odwzorowuje wiÄ™c gradient od jasnych do ciemnych fenotypÃ³w. OÅ› pionowa, wyjaÅ›niajÄ…ca znacznie mniejszÄ… czÄ™Å›Ä‡ inercji, wprowadza drugi â€” subtelniejszy â€” wymiar struktury. W gÃ³rnej czÄ™Å›ci wykresu znajdujÄ… siÄ™ â€Redâ€ oraz â€Greenâ€, co sugeruje, Å¼e te rzadziej wystÄ™pujÄ…ce cechy wspÃ³Å‚wystÄ™pujÄ… czÄ™Å›ciej niÅ¼ wynikaÅ‚oby to z modelu niezaleÅ¼noÅ›ci. Z kolei â€Hazelâ€ pozycjonuje siÄ™ w dodatniej czÄ™Å›ci osi pierwszej i jednoczeÅ›nie nieco dodatniej czÄ™Å›ci osi drugiej, co wskazuje, Å¼e jej profil jest nieznacznie zbliÅ¼ony do ciemniejszych kolorÃ³w wÅ‚osÃ³w, choÄ‡ z pewnym odchyleniem kierunkowym widocznym na drugiej osi. Punkty â€Brown hairâ€ oraz â€Brown eyesâ€ znajdujÄ… siÄ™ bliÅ¼ej Å›rodka niÅ¼ pozostaÅ‚e kategorie. Interpretujemy to jako cechy bardziej â€przeciÄ™tneâ€, ktÃ³rych profile nie odbiegajÄ… silnie od profili przeciÄ™tnych.\nMapa ta potwierdza klasycznÄ… strukturÄ™ zaleÅ¼noÅ›ci fenotypowych: jasne wÅ‚osy czÄ™Å›ciej wspÃ³Å‚wystÄ™pujÄ… z jasnymi oczami, ciemne wÅ‚osy z ciemnymi oczami, a odmienne kombinacje sÄ… rzadsze. Drugi wymiar wyodrÄ™bnia rzadkie kombinacje zwiÄ…zane z czerwonymi wÅ‚osami oraz zielonymi oczami, tworzÄ…c charakterystyczny kierunek dodatkowy. Taka konfiguracja jest typowa dla danych fenotypowych, w ktÃ³rych dominujÄ… jedna lub dwie silne osie zrÃ³Å¼nicowania, a dodatkowe kierunki odzwierciedlajÄ… rzadsze, ale strukturalnie spÃ³jne wzorce wspÃ³Å‚wystÄ™powania.\n\n\n\n\n\nBenzÃ©cri, J. P. 1977. â€Histoire et prÃ©histoire de lâ€™analyse des donnÃ©es. Partie V Lâ€™analyse des correspondancesâ€. Les cahiers de lâ€™analyse des donnÃ©es 2 (1): 9â€“40. https://www.numdam.org/item/CAD_1977__2_1_9_0/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Analiza korespondencji</span>"
    ]
  },
  {
    "objectID": "loglinear.html",
    "href": "loglinear.html",
    "title": "Modele log-liniowe",
    "section": "",
    "text": "Rys historyczny\nModele log-liniowe wyksztaÅ‚ciÅ‚y siÄ™ w statystyce wielowymiarowej jako narzÄ™dzie analizy zaleÅ¼noÅ›ci pomiÄ™dzy zmiennymi jakoÅ›ciowymi reprezentowanymi w tabelach kontyngencji. Ich rozwÃ³j przypadÅ‚ na lata 60. i 70. XX wieku, kiedy to poszukiwano metod analogicznych do regresji, zdolnych do opisu wielowymiarowych zaleÅ¼noÅ›ci miÄ™dzy kategoriami zmiennych nominalnych. SzczegÃ³lnÄ… rolÄ™ odegraÅ‚y prace Goodmana (Goodman 1964, 1981, 1968, 1970, 1971; Goodman i Kruskal 1954), ktÃ³re wprowadziÅ‚y systematycznÄ… parametryzacjÄ™ modeli log-liniowych oraz umoÅ¼liwiÅ‚y stosowanie podejÅ›cia opartego na maksymalnym prawdopodobieÅ„stwie i testowaniu hierarchicznych efektÃ³w. W kolejnych dekadach log-liniowe podejÅ›cia staÅ‚y siÄ™ standardowym narzÄ™dziem analizy wielowymiarowych danych kategoryzowanych, a ich teoria ugruntowaÅ‚a siÄ™ w literaturze statystycznej poÅ›wiÄ™conej analizie tabel wielodzielczych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "loglinear.html#cel",
    "href": "loglinear.html#cel",
    "title": "Modele log-liniowe",
    "section": "Cel",
    "text": "Cel\nGÅ‚Ã³wny cel stosowania modeli log-liniowych polegaÄ‡ na opisaniu i wyjaÅ›nieniu struktury zaleÅ¼noÅ›ci pomiÄ™dzy wiÄ™cej niÅ¼ dwoma zmiennymi jakoÅ›ciowymi. Reprezentacja danych w postaci tabel wielodzielczych prowadzi do koniecznoÅ›ci identyfikacji tych komponentÃ³w, ktÃ³re odzwierciedlajÄ… zaleÅ¼noÅ›ci parowe, trÃ³jwymiarowe czy bardziej zÅ‚oÅ¼one interakcje miÄ™dzy kategoriami. Modele log-liniowe pozwalajÄ… odwzorowaÄ‡ rozkÅ‚ad czÄ™stoÅ›ci poprzez dekompozycjÄ™ logarytmu oczekiwanych licznoÅ›ci w funkcje efektÃ³w gÅ‚Ã³wnych i interakcji, dziÄ™ki czemu moÅ¼liwe staje siÄ™ testowanie hipotez o niezaleÅ¼noÅ›ci lub o strukturze oddziaÅ‚ywaÅ„. Stosuje siÄ™ je zarÃ³wno do klasycznych tabel kontyngencji, jak i do danych pochodzÄ…cych z ankiet, eksperymentÃ³w lub badaÅ„ obserwacyjnych, w ktÃ³rych istotna jest struktura wspÃ³Å‚wystÄ™powania kategorii.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "loglinear.html#definicja-modelu",
    "href": "loglinear.html#definicja-modelu",
    "title": "Modele log-liniowe",
    "section": "Definicja modelu",
    "text": "Definicja modelu\nMatematyczna definicja modelu log-liniowego opiera siÄ™ na zaÅ‚oÅ¼eniu, Å¼e logarytm wartoÅ›ci oczekiwanej dla kaÅ¼dej komÃ³rki tabeli kontyngencji moÅ¼na wyraziÄ‡ jako suma parametrÃ³w zwiÄ…zanych z poziomami poszczegÃ³lnych zmiennych oraz ich wspÃ³Å‚dziaÅ‚aniem. Dla przykÅ‚adowej tabeli trÃ³jwymiarowej z kategoriami \\(i\\), \\(j\\) i \\(k\\) model definiujemy w postaci\n\\[\n\\log \\hat{n}_{ijk} = \\lambda + \\lambda^X_i + \\lambda^Y_j + \\lambda^Z_k + \\lambda^{XY}_{ij} + \\lambda^{XZ}_{ik} + \\lambda^{YZ}_{jk} + \\lambda^{XYZ}_{ijk},\n\\] gdzie \\(\\hat{n}_{ijk}\\) oznacza wartoÅ›Ä‡ oczekiwanÄ… czÄ™stoÅ›ci w danej komÃ³rce, a poszczegÃ³lne parametry \\(\\lambda\\) odpowiadajÄ… efektom gÅ‚Ã³wnym i interakcjom efektÃ³w W modelach hierarchicznych brak okreÅ›lonego skÅ‚adnika interakcji traktujemy jako hipotezÄ™ o niezaleÅ¼noÅ›ci, np. wyeliminowanie \\(\\lambda^{XY}_{ij}\\) oznacza neutralnoÅ›Ä‡ w relacji \\(X\\) i \\(Y\\). Dla jednoznacznej definicji modelu narzuca siÄ™ warunki sumowalnoÅ›ci, takie jak sumowanie wspÃ³Å‚czynnikÃ³w do zera w ramach kaÅ¼dej zmiennej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "loglinear.html#struktura-hierarchiczna",
    "href": "loglinear.html#struktura-hierarchiczna",
    "title": "Modele log-liniowe",
    "section": "Struktura hierarchiczna",
    "text": "Struktura hierarchiczna\nW analizie wielowymiarowych tabel kontyngencji model log-liniowy opisuje logarytm wartoÅ›ci oczekiwanych liczebnoÅ›ci w komÃ³rkach tabeli poprzez sumÄ™ efektÃ³w gÅ‚Ã³wnych i interakcji. Dla trzech zmiennych jakoÅ›ciowych \\(X\\), \\(Y\\) i \\(Z\\) o kategoriach indeksowanych odpowiednio przez \\(i\\), \\(j\\) i \\(k\\), ogÃ³lna postaÄ‡ modelu peÅ‚nego przyjmuje formÄ™ \\[\n\\log \\hat{n}_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda^{XY}_{ij} + \\lambda^{XZ}_{ik} + \\lambda^{YZ}_{jk} + \\lambda^{XYZ}_{ijk}.\n\\] SkÅ‚adniki te interpretujemy jako:\n\nefekt ogÃ³lny \\(\\lambda\\),\nefekty gÅ‚Ã³wne zmiennych \\(X, Y, Z\\),\ninterakcje dwuwymiarowe \\((XY), (XZ), (YZ)\\),\ninterakcja trÃ³jwymiarowa \\((XYZ)\\).\n\nModel peÅ‚ny zawiera wszystkie moÅ¼liwe efekty i interakcje.\nModele z ograniczeniami\nUsuwanie wybranych interakcji prowadzi do modeli uproszczonych. PrzykÅ‚adowo model bez interakcji trzeciego rzÄ™du ma postaÄ‡ \\[\n\\log \\hat{n}_{ijk} = \\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda^{XY}_{ij} + \\lambda^{XZ}_{ik} + \\lambda^{YZ}_{jk}.\n\\] Model, ktÃ³ry zakÅ‚ada tylko pojedynczÄ… interakcjÄ™ dwuwymiarowÄ…, moÅ¼e mieÄ‡ postaÄ‡ \\[\n\\log \\hat{n}_{ijk} =\\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda^{XY}_{ij}.\n\\] Inne modele eliminujÄ… kolejne interakcje lub zachowujÄ… tylko czÄ™Å›Ä‡ z nich. KaÅ¼dy taki model musi jednak speÅ‚niaÄ‡ zasadÄ™ hierarchicznoÅ›ci.\nZasada hierarchicznoÅ›ci modeli log-liniowych\nModel log-liniowy jest hierarchiczny wtedy i tylko wtedy, gdy obecnoÅ›Ä‡ jakiejkolwiek interakcji wyÅ¼szego rzÄ™du wymusza obecnoÅ›Ä‡ wszystkich jej podinterakcji niÅ¼szego rzÄ™du. Formalnie, jeÅ¼eli model zawiera interakcjÄ™ okreÅ›lonÄ… przez zbiÃ³r zmiennych \\[\n{A_1, A_2, \\dots, A_r},\n\\] to musi on zawieraÄ‡ takÅ¼e wszystkie interakcje zdefiniowane na kaÅ¼dym podzbiorze \\[\n{A_{i_1}, A_{i_2}, \\dots, A_{i_s}}\\subset {A_1, \\dots, A_r},\n\\] a wiÄ™c rÃ³wnieÅ¼ wszystkie efekty gÅ‚Ã³wne tworzÄ…cych tÄ™ interakcjÄ™ zmiennych.\nIlustracja hierarchicznoÅ›ci\nJeÅ¼eli w modelu pozostawia siÄ™ interakcjÄ™ trzeciego rzÄ™du \\(\\lambda^{XYZ}_{ijk},\\) to model musi dodatkowo zawieraÄ‡:\n\ninterakcje dwuwymiarowe \\[\n\\lambda^{XY}_{ij}, \\ \\lambda^{XZ}_{ik}, \\ \\lambda^{YZ}_{jk},\n\\]\n\noraz efekty gÅ‚Ã³wne: \\[\n\\lambda_i^X,\\ \\lambda_j^Y,\\ \\lambda_k^Z,\n\\]\n\na takÅ¼e efekt ogÃ³lny \\(\\lambda\\).\n\nAnalogicznie, jeÅ¼eli model zawiera interakcjÄ™ dwuwymiarowÄ…, np. \\(\\lambda^{XY}_{ij}\\), to musi jednoczeÅ›nie zawieraÄ‡: \\[\n\\lambda_i^X,\\qquad \\lambda_j^Y,\\qquad \\lambda.\n\\] Model, w ktÃ³rym te warunki nie sÄ… speÅ‚nione, nie jest modelem hierarchicznym.\nZnaczenie hierarchicznoÅ›ci\nHierarchicznoÅ›Ä‡ zapewnia:\n\nspÃ³jnÄ…, poprawnÄ… interpretacjÄ™ parametrÃ³w,\nistnienie estymatorÃ³w maksymalnego prawdopodobieÅ„stwa,\nprawidÅ‚owe dziaÅ‚anie algorytmu iteracyjnego dopasowania proporcjonalnego (IPF),\nlogicznÄ… strukturÄ™ zaleÅ¼noÅ›ci pomiÄ™dzy zmiennymi.\n\nW efekcie stanowi podstawowÄ… zasadÄ™ konstrukcji modeli log-liniowych i gwarantuje ich poprawnoÅ›Ä‡ statystycznÄ….",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "loglinear.html#estymacja-parametrÃ³w",
    "href": "loglinear.html#estymacja-parametrÃ³w",
    "title": "Modele log-liniowe",
    "section": "Estymacja parametrÃ³w",
    "text": "Estymacja parametrÃ³w\nW modelach log-liniowych parametry estymuje siÄ™ w oparciu o zasadÄ™ maksymalizacji funkcji wiarygodnoÅ›ci. Przyjmuje siÄ™, Å¼e licznoÅ›ci tabeli kontyngencji majÄ… rozkÅ‚ad Poissona z parametrami \\(\\hat{n}_{ijk}\\), ktÃ³re odpowiadajÄ… wartoÅ›ciom oczekiwanym wynikajÄ…cym z przyjÄ™tego modelu log-liniowego. Konstrukcja funkcji wiarygodnoÅ›ci opiera siÄ™ na fakcie, Å¼e komÃ³rki sÄ… traktowane jako niezaleÅ¼ne, co prowadzi do iloczynu gÄ™stoÅ›ci Poissona. Logarytm takiej funkcji dla tabeli trÃ³jwymiarowej przyjmuje postaÄ‡ \\[\n\\ell(\\lambda) = \\sum_{ijk}\n\\left[\nn_{ijk} \\log(\\hat{n}_{ijk}) - \\hat{n}_{ijk} - \\log(n_{ijk}!)\n\\right],\n\\]\ngdzie \\(n_{ijk}\\) oznacza liczbÄ™ obserwacji w komÃ³rce \\(ijk\\), a \\(\\hat{n}_{ijk}\\) wartoÅ›Ä‡ oczekiwanÄ… okreÅ›lonÄ… przez model. Po odrzuceniu staÅ‚ej \\(\\log(n_{ijk}!)\\) zadanie estymacji sprowadza siÄ™ do maksymalizacji \\[\n\\ell(\\lambda) =\\sum_{ijk}\n\\left[\nn_{ijk},\\log(\\hat{n}_{ijk}) - \\hat{n}_{ijk}\n\\right],\n\\] przy czym \\(\\hat{n}_{ijk}\\) wyraÅ¼ane sÄ… w funkcji parametrÃ³w \\(\\lambda\\), poniewaÅ¼ model okreÅ›la \\[\n\\log(\\hat{n}_{ijk}) = \\lambda + \\lambda_{i}^A + \\lambda_{j}^B + \\lambda_{k}^C + \\lambda_{ij}^{AB} + \\dots\n\\] Wobec nieliniowoÅ›ci tej struktury warunki pierwszego rzÄ™du nie majÄ… zamkniÄ™tego rozwiÄ…zania algebraicznego, co prowadzi do koniecznoÅ›ci wykorzystania metod iteracyjnych. W zastosowaniach modeli log-liniowych wykorzystuje siÄ™ dwie klasy procedur. Pierwsza opiera siÄ™ na iteracyjnych metodach gradientowych, takich jak Newtonâ€“Raphson, w ktÃ³rych aktualizuje siÄ™ wartoÅ›ci parametrÃ³w poprzez dodawanie poprawek proporcjonalnych do pochodnej pierwszej i odwrotnoÅ›ci hessianu. Druga klasa, szczegÃ³lnie rozpowszechniona w analizie tabel wielodzielczych, wykorzystuje algorytm iterative proportional fitting (IPF).\n\n\n\n\n\n\nAdnotacja\n\n\n\nAlgorytm iterative proportional fitting (IPF) stosuje siÄ™ do wyznaczania wartoÅ›ci oczekiwanych \\(n_{i_1 i_2 \\dots i_p}\\) w modelach log-liniowych tak, aby byÅ‚y one zgodne z zadanymi rozkÅ‚adami brzegowymi wynikajÄ…cymi z przyjÄ™tej struktury modelu hierarchicznego. Sednem tej procedury jest kolejne skalowanie fragmentÃ³w tabeli oczekiwanej w taki sposÃ³b, aby dopasowaÄ‡ ich marginesy do marginesÃ³w danych empirycznych. Iteracja po iteracji kolejne rozkÅ‚ady brzegowe zaczynajÄ… odpowiadaÄ‡ odpowiednim sumom brzegowym obserwacji, co w rezultacie prowadzi do wartoÅ›ci oczekiwanych maksymalizujÄ…cych funkcjÄ™ wiarygodnoÅ›ci.\nPrzykÅ‚adowo dla modelu bez interakcji trzeciego rzÄ™du \\[\n\\log(\\hat{n}_{ijk}) = \\lambda + \\lambda_i^X + \\lambda_j^Y + \\lambda_k^Z + \\lambda_{ij}^{XY} + \\lambda_{ik}^{XZ} + \\lambda_{jk}^{YZ},\n\\] czyli model, w ktÃ³rym wyklucza siÄ™ obecnoÅ›Ä‡ interakcji trÃ³jwymiarowej \\((XYZ)\\). W takim modelu estymatory wartoÅ›ci oczekiwanych \\(\\hat{n}_{ijk}\\) muszÄ… speÅ‚niaÄ‡ trzy zestawy warunkÃ³w brzegowych: \\[\n\\hat{n}_{ij\\cdot}=n_{ij\\cdot},\\qquad\n\\hat{n}_{i\\cdot k}=n_{i\\cdot k},\\qquad\n\\hat{n}_{\\cdot jk}=n_{\\cdot jk}.\n\\] Warunki te nie mogÄ… zostaÄ‡ speÅ‚nione przez bezpoÅ›rednie podstawienie parametrÃ³w do rÃ³wnania log-liniowego, dlatego stosuje siÄ™ procedurÄ™ iteracyjnÄ… IPF.\nKrok 0. WybÃ³r wartoÅ›ci poczÄ…tkowych\nProcedurÄ™ rozpoczyna siÄ™ od zadania poczÄ…tkowych wartoÅ›ci liczebnoÅ›ci oczekiwanych: \\[\n\\hat{n}_{ijk}^{(0)} = 1 \\qquad \\text{dla wszystkich } i,j,k.\n\\]\nKrok 1. Dopasowanie marginesÃ³w \\(\\hat{n}_{ij\\cdot}\\)\n\nAby wymusiÄ‡ speÅ‚nienie zaleÅ¼noÅ›ci \\[\n\\hat{n}_{ij\\cdot} = n_{ij\\cdot},\n\\] stosuje siÄ™ skalowanie proporcjonalne w kaÅ¼dej warstwie \\(k\\) \\[\n\\hat{n}_{ijk}^{(1)} = \\frac{\\hat{n}_{ijk}^{(0)}\\cdot n_{ij\\cdot}}{\\hat{n}_{ij\\cdot}^{(0)}}.\n\\]\nWartoÅ›ci \\(\\hat{n}_{ij\\cdot}^{(0)}\\) oznaczajÄ… marginesy wyliczone z tabeli \\(\\hat{n}^{(0)}\\). Po zastosowaniu tego skalowania nowe liczebnoÅ›ci \\(\\hat{n}^{(1)}\\) majÄ… prawidÅ‚owe marginesy \\((XY)\\).\nKrok 2. Dopasowanie marginesÃ³w \\(\\hat{n}_{i\\cdot k}\\)\n\nTeraz narzuca siÄ™ speÅ‚nienie warunku \\[\n\\hat{n}_{i\\cdot k} = n_{i\\cdot k}.\n\\]\nWykorzystujÄ…c liczebnoÅ›ci \\(\\hat{n}_{ijk}^{(1)}\\) z poprzedniego kroku, dokonuje siÄ™ skalowania \\[\n\\hat{n}_{ijk}^{(2)} = \\frac{\\hat{n}_{ijk}^{(1)}\\cdot n_{i\\cdot k}}{\\hat{n}_{i\\cdot k}^{(1)}}.\n\\] Po tym kroku wartoÅ›ci oczekiwane \\(\\hat{n}^{(2)}\\) speÅ‚niajÄ… ograniczenia dotyczÄ…ce marginesÃ³w \\((XZ)\\).\nKrok 3. Dopasowanie marginesÃ³w \\(\\hat{n}_{\\cdot jk}\\)\n\nAby speÅ‚niÄ‡ trzecie ograniczenie \\[\n\\hat{n}_{\\cdot jk} = n_{\\cdot jk},\n\\] stosuje siÄ™ kolejne skalowanie \\[\n\\hat{n}_{ijk}^{(3)} = \\frac{\\hat{n}_{ijk}^{(2)}\\cdot n_{\\cdot jk}}{\\hat{n}_{\\cdot jk}^{(2)}}.\n\\] Po tym kroku liczebnoÅ›ci \\(\\hat{n}^{(3)}\\) majÄ… prawidÅ‚owe marginesy \\((YZ)\\).\nKrok 4. Kolejne cykle\nAby zakoÅ„czyÄ‡ pierwszy peÅ‚ny cykl dopasowaÅ„, wraca siÄ™ do kroku 1, podstawiajÄ…c \\(\\hat{n}_{ijk}^{(3)}\\) jako wartoÅ›ci wejÅ›ciowe \\[\n\\hat{n}_{ijk}^{(3)} \\longrightarrow \\hat{n}_{ijk}^{(0)} \\text{ w kolejnym cyklu}.\n\\]\nProcedura powtarza siÄ™ w kolejnoÅ›ci:\n\ndopasowanie \\((XY)\\),\ndopasowanie \\((XZ)\\),\ndopasowanie \\((YZ)\\),\n\naÅ¼ do osiÄ…gniÄ™cia zbieÅ¼noÅ›ci.\nZakoÅ„czenie algorytmu\nIteracje kontynuuje siÄ™ do momentu, w ktÃ³rym rÃ³Å¼nice miÄ™dzy kolejnymi tabelami \\(\\hat{n}^{(t)}\\) i \\(\\hat{n}^{(t+1)}\\) nie przekraczajÄ… ustalonej tolerancji, np.: \\[\n|\\hat{n}_{ijk}^{(t+1)} - \\hat{n}_{ijk}^{(t)}| &lt; 0.01\n\\]\ndla wszystkich \\(i,j,k\\).\nUwagi koÅ„cowe\n\nJeÅ›li brzegowe liczebnoÅ›ci oczekiwane mogÄ… byÄ‡ bezpoÅ›rednio wyliczone z danych empirycznych (np. w modelach peÅ‚nych), algorytm IPF nie jest konieczny.\nW modelach hierarchicznych brakujÄ…cych interakcji (np. brak \\((XYZ)\\)) IPF jest jedynÄ… wygodnÄ… metodÄ… prowadzÄ…cÄ… do estymacji maksymalnego prawdopodobieÅ„stwa.\nDowodzi siÄ™, Å¼e IPF zawsze zbiega do unikalnego rozwiÄ…zania MLE, o ile marginesy nie zawierajÄ… niemoÅ¼liwych konfiguracji (Haberman, 1974).",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "loglinear.html#ocena-dopasowania",
    "href": "loglinear.html#ocena-dopasowania",
    "title": "Modele log-liniowe",
    "section": "Ocena dopasowania",
    "text": "Ocena dopasowania\nOcena poprawnoÅ›ci modelu moÅ¼liwa jest poprzez porÃ³wnanie wartoÅ›ci oczekiwanych i zaobserwowanych licznoÅ›ci. NajczÄ™Å›ciej stosuje siÄ™ test statystyki ilorazu wiarygodnoÅ›ci, \\[\nG^2 = 2 \\sum_{ijk} n_{ijk} \\log\\left(\\frac{n_{ijk}}{\\hat{n}_{ijk}}\\right),\n\\]\nlub test chi-kwadrat Pearsona (rzadziej uÅ¼ywany), \\[\n\\chi^2 = \\sum_{ijk} \\frac{(n_{ijk} - \\hat{n}_{ijk})^2}{\\hat{n}_{ijk}}.\n\\] Obie statystyki pozwalajÄ… oceniÄ‡ zgodnoÅ›Ä‡ modelu z danymi, przy czym mniejsze wartoÅ›ci \\(G^2\\) i \\(\\chi^2\\) wskazujÄ… na lepsze dopasowanie. Kryteria informacyjne, takie jak AIC i BIC, umoÅ¼liwiajÄ… porÃ³wnanie modeli o rÃ³Å¼nej strukturze interakcji, natomiast analiza reszt pozwala wychwyciÄ‡ lokalne odchylenia. Sprawdza siÄ™ takÅ¼e warunki hierarchicznoÅ›ci oraz stabilnoÅ›Ä‡ parametrÃ³w wzglÄ™dem zmian tabeli.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "loglinear.html#interpretacja-parametrÃ³w",
    "href": "loglinear.html#interpretacja-parametrÃ³w",
    "title": "Modele log-liniowe",
    "section": "Interpretacja parametrÃ³w",
    "text": "Interpretacja parametrÃ³w\nInterpretacja modelu log-liniowego odwoÅ‚uje siÄ™ do koncepcji proporcji i wspÃ³Å‚czynnikÃ³w intensywnoÅ›ci zaleÅ¼noÅ›ci. Parametry w skali logarytmicznej odpowiadajÄ… ilorazom intensywnoÅ›ci wspÃ³Å‚wystÄ™powania kategorii. Dla efektÃ³w gÅ‚Ã³wnych porÃ³wnuje siÄ™ poziomy zmiennych w odniesieniu do kategorii bazowych, natomiast dla interakcji parametry wskazujÄ… na istnienie lub brak oddziaÅ‚ywania pomiÄ™dzy zmiennymi. Na przykÅ‚ad dodatnia wartoÅ›Ä‡ parametru interakcji \\(\\lambda^{XY}_{ij}\\) oznacza, Å¼e kombinacja kategorii \\(i\\) i \\(j\\) wystÄ™puje czÄ™Å›ciej niÅ¼ wynikaÅ‚oby to z ich niezaleÅ¼noÅ›ci, natomiast wartoÅ›ci ujemne sugerujÄ… niedoreprezentowanie tej pary. Interpretacja w kontekÅ›cie ilorazÃ³w szans umoÅ¼liwia przeksztaÅ‚cenie parametrÃ³w do skali relatywnej, uÅ¼ytecznej w analizach porÃ³wnawczych.\nModel log-liniowy stanowi zatem metodÄ™ pozwalajÄ…cÄ… odwzorowaÄ‡ strukturÄ™ zaleÅ¼noÅ›ci w danych jakoÅ›ciowych poprzez analizÄ™ efektÃ³w gÅ‚Ã³wnych i interakcji zmiennych. ÅÄ…czy zasady modeli Poissona, hierarchicznej struktury tabeli oraz testowania hipotez o niezaleÅ¼noÅ›ci, stanowiÄ…c uniwersalne narzÄ™dzie analityczne w badaniach wielowymiarowych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "loglinear.html#zera-strukturalne-i-prÃ³bkowe",
    "href": "loglinear.html#zera-strukturalne-i-prÃ³bkowe",
    "title": "Modele log-liniowe",
    "section": "Zera strukturalne i prÃ³bkowe",
    "text": "Zera strukturalne i prÃ³bkowe\nW analizie tabel wielodzielczych zera strukturalne i zera prÃ³bkowe odnoszÄ… siÄ™ do komÃ³rek o liczebnoÅ›ci rÃ³wnej zero, lecz rÃ³Å¼niÄ… siÄ™ znaczeniem i konsekwencjami dla modelowania. W obu przypadkach komÃ³rka ma wartoÅ›Ä‡ 0, ale interpretacja tego zera jest zupeÅ‚nie inna, co wpÅ‚ywa na moÅ¼liwoÅ›Ä‡ stosowania modeli log-liniowych oraz na ich dopasowanie.\nZerem strukturalnym nazywa siÄ™ takÄ… komÃ³rkÄ™ tabeli, w ktÃ³rej liczebnoÅ›Ä‡ musi wynosiÄ‡ zero ze wzglÄ™du na logikÄ™ lub reguÅ‚y konstrukcji zjawiska. Oznacza to, Å¼e taka kategoria w ogÃ³le nie moÅ¼e wystÄ…piÄ‡ w populacji i obserwacja jej byÅ‚aby sprzeczna z definicjÄ… zmiennych. PrzykÅ‚adowo, w tabeli krzyÅ¼ujÄ…cej pÅ‚eÄ‡ biologicznÄ… (mÄ™Å¼czyzna, kobieta) z kategoriÄ… â€ciÄ…Å¼aâ€ (tak, nie), komÃ³rka â€mÄ™Å¼czyzna, ciÄ…Å¼a = takâ€ jest zerem strukturalnym, bo z definicji nie moÅ¼e byÄ‡ tam Å¼adnej obserwacji. Podobnie w tabeli krzyÅ¼ujÄ…cej wiek w kategoriach (0â€“17, 18+) z â€posiadanie prawa wyborczegoâ€ (tak, nie), komÃ³rka â€wiek 0â€“17, prawo wyborcze = takâ€ jest zerem strukturalnym.\nZera prÃ³bkowe (ang. sampling zeros) oznaczajÄ… tych przypadki, w ktÃ³rych liczebnoÅ›Ä‡ komÃ³rki wynosi zero tylko dlatego, Å¼e w danym losowaniu lub prÃ³bie nie zaobserwowano Å¼adnej jednostki pasujÄ…cej do danej kombinacji kategorii, choÄ‡ w populacji jest ona moÅ¼liwa. PrzykÅ‚adem moÅ¼e byÄ‡ komÃ³rka tabeli wiÄ…Å¼Ä…cej wyksztaÅ‚cenie i rodzaj wykonywanej pracy, gdzie w prÃ³bie nie odnotowano osoby z wyksztaÅ‚ceniem podstawowym zatrudnionej jako programista, mimo Å¼e taka kombinacja jest realnie moÅ¼liwa. Innym przykÅ‚adem jest tabela preferencji zakupowych, w ktÃ³rej Å¼adna osoba w prÃ³bie nie wybraÅ‚a poÅ‚Ä…czenia â€pÅ‚eÄ‡ mÄ™skaâ€ i â€zakup kosmetykÃ³w naturalnychâ€, mimo Å¼e taka decyzja zakupowa moÅ¼e wystÄ…piÄ‡ w populacji.\nRÃ³Å¼nica miÄ™dzy tymi dwoma typami zer jest istotna, poniewaÅ¼ zera strukturalne wymagajÄ… wyÅ‚Ä…czenia odpowiednich komÃ³rek z analizy lub uÅ¼ycia modeli uwzglÄ™dniajÄ…cych ograniczenia, natomiast zera prÃ³bkowe moÅ¼na traktowaÄ‡ jako wynik losowej fluktuacji w prÃ³bie i uwzglÄ™dniaÄ‡ normalnie w dopasowaniu modeli log-liniowych.\nCo robimy w modelu log-liniowym:\n\nw przypadku niekompletnych tablic z zerami strukturalnymi liczbÄ™ stopni swobody rozkÅ‚adu \\(\\chi^2\\) statystyki \\(\\chi^2\\) (lub \\(G^2\\)) okreÅ›la formuÅ‚a \\(df= n_1âˆ’n_2âˆ’n_3,\\) gdzie \\(n_1\\) to liczba komÃ³rek w tabeli, \\(n_2\\) to liczba parametrÃ³w w modelu wymagajÄ…cych estymacji, a \\(n_3\\) to liczba zer a priori.\nrozwiÄ…zaniem problemu wystÄ™powania zer prÃ³bkowych jest zwiÄ™kszenie liczebnoÅ›ci prÃ³bki lub ewentualnie, jeÅ›li jest to niemoÅ¼liwe, zwiÄ™kszenie wszystkich liczebnoÅ›ci oczekiwanych przez dodanie maÅ‚ej staÅ‚ej, zwykle \\(\\Delta = 0.5\\), do kaÅ¼dej komÃ³rki tabeli (tzw. korekta Haldaneâ€™).\n\n\nPrzykÅ‚ad 8.1 Â \n\nDane i pytanie badawcze\n\nWbudowany zbiÃ³r HairEyeColor opisuje liczbÄ™ osÃ³b (uczestnikÃ³w badania), skategoryzowanych wedÅ‚ug:\n\n\nHair: kolor wÅ‚osÃ³w (Black, Brown, Red, Blond),\n\nEye: kolor oczu (Brown, Blue, Hazel, Green),\n\nSex: pÅ‚eÄ‡ (Male, Female).\n\nPytania badawcze:\n\nczy kolor wÅ‚osÃ³w i oczu sÄ… od siebie niezaleÅ¼ne?\nczy zaleÅ¼noÅ›Ä‡ wÅ‚osyâ€“oczy jest taka sama dla kobiet i mÄ™Å¼czyzn?\nczy pÅ‚eÄ‡ wnosi jakÄ…Å› istotnÄ… interakcjÄ™?\n\n\nWczytanie danych i podstawowa eksploracja\n\n\nKod# wczytanie danych\ndata(HairEyeColor)\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\nZamieniamy go na ramkÄ™ danych:\n\nKodhec &lt;- as.data.frame(HairEyeColor)\nhead(hec)\n\n   Hair   Eye  Sex Freq\n1 Black Brown Male   32\n2 Brown Brown Male   53\n3   Red Brown Male   10\n4 Blond Brown Male    3\n5 Black  Blue Male   11\n6 Brown  Blue Male   50\n\n\nMoÅ¼na teÅ¼ sprawdziÄ‡ proste tablice brzegowe:\n\nKod# tablica 2D wÅ‚osy Ã— oczy (bez rozrÃ³Å¼nienia pÅ‚ci)\n(HairEye &lt;- margin.table(HairEyeColor, margin = c(1, 2)))\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    68   20    15     5\n  Brown   119   84    54    29\n  Red      26   17    14    14\n  Blond     7   94    10    16\n\nKod# tablica 2D wÅ‚osy Ã— pÅ‚eÄ‡\nmargin.table(HairEyeColor, margin = c(1, 3))\n\n       Sex\nHair    Male Female\n  Black   56     52\n  Brown  143    143\n  Red     34     37\n  Blond   46     81\n\nKod# tablica 2D oczy Ã— pÅ‚eÄ‡\nmargin.table(HairEyeColor, margin = c(2, 3))\n\n       Sex\nEye     Male Female\n  Brown   98    122\n  Blue   101    114\n  Hazel   47     46\n  Green   33     31\n\n\nJuÅ¼ na tym etapie moÅ¼na zauwaÅ¼yÄ‡, Å¼e np. brÄ…zowe wÅ‚osy i brÄ…zowe oczy sÄ… najczÄ™stsze.\n\nIdea modelu log-liniowego\n\n\nZmienna Freq (liczebnoÅ›Ä‡ komÃ³rki tabeli) traktowana jest jako zmienna o rozkÅ‚adzie Poissona z wartoÅ›ciÄ… oczekiwanÄ… \\(n_{ijk}\\). Model log-liniowy zakÅ‚ada, Å¼e \\[\n\\log \\hat{n}_{ijk} = \\lambda + \\lambda_i^{Hair} + \\lambda_j^{Eye} + \\lambda_k^{Sex} + \\lambda_{ij}^{Hair:Eye} + \\lambda_{ik}^{Hair:Sex} +  \\lambda_{jk}^{Eye:Sex} + \\lambda_{ijk}^{Hair:Eye:Sex}.\n\\] To jest model peÅ‚ny (nasycony, ang. saturated model), ktÃ³ry zawsze perfekcyjnie odtwarza liczebnoÅ›ci obserwowane. ChcÄ…c znaleÅºÄ‡ model optymalny, eliminujemy z modelu nasyconego poszczegÃ³lne efekty interakcji, zaczynajÄ…c od interakcji najwyÅ¼szego rzÄ™du.\n\nModel peÅ‚ny (nasycony)\n\nDo estymacji uÅ¼yjemy funkcji loglm z pakietu MASS.\n\nKodlibrary(MASS)\nm_full &lt;- loglm(Freq ~ Hair * Eye * Sex, data = hec)\nm_full\n\nCall:\nloglm(formula = Freq ~ Hair * Eye * Sex, data = hec)\n\nStatistics:\n                 X^2 df P(&gt; X^2)\nLikelihood Ratio   0  0        1\nPearson            0  0        1\n\n\nW formule symbol * oznacza wszystkie efekty gÅ‚Ã³wne i interakcje do danego rzÄ™du. Hair * Eye * Sex odpowiada zapisowi Hair + Eye + Sex + Hair:Eye + Hair:Sex + Eye:Sex + Hair:Eye:Sex\nJak widaÄ‡ na podstawie statystyk dopasowania \\(G^2\\) i \\(\\chi^2\\) model idealnie dopasowuje siÄ™ do danych (deviance = 0, p-value = 1).\n\nModel niezaleÅ¼noÅ›ci wszystkich trzech zmiennych\n\nTeraz bardzo restrykcyjny model:\n\nzakÅ‚ada siÄ™ peÅ‚nÄ… niezaleÅ¼noÅ›Ä‡ Hair, Eye i Sex,\nbrak jakichkolwiek interakcji.\n\n\nKodm_indep &lt;- loglm(Freq ~ Hair + Eye + Sex, data = hec)\nm_indep\n\nCall:\nloglm(formula = Freq ~ Hair + Eye + Sex, data = hec)\n\nStatistics:\n                      X^2 df P(&gt; X^2)\nLikelihood Ratio 166.3001 24        0\nPearson          164.9247 24        0\n\n\nNa podstawie wynikÃ³w moÅ¼na stwierdziÄ‡, Å¼e model zÅ‚oÅ¼ony jedynie z efektÃ³w brzegowych jest niewystarczajÄ…cy do opisu zwiÄ…zkÃ³w miÄ™dzy cechami.\n\nModel testujÄ…cy czy kolor wÅ‚osÃ³w i oczu sÄ… od siebie niezaleÅ¼ne\n\n\nKodm_HE_indep &lt;- loglm(Freq ~ Hair + Eye, data = hec)\nm_HE_indep\n\nCall:\nloglm(formula = Freq ~ Hair + Eye, data = hec)\n\nStatistics:\n                      X^2 df P(&gt; X^2)\nLikelihood Ratio 168.2539 25        0\nPearson          171.5907 25        0\n\n\nModel m_HE_indep reprezentuje model niezaleÅ¼noÅ›ci miÄ™dzy kolorem wÅ‚osÃ³w (Hair) i kolorem oczu (Eye). Odczytane statystyki pozwalajÄ… sprawdziÄ‡, czy hipoteza niezaleÅ¼noÅ›ci jest zgodna z danymi.\n\nModel zakÅ‚ada, Å¼e \\(H_0: Hair \\perp Eye\\) czyli brak interakcji Hair:Eye, co oznacza, Å¼e struktura czÄ™stoÅ›ci wÅ‚osÃ³w i oczu powinna wynikaÄ‡ wyÅ‚Ä…cznie z ich rozkÅ‚adÃ³w marginalnych.\nStatystyki dopasowania \\(G^2 = 168.25\\), \\(p = 0\\) oraz \\(\\chi^2 = 171.59\\), \\(p = 0\\) wskazujÄ… na bardzo sÅ‚abe dopasowanie modelu do danych.\nKolor wÅ‚osÃ³w i kolor oczu sÄ… wyraÅºnie zaleÅ¼ne. W tabeli wystÄ™pujÄ… kombinacje kolorÃ³w wÅ‚osÃ³w i oczu, ktÃ³re pojawiajÄ… siÄ™ czÄ™Å›ciej lub rzadziej, niÅ¼ przewiduje model niezaleÅ¼noÅ›ci.\n\n\nKodfitted(m_HE_indep)\n\nRe-fitting to get fitted values\n\n\n, , .Within. = 1\n\n       Eye\nHair       Brown     Blue     Hazel     Green\n  Black 20.06757 19.61149  8.483108  5.837838\n  Brown 53.14189 51.93412 22.464527 15.459459\n  Red   13.19257 12.89274  5.576858  3.837838\n  Blond 23.59797 23.06166  9.975507  6.864865\n\n, , .Within. = 2\n\n       Eye\nHair       Brown     Blue     Hazel     Green\n  Black 20.06757 19.61149  8.483108  5.837838\n  Brown 53.14189 51.93412 22.464527 15.459459\n  Red   13.19257 12.89274  5.576858  3.837838\n  Blond 23.59797 23.06166  9.975507  6.864865\n\n\nWartoÅ›ci przedstawione jako fitted(m_HE_indep) to liczebnoÅ›ci oczekiwane w modelu zakÅ‚adajÄ…cym niezaleÅ¼noÅ›Ä‡ koloru wÅ‚osÃ³w i koloru oczu. Otrzymane tablice majÄ… identyczne wartoÅ›ci dla obu poziomÃ³w pÅ‚ci, poniewaÅ¼ model niezaleÅ¼noÅ›ci nie uwzglÄ™dnia pÅ‚ci â€“ liczy wyÅ‚Ä…cznie to, jakie liczebnoÅ›ci powinny wystÄ…piÄ‡ przy produkcie marginesÃ³w Hair i Eye.\nKaÅ¼da komÃ³rka pokazuje zatem, ile osÃ³b powinno znaleÅºÄ‡ siÄ™ w danej kombinacji kolorÃ³w wÅ‚osÃ³w i oczu, gdyby te cechy byÅ‚y od siebie niezaleÅ¼ne. PorÃ³wnanie tych wartoÅ›ci z liczebnoÅ›ciami obserwowanymi (ktÃ³re silnie od nich odbiegajÄ…) ilustruje, Å¼e model niezaleÅ¼noÅ›ci nie opisuje danych poprawnie, co potwierdza wczeÅ›niejszy test odrzucajÄ…cy hipotezÄ™ niezaleÅ¼noÅ›ci Hairâ€“Eye.\n\nModel testujÄ…cy czy zaleÅ¼noÅ›Ä‡ Hairâ€“Eye jest taka sama dla kobiet i mÄ™Å¼czyzn\n\n\nKodm_no3way &lt;- loglm(Freq ~ (Hair + Eye + Sex)^2, data = hec)\nm_no3way\n\nCall:\nloglm(formula = Freq ~ (Hair + Eye + Sex)^2, data = hec)\n\nStatistics:\n                      X^2 df  P(&gt; X^2)\nLikelihood Ratio 6.761258  9 0.6619600\nPearson          6.868292  9 0.6508299\n\nKod# PorÃ³wnanie modeli zagnieÅ¼dÅ¼onych (test LRT)\nanova(m_no3way, m_full, test = \"Chisq\")\n\nLR tests for hierarchical log-linear models\n\nModel 1:\n Freq ~ (Hair + Eye + Sex)^2 \nModel 2:\n Freq ~ Hair * Eye * Sex \n\n          Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev)\nModel 1   6.761258  9                                    \nModel 2   0.000000  0   6.761258         9        0.66196\nSaturated 0.000000  0   0.000000         0        1.00000\n\n\nModel zawierajÄ…cy wszystkie interakcje drugiego rzÄ™du, ale pozbawiony interakcji trzeciego rzÄ™du, charakteryzuje siÄ™ bardzo dobrym dopasowaniem do danych. Wysokie wartoÅ›ci p-value zarÃ³wno dla statystyki ilorazu wiarygodnoÅ›ci, jak i testu Pearsona wskazujÄ…, Å¼e rÃ³Å¼nice pomiÄ™dzy wartoÅ›ciami obserwowanymi a oczekiwanymi nie sÄ… istotne. Oznacza to, Å¼e model w tej postaci potrafi odtworzyÄ‡ strukturÄ™ danych i nie ma podstaw do wnioskowania, Å¼e brakuje w nim istotnej zaleÅ¼noÅ›ci.\nPorÃ³wnanie tego modelu z modelem nasyconym, ktÃ³ry dodatkowo zawiera interakcjÄ™ trzeciego rzÄ™du, rÃ³wnieÅ¼ nie pokazuje istotnych rÃ³Å¼nic. WartoÅ›Ä‡ \\(p\\) dla rÃ³Å¼nicy deviance pomiÄ™dzy modelami jest wysoka, co wskazuje, Å¼e dodanie interakcji trÃ³jwymiarowej nie poprawia jakoÅ›ci dopasowania. Oznacza to, Å¼e zaleÅ¼noÅ›ci pomiÄ™dzy parami zmiennych w peÅ‚ni wyjaÅ›niajÄ… obserwowanÄ… strukturÄ™, a interakcja trzeciego rzÄ™du nie wnosi dodatkowej informacji.\nWynik testÃ³w oznacza, Å¼e zaleÅ¼noÅ›Ä‡ miÄ™dzy kolorem wÅ‚osÃ³w a kolorem oczu jest taka sama dla kobiet i mÄ™Å¼czyzn.\n\nKodfitted(m_no3way)\n\nRe-fitting to get fitted values\n\n\n, , Sex = Male\n\n       Eye\nHair        Brown      Blue     Hazel     Green\n  Black 32.789022 11.743604  8.443926  3.018481\n  Brown 52.523050 45.931148 28.195250 16.348213\n  Red   10.760485  8.819775  6.916602  7.502671\n  Blond  1.927443 34.505472  3.444222  6.130636\n\n, , Sex = Female\n\n       Eye\nHair        Brown      Blue     Hazel     Green\n  Black 35.208836  8.258511  6.556304  1.981916\n  Brown 66.477232 38.072151 25.804202 12.652225\n  Red   15.239924  8.180608  7.083293  6.497414\n  Blond  5.074007 59.488731  6.556201  9.868446\n\n\nWartoÅ›ci fitted(m_no3way) przedstawiajÄ… liczebnoÅ›ci oczekiwane w modelu, ktÃ³ry uwzglÄ™dnia wszystkie interakcje drugiego rzÄ™du (Hair:Eye, Hair:Sex, Eye:Sex), ale pomija interakcjÄ™ trzeciego rzÄ™du. Oczekiwane liczebnoÅ›ci rÃ³Å¼niÄ… siÄ™ miÄ™dzy kobietami a mÄ™Å¼czyznami, poniewaÅ¼ model dopuszcza wpÅ‚yw pÅ‚ci na rozkÅ‚ad wÅ‚osÃ³w oraz na rozkÅ‚ad oczu, a takÅ¼e oddzielnie na zaleÅ¼noÅ›Ä‡ kaÅ¼dej z tych cech z pÅ‚ciÄ….\nRozkÅ‚ad liczebnoÅ›ci oczekiwanych w kaÅ¼dej pÅ‚ci odzwierciedla rÃ³Å¼nice marginesÃ³w i dwuwymiarowych zaleÅ¼noÅ›ci w podtabelach, ale brak interakcji trzeciego rzÄ™du sprawia, Å¼e wzorce zaleÅ¼noÅ›ci miÄ™dzy kolorem wÅ‚osÃ³w i kolorem oczu sÄ… nakÅ‚adane symetrycznie w kaÅ¼dej pÅ‚ci. Oznacza to, Å¼e model dopuszcza rÃ³Å¼nice w czÄ™stoÅ›ci kategorii wÅ‚osÃ³w i oczu miÄ™dzy kobietami a mÄ™Å¼czyznami, lecz zakÅ‚ada, Å¼e trÃ³jwymiarowy wzorzec wspÃ³Å‚wystÄ™powania tych cech nie zawiera dodatkowej struktury uzaleÅ¼nionej od pÅ‚ci.\n\nKodexp(m_no3way$param$Eye.Sex)\n\n       Sex\nEye          Male    Female\n  Brown 0.8564811 1.1675681\n  Blue  1.0583498 0.9448672\n  Hazel 1.0072147 0.9928370\n  Green 1.0952946 0.9129963\n\nKodexp(m_no3way$param$Hair.Sex)\n\n       Sex\nHair         Male    Female\n  Black 1.1820456 0.8459910\n  Brown 1.0887655 0.9184714\n  Red   1.0292488 0.9715824\n  Blond 0.7549376 1.3246128\n\nKodexp(m_no3way$param$Hair.Eye)\n\n       Eye\nHair        Brown      Blue    Hazel     Green\n  Black 2.7189690 0.6650995 1.109218 0.4985309\n  Brown 1.3349516 0.7973193 1.135242 0.8275872\n  Red   1.0601658 0.5934837 1.079522 1.4722643\n  Blond 0.2598701 3.1774030 0.735636 1.6462976\n\n\nWartoÅ›ci parametrÃ³w w postaci wykÅ‚adniczej (czyli po zastosowaniu funkcji exp) pokazujÄ… wzglÄ™dne ilorazy czÄ™stoÅ›ci dla poszczegÃ³lnych efektÃ³w dwuwymiarowych obecnych w modelu. Dla testowanej hipotezy dotyczÄ…cej obecnoÅ›ci lub braku istotnej interakcji z pÅ‚ciÄ… wartoÅ›ciowe sÄ… wyÅ‚Ä…cznie parametry dotyczÄ…ce relacji Eye:Sex oraz Hair:Sex. To one informujÄ…, czy rozkÅ‚ad kategorii oczu lub wÅ‚osÃ³w rÃ³Å¼ni siÄ™ pomiÄ™dzy kobietami i mÄ™Å¼czyznami po uwzglÄ™dnieniu pozostaÅ‚ych efektÃ³w w modelu. Ilorazy wiÄ™ksze od 1 wskazujÄ…, Å¼e dana kombinacja (np. kategoria oczu u mÄ™Å¼czyzn) wystÄ™puje czÄ™Å›ciej, niÅ¼ wynikaÅ‚oby to z modelu bez tej interakcji, natomiast wartoÅ›ci mniejsze od 1 informujÄ… o relatywnym niedoborze obserwacji. PrzykÅ‚adowo, wartoÅ›Ä‡ 0.856 oznacza, Å¼e liczebnoÅ›Ä‡ mÄ™Å¼czyzn o brÄ…zowych oczach jest o okoÅ‚o 14 procent niÅ¼sza, niÅ¼ wynikaÅ‚oby to z modelu bez interakcji Eye:Sex, przy uwzglÄ™dnieniu pozostaÅ‚ych efektÃ³w w modelu. Natomiast wartoÅ›Ä‡ 1.17 dla kobiet o brÄ…zowych oczach wskazuje, Å¼e taka kombinacja wystÄ™puje ponadprzeciÄ™tnie (czÄ™Å›ciej o okoÅ‚o 17%) niÅ¼ wynikaÅ‚oby to z modelu bez tej interakcji.\nParametry Hair:Eye dotyczÄ… relacji miÄ™dzy kolorem wÅ‚osÃ³w a kolorem oczu i nie majÄ… znaczenia dla oceny roli pÅ‚ci w strukturze zaleÅ¼noÅ›ci. StanowiÄ… natomiast wartoÅ›ciowe podsumowanie siÅ‚y i kierunku powiÄ…zaÅ„ pomiÄ™dzy wÅ‚osami i oczami, pokazujÄ…c, ktÃ³re kombinacje sÄ… nadreprezentowane, a ktÃ³re pojawiajÄ… siÄ™ rzadziej od oczekiwaÅ„.\n\nModel testujÄ…cy czy pÅ‚eÄ‡ wnosi jakÄ…Å› istotnÄ… interakcjÄ™?\n\n\nKod# Model bez interakcji z pÅ‚ciÄ…:\nm_noSexInt &lt;- loglm(Freq ~ Hair * Eye + Sex, data = hec)\nm_noSexInt\n\nCall:\nloglm(formula = Freq ~ Hair * Eye + Sex, data = hec)\n\nStatistics:\n                      X^2 df  P(&gt; X^2)\nLikelihood Ratio 19.85656 15 0.1775045\nPearson          19.56712 15 0.1891745\n\nKod# PorÃ³wnanie: czy dodanie interakcji z Sex poprawia dopasowanie?\nanova(m_noSexInt, m_no3way, test = \"Chisq\")\n\nLR tests for hierarchical log-linear models\n\nModel 1:\n Freq ~ Hair * Eye + Sex \nModel 2:\n Freq ~ (Hair + Eye + Sex)^2 \n\n           Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev)\nModel 1   19.856561 15                                    \nModel 2    6.761258  9  13.095303         6        0.04155\nSaturated  0.000000  0   6.761258         9        0.66196\n\n\nModel zawierajÄ…cy interakcjÄ™ miÄ™dzy kolorem wÅ‚osÃ³w i kolorem oczu oraz efekty gÅ‚Ã³wne wszystkich zmiennych, ale pozbawiony interakcji z pÅ‚ciÄ…, wykazuje dobre dopasowanie do danych. Wysokie wartoÅ›ci p-value obu testÃ³w dopasowania wskazujÄ…, Å¼e rÃ³Å¼nice miÄ™dzy liczebnoÅ›ciami obserwowanymi i oczekiwanymi nie sÄ… istotne statystycznie. Oznacza to, Å¼e uproszczona struktura modelu jest wystarczajÄ…ca do odtworzenia ukÅ‚adu czÄ™stoÅ›ci w tabeli.\nZ takiego wyniku wynika, Å¼e obecnoÅ›Ä‡ pÅ‚ci nie wprowadza istotnych modyfikacji zaleÅ¼noÅ›ci miÄ™dzy kolorem wÅ‚osÃ³w a kolorem oczu. Sam efekt pÅ‚ci, uwzglÄ™dniony jako efekt gÅ‚Ã³wny, pozwala odtworzyÄ‡ zmiennoÅ›Ä‡ liczebnoÅ›ci bez koniecznoÅ›ci modelowania interakcji z pozostaÅ‚ymi zmiennymi. StrukturÄ™ wspÃ³Å‚wystÄ™powania wÅ‚osÃ³w i oczu moÅ¼na wiÄ™c traktowaÄ‡ jako stabilnÄ… niezaleÅ¼nie od pÅ‚ci, a rÃ³Å¼nice pomiÄ™dzy poziomami tej zmiennej nie wykraczajÄ… poza proste rÃ³Å¼nice w liczebnoÅ›ciach marginalnych.\nPorÃ³wnanie modeli wskazuje, Å¼e dodanie interakcji z pÅ‚ciÄ… (Hair:Sex oraz Eye:Sex) istotnie poprawia dopasowanie w stosunku do modelu, ktÃ³ry takiej interakcji nie zawiera. JednoczeÅ›nie interakcja trzeciego rzÄ™du pozostaje nieistotna, co oznacza, Å¼e pÅ‚eÄ‡ wpÅ‚ywa na rozkÅ‚ad wÅ‚osÃ³w i oczu, ale nie modyfikuje zaleÅ¼noÅ›ci miÄ™dzy tymi dwiema cechami.\n\nKodfitted(m_noSexInt)\n\nRe-fitting to get fitted values\n\n\n, , Sex = Male\n\n       Eye\nHair        Brown      Blue     Hazel     Green\n  Black 32.047297  9.425676  7.069257  2.356419\n  Brown 56.082770 39.587838 25.449324 13.667230\n  Red   12.253378  8.011824  6.597973  6.597973\n  Blond  3.298986 44.300676  4.712838  7.540541\n\n, , Sex = Female\n\n       Eye\nHair        Brown      Blue     Hazel     Green\n  Black 35.952703 10.574324  7.930743  2.643581\n  Brown 62.917230 44.412162 28.550676 15.332770\n  Red   13.746622  8.988176  7.402027  7.402027\n  Blond  3.701014 49.699324  5.287162  8.459459\n\n\nWartoÅ›ci fitted(m_noSexInt) przedstawiajÄ… liczebnoÅ›ci oczekiwane w modelu, ktÃ³ry uwzglÄ™dnia jedynie interakcjÄ™ Hair:Eye, a pÅ‚eÄ‡ traktuje wyÅ‚Ä…cznie jako efekt gÅ‚Ã³wny. Oznacza to, Å¼e model dopuszcza rÃ³Å¼nice w ogÃ³lnej liczbie mÄ™Å¼czyzn i kobiet, ale zakÅ‚ada, Å¼e struktura wspÃ³Å‚wystÄ™powania kolorÃ³w wÅ‚osÃ³w i oczu jest identyczna w obu pÅ‚ciach.\nW praktyce oznacza to, Å¼e rÃ³Å¼nice pomiÄ™dzy tablicami dla mÄ™Å¼czyzn i kobiet wynikajÄ… wyÅ‚Ä…cznie z proporcji pÅ‚ci w prÃ³bie oraz z rozkÅ‚adÃ³w marginalnych, a nie z tego, Å¼e pÅ‚eÄ‡ modyfikuje zaleÅ¼noÅ›Ä‡ miÄ™dzy wÅ‚osami a oczami. KaÅ¼dy â€ksztaÅ‚tâ€ tabeli dla danej pÅ‚ci jest zatem skalowanÄ… wersjÄ… tej samej struktury Hairâ€“Eye. JeÅ›li pewna kombinacja wÅ‚osÃ³w i oczu jest relatywnie czÄ™sta u mÄ™Å¼czyzn, to model zakÅ‚ada, Å¼e powinna byÄ‡ tak samo relatywnie czÄ™sta u kobiet, tylko w innej skali wynikajÄ…cej z ogÃ³lnej liczebnoÅ›ci kobiet. DziÄ™ki temu wynik Å‚atwo interpretowaÄ‡ jako konsekwencjÄ™ zaÅ‚oÅ¼enia, Å¼e pÅ‚eÄ‡ nie zmienia wzorca wspÃ³Å‚wystÄ™powania kolorÃ³w wÅ‚osÃ³w i oczu.\n\nKodexp(m_noSexInt$param$Hair.Eye)\n\n       Eye\nHair       Brown      Blue     Hazel     Green\n  Black 2.651732 0.6712141 1.1104285 0.5059628\n  Brown 1.318449 0.8009509 1.1357654 0.8337621\n  Red   1.056148 0.5943065 1.0795875 1.4757306\n  Blond 0.270821 3.1298428 0.7344512 1.6063203\n\n\nParametry exp(Hair:Eye) pokazujÄ…, jak bardzo dana kombinacja koloru wÅ‚osÃ³w i koloru oczu jest nad- lub niedoreprezentowana wzglÄ™dem sytuacji niezaleÅ¼noÅ›ci tych cech, po uwzglÄ™dnieniu efektÃ³w gÅ‚Ã³wnych. WartoÅ›ci wiÄ™ksze od 1 wskazujÄ… na wzglÄ™dny nadmiar obserwacji, natomiast wartoÅ›ci mniejsze od 1 oznaczajÄ… wzglÄ™dny niedobÃ³r danej kombinacji w stosunku do oczekiwaÅ„ modelowych.\nDla czarnych wÅ‚osÃ³w wspÃ³Å‚czynnik rÃ³wny 2.65 dla brÄ…zowych oczu oznacza, Å¼e osoby o tej kombinacji pojawiajÄ… siÄ™ w danych ponad dwukrotnie czÄ™Å›ciej, niÅ¼ wynikaÅ‚oby to z niezaleÅ¼nego rozkÅ‚adu kategorii. JednoczeÅ›nie parametr 0.67 dla czarnych wÅ‚osÃ³w i niebieskich oczu wskazuje, Å¼e ta kombinacja pojawia siÄ™ znacznie rzadziej, niÅ¼ sugerowaÅ‚by model bez interakcji. WartoÅ›Ä‡ 1.11 dla pary czarne wÅ‚osy i piwne oczy jest bliska jednoÅ›ci, co oznacza, Å¼e liczebnoÅ›Ä‡ tej kombinacji odpowiada temu, co przewidywaÅ‚by model niezaleÅ¼noÅ›ci.\nZ kolei dla wÅ‚osÃ³w blond bardzo wysoki wspÃ³Å‚czynnik 3.13 przy niebieskich oczach Å›wiadczy o silnej nadreprezentacji tej kombinacji w danych. Tymczasem wartoÅ›Ä‡ 0.27 przy brÄ…zowych oczach oznacza, Å¼e osoby o blond wÅ‚osach i brÄ…zowych oczach pojawiajÄ… siÄ™ znacznie rzadziej, niÅ¼ moÅ¼na by oczekiwaÄ‡, gdyby wÅ‚osy i oczy byÅ‚y od siebie niezaleÅ¼ne. Analogicznie, parametr 1.61 przy zielonych oczach sygnalizuje umiarkowany nadmiar tej kombinacji dla osÃ³b o jasnych wÅ‚osach. WartoÅ›ci te pokazujÄ…, Å¼e zaleÅ¼noÅ›Ä‡ miÄ™dzy kolorem wÅ‚osÃ³w i oczu ma wyraÅºny charakter i prowadzi do zrÃ³Å¼nicowanych wzorcÃ³w wspÃ³Å‚wystÄ™powania w rÃ³Å¼nych parach kategorii.\n\n\n\n\n\nGoodman, Leo A. 1964. â€Simultaneous Confidence Intervals for Contrasts Among Multinomial Populationsâ€. The Annals of Mathematical Statistics 35 (2): 716â€“25. https://doi.org/10.1214/aoms/1177703569.\n\n\nâ€”â€”â€”. 1968. â€The Analysis of Cross-Classified Data: Independence, Quasi-Independence, and Interactions in Contingency Tables with or Without Missing Entriesâ€. Journal of the American Statistical Association 63 (324): 1091â€“131. https://doi.org/10.2307/2285873.\n\n\nâ€”â€”â€”. 1970. â€The Multivariate Analysis of Qualitative Data: Interactions among Multiple Classificationsâ€. Journal of the American Statistical Association 65 (329): 226â€“56. https://doi.org/10.1080/01621459.1970.10481076.\n\n\nâ€”â€”â€”. 1971. â€The Analysis of Multidimensional Contingency Tables: Stepwise Procedures and Direct Estimation Methods for Building Models for Multiple Classificationsâ€. Technometrics 13 (1): 33â€“61. https://doi.org/10.1080/00401706.1971.10488753.\n\n\nâ€”â€”â€”. 1981. â€Association Models and the Bivariate Normal for Contingency Tables with Ordered Categoriesâ€. Biometrika 68 (2): 347â€“55. https://doi.org/10.2307/2335579.\n\n\nGoodman, Leo A., i William H. Kruskal. 1954. â€Measures of Association for Cross Classificationsâ€. Journal of the American Statistical Association 49 (268): 732â€“64. https://doi.org/10.2307/2281536.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Modele log-liniowe</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Anderson, T. W. 1992. â€œIntroduction to Hotelling (1931) the\nGeneralization of Studentâ€™s Ratio.â€ In, 45â€“53.\nSpringer New York. https://doi.org/10.1007/978-1-4612-0919-5_3.\n\n\nBartlett, M. S. 1951. â€œThe Effect of Standardization on a Ï‡ 2\nApproximation in Factor Analysis.â€ Biometrika 38 (3/4):\n337. https://doi.org/10.2307/2332580.\n\n\nBenzÃ©cri, J. P. 1977. â€œHistoire Et PrÃ©histoire de lâ€™analyse Des\nDonnÃ©es. Partie V Lâ€™analyse Des Correspondances.â€ Les\nCahiers de lâ€™analyse Des DonnÃ©es 2 (1): 9â€“40. https://www.numdam.org/item/CAD_1977__2_1_9_0/.\n\n\nBollen, Kenneth A. 1989. â€œStructural Equation Models with Observed\nVariables.â€ Structural Equations with Latent Variables,\nApril, 80â€“150. https://doi.org/10.1002/9781118619179.ch4.\n\n\nCattell, Raymond B. 1966. â€œThe Scree Test For The Number Of\nFactors.â€ Multivariate Behavioral Research 1 (2):\n245â€“76. https://doi.org/10.1207/s15327906mbr0102_10.\n\n\nComon, Pierre. 1994. â€œIndependent Component Analysis, A New\nConcept?â€ Signal Processing 36 (3): 287â€“314. https://doi.org/10.1016/0165-1684(94)90029-9.\n\n\nEveritt, B. S., and A. Yates. 1989. â€œMultivariate Exploratory Data\nAnalysis: A Perspective on Exploratory Factor Analysis.â€\nBiometrics 45 (1): 342. https://doi.org/10.2307/2532065.\n\n\nGoodman, Leo A. 1964. â€œSimultaneous Confidence Intervals for\nContrasts Among Multinomial Populations.â€ The Annals of\nMathematical Statistics 35 (2): 716â€“25. https://doi.org/10.1214/aoms/1177703569.\n\n\nâ€”â€”â€”. 1968. â€œThe Analysis of Cross-Classified Data: Independence,\nQuasi-Independence, and Interactions in Contingency Tables with or\nWithout Missing Entries.â€ Journal of the American Statistical\nAssociation 63 (324): 1091â€“131. https://doi.org/10.2307/2285873.\n\n\nâ€”â€”â€”. 1970. â€œThe Multivariate Analysis of Qualitative Data:\nInteractions Among Multiple Classifications.â€ Journal of the\nAmerican Statistical Association 65 (329): 226â€“56. https://doi.org/10.1080/01621459.1970.10481076.\n\n\nâ€”â€”â€”. 1971. â€œThe Analysis of Multidimensional Contingency Tables:\nStepwise Procedures and Direct Estimation Methods for Building Models\nfor Multiple Classifications.â€ Technometrics 13 (1):\n33â€“61. https://doi.org/10.1080/00401706.1971.10488753.\n\n\nâ€”â€”â€”. 1981. â€œAssociation Models and the Bivariate Normal for\nContingency Tables with Ordered Categories.â€ Biometrika\n68 (2): 347â€“55. https://doi.org/10.2307/2335579.\n\n\nGoodman, Leo A., and William H. Kruskal. 1954. â€œMeasures of\nAssociation for Cross Classifications.â€ Journal of the\nAmerican Statistical Association 49 (268): 732â€“64. https://doi.org/10.2307/2281536.\n\n\nGrieder, Silvia, and Markus D. Steiner. 2020. â€œAlgorithmic Jingle\nJungle: A Comparison of Implementations of Principal Axis Factoring and\nPromax Rotation in r and SPSS.â€ http://dx.doi.org/10.31234/osf.io/7hwrm.\n\n\nHarman, Harry H., and Wayne H. Jones. 1966. â€œFactor Analysis by\nMinimizing Residuals (Minres).â€ Psychometrika 31 (3):\n351â€“68. https://doi.org/10.1007/bf02289468.\n\n\nHendrickson, Alan E., and Paul Owen White. 1964. â€œPROMAX: A QUICK\nMETHOD FOR ROTATION TO OBLIQUE SIMPLE STRUCTURE.â€ British\nJournal of Statistical Psychology 17 (1): 65â€“70. https://doi.org/10.1111/j.2044-8317.1964.tb00244.x.\n\n\nHorn, John L. 1965. â€œA Rationale and Test for the Number of\nFactors in Factor Analysis.â€ Psychometrika 30 (2):\n179â€“85. https://doi.org/10.1007/bf02289447.\n\n\nâ€”â€”â€”. 1969. â€œHarry H. Harman Modern Factor Analysis (Second\nEdition, Revised). Chicago and London: University of Chicago Press,\n1967. Pp. Xx + 474. $12.50.â€ Psychometrika\n34 (1): 134â€“38. https://doi.org/10.1017/s0033312300004580.\n\n\nHotelling, Harold. 1936. â€œRelations Between Two Sets of\nVariates.â€ Biometrika 28 (3/4): 321. https://doi.org/10.2307/2333955.\n\n\nâ€”â€”â€”. 1992. â€œThe Generalization of Studentâ€™s\nRatio.â€ In, 54â€“65. Springer.\n\n\nHuang, Yafei, and Peter M. Bentler. 2015. â€œBehavior of\nAsymptotically Distribution Free Test Statistics in Covariance Versus\nCorrelation Structure Analysis.â€ Structural Equation\nModeling: A Multidisciplinary Journal 22 (4): 489â€“503. https://doi.org/10.1080/10705511.2014.954078.\n\n\nHuberty, Carl J., and John D. Morris. 1989. â€œMultivariate Analysis\nVersus Multiple Univariate Analyses.â€ Psychological\nBulletin 105 (2): 302â€“8. https://doi.org/10.1037/0033-2909.105.2.302.\n\n\nâ€œIntroduction to Factor Analysis.â€ 2020. In, 1â€“12. SAGE\nPublications, Inc. https://doi.org/10.4135/9781544339900.n4.\n\n\nJacobucci, Ross, and Kevin J. Grimm. 2018. â€œComparison of\nFrequentist and Bayesian Regularization in Structural Equation\nModeling.â€ Structural Equation Modeling: A Multidisciplinary\nJournal 25 (4): 639â€“49. https://doi.org/10.1080/10705511.2017.1410822.\n\n\nJennrich, R. I., and P. F. Sampson. 1966. â€œRotation for Simple\nLoadings.â€ Psychometrika 31 (3): 313â€“23. https://doi.org/10.1007/bf02289465.\n\n\nJÃ¶reskog, Karl G., and Arthur S. Goldberger. 1972. â€œFactor\nAnalysis by Generalized Least Squares.â€ Psychometrika 37\n(3): 243â€“60. https://doi.org/10.1007/bf02306782.\n\n\nKaiser, Henry F. 1958. â€œThe Varimax Criterion for Analytic\nRotation in Factor Analysis.â€ Psychometrika 23 (3):\n187â€“200. https://doi.org/10.1007/bf02289233.\n\n\nâ€”â€”â€”. 1970. â€œA Second Generation Little Jiffy.â€\nPsychometrika 35 (4): 401â€“15. https://doi.org/10.1007/bf02291817.\n\n\nKiers, Henk A. L. 1994. â€œSimplimax: Oblique Rotation to an Optimal\nTarget with Simple Structure.â€ Psychometrika 59 (4):\n567â€“79. https://doi.org/10.1007/bf02294392.\n\n\nKILIÃ‡, Abdullah, Ä°brahim UYSAL, and Burcu ATAR. 2020. â€œComparison\nof Confirmatory Factor Analysis Estimation Methods on Binary\nData.â€ International Journal of Assessment Tools in\nEducation 7 (3): 451â€“87. https://doi.org/10.21449/ijate.660353.\n\n\nKonopka, Tomasz. 2018. â€œUmap: Uniform Manifold Approximation and\nProjection.â€ The R Foundation. https://doi.org/10.32614/cran.package.umap.\n\n\nKruskal, J. B. 1964. â€œMultidimensional Scaling by Optimizing\nGoodness of Fit to a Nonmetric Hypothesis.â€\nPsychometrika 29 (1): 1â€“27. https://doi.org/10.1007/bf02289565.\n\n\nKyriazos, Theodoros, and Mary Poga-Kyriazou. 2023. â€œApplied\nPsychometrics: Estimator Considerations in Commonly Encountered\nConditions in CFA, SEM, and EFA Practice.â€ Psychology 14\n(05): 799â€“828. https://doi.org/10.4236/psych.2023.145043.\n\n\nLatan, Hengky, and Richard Noonan, eds. 2017. Partial Least Squares\nPath Modeling. Springer International Publishing. https://doi.org/10.1007/978-3-319-64069-3.\n\n\nLawley, D. N. 1940. â€œVI.The Estimation of Factor\nLoadings by the Method of Maximum Likelihood.â€ Proceedings of\nthe Royal Society of Edinburgh 60 (1): 64â€“82. https://doi.org/10.1017/s037016460002006x.\n\n\nLi, Cheng-Hsien. 2015. â€œConfirmatory Factor Analysis with Ordinal\nData: Comparing Robust Maximum Likelihood and Diagonally Weighted Least\nSquares.â€ Behavior Research Methods 48 (3): 936â€“49. https://doi.org/10.3758/s13428-015-0619-7.\n\n\nâ€”â€”â€”. 2021. â€œStatistical Estimation of Structural Equation Models\nwith a Mixture of Continuous and Categorical Observed Variables.â€\nBehavior Research Methods 53 (5): 2191â€“2213. https://doi.org/10.3758/s13428-021-01547-z.\n\n\nLu, Zhao-Hua, Sy-Miin Chow, and Eric Loken. 2016. â€œBayesian Factor\nAnalysis as a Variable-Selection Problem: Alternative Priors and\nConsequences.â€ Multivariate Behavioral Research 51 (4):\n519â€“39. https://doi.org/10.1080/00273171.2016.1168279.\n\n\nMaaten, Laurens van der, and Geoffrey Hinton. 2008. â€œVisualizing\nData Using t-SNE.â€ Journal of Machine Learning Research\n9 (86): 2579â€“2605. http://jmlr.org/papers/v9/vandermaaten08a.html.\n\n\nMarriott, F. H. C., and R. Gittins. 1986. â€œCanonical Analysis: A\nReview with Applications in Ecology; Biomathematics, Vol. 12.â€\nBiometrics 42 (1): 222. https://doi.org/10.2307/2531264.\n\n\nâ€œOmnibus MANOVA Tests.â€ 1985. In, 14â€“39. SAGE Publications,\nInc. https://doi.org/10.4135/9781412985222.d16.\n\n\nPearson, Karl. 1901. â€œLIII. On Lines and Planes of\nClosest Fit to Systems of Points in Space.â€ The\nLondon, Edinburgh, and Dublin Philosophical Magazine and Journal of\nScience 2 (11): 559â€“72. https://doi.org/10.1080/14786440109462720.\n\n\nRencher, Alvin C. 1998. Multivariate Statistical Inference and\nApplications. Vol. 635. Wiley New York.\n\n\nSchweizer, Karl, and Christine DiStefano, eds. 2016. Principles and\nMethods of Test Construction. Hogrefe Publishing. https://doi.org/10.1027/00449-000.\n\n\nSpearman, C. 1961. â€œ\"General\nIntelligence\" Objectively Determined and Measured.â€\nIn, 59â€“73. Appleton-Century-Crofts. https://doi.org/10.1037/11491-006.\n\n\nâ€œSupplemental Material for The Performance of ML, DWLS, and ULS\nEstimation With Robust Corrections in Structural Equation Models With\nOrdinal Variables.â€ 2016. Psychological Methods. https://doi.org/10.1037/met0000093.supp.\n\n\nTarka, Piotr. 2017. â€œAn Overview of Structural Equation Modeling:\nIts Beginnings, Historical Development, Usefulness and Controversies in\nthe Social Sciences.â€ Quality & Quantity 52 (1):\n313â€“54. https://doi.org/10.1007/s11135-017-0469-8.\n\n\nThurstone, L. L. 1931. â€œMultiple Factor Analysis.â€\nPsychological Review 38 (5): 406â€“27. https://doi.org/10.1037/h0069792.\n\n\nTorgerson, Warren S. 1952. â€œMultidimensional Scaling: I. Theory\nand Method.â€ Psychometrika 17 (4): 401â€“19. https://doi.org/10.1007/bf02288916.\n\n\nTurney, A. H. 1939. â€œFactor Analysis Makes ProgressA\nStudy in Factor Analysis: The Stability of a Bi-Factor\nSolution. Karl J. Holzinger , Frances Swineford.â€\nThe School Review 47 (9): 709â€“11. https://doi.org/10.1086/440440.\n\n\nVelicer, Wayne F. 1976. â€œDetermining the Number of Components from\nthe Matrix of Partial Correlations.â€ Psychometrika 41\n(3): 321â€“27. https://doi.org/10.1007/bf02293557.\n\n\nWang, Xiaojing, Candace M. Kammerer, Stewart Anderson, Jiang Lu, and\nEleanor Feingold. 2008. â€œA Comparison of Principal Component\nAnalysis and Factor Analysis Strategies for Uncovering Pleiotropic\nFactors.â€ Genetic Epidemiology 33 (4): 325â€“31. https://doi.org/10.1002/gepi.20384.\n\n\nWright, Sewall. 1934. â€œThe Method of Path Coefficients.â€\nThe Annals of Mathematical Statistics 5 (3): 161â€“215. https://doi.org/10.1214/aoms/1177732676.",
    "crumbs": [
      "Literatura"
    ]
  }
]