---
output: html_document
number-sections: false
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

# Metody redukcji wymiarowości

Historia metod redukcji wymiarowości jest ściśle związana z rozwojem statystyki, psychometrii, a następnie uczenia maszynowego i eksploracji danych. Już na początku XX wieku zaczęto poszukiwać narzędzi pozwalających na uproszczenie złożonych zbiorów danych, w których liczba zmiennych była zbyt duża, aby dało się je analizować bezpośrednio. Głównym celem było uchwycenie istotnych wzorców i zależności przy zachowaniu możliwie dużej ilości informacji.

Jednym z pierwszych i do dziś najczęściej stosowanych podejść jest analiza głównych składowych (*Principal Component Analysis*, PCA). Jej początki sięgają pracy Karla Pearsona z 1901 roku, który zaproponował metodę znajdowania „linii najlepszego dopasowania” w przestrzeni wielowymiarowej. Została ona następnie rozwinięta przez Harolda Hotellinga w latach 30. XX wieku, który sformalizował PCA jako metodę przekształcania skorelowanych zmiennych w nowy zbiór nieskorelowanych składowych, uporządkowanych według wariancji. PCA szybko znalazła zastosowanie w psychometrii i naukach społecznych, a następnie w genetyce, obrazowaniu i ekonomii.

W latach powojennych, wraz z rozwojem psychologii eksperymentalnej i neuronauk, pojawiła się potrzeba metod lepiej uchwytujących niezależne źródła sygnału. Doprowadziło to do opracowania analizy niezależnych składowych (*Independent Component Analysis*, ICA). Choć koncepcje matematyczne stojące za ICA sięgają teorii informacji z połowy XX wieku, to metoda została sformalizowana dopiero w latach 80. i 90. XX wieku, m.in. dzięki pracom Jeana-Françoisa Cardoso czy Aapa Hyvärinena. ICA stała się niezwykle użyteczna w problemach takich jak separacja źródeł w sygnałach biomedycznych (np. EEG, fMRI), odszumianie danych czy analiza obrazów.

Równolegle rozwijały się metody oparte na odległościach i podobieństwach, takie jak skalowanie wielowymiarowe (*Multidimensional Scaling*, MDS). Pierwsze idee pojawiły się w psychometrii w latach 50., a szczególnie w pracach Torgersona i Kruskala. Celem MDS było odwzorowanie obiektów opisanych macierzą podobieństw lub odległości w przestrzeni niskowymiarowej w taki sposób, aby zachować relacje strukturalne. Metoda ta znalazła szerokie zastosowanie w badaniach percepcji, marketingu, biologii oraz w analizie sieci społecznych.

Od końca XX wieku rozwój metod redukcji wymiarowości przyspieszył, co było związane z eksplozją danych wysokowymiarowych w biologii molekularnej, informatyce czy analizie obrazów. Oprócz klasycznych metod liniowych zaczęto rozwijać techniki nieliniowe, takie jak t-SNE (2008, Laurens van der Maaten i Geoffrey Hinton) czy UMAP (2018, McInnes, Healy i Melville), które pozwalają zachować lokalne struktury danych w niskowymiarowej przestrzeni wizualizacji. Metody te zrewolucjonizowały analizę danych w uczeniu maszynowym i biologii obliczeniowej, np. w analizie danych pojedynczych komórek.

Dziś redukcja wymiarowości jest nie tylko techniką wspomagającą wizualizację danych, lecz także kluczowym elementem przetwarzania wstępnego w wielu modelach uczenia maszynowego. Od klasycznych metod PCA i MDS po nowoczesne techniki oparte na sieciach neuronowych, takie jak autoenkodery, rozwój tego obszaru odzwierciedla rosnącą potrzebę uproszczenia i interpretacji złożoności współczesnych danych.

## PCA [@pearson1901]

### Matematyczna definicja modelu

Punktem wyjścia analizy głównych składowych jest problem odwzorowania wielowymiarowego zbioru danych w przestrzeni o mniejszej liczbie wymiarów przy możliwie minimalnej stracie informacji. W praktyce dąży się do kompresji i odszumiania sygnału, usuwania współliniowości, stabilizacji dalszych modeli (np. regresji), a także do wizualizacji struktur klasowych i gradientów zmienności. Przykładowo, dla dwóch silnie skorelowanych cech pierwsza składowa główna jest skierowana wzdłuż linii największego rozrzutu (blisko prostej $y \approx x$), a redukcja do jednego wymiaru zachowuje większą część wariancji niż dowolna inna projekcja.

Matematyczna definicja poprzez maksymalizację wariancji i dekompozycję spektralną polega na transformacji scentralizowanej macierzy danych $X \in \mathbb{R}^{n\times p}$ (każdą kolumnę odjąć o jej średnią). Niech $\Sigma=\frac{1}{n-1}X^\top X$ oznacza empiryczną macierz kowariancji. Pierwszą składową wyznaczamy jako kierunek $w\in\mathbb{R}^{p}$ rozwiązujący zadanie maksymalizacji wariancji projekcji, czyli maksymalizacji $\mathrm{Var}(Xw)=w^\top\Sigma w$ przy ograniczeniu $\|w\|_{2}=1$. Zastosowanie mnożników Lagrange’a prowadzi do warunku stacjonarności $\Sigma w=\lambda w$, a więc $w$ jest wektorem własnym $\Sigma$, zaś $\lambda$ jest odpowiadającą mu wartością własną. Wybieramy największą wartość własną $\lambda_{1}$ i jej wektor $w_{1}$, wówczas wariancja pierwszych wyników projekcji $z_{1}=Xw_{1}$ równa się $\lambda_{1}$. Kolejne składowe otrzymujemy analogicznie jako rozwiązania tego samego problemu z dodatkowymi ograniczeniami ortogonalności $w_{j}^\top w_{k}=0$ dla $k<j$, co ustawia kolejne wektory własne $\Sigma$ w porządku malejących wartości własnych $\lambda_{1}\ge \lambda_{2}\ge \dots \ge \lambda_{p}$. Wektor wyników projekcji $z_{j}$ nazywamy w praktyce *scores*, a $w_{j}$ — wektorem ładunków (*loadings*). Kumulatywny udział wariancji wyjaśnianej przez pierwsze $k$ składowych wynosi wówczas $\sum_{j=1}^{k}\lambda_{j}\big/\sum_{j=1}^{p}\lambda_{j}$ i służy na często do doboru $k$.

Równoważne wyprowadzenie modelu przez rozkład na wartości osobliwe, czyli SVD (ang. *Singular Value Decomposition*), opiera się na faktoryzacji $X=UDV^\top$, gdzie $U\in\mathbb{R}^{n\times r}$ i $V\in\mathbb{R}^{p\times r}$ mają ortonormalne kolumny, $D=\mathrm{diag}(d_{1},\dots,d_{r})$ zawiera uporządkowane wartości osobliwe $d_{1}\ge \dots \ge d_{r}>0$, a $r=\mathrm{rank}(X)$. Wówczas kolumny $V$ pokrywają się (co do znaku) z wektorami ładunków $w_{j}$, zaś macierz wyników projekcji $T=XV$ równa się $UD$. Związek między oboma podejściami jest ścisły: $\lambda_{j}=d_{j}^{2}/(n-1)$, a więc wariancje składowych odwzorowuje się przez kwadraty wartości osobliwych przeskalowane czynnikiem $1/(n-1)$. Projekcja do $k$ wymiarów przyjmuje wówczas postać $X\mapsto T_{k}=UD_{k}$, a rekonstrukcja rzędu $k$ ma postać $$
X_{k}=T_{k}V_{k}^\top=U_{k}D_{k}V_{k}^\top.
$$ Z twierdzenia Eckarta–Younga–Mirsky’ego wynika, że $X_{k}$ minimalizuje błąd Frobeniusa $\|X-Y\|_{F}$ w klasie macierzy $Y$ o rządzie co najwyżej $k$, czyli PCA daje najlepszą aproksymację niskorangową w sensie średniokwadratowym (jest to tzw. obcięte SVD). Ta równoważność łączyć dwie intuicje: maksymalizacja przechwyconej wariancji i minimalizacja błędu rekonstrukcji.

::: {#thm-1}
## Twierdzenie Eckarta–Younga–Mirsky

Niech $X\in\mathbb{R}^{n\times p}$ i niech $r=\mathrm{rank}(X)$. Dla $k<r$ niech $X_{k}=U_{k}D_{k}V_{k}^\top$ będzie obciętym rozkładem SVD rzędu $k$. Wówczas $X_{k}$ jest jedyną macierzą o $\mathrm{rank}(X_{k})=k$, która minimalizuje błąd Frobeniusa[^pca-1] $\|X-Y\|_{F}$ w klasie macierzy $Y\in\mathbb{R}^{n\times p}$ o $\mathrm{rank}(Y)\le k$. Ponadto zachodzi równość $\|X-X_{k}\|_{F}^{2}=\sum_{j=k+1}^{r}d_{j}^{2}$.
:::

[^pca-1]: Błąd Frobeniusa $\|A\|_{F}$ macierzy $A$ definiujemy jako $\|A\|_{F}=\sqrt{\sum_{i,j}a_{ij}^{2}}=\sqrt{\mathrm{tr}(A^\top A)}.$

Zadania optymalizacyjne wyraża się zarówno w wersji wektorowej, jak i macierzowej. Dla pierwszej składowej rozwiązujemy problem maksymalizacji $w^\top\Sigma$ w przy $\|w\|_{2}=1$, co prowadzi do największej wartości własnej. Dla $k$ składowych poszukujemy macierzy $W\in\mathbb{R}^{p\times k}$ o kolumnach ortonormalnych, która maksymalizuje $\mathrm{tr}(W^\top\Sigma W)$, skąd wynika wybór $k$ wektorów własnych $\Sigma$. Równoważnie, szukamy projekcji $P=WW^\top$ minimalizującej błąd rekonstrukcji $\|X-XWW^\top\|_{F}^{2}$. W notacji SVD rozwiązanie ma postać $W=V_{k}$, a więc projekcja działa przez mnożenie przez $V_{k}V_{k}^\top$.

Gdy cechy mierzymy w różnych jednostkach i skalach, zaleca się stosować macierz korelacji zamiast kowariancji, co jest równoważne standaryzacji kolumn $X$ do wariancji 1. Wiele implementacji (np. w `R` funkcja `prcomp`) wykorzystuje SVD na scentralizowanych i ewentualnie standaryzowanych danych, co zapewnia numeryczną stabilność, zwłaszcza gdy $p\gg n$. W sytuacji $p\gg n$ korzystniejsze bywa liczenie mniejszych rozkładów: albo dual PCA[^pca-2] na macierzy $XX^\top\in\mathbb{R}^{n\times n}$, albo bezpośrednio obciętego SVD. Dla danych zaburzonych wartościami odstającymi rozważa się wersje odporne, np. zastępuje się $\Sigma$ estymatorem odpornym (ang. *Minimum Covariance Determinant*, MCD)[^pca-3] lub stosuje się *robust* PCA i dekompozycje oparte na normie jądra i normie $L_{1}$[^pca-4].

[^pca-2]: Wówczas wektory własne $\tilde{w}_{j}$ macierzy $XX^\top\in\mathbb{R}^{n\times n}$ (która jest niższego wymiaru niż $X^\top X$ a co za tym idzie lepiej się zachowuje numerycznie) przekształca się w wektory własne $\Sigma$ przez $w_{j}=X^\top \tilde{w}_{j}/\sqrt{(n-1)\tilde{\lambda}_{j}}$, gdzie $\tilde{\lambda}_{j}$ jest odpowiadającą wartością własną.

[^pca-3]: Estymator MCD polega na znalezieniu podzbioru $h$ obserwacji (zwykle $h \approx 0.75 n$) o najmniejszym wyznaczniku macierzy kowariancji, a następnie obliczeniu średniej i kowariancji na tym podzbiorze. Jest odporny na wartości odstające, ponieważ ignoruje obserwacje, które znacznie zwiększają wyznacznik.

[^pca-4]: Metoda ta zakłada, że macierz danych $X$ ma postać $X=L+S+E$, gdzie $L$ jest macierzą niskorangową (sygnał), $S$ jest macierzą rzadką (wartości odstające), a $E$ jest szumem o małej wariancji. Celem jest odzyskanie $L$ poprzez minimalizację funkcji celu $\|L\|_{*}+\lambda\|S\|_{1}$ przy ograniczeniu $X=L+S$, gdzie $\|L\|_{*}$ jest normą jądra (suma wartości osobliwych $L$), a $\|S\|_{1}$ jest normą $L_{1}$ macierzy $S$.

Podsumowując, PCA można sformułować trojako: jako maksymalizację wariancji projekcji przy ograniczeniach ortogonalności, jako dekompozycję spektralną macierzy kowariancji oraz jako obcięte SVD zapewniające najlepszą aproksymację niskorangową.

### Założenia modelu

Założenia dotyczące danych wejściowych do analizy głównych składowych (PCA) są stosunkowo słabe, ale mają istotny wpływ na jakość wyników i interpretację. Można je podzielić na kilka grup:

1.  Struktura danych
    -   Liniowość – PCA zakłada, że główne wzorce zmienności w danych można uchwycić przez liniowe kombinacje zmiennych wejściowych. Jeśli zależności są silnie nieliniowe (np. dane leżą na zakrzywionej rozmaitości), PCA nie odwzoruje ich poprawnie – lepiej wtedy stosować *kernel* PCA albo metody sąsiedztwa (np. t-SNE, UMAP).
    -   Współzależność zmiennych – metoda ma sens tylko wtedy, gdy między cechami istnieją korelacje. Jeśli wszystkie zmienne są niezależne, PCA nie zredukuje wymiarów i każda składowa odpowiadać będzie jednej zmiennej.
2.  Jednostki i skale pomiarowe
    -   PCA jest wrażliwa na skalę zmiennych, ponieważ opiera się na wariancji.
    -   Jeśli cechy mierzone są w różnych jednostkach (np. temperatura w °C i masa w kg), należy je standaryzować (np. do średniej 0 i wariancji 1).
    -   Gdy wszystkie cechy są w tej samej skali, można pracować na macierzy kowariancji; w przeciwnym razie lepiej korzystać z macierzy korelacji.
3.  Rozkład danych
    -   Normalność wielowymiarowa nie jest wymagana, ale jeżeli dane mają rozkład wielowymiarowo normalny, to składowe główne są niezależne (nie tylko nieskorelowane), co upraszcza interpretację. Naruszenie założenia o normalności nie sprawia, że PCA nie działa, lecz niezależność składowych nie jest zagwarantowana.
    -   Brak wartości odstających – PCA jest bardzo wrażliwa na *outliery*, które mogą wpłynąć na kierunki głównych składowych, bo opiera się na kowariancji. Dlatego dane powinny być oczyszczone lub należy stosować wersje metody odporne (*robust* PCA).
4.  Liczebność próby
    -   Aby oszacować macierz kowariancji, liczba obserwacji $n$ powinna być odpowiednio duża względem liczby zmiennych $p$.
    -   Gdy $p \gg n$, klasyczna PCA bywa niestabilna i stosuje się wtedy *dual* PCA albo obcięte SVD.
5.  Braki danych - PCA wymaga pełnej macierzy danych (bez braków). W przypadku braków stosuje się najczęściej imputację (np. metodą średnich czy metody oparte na modelach).

### Interpretacja graficzna i praktyczna

```{r}
# Pakiety
library(MASS)
library(tidyverse)
library(scales)

set.seed(44)

# 1) Dane 2D o eliptycznym rozkładzie (silna współzmienność)
n  <- 300
mu <- c(0, 0)
sd1 <- 2
sd2 <- 1
rho <- 0.8
Sigma <- matrix(c(sd1^2, rho*sd1*sd2,
                  rho*sd1*sd2, sd2^2), nrow = 2, byrow = TRUE)

X <- MASS::mvrnorm(n, mu = mu, Sigma = Sigma) %>%
  as_tibble(.name_repair = ~c("x1","x2"))

# 2) PCA na danych scentralizowanych (bez standaryzacji)
pca <- prcomp(X, center = TRUE, scale. = FALSE)

# Wartości własne i wektory (ładunki)
lambda <- pca$sdev^2
V <- pca$rotation    # kolumny: PC1, PC2
center <- colMeans(X)

# 3) Punkty końcowe wektorów PC1 i PC2 (skalować długością ~ odchylenie wzdłuż składowej)
# Skala wektora: k * sd wzdłuż danej składowej (tu k = 2 dla czytelności)
k <- 2
pc1_end <- center + k * pca$sdev[1] * V[,1]
pc2_end <- center + k * pca$sdev[2] * V[,2]

# 4) Ramy wykresu i linie osi oryginalnego układu
xr <- range(X$x1); yr <- range(X$x2)

# 5) Dane pomocnicze do geometrii
arrows_df <- tribble(
  ~x,          ~y,          ~xend,        ~yend,     ~label,
  center[1],   center[2],   pc1_end[1],   pc1_end[2], "PC1",
  center[1],   center[2],   pc2_end[1],   pc2_end[2], "PC2"
)

# Opisy udziału wariancji
expl <- percent(lambda / sum(lambda), accuracy = 0.1)

# 6) Wykres
ggplot(X, aes(x = x1, y = x2)) +
  # chmura punktów
  geom_point(alpha = 0.5, size = 1.6) +
  # elipsa rozrzutu (1 odchylenie standardowe ~ poziom 0.68)
  stat_ellipse(type = "norm", level = 0.68, linewidth = 0.8) +
  # oryginalne osie układu współrzędnych (przez (0,0))
  geom_hline(yintercept = 0, linetype = 3, linewidth = 0.5) +
  geom_vline(xintercept = 0, linetype = 3, linewidth = 0.5) +
  # wektory składowych głównych (wychodzące ze środka danych)
  geom_segment(data = arrows_df,
               aes(x = x, y = y, xend = xend, yend = yend),
               arrow = arrow(length = unit(0.25, "cm")),
               linewidth = 1) +
  # etykiety PC z udziałem wariancji
  geom_text(data = arrows_df %>%
              mutate(txt = ifelse(label=="PC1",
                                  paste0("PC1 (", expl[1], ")"),
                                  paste0("PC2 (", expl[2], ")"))),
            aes(x = xend, y = yend, label = txt),
            nudge_x = 0.05, nudge_y = 0.05, hjust = 0, vjust = 0,
            size = 3.5) +
  # punkt środka
  geom_point(aes(x = center[1], y = center[2]), color = "black", size = 2) +
  coord_fixed() +
  labs(x = "x1 (oś oryginalna)",
       y = "x2 (oś oryginalna)",
       title = "Oryginalne osie, dane oraz dwie składowe główne (2D)",
       subtitle = paste0("Udział wariancji: PC1 = ", expl[1], ", PC2 = ", expl[2])) +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "plain"),
        plot.subtitle = element_text(face = "plain"))
```

Dla dwóch wymiarów elipsa rozrzutu danych ma osie ustawione dokładnie wzdłuż $w_{1}$ i $w_{2}$, a ich długości proporcjonalne do $\sqrt{\lambda_{1}}$ i $\sqrt{\lambda_{2}}$. Transformacja do przestrzeni składowych odpowiada obrotowi układu współrzędnych tak, by oś $X_{1}'$ leżała w kierunku największego rozrzutu, a $X_{2}'$ — w kierunku pozostałej zmienności. Projekcja do $k<p$ wymiarów działa jak rzut ortogonalny na podprzestrzeń rozpiętą przez pierwsze $k$ osi i „spłaszczenie” w pominiętych kierunkach, co minimalizuje błąd rekonstrukcji w sensie średniokwadratowym. Wykresy *scores* prezentują obiekty w przestrzeni składowych głównych i często ujawniają skupiska lub obserwacje odstające. Wektory *loadings* są przedstawiane na tzw. kole korelacji, gdzie końce strzałek leżą na okręgu jednostkowym, a ich długości i kąty odzwierciedlają korelacje zmiennych oryginalnych ze składowymi. Zmienne wskazujące podobne kierunki tworzą grupy, co pomaga rozumieć współzmienność. Wykres *biplot* łączy obie perspektywy: punkty obiektów i kierunki zmiennych w tej samej płaszczyźnie, dzięki czemu można podejrzeć jednocześnie relacje między obiektami i kontrybucje cech. Dodatkowo wykres udziału wariancji, czyli *scree plot*, porządkuje $\lambda_{j}$ i pomagać wyznaczyć $k$ przez identyfikację „łokcia” krzywej lub przez osiągnięcie założonego poziomu wariancji kumulatywnej.

Ładunek $w_{jk}$ to współczynnik liniowej kombinacji $j$-tej składowej dla $k$-tej zmiennej; jego znak i wartość bezwzględna informują o kierunku i sile związku. Korelację zmiennej z $j$-tą składową szacujemy jako cosinus kąta między wektorem zmiennej a osią składowej na kole korelacji; duże wartości sugeruję dużą kontrybucję tej cechy do składowej. Rekonstrukcja obiektu $i$-tego z $k$ składowych ma postać $\hat{x}_{i}=\sum_{j=1}^{k} t_{ij} \, w_{j}^\top$, gdzie $t_{ij} = x_i^\top w_j$, co pozwala na analizę błędów rekonstrukcji i odszumianie przez odcięcie składowych o małych $d_{j}$. W regresji, gdy predyktory są współliniowe, stosuje się regresję na składowych głównych albo regresję grzbietową w przestrzeni *scores*, co poprawia stronę obliczeniową i zmniejszać wariancję estymatorów.

### Kryteria doboru liczby składowych głównych

Dobór liczby składowych głównych ($k$) jest jednym z kluczowych etapów analizy PCA, ponieważ decyduje o tym, ile informacji (wariancji) zostanie zachowane przy redukcji wymiarowości. Zbyt mała liczba składowych prowadzi do utraty istotnych informacji, a zbyt duża – do utrzymania szumu i nadmiarowej redundancji. W praktyce stosuje się zestaw kryteriów ilościowych i jakościowych, które można podzielić na kilka grup.

#### Kryteria oparte na wariancji wyjaśnianej

Najbardziej klasyczne podejście polega na analizie udziału wariancji przechwyconej przez pierwsze $k$ składowych. Dla każdej składowej liczy się wartość własną $\lambda_j$ macierzy kowariancji, a udział wariancji wyjaśnianej przez pierwsze $k$ składowych to $$
\eta(k) = \frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j}.
$$ Stosowane reguły:

-   Reguła progu wariancji - wybiera się najmniejsze $k$, dla którego $\eta(k)$ przekracza ustalony próg, np. 80%, 90% lub 95% (w literaturze nie ma jednego progu). Gdy dane silnie skorelowane – wystarczą 2–3 składowe, a gdy dane są bardziej złożone – potrzeba więcej (5–10 i więcej).
-   Wykres osypiska (*scree plot*) – wykres wartości własnych $\lambda_j$ uporządkowanych malejąco. Wybiera się punkt, w którym tempo spadku gwałtownie maleje („łokieć krzywej”).
-   Wskaźnik udziału marginalnego - $\Delta \eta_j = \eta(j) - \eta(j-1)$. Gdy przyrost staje się znikomy, dalsze składowe nie wnoszą istotnej informacji.

#### Kryteria algebraiczne

Kryterium wartości własnej (Kaisera–Guttmana) oparte jest na macierzy korelacji, które mówi, że zachowuje się tylko te składowe, których wartości własne $\lambda_j > 1$. Oznacza to, że dana składowa wyjaśnia więcej wariancji niż pojedyncza standaryzowana zmienna. Reguła ta jest prosta, ale często zbyt konserwatywna (tendencja do wyboru zbyt wielu składowych).

#### Kryteria statystyczne i walidacyjne

-   Analiza równoległa (*Parallel Analysis*) - polega na porównaniu wartości własnych uzyskanych z danych rzeczywistych z wartościami własnymi uzyskanymi z wielu symulowanych zestawów danych o tych samych wymiarach, ale z losowym szumem. Zachowuje się tylko te składowe, których wartości własne przekraczają średnią (lub kwantyl) z rozkładu symulowanego. Ta metoda ogranicza ryzyko wyboru składowych wynikających z przypadku.
-   Walidacja krzyżowa (*Cross-Validation*) - gdy PCA wykorzystuje się w kontekście modelowania predykcyjnego (np. *PCA regression*). Wybiera się takie $k$, które minimalizuje błąd predykcji (np. RMSE) obliczany metodą walidacji krzyżowej.
-   *Bartlett’s Test of Sphericity* sprawdza, czy korelacje są wystarczająco silne, by PCA miała sens.
-   *Broken Stick Model* porównuje udział wariancji każdej składowej z oczekiwaną wartością przy losowym rozkładzie wariancji – zachowuje się tylko te składowe, które przekraczają tę wartość ($E_k=\frac{1}{p}\sum_{j=k}^{p}\frac{1}{j}$).

#### Kryteria interpretacyjne i dziedzinowe

Czasami najważniejszy jest nie wynik numeryczny, lecz użyteczność interpretacyjna:

-   Wybiera się tyle składowych, ile da się sensownie zinterpretować (np. odpowiadających znanym procesom fizycznym, ekonomicznym, biologicznym).
-   W analizie wizualnej (np. w eksploracji danych) często wybiera się 2 lub 3 pierwsze składowe, które umożliwiają wykresy 2D lub 3D.

| Kryterium | Opis | Zalety | Ograniczenia |
|------------------|------------------|------------------|------------------|
| **Udział wariancji (np. ≥90%)** | Zachowaj tyle składowych, by wyjaśnić określony procent całkowitej wariancji | Proste i intuicyjne | Wybór progu bywa arbitralny |
| **Wykres osypiska (*scree plot*)** | Wybór punktu „kolana” na krzywej wartości własnych | Wizualnie czytelne | Subiektywne, zależy od interpretacji obserwatora |
| **Wartość własna \> 1 (Kaiser–Guttman)** | Zachowaj składowe, których wartości własne przekraczają 1 (dla macierzy korelacji) | Łatwe obliczeniowo | Często zbyt liberalne – wybiera zbyt wiele składowych |
| **Analiza równoległa (*Parallel Analysis*)** | Porównanie wartości własnych z rozkładem uzyskanym z danych losowych | Statystycznie uzasadnione, ogranicza wybór przypadkowych komponentów | Wymaga symulacji lub dedykowanego oprogramowania |
| **Walidacja krzyżowa (*Cross-Validation*)** | Wybór liczby składowych minimalizującej błąd predykcji (np. RMSE) | Najlepsza w kontekście modeli predykcyjnych | Kosztowna obliczeniowo, wymaga podziału danych |
| **Model *Broken Stick*** | Porównanie udziału wariancji składowych z oczekiwanym rozkładem losowym | Uzasadnione teoretycznie, ogranicza przeuczenie | Mniej intuicyjne, rzadziej używane |
| **Kryterium interpretacyjne** | Wybór liczby składowych możliwych do sensownej interpretacji | Praktyczne i kontekstowe | Subiektywne i zależne od wiedzy dziedzinowej |

::: {#exm-1}
## PCA na danych irysów

```{r}
library(factoextra)
library(easystats)
library(gt)

pca_iris <- prcomp(iris[,-5], center = TRUE, scale. = TRUE) 
# albo
pca <- principal_components(iris, n = 4, rotate = "none") # domyślnie standaryzuje zmienne

# Wykres osypiska
fviz_eig(pca_iris, addlabels = TRUE)
```

Jak widać z powyższego wykresu osypiska pierwsza składowa wyjaśnia około 73% całkowitej wariancji, a druga 23%. Jeśli chcieć opierać wybór liczby składowych głównych na kryteriach (również takich, które nie były prezentowane powyżej), to można użyć funkcji `n_components()` pakietu `parameters` w ekosystemie `easystats`.

```{r}
k <- n_components(iris[,-5])
as.data.frame(k)

plot(k)
```

Choć większość kryteriów wskazuje na 1 składową, to na potrzeby przykładu wykorzystamy dwie składowe. Wyjaśniają one blisko 96% całkowitej wariancji (patrz poniżej). Możemy teraz przejrzeć wyniki PCA, czyli macierz ładunków (wektorów własnych) i macierz wyników projekcji (*scores*).

```{r}
# Udział wariancji
summary(pca_iris)

# Ładunki (wektory własne)
pca_iris$rotation

# scores
head(pca_iris$x)
```

Pierwsza składowa główna (`PC1`) jest kombinacją liniową wszystkich czterech zmiennych, przy czym trzy z nich — `Sepal.Length`, `Petal.Length` oraz `Petal.Width` — mają dodatnie ładunki, natomiast `Sepal.Width` ma ładunek ujemny. Oznacza to, że składowa ta rośnie, gdy długość działki kielicha oraz długość i szerokość płatków są duże, a maleje, gdy szerokość działki jest duża. Można zatem interpretować `PC1` jako wymiar opisujący ogólny rozmiar kwiatu: kwiaty o większych płatkach i węższych działkach uzyskują wyższe wartości tej składowej. W zbiorze iris `PC1` bardzo dobrze rozdziela gatunki – `setosa` charakteryzuje się niskimi wartościami tej składowej (krótkie płatki, szerokie działki), natomiast `versicolor` i `virginica` mają wartości wysokie, co odpowiada większym rozmiarom kwiatów.

Druga składowa główna (`PC2`) ma zupełnie inną strukturę ładunków. Zdominowana jest przez bardzo silny ujemny współczynnik dla `Sepal.Width` oraz mniejszy, również ujemny, dla `Sepal.Length`. Wpływ płatków na tę składową jest niewielki. PC2 odzwierciedla zatem zmienność w obrębie kształtu działki kielicha, a zwłaszcza jej proporcji długości do szerokości. Kwiaty o węższych działkach mają wyższe wartości PC2, natomiast te o szerszych – niższe.

Interpretując wspólnie obie składowe, można stwierdzić, że `PC1` opisuje rozmiar kwiatu, natomiast `PC2` – proporcje i kształt działki. W przestrzeni `PC1–PC2` dane tworzą układ, w którym `setosa` jest wyraźnie oddzielona od pozostałych gatunków poprzez niskie wartości `PC1` i wysokie `PC2`, a `versicolor` i `virginica` różnią się między sobą głównie wzdłuż drugiej osi. W rezultacie te dwie składowe pozwalają na niemal pełne odwzorowanie i wizualne rozdzielenie gatunków, przy czym `PC1` odpowiada za wymiar wielkościowy, a `PC2` – za wymiar kształtowy. Na potrzeby wizualizacji możemy narysować wykres *biplot* łączący obiekty i zmienne w przestrzeni dwóch pierwszych składowych.

```{r}
fviz_pca_biplot(pca_iris, repel = TRUE,
                col.var = "blue", # kolor zmiennych
                col.ind = iris$Species) + # kolor obiektów wg gatunku
  theme_minimal()
```
:::

## ICA [@comon1994]

Podstawowy model ICA (ang. *Independent Component Analysis*) zakłada, że wektor obserwacji $X \in \mathbb{R}^p$ powstaje poprzez liniowe i natychmiastowe wymieszanie wektora ukrytych źródeł $s \in \mathbb{R}^m$ o statystycznie niezależnych składowych $$
X = A s,\qquad A \in \mathbb{R}^{p\times m},
$$ przy czym $m \le \min(p,n)$ oraz macierz mieszająca $A$ ma pełny rząd. Celem jest oszacowanie macierzy demiksującej $W \in \mathbb{R}^{m\times p}$ tak, aby $y = W X$ aproksymować $s$ składowymi możliwie niezależnymi w sensie probabilistycznym. Z istoty problemu rozwiązanie identyfikowalne jest jedynie do permutacji i skalowania - kolejność oraz skale (a więc i znaki) składowych nie są odzyskiwalne.

Wyprowadzenie algorytmów ICA rozpoczynamy od scentralizowania danych i ich *whiteningu.* Niech $\Sigma_X = \tfrac{1}{n}\sum_i (X_i-\bar X)(X_i-\bar X)^\top$ oraz niech $V$ oznacza macierz *whitening* taką, że $Z=V(X-\bar X)$ spełnia $\operatorname{Cov}(Z)=I_p$. W praktyce przyjmujemy $V=\Lambda^{-1/2}U^\top$ z dekompozycji $\Sigma_X=U\Lambda U^\top$. W przestrzeni *whitened* model przyjmuje postać $$
Z = V A s \equiv R\, s,
$$ gdzie $R$ jest macierzą ortogonalną (dla przypadku $m=p$). Poszukujemy więc wektorów o jednostkowej normie, dla których skalarna projekcja $y=w^\top Z$ jest możliwie „nienormalna” (niesymetryczna lub ciężkoogonowa), co stanowi praktyczne kryterium niezależności.

```{r}
#| fig-height: 6
#| fig-width: 15

# Pakiety
library(fastICA)
library(patchwork)

set.seed(44)

# Ustalenia wymiarów zgodnie z opisem:
# p = liczba obserwowanych zmiennych, m = liczba źródeł, n = liczba obserwacji
n <- 3000
m <- 2
p <- 2

# 1) Generowanie niezależnych źródeł S ∈ R^{m×n} (kolumny = obserwacje, wiersze = źródła)
s1 <- rexp(n, rate = 1) - 1       # niegaussowskie, centrowane do ~0
s2 <- runif(n, -2, 2)             # niegaussowskie
S  <- rbind(s1, s2)               # S ma wymiar m×n

# Centrowanie źródeł (dla wygody): E[s] ≈ 0 w każdym źródle
S  <- S - rowMeans(S)

# 2) Mieszanie: X = A S, gdzie A ∈ R^{p×m}, X ∈ R^{p×n}
A <- matrix(c(1, 2,
              2, 1), nrow = p, byrow = TRUE)

X <- A %*% S                      # X ma wymiar p×n (kolumny = obserwacje)

# 3) Centrowanie danych obserwowanych: Xc = X - \bar X
# \bar X jest wektorem średnich po kolumnach (po obserwacjach) dla każdej zmiennej
X_bar <- rowMeans(X)              # wektor długości p
Xc <- X - X_bar                   # odjęcie wektora średnich od każdej kolumny

# 4) Whitening: Z = V Xc, gdzie V = Λ^{-1/2} U^T z dekompozycji Σ_X = U Λ U^T
# Zgodnie z opisem: Σ_X = (1/n) Σ_i (X_i - \bar X)(X_i - \bar X)^T = (1/n) Xc Xc^T
SigmaX <- (1 / n) * (Xc %*% t(Xc))    # p×p

e <- eigen(SigmaX)
U <- e$vectors                          # p×p
Lambda <- diag(e$values)                # p×p

V <- solve(sqrt(Lambda)) %*% t(U)       # Λ^{-1/2} U^T, wymiar p×p
Z <- V %*% Xc                           # p×n

# Kontrola: Cov(Z) = I_p przy tej samej normalizacji 1/n
SigmaZ <- (1 / n) * (Z %*% t(Z))
round(SigmaZ, 3)

# 5) (Opcjonalnie) ICA na danych obserwowanych.
# Uwaga: fastICA oczekuje macierzy w układzie "wiersze = obserwacje, kolumny = zmienne",
# czyli tu przekazujemy t(Xc), które ma wymiar n×p.
ica <- fastICA(t(Xc), n.comp = m)

# Źródła estymowane przez fastICA są w układzie n×m (wiersze = obserwacje),
# więc dla zgodności z notacją S ∈ R^{m×n} transponujemy:
S_hat <- t(ica$S)                         # m×n

# Macierz mieszająca estymowana przez fastICA ma wymiar p×m (zgodne z A ∈ R^{p×m})
A_hat <- ica$A

# Dla przypadku kwadratowego (m = p) można zdefiniować macierz demiksującą jako W ≈ A^{-1}
# (z zastrzeżeniem permutacji i skali, które są nieidentyfikowalne w ICA).
W_hat <- solve(A_hat)                     # m×p

# 6) Prosta diagnostyka: korelacje między prawdziwymi źródłami a estymowanymi
# (pokaże permutację/znak)
cor(t(S), t(S_hat))

# Y jako odzyskane źródła (m×n), spójnie z notacją kolumnową
Y <- t(ica$S)   # fastICA zwraca S jako n×m, więc transponujemy do m×n

# 7) Przygotowanie danych do wykresów
# M jest w formacie "zmienne × obserwacje" (p×n lub m×n),
# więc do data.frame robimy transpozycję: "obserwacje × zmienne"
to_df <- function(M, name){
  as.data.frame(t(M)) |>
    setNames(c("c1","c2")) |>
    mutate(stage = name)
}

df_X <- to_df(Xc, "X: dane zmieszane (wycentrowane)")  # używam Xc, bo model whiteningu jest dla X - X̄
df_Z <- to_df(Z,  "Z: po whitening")
df_Y <- to_df(Y,  "Y: po ICA (źródła)")

df_all <- bind_rows(df_X, df_Z, df_Y)

# 6) Osie układu i wektory bazowe (do wizualizacji)
axes_df <- function(scale_len = 2){
  data.frame(
    x = c(0, 0), y = c(0, 0),
    xend = c(scale_len, 0), yend = c(0, scale_len),
    label = c("e1", "e2")
  )
}
axesZ <- axes_df()

# 7) Wykresy: chmury punktów w 2D (X, Z, Y)
pX <- ggplot(df_X, aes(c1, c2)) +
  geom_point(alpha = 0.25, size = 0.8, color = "orange") +
  coord_equal() +
  labs(title = "Przed whiteningiem (Xc)",
       x = "Xc[1, ]", y = "Xc[2, ]") +
  theme_minimal()

pZ <- ggplot(df_Z, aes(c1, c2)) +
  geom_point(alpha = 0.25, size = 0.8, color = 'darkblue') +
  geom_segment(
    data = axesZ,
    aes(x = x, y = y, xend = xend, yend = yend),
    arrow = arrow(length = unit(0.18, "cm")),
    linewidth = 0.8
  ) +
  geom_text(
    data = axesZ,
    aes(x = xend, y = yend, label = label),
    nudge_x = 0.05, nudge_y = 0.05, size = 3
  ) +
  coord_equal() +
  labs(title = "Po whitening (Z): Cov ≈ I",
       x = "Z[1, ]", y = "Z[2, ]") +
  theme_minimal()

pY <- ggplot(df_Y, aes(c1, c2)) +
  geom_point(alpha = 0.25, size = 0.8, color = "darkgreen") +
  coord_equal() +
  labs(title = "Po ICA (Y): odzyskane źródła",
       x = "Y[1, ]", y = "Y[2, ]") +
  theme_minimal()

(pX | pZ | pY)

# 8) Marginalne histogramy pokazujące „nienormalność”
hX <- df_X |>
  pivot_longer(c(c1, c2), names_to = "col", values_to = "val") |>
  mutate(stage = "Xc")

hZ <- df_Z |>
  pivot_longer(c(c1, c2), names_to = "col", values_to = "val") |>
  mutate(stage = "Z")

hY <- df_Y |>
  pivot_longer(c(c1, c2), names_to = "col", values_to = "val") |>
  mutate(stage = "Y")

h_all <- bind_rows(hX, hZ, hY) |>
  mutate(stage = factor(stage, levels = c("Xc", "Z", "Y")))

ggplot(h_all, aes(val)) +
  geom_histogram(bins = 60, fill = "grey70", color = "white") +
  facet_grid(stage ~ col, scales = "free_y") +
  labs(title = "Marginalne rozkłady: przed whiteningiem, po whitening, po ICA",
       x = "wartość", y = "liczność") +
  theme_minimal()

# 9) Krótka kontrola: kowariancje i korelacje
# cov() i cor() zakładają, że obserwacje są w wierszach, a zmienne w kolumnach,
# więc dla macierzy p×n stosujemy transpozycję.
cat("\nKowariancja Xc:\n"); print(round(cov(t(Xc)), 3))

cat("\nKowariancja Z (powinna być bliska I przy normalizacji 1/(n-1) w cov()):\n")
print(round(cov(t(Z)), 3))

cat("\nKorelacje pomiędzy składowymi Y (powinny być bliskie 0; niezależność jest silniejsza niż brak korelacji):\n")
print(round(cor(t(Y)), 3))
```

Podejście maksymalizujące nienormalność opiera się na kurtozie lub na przybliżonej negatywnej entropii (ang. *negentropy*). Dla $\operatorname{Var}(y)=1$ kurtoza $\kappa(y)=\mathbb{E}\{y^4\}-3$ przyjmuje wartości 0 dla rozkładu normalnego i wartości odległe od zera dla rozkładów nienormalnych. Maksymalizacja $|\kappa(w^\top Z)|$ prowadzi do składowych niezależnych. Stabilniejsze i bardziej ogólne kryterium stanowi *negentropy* $J(y)=H(y_{\text{gauss}})-H(y)$, gdzie $H$ oznacza entropię. W praktyce stosuje się aproksymacje postaci $$
J(y)\approx \Big(\mathbb{E}\,G(y)-\mathbb{E}\,G(v)\Big)^2,
$$ z dobraną nieliniowością $G$ oraz $v\sim \mathcal N(0,1)$. Maksymalizacja $J$ przy ograniczeniu $\|w\|=1$ zapewnia poszukiwanie najbardziej nienormalnych kierunków. Częste wybory $G$, to

-   $G(u)=\frac{1}{a_1}\log\cosh(a_1 u)$ (np. $a_1=1$) - uniwersalna,
-   $G(u)=-\exp(-u^2/2)$ - dla rozkładów o ciężkich ogonach,
-   $G(u)=\frac{1}{4}u^4$ - dla rozkładów o lekkich ogonach.

Z kryteriów tych wynika algorytm *FastICA* jako iteracyjne poszukiwanie stałego punktu. Dla jednego komponentu w przestrzeni *whitened* stosujemy aktualizację $$
w \leftarrow \mathbb{E}\{Z\,g(w^\top Z)\}-\mathbb{E}\{g’(w^\top Z)\}\, w,\qquad \text{następnie } w\leftarrow \frac{w}{\|w\|},
$$ gdzie $g=G’$ jest *score function* (np. $g(u)=\tanh(u)$, $g(u)=u^3$ lub $g(u)=u\exp(-u^2/2)$). Pierwszy składnik $\mathbb{E}\{Z\,g(w^\top Z)\}$ jest „uogólnioną” wersją gradientu kryterium niegaussowskości: wzmacnia te kierunki $w$, w których projekcja $w^\top Z$ daje rozkład o dużej wartości wybranej miary niegaussowskości. Drugi składnik $-\mathbb{E}\{g’(w^\top Z)\}\,$ w pełni rolę korekty wynikającej z ograniczenia normy oraz z faktu, że pracuje się w przestrzeni *whitened.* Dzięki temu iteracja nie „puchnie” wzdłuż tego samego kierunku i ma poprawną geometrię z punktu widzenia warunku $\|w\|=1$. W praktyce oba wyrażenia oblicza się jako średnie z próby: wartości oczekiwane $\mathbb{E}[\cdot]$ zastępuje się średnimi po obserwacjach $Z_i.$

Dla wielu składowych stosujemy równoległe aktualizacje i ortogonalizację w kolejnych krokach, np. metodą rzutów Grama–Schmidta lub przez dekompozycję symetryczną $W\leftarrow (WW^\top)^{-1/2}W$, co zachowuje wzajemną ortogonalność wektorów w przestrzeni *whitened* i zapobiega zbieżności do tej samej składowej.

Alternatywne wyprowadzenie pochodzi z maksymalizacji funkcji wiarygodności (*maximum likelihood*). Zakładając niezależność źródeł z gęstościami $p_{s_i}$ i (dla prostoty) brak szumu, otrzymujemy logarytm funkcji wiarygodności $$
\mathcal L(W)=\sum_{t=1}^n\Bigg(\sum_{i=1}^m \log p_{s_i}\big((W X_t)_i\big)\Bigg) + n\log|\det W|.
$$ Jej gradient prowadzi do zasady *Infomax*, która brzmi: dobrać $W$ tak, aby wyjścia miały jak największą sumę entropii (co przy zachowaniu $\log|\det W|$ jest równoważne maksymalizacji wspólnej niezależności). W praktyce wybór rodziny $p_{s_i}$ implikuje odpowiednie nieliniowości w regule uczenia, formalnie zbieżne z powyższymi kontrastami na *negentropy.*

W obecności szumu addytywnego $X = A s + \varepsilon$ z $\varepsilon\sim \mathcal N(0,\sigma^2 I)$ problem staje się trudniejszy. Stosuje się wówczas rozszerzone modele ICA z estymacją rzędu i składowej szumowej, warianty bayesowskie, lub metody wykorzystujące dodatkowe własności źródeł (np. niezależność czasową wyższych rzędów, jak w SOBI wykorzystującym autokowariancje).

Założenia identyfikowalności obejmują liniowość i natychmiastowość mieszania, niezależność składowych źródłowych, co najwyżej jedną składową o rozkładzie normalnym (inaczej problem staje się nierozwiązywalny z powodu nieodróżnialności kierunków gaussowskich), pełny rząd macierzy $A$ oraz wystarczającą nienormalność źródeł, aby kontrasty informacyjne miały sens. Zwyczajowo zakłada się również stacjonarność w czasie, o ile wykorzystujemy momenty lub autokorelacje do estymacji.

Dobór liczby składowych w ICA nie opiera się na udziale wariancji, jak w PCA, ponieważ ICA nie porządkuje komponentów według wariancji. W praktyce najpierw wybiera się wymiar *whiteningu* $m$ (efektywny rząd sygnału), a następnie ekstrahuje $m$ składowych niezależnych. Kryteria wyboru $m$ obejmują informacyjne miary rzędu macierzy kowariancji, takie jak MDL/BIC dopasowane do modelu składowej szumowej i niezerowych wartości własnych, testy istotności dla wartości własnych po *whiteningu* (warianty analizy równoległej, permutacyjne testy mierzące losowość), walidację na podstawie wiarygodności w modelu ML-ICA z różnymi $m$ oraz kryteria stabilności. Kryteria stabilności polegają na wielokrotnym uruchomieniu algorytmu z różnymi inicjalizacjami i grupowaniu uzyskanych komponentów. Liczba dobrze replikujących się grup daje oszacowanie na $m$. Dodatkowo stosuje się testy resztowej zależności między oszacowanymi źródłami (np. testy niezależności na bazie informacji wzajemnej). Jeśli po dodaniu kolejnej składowej informacja wzajemna między „źródłami” nie maleje, zwiększanie $m$ nie przynosi korzyści. W zastosowaniach z szumem wybieramy $m$ tak, by oddzielać podprzestrzeń sygnałową od szumowej, co praktycznie sprowadza się do analizy spektrum wartości własnych i modelowania ogona jako białego szumu.

Interpretacja wyników ICA różni się od PCA. Składowe ICA $y_i$ stanowią oceny źródeł o maksymalnej niezależności, a wiersze $W$ definiują filtry demiksujące, podczas gdy kolumny $A$ (przyjmując $A\approx W^{-1}$) reprezentują wzorce mieszania, czyli „mapy obciążenia” źródeł na czujniki/cechy. Skale i znaki składowych są arbitralne, co wymaga interpretować je względnie: znormalizować wariancję lub maksymalną wartość, a znak dobrać tak, by ułatwić opis dziedzinowy[^pca-5]. W przeciwieństwie do PCA, składowe ICA nie muszą być ortogonalne, a ich wariancje nie są uporządkowane[^pca-6].

[^pca-5]: Wyobraźmy sobie, że ICA rozdziela dwa źródła dźwięku — skrzypce i fortepian. Jeśli algorytm zwróci sygnał, który jest odwrócony w fazie (czyli pomnożony przez -1), to dźwięk fortepianu jest ten sam fizycznie, tylko wszystkie amplitudy mają odwrotny znak. Dlatego znak (i skala) nie mają znaczenia dla jakości separacji — są arbitralne.

[^pca-6]: W sygnałach biologicznych, takich jak EEG, ICA może oddzielić artefakty ruchowe od sygnałów mózgowych. Artefakty te mogą być silnie nienormalne i niezależne od sygnałów mózgowych, co czyni ICA skuteczną metodą ich identyfikacji i usunięcia.

::: {#exm-2}
## ICA na mieszance sygnałów

```{r}
data("EuStockMarkets")
P <- as.data.frame(EuStockMarkets)              # poziomy indeksów: DAX, SMI, CAC, FTSE
R <- as.data.frame(apply(P, 2, function(x) diff(log(x))))  # dzienne log-zwroty
colnames(R) <- colnames(P)

# ICA na dziennych zwrotach
set.seed(123)
ica_res <- fastICA(R, n.comp = 4, method = "C")
S_est <- as.data.frame(ica_res$S)               # odzyskane źród
colnames(S_est) <- paste0("IC", 1:4)
A_est <- ica_res$A                              # macierz mieszająca
W_est <- ica_res$K %*% ica_res$W                # macierz demiksująca
```

Macierz `A_est`, czyli macierz mieszania, opisuje sposób, w jaki oryginalne zmienne obserwowalne — w tym przypadku cztery indeksy giełdowe: DAX, SMI, CAC i FTSE — powstają jako liniowe kombinacje ukrytych, niezależnych czynników. Każdy wiersz tej macierzy odpowiada jednemu indeksowi, a każda kolumna jednej składowej niezależnej. Wartości liczbowe oznaczają współczynniki liniowych kombinacji, czyli wpływ danej składowej na dany indeks. Wartość dodatnia wskazuje, że wzrost komponentu powoduje wzrost indeksu, wartość ujemna — że ruch komponentu przekłada się na spadek indeksu, a wartość bliska zeru oznacza brak istotnego związku.

```{r}
round(A_est, 3)
```

-   Pierwszy komponent (`IC1`) najsilniej ładuje się na indeks CAC, a w mniejszym stopniu na SMI. Znak ujemny dla CAC i dodatni dla SMI sugeruje, że komponent ten uchwyca różnicę pomiędzy rynkami strefy euro a rynkiem szwajcarskim, czyli czynnik kontrastujący. W praktyce oznacza to, że wzrost aktywności na rynkach kontynentalnych wiązać się może z relatywnym osłabieniem rynku SMI lub odwrotnie.
-   Drugi komponent (`IC2`) również oddziałuje na CAC i SMI w kierunku ujemnym, co może świadczyć o uchwyceniu wspólnego czynnika kontynentalnego o mniejszej amplitudzie. Dodatnie, choć niewielkie wartości dla FTSE wskazują, że komponent ten częściowo kontrastuje rynki kontynentalne z brytyjskim.
-   Trzeci komponent (`IC3`) ma wyraźnie odmienną strukturę. Dla FTSE współczynnik jest dodatni i największy, natomiast dla pozostałych indeksów ujemny. Komponent ten rozdziela zatem rynek brytyjski od reszty Europy i można go interpretować jako czynnik geograficzny lub walutowy, związany z odmiennym otoczeniem gospodarczym Wielkiej Brytanii.
-   Czwarty komponent (`IC4`) ma współczynniki bardzo małe, rzędu 10^-3^–10^-2^, co sugeruje, że jego wpływ na strukturę indeksów jest marginalny. Prawdopodobnie odpowiada on za szum lub krótkotrwałe, lokalne fluktuacje, które nie mają znaczenia ekonomicznego.

```{r}
round(W_est, 3)
```

Macierz `W_est`, czyli macierz demiksująca, zawiera współczynniki liniowych kombinacji oryginalnych zmiennych (indeksów giełdowych), które pozwalają uzyskać poszczególne niezależne komponenty. Każdy wiersz tej macierzy odpowiada jednemu komponentowi ICA (IC1–IC4), a każda kolumna — jednej zmiennej obserwowalnej (DAX, SMI, CAC, FTSE). Wartości w tej macierzy można zatem interpretować jako wagi, z jakimi poszczególne indeksy uczestniczą w tworzeniu danego odzyskanego źródła.

-   Pierwszy komponent (`IC1`) ma duże dodatnie wagi dla indeksów DAX i SMI oraz silnie ujemną wagę dla FTSE. Oznacza to, że `IC1` odzwierciedla kontrast pomiędzy rynkami kontynentalnymi (Niemcy, Szwajcaria) a rynkiem brytyjskim. Wzrost wartości `IC1` odpowiada sytuacji, w której indeksy kontynentalne zachowują się silniej niż FTSE — można więc interpretować ten czynnik jako różnicowy, typu „Europa kontynentalna kontra Wielka Brytania”.
-   Drugi komponent (`IC2`) pokazuje odwrotny schemat: dodatni wpływ DAX, silnie ujemny SMI, a słaby wpływ pozostałych indeksów. Można go interpretować jako czynnik rozróżniający zachowanie rynku niemieckiego i szwajcarskiego, który w ICA często ujawnia się jako efekt odmiennych warunków walutowych i struktury gospodarczej.
-   Trzeci komponent (`IC3`) ma wszystkie wagi ujemne, z wyjątkiem niewielkich dodatnich dla CAC i FTSE. Oznacza to, że `IC3` reprezentuje wspólny kierunek zmian większości indeksów (ruch globalny), ale w konstrukcji demiksującej występuje ze znakiem ujemnym. W praktyce odpowiada to czynnikowi rynkowemu o charakterze ogólnym — globalnemu impulsowi, który oddziałuje w podobny sposób na większość rynków.
-   Czwarty komponent (`IC4`) ma wysoką dodatnią wagę dla CAC oraz ujemne dla pozostałych indeksów, co sugeruje, że może on odzwierciedlać czynnik specyficzny dla rynku francuskiego — reakcje lokalne lub sektorowe, które nie są wspólne dla innych giełd..

Znaki współczynników w ICA są arbitralne (zmiana wszystkich znaków w jednym wierszu nie zmienia modelu), dlatego przy interpretacji należy zwracać uwagę na względne zależności między indeksami, a nie na samą polaryzację znaków. Wartości bezwzględne wag pokazują natomiast, które indeksy mają największy udział w kształtowaniu danego czynnika.

```{r}
kurt <- apply(S_est, 2, function(x) mean(x^4) - 3)  # kurtozy odzyskanych źródeł
print(round(kurt, 3))                           # kurtozy (nienormalność)
```

Wartości kurtozy stanowią miarę niegaussowskości rozkładu — czyli tego, jak bardzo dany sygnał odbiega od kształtu rozkładu normalnego. Wartości dodatnie oznaczają rozkłady o „cięższych ogonach” i bardziej spiczastym kształcie (tzw. leptokurtyczne), co jest typowe dla sygnałów rzadkich, zawierających wyraźne piki i okresy stabilności. W kontekście ICA wysoka kurtoza jest pożądana, ponieważ algorytm poszukuje właśnie takich komponentów — maksymalnie odmiennych od normalnych, a więc potencjalnie niezależnych źródeł.

-   `IC1` (5.601) ma bardzo wysoką kurtozę, co wskazuje na silną niegaussowskość. Komponent ten prawdopodobnie reprezentuje główny, „rzadki” czynnik ekonomiczny, który reaguje gwałtownie w momentach istotnych zmian rynkowych. Może to być globalny impuls rynkowy lub okresowe szoki finansowe.
-   `IC2` (1.985) ma umiarkowanie dodatnią kurtozę, sugerującą rozkład jedynie lekko leptokurtyczny. Oznacza to, że komponent jest bliższy rozkładowi normalnemu, a zatem mniej „niezależny” w sensie ICA. Może reprezentować łagodniejszy czynnik wspólny, np. codzienną zmienność lub trend regionalny.
-   `IC3` (8.206) ma najwyższą kurtozę spośród wszystkich komponentów. Jest to bardzo silny sygnał niegaussowski, typowy dla źródła zawierającego rzadkie, intensywne zdarzenia — w kontekście finansowym mogą to być momenty skokowych zmian cen lub kryzysów, wpływające selektywnie na część indeksów. Ten komponent można traktować jako najbardziej „czyste” źródło w sensie ICA.
-   `IC4` (2.274) wykazuje umiarkowaną kurtozę, zbliżoną do IC2. Można go interpretować jako dodatkowy, mniej wyraźny czynnik poboczny, który w pewnym stopniu odbiega od normalności, ale nie ma charakteru dominującego.

```{r}
# Sprawdzenie korelacji między oryginalnymi a odzyskanymi sygnałami
cor_matrix <- cor(R, S_est)
print(round(cor_matrix, 3))
```

Macierz korelacji między oryginalnymi indeksami giełdowymi a odzyskanymi komponentami niezależnymi (`cor_matrix`) pokazuje, jak silnie i w jakim kierunku (znak dodatni lub ujemny) każdy z indeksów jest powiązany z danym źródłem ICA. Wysokie wartości bezwzględne wskazują, że dany komponent w dużym stopniu tłumaczy zmienność danego indeksu, natomiast wartości bliskie zera oznaczają słaby związek.

-   Najsilniejsze korelacje obserwuje się dla komponentu `IC3`, który ma wartości ujemne i bardzo wysokie w module: DAX (−0.954), SMI (−0.871) i CAC (−0.686). Oznacza to, że `IC3` stanowi wspólny czynnik dominujący dla trzech kontynentalnych indeksów europejskich. Wszystkie trzy reagują w tym samym kierunku (ujemny znak jest konwencjonalny, jego odwrócenie nie zmienia interpretacji). Można zatem uznać, że `IC3` reprezentuje globalny czynnik rynkowy, wspólny dla głównych giełd kontynentalnych, a jego wysoka kurtoza (8.206) wskazuje, że czynnik ten cechuje się silnymi, epizodycznymi wahaniami — typowymi dla okresów zawirowań finansowych.
-   Komponent `IC1` wykazuje wyraźną ujemną korelację z FTSE (−0.788), przy braku silnych zależności z pozostałymi indeksami. Oznacza to, że `IC1` można interpretować jako czynnik specyficzny dla rynku brytyjskiego, niezależny od ruchów kontynentalnych. Wysoka wartość bezwzględna korelacji sugeruje, że ten komponent odpowiada za znaczną część zmienności FTSE, co dobrze współgra z interpretacją wcześniejszej macierzy mieszania — `IC1` oddzielał Wielką Brytanię od reszty Europy.
-   Komponent `IC2` wykazuje umiarkowane korelacje o różnych znakach: dodatnią z DAX (0.287) i ujemną ze SMI (−0.475). Można go zatem interpretować jako czynnik różnicowy pomiędzy rynkami Niemiec i Szwajcarii. W praktyce może on odzwierciedla odmienną reakcję tych rynków na czynniki lokalne, np. różnice w strukturze sektorowej lub polityce monetarnej.
-   Komponent `IC4` ma umiarkowaną dodatnią korelację z CAC (0.689), a pozostałe indeksy reagują na niego słabo. Oznacza to, że IC4 może być czynnikiem częściowo specyficznym dla rynku francuskiego, prawdopodobnie o charakterze lokalnym lub szumowym.

Na koniec wizualizacja ICA.

```{r}
#| fig-width: 12
#| fig-height: 12
# odzyskane komponenty 
S_est_long <- S_est |>
  mutate(Time = 1:nrow(S_est)) |>
  pivot_longer(cols = starts_with("IC"),
                      names_to = "Component", values_to = "Value")

p1 <- ggplot(S_est_long, aes(x = Time, y = Value, color = Component)) +
  geom_line() +
  facet_wrap(~ Component, ncol = 1, scales = "free_y") +
  labs(title = "Odzyskane niezależne komponenty (ICA)", x = "Czas", y = "Wartość") +
  theme_minimal() +
  theme(legend.position = "none")

R_long <- R |>
  mutate(Time = 1:nrow(R)) |>
  pivot_longer(cols = -Time,    
                      names_to = "Index", values_to = "Return")

p2 <- ggplot(R_long, aes(x = Time, y = Return, color = Index)) +
  geom_line() +
  facet_wrap(~ Index, ncol = 1, scales = "free_y") +
  labs(title = "Oryginalne dzienne log-zwroty indeksów", x = "Czas", y = "Log-zwrot") +
  scale_color_flat_d() +
  theme_minimal() +
  theme(legend.position = "none")

p1 | p2
```
:::

## MDS

Metoda *Multidimensional Scaling* (MDS), czyli skalowanie wielowymiarowe, stanowi rodzinę technik służących do odwzorowania danych opisanych za pomocą macierzy odległości (lub podobieństw) w przestrzeń o niskim wymiarze — najczęściej dwuwymiarową lub trójwymiarową — w taki sposób, aby relacje między obiektami zostały zachowane możliwie wiernie. Celem jest więc konstrukcja wektorowej reprezentacji obiektów, która odtwarza zadane odległości.

Istnieją dwie podstawowe wersje MDS: metryczna i niemetryczna, różniące się sposobem odwzorowania wartości wejściowych i kryterium dopasowania.

### Wersja metryczna (klasyczna MDS) [@torgerson1952]

#### Założenia

Dane wejściowe stanowią macierz odległości euklidesowych $$
\Delta = [\delta_{ij}]_{n\times n}, \quad \delta_{ij} \ge 0, \quad \delta_{ii}=0, \quad \delta_{ij}=\delta_{ji}.
$$ Celem jest znalezienie konfiguracji punktów $X \in \mathbb{R}^{n\times p}$, takiej że odległości między punktami $d_{ij}(X) = \|x_i - x_j\|$ są jak najbardziej zbliżone do odległości zadanych $\delta_{ij}.$

#### Wyprowadzenie modelu

Z klasycznej geometrii euklidesowej wynika, że iloczyn skalarny między wektorami można zapisać przez odległości $$
x_i^\top x_j = \frac{1}{2}\bigl(\|x_i\|^2 + \|x_j\|^2 - \|x_i - x_j\|^2 \bigr).
$$ Jeżeli dane są wyrażone przez odległości $\delta_{ij}$, to można odtworzyć tzw. macierz iloczynów skalarnych $B = XX^\top$, która określa współrzędne punktów po odpowiednim scentralizowaniu układu współrzędnych. Operacja ta nazywa się podwójnym centrowaniem (*double centering*) $$
B = -\frac{1}{2} J \Delta^{(2)} J,
$$ gdzie $\Delta^{(2)} = [\delta_{ij}^2]$ to macierz kwadratów odległości, a $J = I_n - \frac{1}{n}\mathbf{1}\mathbf{1}^\top$ to macierz centrowania (usuwa środek ciężkości układu).

Macierz $B$ powinna być dodatnio półokreślona (w przypadku euklidesowym). Następnie wykonuje się jej rozkład spektralny $$
B = V \Lambda V^\top,
$$ gdzie $\Lambda = \operatorname{diag}(\lambda_1, \ldots, \lambda_n)$ zawiera wartości własne uporządkowane malejąco, a $V$ to odpowiadające im wektory własne. Współrzędne punktów w przestrzeni o wymiarze $p$ wyznacza się przez $$
X_p = V_p \Lambda_p^{1/2},
$$ gdzie $V_p$ zawiera $p$ pierwszych wektorów własnych, a $\Lambda_p$ odpowiadające im największe wartości własne. Otrzymana konfiguracja $X_p$ odwzorowuje relacje odległości w sposób minimalizujący błąd w sensie średniokwadratowym.

#### Kryterium dopasowania

W wersji metrycznej minimalizuje się błąd rekonstrukcji odległości

$$
\min_{X} \sum_{i<j} \bigl( d_{ij}(X) - \delta_{ij} \bigr)^2.
$$

Rozwiązanie klasycznej wersji powyższego problemu daje wprost powyższa dekompozycja macierzy $B$, dlatego często określa się ją jako *Classical Scaling* lub *Principal Coordinates Analysis* (PCoA). Metoda ta jest w pełni analityczna i odpowiada PCA zastosowanej do macierzy odległości.

### Wersja niemetryczna (Non-metric MDS) [@kruskal1964]

#### Założenia

W wersji niemetrycznej zakłada się, że informacja w danych ma przede wszystkim charakter porządkowy: jeżeli $\delta_{ij} > \delta_{kl}$, to para $(i,j)$ ma być „bardziej niepodobna” niż $(k,l)$, ale nie wymaga się, aby różnice $\delta_{ij}-\delta_{kl}$ miały sens ilościowy. Z tego powodu nie dopasowuje się wprost relacji $\delta_{ij}\approx d_{ij}(X)$, tylko dopuszcza się monotoniczne przekształcenie $f$, które „przekłada” odległości w konfiguracji na skalę zgodną z porządkiem danych. Funkcja $f$ jest w praktyce wynikiem tzw. regresji izotonicznej: dobiera się ją tak, aby była monotoniczna i aby możliwie najlepiej dopasowywała wartości w sensie najmniejszych kwadratów przy zachowaniu porządku. To pozwala rozdzielić dwa problemy: (1) znalezienie geometrii $X$, która poprawnie porządkuje pary, oraz (2) dopasowanie skali poprzez $f$, które jest „swobodne”, o ile nie łamie rang.

#### Model i funkcja stresu

Optymalizuje się wtedy miarę stresu Kruskala (STRESS - STandardized REsidual Sum of Squares) $$
S(X) = \sqrt{\frac{\sum_{i<j} \bigl(f(\delta_{ij}) - d_{ij}(X)\bigr)^2}{\sum_{i<j} d_{ij}(X)^2}}.
$$

Minimalizacja stresu odbywa się iteracyjnie — zmienia się położenie punktów $x_i$, aby zmniejszyć różnicę między rangami obserwowanych i odwzorowanych odległości. Pełni ona trzy role.

Po pierwsze, w liczniku sumuje kwadraty reszt $\bigl(f(\delta_{ij}) - d_{ij}(X)\bigr)$, czyli mierzy, jak duże są rozbieżności między odległościami w przestrzeni a wartościami „najlepiej dopasowanymi” do porządku danych. Kwadrat powoduje, że większe rozbieżności są karane silniej i że kryterium jest gładkie w sensie optymalizacyjnym (co sprzyja iteracyjnym algorytmom).

Po drugie, w mianowniku normalizuje się tę sumę przez $\sum_{i<j} d_{ij}(X)^2$, aby miara była niezależna od arbitralnej skali konfiguracji. Bez normalizacji dałoby się sztucznie zmniejszać błąd przez „ściskanie” lub „rozciąganie” konfiguracji (a przy pewnych formach dopasowania nawet doprowadzić do trywialnych rozwiązań). Normalizacja sprawia, że STRESS jest miarą względną: odpowiada proporcji „energii reszt” do „energii odległości”, dzięki czemu można porównywać wyniki dla różnych konfiguracji i różnych zbiorów danych.

Po trzecie, STRESS zamienia problem zachowania rang w problem optymalizacyjny, który da się rozwiązywać algorytmicznie. W samym warunku rangowym („zachować porządek”) nie ma naturalnej funkcji celu, którą można różniczkować lub minimalizować. STRESS daje konkretną liczbę, którą można iteracyjnie zmniejszać, przesuwając punkty $x_i$. Procedura wygląda schematycznie tak: dla ustalonego $X$ wyznacza się $d_{ij}(X)$, następnie dobiera się monotoniczne $f$, po czym aktualizuje się $X$, aby zmniejszyć STRESS i tak do zbieżności. Dzięki temu niemetryczny MDS staje się metodą praktycznie wykonalną.

| Wartość STRESS | Ocena dopasowania                        |
|----------------|------------------------------------------|
| \< 0.05        | Doskonałe                                |
| 0.05–0.10      | Bardzo dobre                             |
| 0.10–0.20      | Umiarkowane                              |
| 0.20–0.30      | Słabe                                    |
| \> 0.30        | Bardzo słabe (odwzorowanie nieadekwatne) |

#### Interpretacja

W niemetrycznym MDS nie dąży się do zachowania dokładnych odległości, lecz do zachowania porządku relacji niepodobieństwa, obiekty podobne mają być blisko siebie, a niepodobne — daleko. Dzięki temu metoda jest odporna na nieliniowe zniekształcenia skali w danych wejściowych.

### Porównanie metod MDS

| Cecha | Metryczny MDS | Niemetryczny MDS |
|------------------------|------------------------|------------------------|
| **Typ danych wejściowych** | Odległości euklidesowe | Dowolne niepodobieństwa, także porządkowe |
| **Zależność między danymi a odległościami** | Liniowa | Monotoniczna (dowolna funkcja porządkowa) |
| **Algorytm** | Dekompozycja własna macierzy centrowanej | Iteracyjna optymalizacja stresu |
| **Kryterium dopasowania** | Minimalizacja błędu kwadratowego odległości | Minimalizacja stresu Kruskala |
| **Interpretacja** | Odtwarza dokładne relacje geometryczne | Odtwarza relacje rangowe (porządek podobieństw) |

W praktyce metryczny MDS jest szybszy, prostszy i równoważny klasycznemu podejściu PCA, gdy dane mają charakter metryczny. Niemetryczny MDS natomiast jest bardziej elastyczny — pozwala odwzorować struktury nieliniowe, zachowując tylko relacje porządkowe między obiektami, co czyni go odpowiednim do danych percepcyjnych, preferencyjnych lub ankietowych.

::: {#exm-3}
## MDS na danych o odległościach między miastami

```{r}
#| fig-height: 15
#| fig-width: 9

library(MASS)        # isoMDS, UScitiesD
library(maps)        # zarysy map
library(ggrepel)


# Dane: odległości drogowe między miastami w USA (w milach)
data("UScitiesD")  # obiekt klasy 'dist' z pakietu MASS

# 1) Metryczny MDS
mds_metric <- cmdscale(UScitiesD, k = 2)
mds_metric

# 2) Niemetryczny MDS
mds_nonmetric <- isoMDS(UScitiesD, k = 2)$points
mds_nonmetric

# 3) Ramy danych do wykresu MDS
mds_df <- data.frame(
  City = rownames(mds_metric),
  Metric_X = mds_metric[,1],
  Metric_Y = mds_metric[,2],
  Nonmetric_X = mds_nonmetric[,1],
  Nonmetric_Y = mds_nonmetric[,2]
)

# 4) Wykresy MDS (po symetrii względem obu osi dla lepszej czytelności)
p1 <- ggplot(mds_df, aes(x = -Metric_X, y = -Metric_Y, label = City)) +
  geom_point(color = "blue", size = 2) +
  geom_text_repel(size = 3) +
  labs(title = "Metryczny MDS na danych o odległościach między miastami",
       x = "Wymiar 1", y = "Wymiar 2") +
  theme_minimal()

p2 <- ggplot(mds_df, aes(x = -Nonmetric_X, y = -Nonmetric_Y, label = City)) +
  geom_point(color = "red", size = 2) +
  geom_text_repel(size = 3) +
  labs(title = "Niemetryczny MDS na danych o odległościach między miastami",
       x = "Wymiar 1", y = "Wymiar 2") +
  theme_minimal()

# 5) Faktyczne położenia miast (long/lat) – nazwy muszą pokrywać się z UScitiesD
city_coords <- tribble(
  ~City,           ~lon,      ~lat,
  "Atlanta",       -84.39,     33.75,
  "Chicago",       -87.63,     41.88,
  "Denver",       -104.99,     39.74,
  "Houston",       -95.37,     29.76,
  "LosAngeles",   -118.24,     34.05,
  "Miami",         -80.19,     25.77,
  "NewYork",       -74.01,     40.71,
  "SanFrancisco", -122.42,     37.77,
  "Seattle",      -122.33,     47.61,
  "Washington",    -77.04,     38.90
)

# 6) Zarys mapy USA
usa_map <- map_data("state")

# 7) Wykres p3: faktyczna mapa z punktami miast
p3 <- ggplot() +
  geom_polygon(data = usa_map,
               aes(x = long, y = lat, group = group),
               fill = "grey95", color = "grey70", linewidth = 0.3) +
  geom_point(data = city_coords,
             aes(x = lon, y = lat),
             size = 2, color = "black") +
  geom_text_repel(data = city_coords,
                  aes(x = lon, y = lat, label = City),
                  size = 3) +
  labs(title = "Faktyczne położenie miast na mapie USA",
       x = "Długość geograficzna", y = "Szerokość geograficzna") +
  theme_minimal()

# 8) Prezentacja obok siebie (patchwork)
p1/ p2 / p3
```
:::

## t-SNE [@vandermaaten08a]

Metoda *t-distributed Stochastic Neighbor Embedding* (t-SNE) została opracowana przez Laurensa van der Maatena i Geoffreya Hintona w 2008 roku jako nieliniowa technika redukcji wymiarowości, której celem jest odwzorowanie lokalnej struktury danych wysokowymiarowych w przestrzeni o mniejszej liczbie wymiarów, zwykle dwuwymiarowej lub trójwymiarowej. W przeciwieństwie do metod liniowych, takich jak PCA, t-SNE nie dąży do maksymalizacji wariancji, lecz do zachowania sąsiedztw pomiędzy punktami – obserwacje, które w przestrzeni oryginalnej są blisko siebie, powinny również pozostawać blisko w przestrzeni odwzorowania.

Niech dane wejściowe tworzą macierz $X = [x_1, x_2, \dots, x_n]^\top$, gdzie każdy wektor $x_i \in \mathbb{R}^p$ reprezentuje jedną obserwację w przestrzeni o wymiarze $p$. Pierwszym krokiem jest przekształcenie danych wysokowymiarowych w macierz podobieństw, która opisuje, jak bardzo punkty są „bliskie” względem siebie. Dla każdego punktu $x_i$ definiuje się rozkład warunkowy $$
p_{j|i} = \frac{\exp\!\left(-\frac{|x_i - x_j|^2}{2\sigma_i^2}\right)}{\sum_{k \neq i} \exp\!\left(-\frac{|x_i - x_k|^2}{2\sigma_i^2}\right)}, \quad p_{i|i} = 0,
$$ gdzie parametr $\sigma_i$ (odpowiednik *bandwidth*) dobiera się tak, aby entropia rozkładu $P_i = (p_{j|i})_j = (p_{1|i}, p_{2|i}, \dots, p_{n|i})$ odpowiadała zadanej *perplexity*, czyli efektywnej liczbie sąsiadów. *Perplexity* jest hiperparametrem kontrolującym zakres lokalności analizowanych relacji.

Następnie konstruuje się symetryczną macierz podobieństw $$
p_{ij} = \frac{p_{i|j} + p_{j|i}}{2n},
$$ która reprezentuje prawdopodobieństwo, że punkty $x_i$ i $x_j$ są bliskimi sąsiadami w przestrzeni oryginalnej.

Kolejnym krokiem jest utworzenie analogicznego rozkładu w przestrzeni odwzorowania $Y = [y_1, y_2, \dots, y_n]^\top$, gdzie $y_i \in \mathbb{R}^q$ i zwykle $q = 2$ lub 3. Dla tych punktów definiuje się rozkład podobieństw oparty na rozkładzie t-Studenta z jednym stopniem swobody $$
q_{ij} = \frac{(1 + |y_i - y_j|^2)^{-1}}{\sum_{k \neq l} (1 + |y_k - y_l|^2)^{-1}}, \quad q_{ii} = 0.
$$ Rozkład t-Studenta ma grube ogony, co umożliwia bardziej realistyczne odwzorowanie relacji między punktami odległymi od siebie i redukuje problem *crowding*, czyli nadmiernego ściskania punktów w centrum przestrzeni odwzorowania.

Celem t-SNE jest minimalizacja dywergencji Kullbacka–Leiblera między rozkładami $P$ i $Q$ $$
C = \operatorname{KL}(P \| Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}.
$$ Optymalizacja tej funkcji, zwykle za pomocą spadku gradientowego, prowadzi do znalezienia takich współrzędnych $Y$, które zachowują lokalne relacje między punktami w jak największym stopniu. Gradient funkcji celu względem współrzędnych $y_i$ ma postać $$
\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij}) (y_i - y_j) (1 + \|y_i - y_j\|^2)^{-1},
$$ a współczynnik 4 pełni rolę skalującą.

Metoda t-SNE nie zakłada liniowości ani rozkładu normalnego danych, lecz wymaga, aby dane były znormalizowane w przypadku różnych jednostek pomiarowych, ponieważ odległości euklidesowe są wrażliwe na skalę. Wskazane jest wcześniejsze zastosowanie PCA w celu usunięcia szumu i zmniejszenia złożoności obliczeniowej. Dane nie powinny zawierać dużej liczby wartości odstających ani duplikatów, które mogłyby zaburzyć lokalne struktury. Kluczowy hiperparametr *perplexity* powinien być dostosowany do liczby obserwacji — zbyt mała wartość prowadzi do przeuczenia lokalnego, a zbyt duża powoduje zatarcie drobnych struktur.

Wyniki t-SNE przedstawione w przestrzeni dwuwymiarowej lub trójwymiarowej nie mają interpretacji metrycznej. Oznacza to, że odległości między klastrami nie są bezpośrednio interpretowalne ilościowo. Interpretacja opiera się głównie na analizie sąsiedztwa: punkty znajdujące się blisko siebie w przestrzeni t-SNE odpowiadają obserwacjom podobnym w oryginalnych cechach, natomiast wyraźne skupiska punktów mogą wskazywać na istnienie klas lub podgrup. Oś pierwsza i druga nie mają znaczenia merytorycznego – są jedynie współrzędnymi w przestrzeni odwzorowania, które zachowuje lokalną strukturę, a nie globalną geometrię danych.

::: callout-note
## Ważne kroki przy stosowaniu t-SNE

1.  Standaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.
2.  Wybór zakresu *perplexity* — zwykle testuje się kilka wartości (np. 5, 15, 30, 50) i ocenia stabilność struktur (czy klastry są rozdzielne, czy stabilne względem permutacji danych).
3.  Ustawienie *learning rate* — zaczyna się od wartości domyślnej (200) i w razie potrzeby zwiększa do 500–1000, jeśli klastry są zbyt zwarte.
4.  Ustalenie liczby iteracji — co najmniej 500; w przypadku dużych zbiorów można zwiększyć do 1000–2000, jeśli rozkład nadal się zmienia.
5.  Porównanie z PCA lub UMAP — warto sprawdzić, czy t-SNE nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.
:::

::: {#exm-4}
## t-SNE na danych `iris`

```{r}
#| fig-width: 14
#| fig-height: 6

library(Rtsne)      # t-SNE

set.seed(44)

# Przygotowanie danych (jak wcześniej)
iris_data <- iris %>%
  select(-Species) %>%
  as.matrix() %>%
  scale()

# t-SNE
tsne_result <- Rtsne(
  iris_data,
  dims = 2, perplexity = 30, verbose = TRUE,
  max_iter = 500, check_duplicates = FALSE
)

tsne_df <- data.frame(
  Dim1 = tsne_result$Y[,1],
  Dim2 = tsne_result$Y[,2],
  Species = iris$Species
)

# PCA na tych samych danych
pca_fit <- prcomp(iris_data, center = FALSE, scale. = FALSE)
pca_df <- data.frame(
  PC1 = pca_fit$x[,1],
  PC2 = pca_fit$x[,2],
  Species = iris$Species
)

# Wykres t-SNE
p1 <- ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = Species)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "t-SNE",
       x = "Wymiar 1", y = "Wymiar 2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")

# Wykres PCA (pierwsze dwie składowe)
p2 <- ggplot(pca_df, aes(x = PC1, y = PC2, color = Species)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "PCA",
       x = "PC1", y = "PC2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")

p1 | p2
```
:::

## UMAP [@konopka2018]

Metoda *Uniform Manifold Approximation and Projection* (UMAP) jest nieliniową techniką redukcji wymiarowości opracowaną przez McInnesa i Healy’ego w 2018 roku. Jej celem jest odwzorowanie danych z przestrzeni wysokowymiarowej w przestrzeń o mniejszej liczbie wymiarów przy zachowaniu struktury geometrycznej — zarówno lokalnej, jak i globalnej — poprzez modelowanie danych jako rozkładu na rozmaitości (*manifold*). W odróżnieniu od metody t-SNE, UMAP opiera się na teorii rozmaitości Riemanna oraz na pojęciach pochodzących z teorii zbiorów rozmytych i topologii algebraicznej.

Niech dane wejściowe stanowią zbiór punktów $$
X = \{x_1, x_2, \dots, x_n\}, \quad x_i \in \mathbb{R}^p.
$$ Zakłada się, że punkty te leżą na rozmaitości $\mathcal{M} \subset \mathbb{R}^p$ o niższym wymiarze rzeczywistym $d < p$, zanurzonej w przestrzeni obserwowalnej. Metoda UMAP tworzy dwie probabilistyczne reprezentacje tej rozmaitości: po pierwsze, graf sąsiedztwa w przestrzeni wysokowymiarowej (*fuzzy simplicial set*), który opisuje lokalne zależności między punktami, oraz po drugie, graf w przestrzeni niskowymiarowej, którego struktura ma jak najlepiej odwzorowywać pierwszy.

W celu konstrukcji grafu w przestrzeni wejściowej dla każdego punktu $x_i$ określa się odległości do jego $k$-najbliższych sąsiadów. Następnie wyznacza się dwa parametry lokalne $$
\rho_i = \min_{j: d(x_i,x_j) > 0} d(x_i, x_j),
$$ czyli najmniejszą dodatnią odległość (umożliwiającą niezerową gęstość), oraz $\sigma_i > 0,$ skalę lokalną dobraną tak, aby spełniony był warunek normalizacji entropii $$
\sum_{j} \exp\!\left(-\frac{\max(0, d(x_i,x_j) - \rho_i)}{\sigma_i}\right) = \log_2(k).
$$ Na tej podstawie definiuje się rozmyte prawdopodobieństwa sąsiedztwa $$
p_{j|i} = \exp\!\left(-\frac{\max(0, d(x_i, x_j) - \rho_i)}{\sigma_i}\right).
$$ Ponieważ macierz tych wartości nie jest symetryczna, łączy się oba kierunki zgodnie z zasadami teorii zbiorów rozmytych $$
p_{ij} = p_{i|j} + p_{j|i} - p_{i|j}\,p_{j|i}.
$$ Tak powstały rozmyty graf sąsiedztwa zawiera wagi $p_{ij}$, które odzwierciedlają siłę połączeń między punktami.

Następnie w przestrzeni wynikowej $Y = \{y_1, y_2, \dots, y_n\} \subset \mathbb{R}^q$, gdzie zwykle $q = 2$ lub 3, definiuje się analogiczny rozmyty graf $q_{ij}$, którego wagi opisuje funkcja jądra typu *heavy-tailed* $$
q_{ij} = \frac{1}{1 + a\,\|y_i - y_j\|^{2b}},
$$ gdzie $a$ i $b$ są parametrami dopasowanymi empirycznie (standardowo $a \approx 1.929,\ b \approx 0.7915$).

Zasadniczym celem UMAP jest znalezienie takiej konfiguracji punktów $Y$, aby rozmyty graf $q_{ij}$ jak najlepiej przybliżał graf $p_{ij}$. Kryterium optymalizacji ma postać minimalizacji rozbieżności krzyżowej (ang. *cross-entropy*) między dwoma rozkładami sąsiedztwa $$
C = \sum_{i < j} \left[ -p_{ij}\log(q_{ij}) - (1 - p_{ij})\log(1 - q_{ij}) \right].
$$ Minimalizacja tej funkcji jest realizowana metodami gradientowymi, zazwyczaj z wykorzystaniem *stochastic gradient descent* (SGD). W wyniku optymalizacji punkty $y_i$ są przesuwane tak, aby utrzymać bliskie relacje w miejscach, gdzie $p_{ij}$ jest duże i rozdzielać punkty, gdzie $p_{ij}$ jest małe.

Założenia metody UMAP są stosunkowo niewielkie, lecz istotne. Zakłada się, że dane leżą na rozmaitości o niskim wymiarze, a więc można je opisać poprzez ciągłą strukturę geometryczną. Przyjmuje się również, że użyta miara odległości (zwykle euklidesowa) odzwierciedla faktyczne podobieństwo obserwacji oraz że rozkład punktów jest gładki, czyli w małych sąsiedztwach struktura jest dobrze przybliżana liniowo.

Interpretacja wyników UMAP nie odnosi się do bezwzględnych wartości współrzędnych, lecz do relacji między punktami. Punkty położone blisko siebie w przestrzeni wynikowej są podobne w przestrzeni oryginalnej, a większe odległości odpowiadają mniejszemu podobieństwu. W przeciwieństwie do t-SNE metoda ta lepiej zachowuje nie tylko lokalne klastry, ale również częściowo strukturę globalną, co umożliwia analizę gradientów i ciągłych przejść między grupami obserwacji.

::: callout-note
## Ważne kroki przy stosowaniu UMAP

1.  Standaryzacja danych — każda zmienna powinna być przeskalowana do średniej 0 i wariancji 1, aby uniknąć dominacji jednej cechy w metryce euklidesowej.
2.  Wybór liczby sąsiadów (*n_neighbors*) — kontroluje lokalność odwzorowania; mniejsze wartości (5–15) podkreślają lokalne struktury, większe (30–50) zachowują więcej globalnych relacji.
3.  Ustawienie wymiaru wynikowego (*n_components*) — zwykle 2 lub 3, w zależności od potrzeb wizualizacji.
4.  Wybór metryki odległości — domyślnie euklidesowa, ale można użyć innych (np. Manhattan, cosine) w zależności od charakteru danych.
5.  Porównanie z PCA lub t-SNE — warto sprawdzić, czy UMAP nie generuje artefaktów (np. sztucznych przerw między klastrami), których nie ma w prostszych odwzorowaniach.
:::

::: {#exm-5}
## UMAP na danych `iris`

```{r}
#| fig-width: 14
#| fig-height: 6
library(uwot)     # UMAP

set.seed(44)

# Przygotowanie danych: oddzielić etykiety klas i standaryzować cechy
X <- iris %>%
  select(-Species) %>%
  scale() %>%
  as.matrix()

y <- iris$Species

# UMAP 2D: podstawowe parametry
# n_neighbors = "skala lokalności", min_dist = "zwartość klastrów", metric = metryka odległości
emb_umap <- umap(
  X,
  n_neighbors = 15,
  min_dist    = 0.1,
  metric      = "euclidean",
  n_components = 2,
  verbose = TRUE
)

# Ramka wynikowa do wykresu
df_umap <- data.frame(
  UMAP1 = emb_umap[, 1],
  UMAP2 = emb_umap[, 2],
  Species = y
)

# Dla porównania: PCA 2D (opcjonalnie)
pca <- prcomp(X, center = FALSE, scale. = FALSE)
df_pca <- data.frame(
  PC1 = pca$x[, 1],
  PC2 = pca$x[, 2],
  Species = y
)

# Wykresy
p_umap <- ggplot(df_umap, aes(x = UMAP1, y = UMAP2, color = Species)) +
  geom_point(size = 2, alpha = 0.8) +
  labs(title = "UMAP",
       x = "UMAP1", y = "UMAP2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")

p_pca <- ggplot(df_pca, aes(x = PC1, y = PC2, color = Species)) +
  geom_point(size = 2, alpha = 0.8) +
  labs(title = "PCA",
       x = "PC1", y = "PC2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")

# Wyświetlenie obok siebie
p_umap | p_pca
```
:::

Poniżej prezentuję zbiorcze porównanie wszystkich omówionych metod redukcji wymiarowości na tym samym zbiorze danych `iris`. Wykorzystuję PCA, ICA, MDS, t-SNE oraz UMAP, aby zobaczyć, jak różne techniki odwzorowują strukturę danych.

```{r}
#| fig-width: 15
#| fig-height: 10

set.seed(44)

# Usuń duplikaty (t-SNE i MDS niemetryczny są na to wrażliwe)
iris_unique <- iris[!duplicated(iris[, -5]), ]
X <- as.matrix(scale(iris_unique[, -5]))
y <- iris_unique$Species

# PCA (2 pierwsze składowe)
pca_fit <- prcomp(X, center = FALSE, scale. = FALSE)
df_pca <- data.frame(
  Dim1 = pca_fit$x[, 1],
  Dim2 = pca_fit$x[, 2],
  Method = "PCA",
  Species = y
)

# ICA (2 komponenty niezależne)
ica_fit <- fastICA(X, n.comp = 2, method = "C")
df_ica <- data.frame(
  Dim1 = ica_fit$S[, 1],
  Dim2 = ica_fit$S[, 2],
  Method = "ICA",
  Species = y
)

# MDS metryczny (klasyczny)
mds_metric <- cmdscale(dist(X), k = 2)
df_mds_metric <- data.frame(
  Dim1 = mds_metric[, 1],
  Dim2 = mds_metric[, 2],
  Method = "MDS (metryczny)",
  Species = y
)

# MDS niemetryczny (isoMDS)
mds_nonmetric <- isoMDS(dist(X), k = 2)$points
df_mds_nonmetric <- data.frame(
  Dim1 = mds_nonmetric[, 1],
  Dim2 = mds_nonmetric[, 2],
  Method = "MDS (niemet.)",
  Species = y
)

# t-SNE
tsne_fit <- Rtsne(
  X, dims = 2, perplexity = 30,
  max_iter = 750, check_duplicates = FALSE, verbose = FALSE
)
df_tsne <- data.frame(
  Dim1 = tsne_fit$Y[, 1],
  Dim2 = tsne_fit$Y[, 2],
  Method = "t-SNE",
  Species = y
)

# UMAP
umap_emb <- umap(
  X,
  n_neighbors = 15,
  min_dist = 0.1,
  metric = "euclidean",
  n_components = 2,
  verbose = FALSE
)
df_umap <- data.frame(
  Dim1 = umap_emb[, 1],
  Dim2 = umap_emb[, 2],
  Method = "UMAP",
  Species = y
)

# Połączenie wszystkich metod
df_all <- bind_rows(
  df_pca,
  df_ica,
  df_mds_metric,
  df_mds_nonmetric,
  df_tsne,
  df_umap
)

# Wykres porównawczy
ggplot(df_all, aes(Dim1, Dim2, color = Species)) +
  geom_point(size = 2, alpha = 0.8) +
  facet_wrap(~ Method, scales = "free", ncol = 3) +
  labs(
    title = "Porównanie metod redukcji wymiarowości na zbiorze iris",
    x = "Wymiar 1", y = "Wymiar 2"
  ) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    strip.text = element_text(face = "bold")
  )
```
