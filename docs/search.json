[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wielowymiarowa analiza danych",
    "section": "",
    "text": "Wstęp\nWielowymiarowa analiza danych stanowi jeden z filarów współczesnej statystyki i eksploracji danych, oferując metody pozwalające zrozumieć strukturę i zależności w zbiorach danych, w których każda obserwacja jest opisana wieloma zmiennymi jednocześnie. W dobie powszechnego dostępu do danych oraz rosnącego zapotrzebowania na ich zaawansowaną analizę, umiejętność stosowania metod wielowymiarowych staje się nieodzowna zarówno w badaniach naukowych, jak i w analizie danych stosowanej w przemyśle, finansach, biologii, medycynie czy naukach społecznych.\nNiniejsza książka została opracowana z myślą o dwóch kierunkach kształcenia akademickiego: matematyce oraz inżynierii i analizie danych. Jej celem jest zapewnienie solidnych podstaw teoretycznych oraz praktycznych umiejętności niezbędnych do stosowania metod wielowymiarowych w rzeczywistych problemach badawczych i aplikacyjnych. Zakres tematyczny książki został dobrany tak, aby uwzględniać zarówno klasyczne metody statystyczne, jak i techniki wykorzystywane we wspsółczesnej analizie danych.\nW pierwszej części książki omówione zostaną testy wielowymiarowe, które stanowią rozszerzenie klasycznych metod statystycznych na przypadki, w których każda obserwacja opisana jest wieloma zmiennymi. Szczególna uwaga zostanie poświęcona testowi Hotellinga T², będącemu odpowiednikiem testu t dla wielu zmiennych, oraz analizie wariancji dla wielu zmiennych (MANOVA), pozwalającej na badanie różnic między grupami z uwzględnieniem współzależności zmiennych. Celem tej części będzie zrozumienie podstaw inferencji w przestrzeni wielowymiarowej i interpretacji wyników testów z uwzględnieniem macierzy kowariancji.\nNastępnie przedstawiona zostanie analiza kanoniczna, która służy do badania zależności pomiędzy dwoma zestawami zmiennych. Czytelnik pozna konstrukcję zmiennych kanonicznych, sposoby ich interpretacji oraz znaczenie wag i korelacji kanonicznych. Analiza ta ma kluczowe znaczenie wszędzie tam, gdzie celem jest znalezienie skorelowanych struktur w dwóch grupach cech, np. w badaniach biologicznych, społecznych lub psychometrycznych.\nKolejna część książki będzie poświęcona analizie czynnikowej (FA), która umożliwia modelowanie współzmienności zestawu zmiennych za pomocą mniejszej liczby zmiennych ukrytych, zwanych czynnikami. Przedstawione zostaną metody estymacji, kryteria wyboru liczby czynników oraz techniki rotacji, które służą lepszej interpretacji wyników. Analiza czynnikowa jest często stosowana w badaniach ankietowych i psychometrycznych, ale znajduje również zastosowanie w analizie danych ekonomicznych i marketingowych.\nW dalszej kolejności wprowadzony zostanie model ścieżkowy oraz jego uogólnienie w postaci modeli równań strukturalnych (SEM). Modele te pozwalają na modelowanie zarówno obserwowalnych, jak i ukrytych zmiennych oraz relacji przyczynowych pomiędzy nimi. Czytelnik pozna strukturę modelu ścieżkowego, pojęcie identyfikowalności, miary dopasowania oraz techniki estymacji parametrów. Modele SEM są obecnie szeroko stosowane w naukach społecznych, biologii, psychologii i ekonomii.\nNastępnie omówione zostaną metody redukcji wymiarowości, których celem jest uproszczenie reprezentacji danych bez utraty istotnej informacji. Kluczową techniką będzie analiza składowych głównych (PCA), która pozwala na znalezienie nowych osi zmienności w danych. Kolejno zaprezentowana zostanie analiza niezależnych składowych (ICA), która poszukuje składników statystycznie niezależnych, co jest szczególnie użyteczne w analizie sygnałów. Obie metody znajdą zastosowanie zarówno w przygotowaniu danych, jak i w ich eksploracji.\nKolejna część książki poświęcona będzie metodom skalowania wielowymiarowego (Multidimensional Scaling, MDS), które umożliwiają odwzorowanie relacji odległościowych pomiędzy obiektami w przestrzeni o mniejszym wymiarze. Wariant metric zakłada zachowanie rzeczywistych wartości odległości, natomiast non-metric koncentruje się na porządku dystansów. Metody te pozwalają uzyskać intuicyjne wizualizacje struktur danych, szczególnie przydatne w psychologii, socjologii czy analizie rynku.\nW uzupełnieniu do klasycznych technik przedstawione zostaną nieliniowe metody redukcji wymiarowości, takie jak t-distributed Stochastic Neighbor Embedding (t-SNE) oraz Uniform Manifold Approximation and Projection (UMAP). Obie techniki pozwalają na odwzorowanie skomplikowanych struktur danych w przestrzeniach dwu- lub trójwymiarowych, zachowując lokalne sąsiedztwa. Choć są to metody przede wszystkim eksploracyjne i wizualizacyjne, ich wartość w analizie dużych zbiorów danych jest trudna do przecenienia.\nNastępnie przedstawiona zostanie analiza skupień, której celem jest odkrywanie naturalnych grup w zbiorze danych. Omówione zostaną zarówno metody hierarchiczne, jak i niehierarchiczne, w tym popularna metoda k-średnich. Poruszona zostanie problematyka doboru liczby skupień oraz oceny stabilności i jakości otrzymanych rozwiązań. Analiza skupień znajduje zastosowanie w segmentacji rynku, biologii molekularnej, diagnostyce medycznej i wielu innych dziedzinach.\nKolejna część książki poświęcona będzie analizie korespondencji, stosowanej do eksploracji związków pomiędzy zmiennymi jakościowymi przedstawionymi w postaci tablicy kontyngencji. Przedstawiona zostanie zarówno analiza korespondencji prosta (dla dwóch zmiennych), jak i złożona (dla więcej niż dwóch). Omówione zostaną interpretacja map percepcyjnych, odwzorowanie profili oraz związki z metodami takimi jak PCA czy MDS.\nOstatni rozdział poświęcony będzie analizie log-liniowej, która umożliwia modelowanie częstości w tablicach wielodzielczych na podstawie interakcji pomiędzy zmiennymi kategorycznymi. Zostaną zaprezentowane modele pełne i uproszczone, zasady testowania złożoności modeli oraz interpretacji parametrów. Analiza log-liniowa jest szczególnie przydatna przy badaniu wielowymiarowych zależności między zmiennymi kategorycznymi w badaniach społecznych, medycznych oraz w analizie zachowań konsumenckich.\nWszystkie metody zostaną zilustrowane przykładami praktycznymi, realizowanymi w języku R. Pozwoli to Czytelnikowi nie tylko zrozumieć teoretyczne podstawy omawianych technik, ale także nabyć umiejętność ich stosowania w praktyce analitycznej.",
    "crumbs": [
      "Wstęp"
    ]
  },
  {
    "objectID": "multi_tests.html",
    "href": "multi_tests.html",
    "title": "Testy wielowymiarowe",
    "section": "",
    "text": "Test Hotellinga dla znanej macierzy kowariancji (Anderson 1992)\nW tradycyjnej analizie statystycznej często koncentrujemy się na porównywaniu grup ze względu na jedną zmienną – np. porównujemy średni wzrost kobiet i mężczyzn, wykorzystując test t-Studenta. Jednak w rzeczywistości badawczej rzadko interesuje nas tylko jedna cecha. Przykładowo, porównując grupy pacjentów, możemy jednocześnie rozważać poziom ciśnienia, cholesterolu i BMI. Albo, analizując dane socjologiczne, chcemy porównać grupy pod względem dochodów, wykształcenia i poziomu zadowolenia z życia.\nUżycie wielu testów jednowymiarowych wydaje się kuszące – testujemy każdą zmienną osobno. Jednak prowadzi to do trzech istotnych problemów (Huberty i Morris 1989):\n\\[\n\\mathbb{P}(\\text{co najmniej jeden błąd I rodzaju}) = 1 - (1 - \\alpha)^p = 1 - 0.95^{10} \\approx 0.40\n\\]\nPowyższe problemy uzasadniają potrzebę stosowania testów wielowymiarowych – uwzględniających strukturę współzmienności między cechami oraz pozwalających na testowanie hipotez dotyczących całych wektorów średnich.\nRozważmy próbkę \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\ldots, \\boldsymbol{y}_n \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), gdzie \\(\\boldsymbol{\\Sigma}\\) jest znana. Oznacza to, że mamy do czynienia z ciągiem niezależnych losowych wektorów \\(\\boldsymbol{y}_i\\), z których każdy ma ten sam wielowymiarowy rozkład normalny o wymiarze \\(p\\), średniej \\(\\boldsymbol{\\mu}\\) i macierzy kowariancji \\(\\boldsymbol{\\Sigma}\\). Każdy wektor \\(\\boldsymbol{y}_i\\) można interpretować jako punkt w przestrzeni \\(\\mathbb{R}^p\\), opisujący \\(p\\) cech (zmiennych) dla jednej obserwacji. Formalnie jest to wektor kolumnowy postaci \\(\\boldsymbol{y}_i = [y_{i1}, y_{i2}, \\ldots, y_{ip}]^\\top\\), gdzie indeks \\(i\\) numeruje jednostki (np. osoby, obiekty pomiaru), a indeksy \\(j = 1, \\ldots, p\\) odpowiadają poszczególnym zmiennym. Wektor ten traktowany jest jako zmienna losowa, ponieważ jego wartości są wynikiem losowego procesu generującego dane. Rozkład \\(\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) jest wielowymiarową wersją rozkładu normalnego. Opisuje on sytuację, w której każda kombinacja liniowa zmiennych losowych w \\(\\boldsymbol{y}_i\\) również ma rozkład normalny, a funkcja gęstości prawdopodobieństwa ma postać zależną od wartości wektora średnich oraz struktury kowariancji. Jest to fundamentalne założenie w klasycznej analizie statystycznej, pozwalające na stosowanie wielu narzędzi statystycznych.\nParametr \\(\\boldsymbol{\\mu} = [\\mu_1, \\mu_2, \\ldots, \\mu_p]^\\top\\) to wektor wartości oczekiwanych każdej z analizowanych zmiennych. Oznacza on przeciętny poziom zmiennej \\(y_{ij}\\) w populacji dla każdej cechy \\(j\\). Jest to parametr istotny z punktu widzenia testowania hipotez, ponieważ wiele testów statystycznych dotyczy właśnie równości lub różnic wektorów średnich między grupami.\nZ kolei macierz \\(\\boldsymbol{\\Sigma}\\) to dodatnio określona, symetryczna macierz kowariancji o wymiarach \\(p \\times p\\). Jej elementy \\(\\sigma_{jj}\\) opisują wariancje poszczególnych zmiennych, natomiast elementy poza główną przekątną \\(\\sigma_{jk}\\) (dla \\(j \\ne k\\)) opisują kowariancje, czyli współzmienność pomiędzy zmiennymi \\(y_{ij}\\) i \\(y_{ik}\\). W analizie wielowymiarowej uwzględnienie tych zależności między cechami jest kluczowe, ponieważ pozwala lepiej zrozumieć strukturę danych i dokonywać bardziej trafnych wniosków statystycznych.\nO próbie \\(\\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n\\) zakładamy, że wszystkie obserwacje są niezależne oraz pochodzą z tego samego rozkładu. Oznacza to, że mamy do czynienia z próbą losową, niezależną o identycznych rozkładach (i.i.d.), co jest podstawowym założeniem wielu testów i metod estymacji. Całą próbkę można przedstawić jako macierz danych o wymiarach \\(n \\times p\\), w której wiersze odpowiadają jednostkom, a kolumny cechom.\nW przypadku, gdy macierz kowariancji \\(\\boldsymbol{\\Sigma}\\) jest znana, możemy zastosować uproszczone wersje testów statystycznych, takie jak klasyczny test Hotellinga \\(T^2\\) dla jednej próby. Jest to jednak sytuacja czysto teoretyczna, ponieważ w praktyce \\(\\boldsymbol{\\Sigma}\\) musi być zazwyczaj estymowana na podstawie danych. Pomimo tego, przypadek znanej macierzy jest użyteczny do budowania intuicji, zrozumienia ról poszczególnych parametrów i wyprowadzania własności statystyk testowych.\nChcemy przetestować hipotezę:\n\\[\nH_0: \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_0 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu} \\neq \\boldsymbol{\\mu}_0\n\\]\nStatystyka testowa opiera się na uogólnionej odległości Mahalanobisa:\n\\[\nT^2 = n (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)^\\top \\boldsymbol{\\Sigma}^{-1} (\\bar{\\boldsymbol{y}} - \\boldsymbol{\\mu}_0)\n\\]\nPod warunkiem spełnienia \\(H_0\\), statystyka \\(T^2 \\sim \\chi^2_p\\). Zatem możemy porównać wartość \\(T^2\\) z odpowiednim kwantylem rozkładu chi-kwadrat.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#założenia",
    "href": "multi_tests.html#założenia",
    "title": "Testy wielowymiarowe",
    "section": "Założenia",
    "text": "Założenia\n\nPróby są niezależne;\nObserwacje w każdej grupie pochodzą z rozkładu wielowymiarowego normalnego;\n\nMacierze kowariancji są równe \\(\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}\\). Jest to kluczowe założenie umożliwiające zbudowanie wspólnego estymatora kowariancji i zastosowanie rozkładu \\(T^2\\) Hotellinga. Choć może być ono naruszone w praktyce, to dla dużych prób test zachowuje swoje właściwości asymptotyczne (Rencher 1998).\n\nWektory średnich z próby wyrażamy jako:\n\\[\n\\bar{\\boldsymbol{y}}_1 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} \\boldsymbol{y}_{1,i}, \\quad\n\\bar{\\boldsymbol{y}}_2 = \\frac{1}{n_2} \\sum_{i=1}^{n_2} \\boldsymbol{y}_{2,i}\n\\]\nNieobciążonym estymatorem macierzy kowariancji (\\(\\boldsymbol{\\Sigma}\\)) jest tzw. połączony estymator kowariancji:\n\\[\n\\mathbf{S} =\n\\frac{(n_1 - 1) \\mathbf{S}_1 + (n_2 - 1) \\mathbf{S}_2}{n_1 + n_2 - 2}\n\\] gdzie \\[\n\\mathbf{S}_1 = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top\n\\]\n\\[\n\\mathbf{S}_2 = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top\n\\]\nZatem:\n\\[\n\\mathbf{S} = \\frac{\\mathbf{W}_1 + \\mathbf{W}_2}{n_1 + n_2 - 2}\n\\] gdzie: \\[\n\\mathbf{W}_1 = \\sum_{i=1}^{n_1} (\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)(\\boldsymbol{y}_{1,i} - \\bar{\\boldsymbol{y}}_1)^\\top = (n_1 - 1)\\mathbf{S}_1\n\\]\n\\[\n\\mathbf{W}_2 = \\sum_{i=1}^{n_2} (\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)(\\boldsymbol{y}_{2,i} - \\bar{\\boldsymbol{y}}_2)^\\top = (n_2 - 1)\\mathbf{S}_2\n\\] Statystyka testowa Hotellinga wówczas ma postać:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)\n\\]\nA gdy \\(H_0\\) jest prawdziwa, to po przekształceniu: \\[\nF = \\frac{(n_1 + n_2 - p - 1)}{p(n_1 + n_2 - 2)} T^2 \\sim F_{p, n_1 + n_2 - p - 1}\n\\]\nAlternatywnie, można zapisać, że: \\[\nT^2 \\sim \\frac{p(n_1 + n_2 - 2)}{n_1 + n_2 - p - 1} F_{p, n_1 + n_2 - p - 1}\n\\]\nHipotezę zerową \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) odrzucamy na poziomie istotności \\(\\alpha\\), jeśli: \\[\nT^2 &gt; T^2_{\\alpha, p, n_1 + n_2 - 2}\n\\] lub równoważnie: \\[\nF &gt; F_{\\alpha, p, n_1 + n_2 - p - 1}\n\\]\nAby test był możliwy do przeprowadzenia, konieczne jest, aby \\(n_1 + n_2 - 2 &gt; p\\), czyli liczba stopni swobody w estymacji wspólnej kowariancji była większa niż wymiar przestrzeni cech.\n\n\n\n\n\n\nAdnotacja\n\n\n\nW praktyce istotne jest, aby przed zastosowaniem testu \\(T^2\\) Hotellinga dla dwóch prób zweryfikować założenie o równości macierzy kowariancji — np. za pomocą testu Boxa. Test M Boxa (ang. Box’s M test) służy do statystycznej weryfikacji hipotezy równości macierzy kowariancji w wielu grupach.\nZałóżmy, że mamy \\(G\\) niezależnych prób z wielowymiarowego rozkładu normalnego:\n\\[\n\\boldsymbol{y}_{g} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_g, \\boldsymbol{\\Sigma}_g), \\quad g = 1, \\ldots, G\n\\] Testujemy hipotezę: \\[\nH_0: \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\ldots = \\boldsymbol{\\Sigma}_G = \\boldsymbol{\\Sigma}\n\\] przeciwko alternatywie: \\[\nH_1: \\exists\\, g, h: \\boldsymbol{\\Sigma}_g \\ne \\boldsymbol{\\Sigma}_h\n\\] Niech\n\n\n\\(\\mathbf{S}_g\\) – macierz kowariancji w grupie \\(g\\),\n\n\\(n_g\\) – liczba obserwacji w grupie \\(g\\),\n\n\\(\\mathbf{S}_p\\) – połączony estymator macierzy kowariancji:\n\n\\[\n\\mathbf{S}_p = \\frac{1}{N - G} \\sum_{g=1}^G (n_g - 1)\\mathbf{S}_g\n\\] gdzie \\(N = \\sum_{g=1}^G n_g\\) – łączna liczba obserwacji. Wówczas, statystyka testowa Boxa ma postać: \\[\nM = (N - G) \\cdot \\ln|\\mathbf{S}_p| - \\sum_{g=1}^G (n_g - 1) \\cdot \\ln|\\mathbf{S}_g|\n\\] Poprawka na skończoną próbkę prowadzi do statystyki: \\[\nC = \\left(1 - c\\right) \\cdot M\n\\] gdzie: \\[\nc = \\frac{1}{3(p + 1)(G - 1)} \\left[ \\sum_{g=1}^G \\frac{1}{n_g - 1} - \\frac{1}{N - G} \\right]\n\\] Statystyka \\(C\\) jest asymptotycznie zbierzna do rozkładu \\(\\chi^2\\left(\\frac{p}{2}(p + 1)(G - 1)\\right)\\) liczbą stopni swobody.\nHipotezę \\(H_0\\) o równości macierzy kowariancji odrzuca się, jeśli: \\[\nC &gt; \\chi^2_{1 - \\alpha, df}\n\\] lub gdy \\(p\\) testu jest mniejsza od poziomu istotności \\(\\alpha\\).\nUwagi praktyczne\n\nTest M Boxa jest wrażliwy na odchylenia od normalności – jeśli dane nie są zbliżone do normalnych, test może dawać mylące wyniki.\nW dużych próbach nawet drobne różnice między macierzami kowariancji mogą prowadzić do odrzucenia \\(H_0\\), choć nie mają istotnego wpływu praktycznego.\nW małych próbach test może być niestabilny – zaleca się ostrożność przy interpretacji.\n\n\n\n\nPrzykład 4.1 (Porównanie dwóch grup za pomocą testu \\(T^2\\) Hotellinga)  \n\nKodlibrary(MASS)\n# Parametry symulacji\nset.seed(44)\np &lt;- 2          # liczba zmiennych\nn1 &lt;- 30        # liczba obserwacji w grupie 1\nn2 &lt;- 35        # liczba obserwacji w grupie 2\n\n# Parametry rozkładu\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1, 1)\nSigma &lt;- matrix(c(1, 0.5,\n                  0.5, 1), nrow = 2)\n\n# Generowanie danych\nY1 &lt;- mvrnorm(n1, mu = mu1, Sigma = Sigma)\nY2 &lt;- mvrnorm(n2, mu = mu2, Sigma = Sigma)\n\n# Średnie z próby\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymatory kowariancji\nS1 &lt;- cov(Y1)\nS2 &lt;- cov(Y2)\n\n# Wspólna kowariancja (połączona)\nSp &lt;- ((n1 - 1)*S1 + (n2 - 1)*S2) / (n1 + n2 - 2)\n\n# Statystyka testowa Hotellinga T^2\ndiff_mean &lt;- y1_bar - y2_bar\nT2 &lt;- (n1 * n2) / (n1 + n2) * t(diff_mean) %*% solve(Sp) %*% diff_mean\nT2 &lt;- as.numeric(T2)\n\n# Przekształcenie do F\ndf1 &lt;- p\ndf2 &lt;- n1 + n2 - p - 1\nF_stat &lt;- (df2 / (df1 * (n1 + n2 - 2))) * T2\n\n# Wartość krytyczna\nalpha &lt;- 0.05\nF_crit &lt;- qf(1 - alpha, df1, df2)\n\n# p-wartość\np_val &lt;- 1 - pf(F_stat, df1, df2)\n\n# Wynik testu\nsprintf(\"Statystyka T² = %.3f\", T2)\n\n[1] \"Statystyka T² = 17.340\"\n\nKodsprintf(\"Statystyka F = %.3f\", F_stat)\n\n[1] \"Statystyka F = 8.532\"\n\nKodsprintf(\"Wartość krytyczna F = %.3f\", F_crit)\n\n[1] \"Wartość krytyczna F = 3.145\"\n\nKodsprintf(\"p-value = %e\", p_val)\n\n[1] \"p-value = 5.330310e-04\"\n\nKod# Wizualizacja \ndf1 &lt;- as.data.frame(Y1) %&gt;%\n  mutate(grupa = \"Grupa 1\")\n\ndf2 &lt;- as.data.frame(Y2) %&gt;%\n  mutate(grupa = \"Grupa 2\")\n\ndf_all &lt;- bind_rows(df1, df2)\ncolnames(df_all)[1:2] &lt;- c(\"X1\", \"X2\")\n\n# Ramka danych ze średnimi\nmeans &lt;- data.frame(\n  X1 = c(y1_bar[1], y2_bar[1]),\n  X2 = c(y1_bar[2], y2_bar[2]),\n  grupa = c(\"Grupa 1\", \"Grupa 2\")\n)\n\n# Wykres\nggplot(df_all, aes(x = X1, y = X2, color = grupa, shape = grupa)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_point(data = means, aes(x = X1, y = X2),\n             shape = c(1, 2), size = 5, stroke = 1.2, show.legend = FALSE) +\n  scale_shape_manual(values = c(16, 17)) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  coord_equal() +\n  theme_minimal() +\n  labs(title = \"Porównanie dwóch grup\",\n       x = \"X1\", y = \"X2\", color = \"Grupa\", shape = \"Grupa\")\n\n\n\n\n\n\n\n\nKod# Alternatywnie, użycie gotowej funkcji z pakietu ICSNP\nlibrary(ICSNP)\nHotellingsT2(rbind(Y1, Y2)~factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Hotelling's two sample T2-test\n\ndata:  rbind(Y1, Y2) by factor(c(rep(1, n1), rep(2, n2)))\nT.2 = 8.5321, df1 = 2, df2 = 62, p-value = 0.000533\nalternative hypothesis: true location difference is not equal to c(0,0)\n\n\nW analizowanym przykładzie zdefiniowaliśmy macierze kowariancji identycznie ale w rzeczywistości należałoby testować hipotezę o równości macierzy kowariancji. Tylko dla celów ćwiczeniowych pokażę jak to zrobić.\n\nKod# Test Boxa na równość macierzy kowariancji\nlibrary(biotools)\nboxM(rbind(Y1, Y2), factor(c(rep(1, n1), rep(2, n2))))\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  rbind(Y1, Y2)\nChi-Sq (approx.) = 2.1261, df = 3, p-value = 0.5466\n\nKod# lub z wykorzystaniem pakietu rstatix\nlibrary(rstatix)\nbox_m(df_all[,-3], df_all[,3])\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                            \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                             \n1      2.13   0.547         3 Box's M-test for Homogeneity of Covariance Matric…\n\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nW sytuacji, gdy założenie o równości macierzy kowariancji jest naruszone, można stosować alternatywne metody, takie jak:\n\nTesty permutacyjne (Hotelling::hotelling.test(Y~Group, perm = TRUE, B = 5000)).\nUogólniony test Hotellinga - test Jamesa (ang. James’s second-order test) lub czasami nazywany również testem Welch-type Hotelling test (Hotelling::hotelling.test(Y~Group, var.equal = FALSE)).\nW przypadku danych charakteryzujących się dużą liczbą zmiennych w stosunku do liczby obserwacji, można rozważyć użycie estymatora Jamesa-Steina do stabilizaji macierzy kowariancji (Hotelling::hotelling.test(Y~Group, shrinkage = TRUE)).\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nOdrzucenie hipotezy zerowej \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) w teście Hotellinga \\(T^2\\) oznacza, że mamy statystycznie istotny dowód na to, iż rozkłady średnich wektorów dwóch populacji wielowymiarowych różnią się, biorąc pod uwagę współzmienność między zmiennymi. W kontekście zastosowań praktycznych, oznacza to, że przynajmniej jedna zmienna (lub kombinacja zmiennych) odróżnia grupy, nawet jeśli nie da się tego wykazać za pomocą testów jednowymiarowych.\nW sytuacji, gdy mamy dane wielowymiarowe \\(\\boldsymbol{y}_{gi} \\in \\mathbb{R}^p\\) z dwóch niezależnych grup (\\(g = 1,2\\)), testujemy:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 \\quad \\text{vs} \\quad H_1: \\boldsymbol{\\mu}_1 \\neq \\boldsymbol{\\mu}_2.\n\\]\nOdrzucenie \\(H_0\\) sugeruje, że istnieje różnica pomiędzy średnimi wektorami, ale nie musi oznaczać, że którakolwiek ze średnich poszczególnych zmiennych \\(\\mu_{1j}, \\mu_{2j}\\) różni się istotnie — zwłaszcza jeśli uwzględnimy korelacje między zmiennymi.\nTesty jednowymiarowe ignorują te współzależności, dlatego mogą nie wykazać istotnych różnic, mimo że ogólny profil wielowymiarowy się różni. Innymi słowy, może nie istnieć żadna istotna różnica w poszczególnych zmiennych, ale pewna kombinacja liniowa tych zmiennych pozwala na rozróżnienie grup.\nRozważmy kombinację liniową:\n\\[\nz = \\boldsymbol{a}^\\top \\boldsymbol{y},\n\\]\ngdzie \\(\\boldsymbol{a} \\in \\mathbb{R}^p\\) to niezerowy wektor wag. Wówczas \\(z\\) jest jednowymiarową zmienną losową będącą projekcją obserwacji \\(\\boldsymbol{y}\\) na kierunek \\(\\boldsymbol{a}\\).\nJeśli hipoteza \\(H_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2\\) jest fałszywa, to istnieje taki wektor \\(\\boldsymbol{a}\\), dla którego:\n\\[\nH_0: \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_1 = \\boldsymbol{a}^\\top \\boldsymbol{\\mu}_2\n\\]\nzostanie odrzucona w teście jednowymiarowym. Dla takiej kombinacji liniowej możemy zdefiniować statystykę \\(t\\)-Studenta:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\bar{z}_1 - \\bar{z}_2}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) s_z^2}},\n\\]\ngdzie:\n\n\n\\(\\bar{z}_g = \\boldsymbol{a}^\\top \\bar{\\boldsymbol{y}}_g\\) – średnia z projekcji grupy \\(g\\),\n\n\\(s_z^2\\) – nieobciążony estymator wariancji \\(z\\), czyli:\n\n\\[\ns_z^2 = \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a},\n\\]\na \\(\\mathbf{S}\\) to wspólna macierz kowariancji.\nStąd pełna postać statystyki:\n\\[\nt(\\boldsymbol{a}) = \\frac{\\boldsymbol{a}^\\top (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)}{\\sqrt{\\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\boldsymbol{a}^\\top \\mathbf{S} \\boldsymbol{a}}}.\n\\]\nPonieważ statystyka \\(t(\\boldsymbol{a})\\) może być zarówno dodatnia, jak i ujemna, stosuje się często jej kwadrat jako miarę istotności:\n\\[\nT^2 = t^2(\\boldsymbol{a}).\n\\]\nStatystyka Hotellinga \\(T^2\\) przyjmuje postać:\n\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2)^\\top \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nJest to forma uogólnionej odległości Mahalanobisa między średnimi wektorami. Można pokazać, że istnieje taki wektor \\(\\boldsymbol{a}\\), który maksymalizuje różnicę \\(t(\\boldsymbol{a})\\) — to tzw. funkcja dyskryminacyjna:\n\\[\n\\boldsymbol{a} = \\mathbf{S}^{-1} (\\bar{\\boldsymbol{y}}_1 - \\bar{\\boldsymbol{y}}_2).\n\\]\nDla tej wartości wektora \\(\\boldsymbol{a}\\), statystyka \\(T^2\\) przyjmuje największą wartość i jest najbardziej czuła na różnice między grupami.\nOdrzucenie \\(H_0\\) oznacza więc, że w przestrzeni \\(\\mathbb{R}^p\\) istnieje kierunek \\(\\boldsymbol{a}\\), dla którego grupy mają różne średnie projekcje. To otwiera drogę do:\n\nkonstrukcji funkcji dyskryminacyjnych (jak w analizie dyskryminacyjnej),\nidentyfikacji zmiennych lub ich kombinacji odpowiedzialnych za różnicę,\ndalszych analiz jednowymiarowych dla projekcji \\(z = \\boldsymbol{a}^\\top \\boldsymbol{y}\\).\n\n\nKodlibrary(gridExtra)  # dla strzałki jako warstwy\n\nset.seed(42)\n\n# Parametry\nn1 &lt;- n2 &lt;- 100\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(1.5, 0.5)\nSigma &lt;- matrix(c(1, 0.8, 0.8, 1), ncol = 2)\n\n# Dane\nY1 &lt;- mvrnorm(n1, mu1, Sigma)\nY2 &lt;- mvrnorm(n2, mu2, Sigma)\n\n# Średnie\ny1_bar &lt;- colMeans(Y1)\ny2_bar &lt;- colMeans(Y2)\n\n# Estymacja wspólnej kowariancji\nS_pooled &lt;- ((n1 - 1) * cov(Y1) + (n2 - 1) * cov(Y2)) / (n1 + n2 - 2)\n\n# Kierunek dyskryminacyjny a\ndiff &lt;- y1_bar - y2_bar\na &lt;- solve(S_pooled, diff)\na_norm &lt;- a / sqrt(sum(a^2))  # normalizacja\n\n# Punkt startowy strzałki (środek między średnimi)\norigin &lt;- (y1_bar + y2_bar) / 2\nscale &lt;- 3  # długość strzałki\narrow_end &lt;- origin + scale * a_norm\n\n# Ramka danych do wykresu\ndf &lt;- rbind(\n  data.frame(X1 = Y1[,1], X2 = Y1[,2], Grupa = \"Grupa 1\"),\n  data.frame(X1 = Y2[,1], X2 = Y2[,2], Grupa = \"Grupa 2\")\n)\n\n# Wektory średnich i strzałka\nmeans_df &lt;- data.frame(rbind(y1_bar, y2_bar))\ncolnames(means_df) &lt;- c(\"X1\", \"X2\")\nmeans_df$Grupa &lt;- c(\"Grupa 1\", \"Grupa 2\")  # te same etykiety co w danych punktów\n\narrow_df &lt;- data.frame(\n  x = origin[1],\n  y = origin[2],\n  xend = arrow_end[1],\n  yend = arrow_end[2]\n)\n\n# Wykres\nggplot(df, aes(x = X1, y = X2, color = Grupa)) +\n  geom_point(alpha = 0.5) +\n  geom_point(data = means_df, aes(x = X1, y = X2),\n             size = 4, shape = 16, show.legend = FALSE) +  # średnie tym samym kolorem; bez legendy\n  geom_segment(data = arrow_df, \n               aes(x = x, y = y, xend = xend, yend = yend),\n               arrow = arrow(length = unit(0.25, \"cm\")), color = \"black\", size = 1, show.legend = FALSE) +\n  labs(\n    title = latex2exp::TeX(r\"(Ilustracja kierunku dyskryminacyjnego $a = S^{-1} (\\bar{y}_1 - \\bar{y}_2)$)\"),\n    x = latex2exp::TeX(r\"($X_1$)\"),\n    y = latex2exp::TeX(r\"($X_2$)\")\n  ) +\n  coord_equal() +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#założenia-i-testowane-hipotezy",
    "href": "multi_tests.html#założenia-i-testowane-hipotezy",
    "title": "Testy wielowymiarowe",
    "section": "Założenia i testowane hipotezy",
    "text": "Założenia i testowane hipotezy\nZakłada się, że obserwacje są niezależne i pochodzą z wielowymiarowego rozkładu normalnego w każdej grupie, tj.:\n\\[\n\\boldsymbol{y}_{ij} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}), \\quad i = 1, \\dots, g,\\ j = 1, \\dots, n_i,\n\\]\ngdzie:\n\n\n\\(\\boldsymbol{y}_{ij} \\in \\mathbb{R}^p\\) — wektor obserwacji w grupie \\(i\\),\n\n\\(\\boldsymbol{\\mu}_i\\) — wektor średnich dla grupy \\(i\\),\n\n\\(\\boldsymbol{\\Sigma}\\) — wspólna macierz kowariancji we wszystkich grupach (założenie homogeniczności).\n\nTestowana jest hipoteza zerowa:\n\\[\nH_0: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\dots = \\boldsymbol{\\mu}_g\n\\]\nwobec alternatywy:\n\\[\nH_1: \\exists\\ i,j\\ \\text{takie, że}\\ \\boldsymbol{\\mu}_i \\ne \\boldsymbol{\\mu}_j.\n\\]\nModel MANOVA opiera się na kilku fundamentalnych założeniach dotyczących danych, których spełnienie warunkuje poprawność i wiarygodność uzyskanych wyników. Ich naruszenie może prowadzić do fałszywych wniosków, zbyt dużej liczby odrzuceń hipotezy zerowej lub do błędnych ocen efektów czynników. Poniżej przedstawiono szczegółowo każde z tych założeń.\n\nPierwszym kluczowym założeniem jest odpowiednia wielkość próby. Przyjmuje się, że liczba obserwacji w każdej grupie (komórce) powinna przekraczać liczbę zmiennych zależnych, które są jednocześnie analizowane. To praktyczne zalecenie pozwala uniknąć problemów z oszacowaniem macierzy kowariancji i zapewnia dostateczną moc statystyczną.\nKolejnym istotnym założeniem jest niezależność obserwacji. Oznacza to, że każda jednostka obserwacyjna (np. osoba) powinna przynależeć wyłącznie do jednej grupy. Obserwacje wewnątrz i pomiędzy grupami nie mogą być ze sobą powiązane. W szczególności, model MANOVA nie jest odpowiedni dla danych z pomiarami powtarzanymi u tych samych obiektów. Dobór próby powinien być dokonany w sposób losowy, bez systematycznych zależności.\nTrzecim wymogiem jest brak obserwacji odstających, zarówno w sensie jednowymiarowym (dla każdej zmiennej z osobna), jak i wielowymiarowym (dla kombinacji wszystkich zmiennych zależnych). Obserwacje odstające mogą silnie zniekształcać wartości średnich i macierzy kowariancji, przez co wyniki MANOVA stają się niestabilne.\nFundamentalnym założeniem MANOVA jest wielowymiarowa normalność rozkładu danych w każdej z grup. Oznacza to, że wektor zmiennych zależnych w każdej grupie powinien mieć rozkład wielowymiarowy normalny. W R można zastosować funkcję mshapiro_test() z pakietu rstatix, aby przeprowadzić test Shapiro–Wilka dla sprawdzenia normalności wielowymiarowej.\nKolejne założenie dotyczy braku współliniowości. Oczekuje się, że zmienne zależne będą ze sobą skorelowane w umiarkowany sposób, ale nie nadmiernie. Wartości współczynników korelacji przekraczające \\(r = 0,90\\) są uznawane za niepożądane i mogą powodować problemy numeryczne oraz błędną interpretację wyników. Jak podają Tabachnick i Fidell (2012), zmienne powinny wnosić unikalne informacje do modelu.\nWażnym wymogiem jest również liniowość zależności między zmiennymi zależnymi w każdej grupie. Oznacza to, że zależności pomiędzy każdą parą zmiennych muszą być dobrze opisane przez funkcję liniową — jest to konieczne, aby poprawnie oszacować strukturę kowariancji.\nDla poprawnego działania MANOVA zakłada się także jednorodność wariancji dla każdej zmiennej zależnej między grupami. Można to testować za pomocą testu Levene’a. Nieistotny wynik testu Levene’a sugeruje, że wariancje są porównywalne w grupach.\nOstatnie, ale bardzo istotne, jest założenie o jednorodności macierzy kowariancji (homogeniczności macierzy wariancji–kowariancji) pomiędzy grupami. Oznacza to, że struktura współzależności między zmiennymi powinna być podobna w każdej grupie. Weryfikację tego założenia umożliwia test Boxa (Box’s M test), który stanowi wielowymiarowy odpowiednik testu Levene’a. Ze względu na dużą czułość testu Boxa na odstępstwa od założeń, przyjmuje się konserwatywny poziom istotności \\(\\alpha = 0,001\\) dla weryfikacji jego wyniku.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "href": "multi_tests.html#konstrukcja-modelu-i-statystyki-testowe",
    "title": "Testy wielowymiarowe",
    "section": "Konstrukcja modelu i statystyki testowe",
    "text": "Konstrukcja modelu i statystyki testowe\nPodobnie jak w jednowymiarowym przypadku, w MANOVA analizuje się rozkład wariancji całkowitej na wariancję międzygrupową i wewnątrzgrupową, ale w postaci macierzy kowariancji:\n\nMacierz wariancji międzygrupowej (ang. between-group SSCP):\n\n\\[\n\\mathbf{B} = \\sum_{i=1}^{g} n_i (\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})(\\bar{\\boldsymbol{y}}_i - \\bar{\\boldsymbol{y}})^\\top\n\\]\n\nMacierz wariancji wewnątrzgrupowej (ang. within-group SSCP):\n\n\\[\n\\mathbf{W} = \\sum_{i=1}^{g} \\sum_{j=1}^{n_i} (\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)(\\boldsymbol{y}_{ij} - \\bar{\\boldsymbol{y}}_i)^\\top\n\\]\nMacierz wariancji całkowitej to: \\(\\mathbf{T} = \\mathbf{B} + \\mathbf{W}\\).\nW celu przeprowadzenia testu MANOVA, wykorzystuje się statystyki oparte na stosunku macierzy:\n\\[\n\\mathbf{W}^{-1} \\mathbf{B}\n\\]\nNajczęściej spotykane statystyki testowe to:\n\nWilks’ Lambda:\n\n\\[\n\\Lambda = \\frac{\\det(\\mathbf{W})}{\\det(\\mathbf{B} + \\mathbf{W})}\n\\]\n\nStatystyka Pillai-Bartletta (Trace):\n\n\\[\nV = \\mathrm{tr}\\left[(\\mathbf{B} + \\mathbf{W})^{-1} \\mathbf{B}\\right]\n\\]\n\nStatystyka Hotellinga–Lawleya (Trace):\n\n\\[\nT = \\mathrm{tr}(\\mathbf{W}^{-1} \\mathbf{B})\n\\]\n\nNajwiększy pierwiastek Roy’a:\n\n\\[\n\\theta_{\\text{max}} = \\text{największa wartość własna}\\ (\\mathbf{W}^{-1} \\mathbf{B})\n\\]\nWybór konkretnej statystyki zależy od liczebności prób, wymiaru przestrzeni i liczby grup. W praktyce Wilks’ Lambda jest najczęściej stosowana.\n\nPrzykład 5.1 Na poziomie istotności \\(\\alpha=0.05\\) zweryfikuj hipotezę, że czynnik grupujący (Group) istotnie różnicuje zmienne Actions i Thoughts jednocześnie.\n\nKodlibrary(gtsummary)\nlibrary(rstatix)\nlibrary(easystats)\nlibrary(gt)\ndane &lt;- rio::import(\"data/OCD.dat\")\n\n\nStatystyki opisowe grup\n\nKoddane %&gt;% \n  tbl_summary(by = Group,\n              statistic = list(where(is.numeric) ~ \"{mean} ({sd})\"),\n              type = list(Actions ~ \"continuous\"),\n              digits = list(everything() ~ 2))\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nBT\nN = 101\n\n\nCBT\nN = 101\n\n\nNo Treatment Control\nN = 101\n\n\n\n\nActions\n3.70 (1.77)\n4.90 (1.20)\n5.00 (1.05)\n\n\nThoughts\n15.20 (2.10)\n13.40 (1.90)\n15.00 (2.36)\n\n\n\n\n1 Mean (SD)\n\n\n\n\nKodp &lt;- dane %&gt;% \n  select(-Group) %&gt;% \n  correlation()\n\np %&gt;% print_html()\n\n\n\n\n\n\nCorrelation Matrix (pearson-method)\n\n\nParameter1\nParameter2\nr\n95% CI\nt(28)\np\n\n\n\nActions\nThoughts\n0.06\n(-0.31, 0.41)\n0.31\n0.758\n\n\np-value adjustment method: Holm (1979); Observations: 30\n\n\n\n\n\nW kontekście zmiennej Actions widzimy najwyższy poziom w grupie No treatment, natomiast najniższy w grupie BT. Dla zmiennej Thoughts najwyższy poziom osiągnięto w grupie BT a najniższy w grupie CBT. Grupy różnią się również zmiennością obu cech. Związek pomiędzy zmiennymi Actions i Thoughts jest niemal niezauważalny. Korelacja pomiędzy tymi cechami jest nieistotnie różna od zera.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  ggplot(aes(x = Group, y = value, fill = name)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(fill = \"Variable\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\nPowyższe wykresy potwierdzają znaczne różnice pomiędzy grupami w kontekście analizowanych cech.\nZałożenia\n\nKoddane %&gt;% \n  group_split(Group) %&gt;% \n  map_df(~mshapiro_test(.x[,2:3])) %&gt;% \n  mutate(Group = dane %&gt;% \n           group_keys(Group) %&gt;% \n           pull(Group),\n         .before = statistic) %&gt;%\n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nstatistic\np.value\n\n\n\nBT\n0.891\n0.175\n\n\nCBT\n0.959\n0.777\n\n\nNo Treatment Control\n0.826\n0.030\n\n\n\n\n\n\nJedynie w grupie No treatment nie jest zachowana wielowymiarowa normalność rozkładu analizowanych cech. Można też przeprowadzić testy normalności poszczególnych zmiennych, ale należy pamiętać, że brak podstaw do odrzucenia hipotezy o normalności brzegowych zmiennych nie jest warunkiem dostatecznym, a jedynie koniecznym.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Actions) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nActions\n0.872\n0.106\n\n\nCBT\nActions\n0.952\n0.691\n\n\nNo Treatment Control\nActions\n0.859\n0.074\n\n\n\n\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  shapiro_test(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nvariable\nstatistic\np\n\n\n\nBT\nThoughts\n0.877\n0.120\n\n\nCBT\nThoughts\n0.914\n0.310\n\n\nNo Treatment Control\nThoughts\n0.826\n0.030\n\n\n\n\n\n\nPodobnie jak w przypadku wielowymiarowym brak normalności zarysował się w grupie No treatment i to tylko dla zmiennej Thoughts. Teraz przechodzimy do testowania jednorodności kowariancji.\n\nKodbox_m(dane[,2:3], dane$Group)  %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n8.893\n0.180\n6.000\nBox's M-test for Homogeneity of Covariance Matrices\n\n\n\n\n\nNa podstawie powyższego testu można stwierdzić, iż nie ma podstaw do odrzucenia hipotezy o jednorodności macierzy kowariancji.\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Actions) \n\n[1] Group      Actions    Thoughts   is.outlier is.extreme\n&lt;0 wierszy&gt; (lub 'row.names' o zerowej długości)\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  identify_outliers(Thoughts) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nGroup\nActions\nThoughts\nis.outlier\nis.extreme\n\n\nNo Treatment Control\n4\n20\nTRUE\nFALSE\n\n\n\n\nKodwhich(dane$Actions == 4 & dane$Thoughts == 20)\n\n[1] 26\n\nKoddane %&gt;% \n  group_by(Group) %&gt;% \n  mahalanobis_distance() %&gt;% \n  filter(is.outlier==TRUE)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: Actions &lt;int&gt;, Thoughts &lt;int&gt;, mahal.dist &lt;dbl&gt;,\n#   is.outlier &lt;lgl&gt;\n\n\nIstnieje jedna obserwacja odstająca w grupie No treatment (obserwacja nr 26). Test wielowymiarowy nie wykrył żadnego elementu odstającego.\nPomimo niespełnienia założenia o wielowymiarowej normalności cech w grupach, zastosujemy test MANOVA.\nManova\n\nKodmod &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane)\nManova(mod) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.32\n4\n54\n2.56\n0.049\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Wilk\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.70\n4\n52\n2.55\n0.050\n\n\nWilks test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Roy\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.33\n2\n27\n4.52\n0.020\n\n\nRoy test statistic Anova Table (Type 2 tests)\n\n\n\n\nKodManova(mod, test = \"Hotelling\") %&gt;% \n  parameters() %&gt;% \n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.41\n4\n50\n2.55\n0.051\n\n\nHotelling-Lawley test statistic Anova Table (Type 2 tests)\n\n\n\n\nKod# model bez obserawcji odstającej\nmod2 &lt;- manova(cbind(Actions, Thoughts)~Group, data = dane[-26,])\nManova(mod2) %&gt;% \n  parameters() %&gt;%\n  print_html()\n\n\n\n\n\n\nModel Summary\n\n\nParameter\ndf\nStatistic\ndf (num.)\ndf (error)\nF\np\n\n\n\nGroup\n2\n0.36\n4\n52\n2.87\n0.032\n\n\nPillai test statistic Anova Table (Type 2 tests)\n\n\n\n\n\nAnalizują wszystkie rodzaje testów Manova, widzimy, że jedynie test Hotellinga-Laweya nie daje podstaw do odrzucenia hipotezy o równości wektorów średnich. Natomiast ponieważ co najmniej jeden z nich wskazał istotność różnic, to przyjmujemy, że są podstawy aby odrzucić hipotezę o równości wektorów średnich pomiędzy grupami. Test wykluczający obserwację odstającą również każe odrzucić hipotezę \\(H_0\\).\nPrzeprowadzimy zatem analizę brzegową.\n\nKoddane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  anova_test(value~Group) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\nActions\nGroup\n2.000\n27.000\n2.771\n0.080\n\n0.170\n\n\nThoughts\nGroup\n2.000\n27.000\n2.154\n0.136\n\n0.138\n\n\n\n\n\n\nAnaliza brzegowa pokazuje ciekawy wynik, mianowicie, dla żadnej z analizowanych cech testy brzegowe nie wykazały istotnych różnic. To pokazuje jak ważne jest stosowanie testów wielowymiarowych w kontekście porównań grup.\nPost-hoc\nPonieważ testy brzegowe ANOVA nie wykazały różnic, to testów post-hoc nie powinno się wykonywać, ale dla celów ćwiczeniowych pokażę jak je wykonać.\n\nKodpwc &lt;- dane %&gt;% \n  pivot_longer(cols = -Group) %&gt;% \n  group_by(name) %&gt;% \n  games_howell_test(value~Group)\npwc %&gt;% \n  select(-.y.) %&gt;% \n  gt() %&gt;%\n  fmt_number(columns = is.double, decimals = 3)\n\n\n\n\n\nname\ngroup1\ngroup2\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\nActions\nBT\nCBT\n1.200\n−0.544\n2.944\n0.209\nns\n\n\nActions\nBT\nNo Treatment Control\n1.300\n−0.394\n2.994\n0.148\nns\n\n\nActions\nCBT\nNo Treatment Control\n0.100\n−1.189\n1.389\n0.979\nns\n\n\nThoughts\nBT\nCBT\n−1.800\n−4.085\n0.485\n0.138\nns\n\n\nThoughts\nBT\nNo Treatment Control\n−0.200\n−2.749\n2.349\n0.978\nns\n\n\nThoughts\nCBT\nNo Treatment Control\n1.600\n−0.852\n4.052\n0.244\nns\n\n\n\n\n\n\nTesty post-hoc potwierdzają wyniki testów brzegowych ANOVA, ponieważ brakuje różnic pomiędzy poziomami zmiennych grupujących.\n\n\n\n\n\nAnderson, T. W. 1992. „Introduction to Hotelling (1931) The Generalization of Student’s Ratio”. W, 45–53. Springer New York. https://doi.org/10.1007/978-1-4612-0919-5_3.\n\n\nHotelling, Harold. 1992. „The generalization of Student’s ratio”. W, 54–65. Springer.\n\n\nHuberty, Carl J., i John D. Morris. 1989. „Multivariate Analysis Versus Multiple Univariate Analyses.” Psychological Bulletin 105 (2): 302–8. https://doi.org/10.1037/0033-2909.105.2.302.\n\n\n„Omnibus MANOVA Tests”. 1985. W, 14–39. SAGE Publications, Inc. https://doi.org/10.4135/9781412985222.d16.\n\n\nRencher, Alvin C. 1998. Multivariate statistical inference and applications. T. 635. Wiley New York.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Testy wielowymiarowe</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anderson, T. W. 1992. “Introduction to Hotelling (1931) the\nGeneralization of Student’s Ratio.” In, 45–53.\nSpringer New York. https://doi.org/10.1007/978-1-4612-0919-5_3.\n\n\nBartlett, M. S. 1951. “The Effect of Standardization on a χ 2\nApproximation in Factor Analysis.” Biometrika 38 (3/4):\n337. https://doi.org/10.2307/2332580.\n\n\nBollen, Kenneth A. 1989. “Structural Equation Models with Observed\nVariables.” Structural Equations with Latent Variables,\nApril, 80–150. https://doi.org/10.1002/9781118619179.ch4.\n\n\nCattell, Raymond B. 1966. “The Scree Test For The Number Of\nFactors.” Multivariate Behavioral Research 1 (2):\n245–76. https://doi.org/10.1207/s15327906mbr0102_10.\n\n\nEveritt, B. S., and A. Yates. 1989. “Multivariate Exploratory Data\nAnalysis: A Perspective on Exploratory Factor Analysis.”\nBiometrics 45 (1): 342. https://doi.org/10.2307/2532065.\n\n\nGrieder, Silvia, and Markus D. Steiner. 2020. “Algorithmic Jingle\nJungle: A Comparison of Implementations of Principal Axis Factoring and\nPromax Rotation in r and SPSS.” http://dx.doi.org/10.31234/osf.io/7hwrm.\n\n\nHarman, Harry H., and Wayne H. Jones. 1966. “Factor Analysis by\nMinimizing Residuals (Minres).” Psychometrika 31 (3):\n351–68. https://doi.org/10.1007/bf02289468.\n\n\nHendrickson, Alan E., and Paul Owen White. 1964. “PROMAX: A QUICK\nMETHOD FOR ROTATION TO OBLIQUE SIMPLE STRUCTURE.” British\nJournal of Statistical Psychology 17 (1): 65–70. https://doi.org/10.1111/j.2044-8317.1964.tb00244.x.\n\n\nHorn, John L. 1965. “A Rationale and Test for the Number of\nFactors in Factor Analysis.” Psychometrika 30 (2):\n179–85. https://doi.org/10.1007/bf02289447.\n\n\n———. 1969. “Harry H. Harman Modern Factor Analysis (Second\nEdition, Revised). Chicago and London: University of Chicago Press,\n1967. Pp. Xx + 474. $12.50.” Psychometrika\n34 (1): 134–38. https://doi.org/10.1017/s0033312300004580.\n\n\nHotelling, Harold. 1936. “Relations Between Two Sets of\nVariates.” Biometrika 28 (3/4): 321. https://doi.org/10.2307/2333955.\n\n\n———. 1992. “The Generalization of Student’s\nRatio.” In, 54–65. Springer.\n\n\nHuang, Yafei, and Peter M. Bentler. 2015. “Behavior of\nAsymptotically Distribution Free Test Statistics in Covariance Versus\nCorrelation Structure Analysis.” Structural Equation\nModeling: A Multidisciplinary Journal 22 (4): 489–503. https://doi.org/10.1080/10705511.2014.954078.\n\n\nHuberty, Carl J., and John D. Morris. 1989. “Multivariate Analysis\nVersus Multiple Univariate Analyses.” Psychological\nBulletin 105 (2): 302–8. https://doi.org/10.1037/0033-2909.105.2.302.\n\n\n“Introduction to Factor Analysis.” 2020. In, 1–12. SAGE\nPublications, Inc. https://doi.org/10.4135/9781544339900.n4.\n\n\nJacobucci, Ross, and Kevin J. Grimm. 2018. “Comparison of\nFrequentist and Bayesian Regularization in Structural Equation\nModeling.” Structural Equation Modeling: A Multidisciplinary\nJournal 25 (4): 639–49. https://doi.org/10.1080/10705511.2017.1410822.\n\n\nJennrich, R. I., and P. F. Sampson. 1966. “Rotation for Simple\nLoadings.” Psychometrika 31 (3): 313–23. https://doi.org/10.1007/bf02289465.\n\n\nJöreskog, Karl G., and Arthur S. Goldberger. 1972. “Factor\nAnalysis by Generalized Least Squares.” Psychometrika 37\n(3): 243–60. https://doi.org/10.1007/bf02306782.\n\n\nKaiser, Henry F. 1958. “The Varimax Criterion for Analytic\nRotation in Factor Analysis.” Psychometrika 23 (3):\n187–200. https://doi.org/10.1007/bf02289233.\n\n\n———. 1970. “A Second Generation Little Jiffy.”\nPsychometrika 35 (4): 401–15. https://doi.org/10.1007/bf02291817.\n\n\nKiers, Henk A. L. 1994. “Simplimax: Oblique Rotation to an Optimal\nTarget with Simple Structure.” Psychometrika 59 (4):\n567–79. https://doi.org/10.1007/bf02294392.\n\n\nKILIÇ, Abdullah, İbrahim UYSAL, and Burcu ATAR. 2020. “Comparison\nof Confirmatory Factor Analysis Estimation Methods on Binary\nData.” International Journal of Assessment Tools in\nEducation 7 (3): 451–87. https://doi.org/10.21449/ijate.660353.\n\n\nKyriazos, Theodoros, and Mary Poga-Kyriazou. 2023. “Applied\nPsychometrics: Estimator Considerations in Commonly Encountered\nConditions in CFA, SEM, and EFA Practice.” Psychology 14\n(05): 799–828. https://doi.org/10.4236/psych.2023.145043.\n\n\nLatan, Hengky, and Richard Noonan, eds. 2017. Partial Least Squares\nPath Modeling. Springer International Publishing. https://doi.org/10.1007/978-3-319-64069-3.\n\n\nLawley, D. N. 1940. “VI.The Estimation of Factor\nLoadings by the Method of Maximum Likelihood.” Proceedings of\nthe Royal Society of Edinburgh 60 (1): 64–82. https://doi.org/10.1017/s037016460002006x.\n\n\nLi, Cheng-Hsien. 2015. “Confirmatory Factor Analysis with Ordinal\nData: Comparing Robust Maximum Likelihood and Diagonally Weighted Least\nSquares.” Behavior Research Methods 48 (3): 936–49. https://doi.org/10.3758/s13428-015-0619-7.\n\n\n———. 2021. “Statistical Estimation of Structural Equation Models\nwith a Mixture of Continuous and Categorical Observed Variables.”\nBehavior Research Methods 53 (5): 2191–2213. https://doi.org/10.3758/s13428-021-01547-z.\n\n\nLu, Zhao-Hua, Sy-Miin Chow, and Eric Loken. 2016. “Bayesian Factor\nAnalysis as a Variable-Selection Problem: Alternative Priors and\nConsequences.” Multivariate Behavioral Research 51 (4):\n519–39. https://doi.org/10.1080/00273171.2016.1168279.\n\n\nMarriott, F. H. C., and R. Gittins. 1986. “Canonical Analysis: A\nReview with Applications in Ecology; Biomathematics, Vol. 12.”\nBiometrics 42 (1): 222. https://doi.org/10.2307/2531264.\n\n\n“Omnibus MANOVA Tests.” 1985. In, 14–39. SAGE Publications,\nInc. https://doi.org/10.4135/9781412985222.d16.\n\n\nRencher, Alvin C. 1998. Multivariate Statistical Inference and\nApplications. Vol. 635. Wiley New York.\n\n\nSchweizer, Karl, and Christine DiStefano, eds. 2016. Principles and\nMethods of Test Construction. Hogrefe Publishing. https://doi.org/10.1027/00449-000.\n\n\nSpearman, C. 1961. “\"General\nIntelligence\" Objectively Determined and Measured.”\nIn, 59–73. Appleton-Century-Crofts. https://doi.org/10.1037/11491-006.\n\n\n“Supplemental Material for The Performance of ML, DWLS, and ULS\nEstimation With Robust Corrections in Structural Equation Models With\nOrdinal Variables.” 2016. Psychological Methods. https://doi.org/10.1037/met0000093.supp.\n\n\nTarka, Piotr. 2017. “An Overview of Structural Equation Modeling:\nIts Beginnings, Historical Development, Usefulness and Controversies in\nthe Social Sciences.” Quality & Quantity 52 (1):\n313–54. https://doi.org/10.1007/s11135-017-0469-8.\n\n\nThurstone, L. L. 1931. “Multiple Factor Analysis.”\nPsychological Review 38 (5): 406–27. https://doi.org/10.1037/h0069792.\n\n\nTurney, A. H. 1939. “Factor Analysis Makes ProgressA\nStudy in Factor Analysis: The Stability of a Bi-Factor\nSolution. Karl J. Holzinger , Frances Swineford.”\nThe School Review 47 (9): 709–11. https://doi.org/10.1086/440440.\n\n\nVelicer, Wayne F. 1976. “Determining the Number of Components from\nthe Matrix of Partial Correlations.” Psychometrika 41\n(3): 321–27. https://doi.org/10.1007/bf02293557.\n\n\nWang, Xiaojing, Candace M. Kammerer, Stewart Anderson, Jiang Lu, and\nEleanor Feingold. 2008. “A Comparison of Principal Component\nAnalysis and Factor Analysis Strategies for Uncovering Pleiotropic\nFactors.” Genetic Epidemiology 33 (4): 325–31. https://doi.org/10.1002/gepi.20384.\n\n\nWright, Sewall. 1934. “The Method of Path Coefficients.”\nThe Annals of Mathematical Statistics 5 (3): 161–215. https://doi.org/10.1214/aoms/1177732676.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "cca.html",
    "href": "cca.html",
    "title": "Analiza kanoniczna",
    "section": "",
    "text": "Przypomnienie z algebry 😉\nAnaliza kanoniczna (ang. Canonical Correlation Analysis, CCA) jest klasyczną techniką statystyczną służącą do badania związków pomiędzy dwoma zestawami zmiennych wielowymiarowych. Jej podstawowym celem jest znalezienie takich kombinacji liniowych zmiennych z obu zestawów, które maksymalizują wzajemną korelację – są to tzw. kanoniczne zmienne lub kanoniczne składniki. Technika ta została wprowadzona przez Harolda Hotellinga w roku 1936, a więc w okresie intensywnego rozwoju metod statystycznych opartych na algebrze macierzy (Hotelling 1936).\nW tym samym czasie powstawały także inne fundamenty analizy wielowymiarowej, takie jak analiza składowych głównych (PCA) czy dyskryminacja liniowa (LDA)1. Analiza kanoniczna stanowi zatem jeden z filarów klasycznej statystyki wielowymiarowej i do dziś pozostaje istotnym narzędziem eksploracji i modelowania złożonych zależności.\nW odróżnieniu od regresji wielorakiej, która przewiduje zestaw zmiennych zależnych na podstawie zestawu predyktorów, analiza kanoniczna traktuje obie grupy zmiennych symetrycznie – nie zakłada istnienia wyraźnego kierunku przyczynowego. Dlatego stosuje się ją w sytuacjach, gdy celem jest ogólna analiza współzależności pomiędzy dwoma zbiorami zmiennych, a nie przewidywanie jednego zestawu na podstawie drugiego.\nTypowe zastosowania analizy kanonicznej obejmują:\nNa potrzeby definicji modeli kanonicznego potrzebne będą nam pewne twierdzenia z zakresu algebry.\nOto matematyczna definicja modelu CCA oraz dowód istnienia rozwiązania, sformułowana ściśle w duchu Twojego tekstu:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#sformułowanie-problemu-własnego",
    "href": "cca.html#sformułowanie-problemu-własnego",
    "title": "Analiza kanoniczna",
    "section": "Sformułowanie problemu własnego",
    "text": "Sformułowanie problemu własnego\nPrzekształćmy zmienne \\[\nc=\\Sigma_{XX}^{1/2}a,\\quad d=\\Sigma_{YY}^{1/2}b.\n\\]\nWówczas \\[\n\\rho(a,b)=\\frac{c^\\top\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}d}{\\sqrt{c^\\top c}\\sqrt{d^\\top d}}.\n\\]\nZ lematu Cauchy’ego–Buniakowskiego–Schwarza mamy \\[\n\\left|c^\\top \\mathbf{M} d\\right| \\le\n\\bigl(c^\\top \\mathbf{M}\\mathbf{M}^\\top c\\bigr)^{1/2}\\bigl(d^\\top d\\bigr)^{1/2},\n\\quad \\text{gdzie }\\mathbf{M}=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}.\n\\tag{5.1}\\]\nZatem \\[\n\\rho(a,b)^2 \\le\n\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}.\n\\]\nPonieważ \\(\\mathbf{M}\\mathbf{M}^\\top=\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\) jest macierzą symetryczną dodatnio określoną, z lematu Rayleigha–Ritza otrzymujemy \\[\n\\max_{c\\neq 0}\\frac{c^\\top\\mathbf{M}\\mathbf{M}^\\top c}{c^\\top c}=\\lambda_1,\n\\] gdzie \\(\\lambda_1\\) to największa wartość własna tej macierzy, osiągana dla \\(c=e_1\\) – jej wektora własnego.\nJeśli \\[\nd \\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1\n\\] to Równanie 5.1 staje się równością.\nWracając do oryginalnych współczynników \\[\na_1=\\Sigma_{XX}^{-1/2}e_1,\\quad\nb_1\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_1.\n\\]\nPierwsza korelacja kanoniczna wynosi wówczas \\[\n\\rho_1=\\sqrt{\\lambda_1}.\n\\]\nAnalogicznie dla kolejnych par, przy założeniu \\(c\\perp e_1,\\dots,e_{k-1}\\), mamy \\[\n\\rho_k=\\sqrt{\\lambda_k},\n\\] gdzie \\(\\lambda_k\\) to kolejne wartości własne macierzy \\(\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}\\), a odpowiadające im wektory własne \\(e_k\\) definiują kolejne wektory kanoniczne \\[\nU_k=e_k^\\top\\Sigma_{XX}^{-1/2}X,\\quad\nV_k=f_k^\\top\\Sigma_{YY}^{-1/2}Y,\\quad\nf_k\\propto \\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1/2}e_k.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#własności-rozwiązań",
    "href": "cca.html#własności-rozwiązań",
    "title": "Analiza kanoniczna",
    "section": "Własności rozwiązań",
    "text": "Własności rozwiązań\nDla każdej pary \\((U_k,V_k)\\) zachodzi \\[\n\\mathrm{Var}(U_k)=\\mathrm{Var}(V_k)=1,\\quad\n\\mathrm{Cov}(U_k,U_l)=\\mathrm{Cov}(V_k,V_l)=\\mathrm{Cov}(U_k,V_l)=0\\quad (k\\neq l).\n\\]\n\n\n\n\n\n\nDowód powyższych równości\n\n\n\nDla danej pary wektorów kanonicznych mamy\n\\[\nU_k = a_k^\\top X = e_k^\\top \\Sigma_{XX}^{-1/2} X,\n    \\qquad\n    V_k = b_k^\\top Y = f_k^\\top \\Sigma_{YY}^{-1/2} Y,\n\\] gdzie:\n\n\n\\(e_k\\) jest ortonormalnym6 wektorem własnym macierzy \\(\\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} \\Sigma_{XX}^{-1/2}\\)\n\n\n\\(f_k \\propto \\Sigma_{YY}^{-1/2} \\Sigma_{YX} \\Sigma_{XX}^{-1/2} e_k\\),\n\n\\(\\rho_k = \\sqrt{\\lambda_k}\\), gdzie \\(\\lambda_k\\) to odpowiadająca wartość własna.\n\nMacierze \\(\\Sigma_{XX}\\), \\(\\Sigma_{YY}\\) są dodatnio określone, więc można wprowadzić transformacje \\[\n\\tilde{X} = \\Sigma_{XX}^{-1/2}X, \\quad \\tilde{Y} = \\Sigma_{YY}^{-1/2}Y.\n\\] Zatem \\[\nU_k = e_k^\\top \\tilde{X},\\quad V_k = f_k^\\top \\tilde{Y}.\n\\]\nNajpierw udowodnimy, że \\(\\operatorname{Var}(U_k) = \\operatorname{Var}(V_k) = 1\\).\nZmienna \\(U_k = e_k^\\top \\tilde{X}\\), więc \\[\n\\operatorname{Var}(U_k) = \\operatorname{Var}(e_k^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_k.\n\\] Zauważmy, że \\[\n\\operatorname{Var}(\\tilde{X}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XX} \\Sigma_{XX}^{-1/2} = I,\n\\] więc \\[\n\\operatorname{Var}(U_k) = e_k^\\top I e_k = e_k^\\top e_k = 1.\n\\] Analogicznie \\[\n\\operatorname{Var}(V_k) = f_k^\\top \\operatorname{Var}(\\tilde{Y}) f_k = f_k^\\top f_k = 1,\n\\] ponieważ \\(\\tilde{Y}\\) ma jednostkową macierz kowariancji, a \\(f_k\\) są znormalizowane.\nTeraz dowiedziemy, że \\(\\operatorname{Cov}(U_k, U_l) = 0\\) dla \\(k \\neq l\\)\n\\[\n\\operatorname{Cov}(U_k, U_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, e_l^\\top \\tilde{X}) = e_k^\\top \\operatorname{Var}(\\tilde{X}) e_l = e_k^\\top e_l.\n\\] Ponieważ \\(e_k\\), \\(e_l\\) są ortonormalnymi wektorami własnymi symetrycznej macierzy, to \\[\ne_k^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, U_l) = 0.\n\\] Podobnie \\[\n\\operatorname{Cov}(V_k, V_l) = f_k^\\top f_l = 0 \\quad \\text{dla } k \\neq l.\n\\]\nNa koniec dowiedźmy, że \\(\\operatorname{Cov}(U_k, V_l) = 0\\) dla \\(k \\neq l\\) \\[\n\\operatorname{Cov}(U_k, V_l) = \\operatorname{Cov}(e_k^\\top \\tilde{X}, f_l^\\top \\tilde{Y}) = e_k^\\top \\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) f_l.\n\\] Z definicji \\[\n\\operatorname{Cov}(\\tilde{X}, \\tilde{Y}) = \\Sigma_{XX}^{-1/2} \\Sigma_{XY} \\Sigma_{YY}^{-1/2} =: M.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = e_k^\\top M f_l.\n\\] Z poprzednich wyprowadzeń \\[\nf_l \\propto M^\\top e_l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) \\propto e_k^\\top M M^\\top e_l.\n\\] Ale macierz \\(MM^\\top\\) jest symetryczna, a \\(e_k\\) są jej ortonormalnymi wektorami własnymi, więc \\[\ne_k^\\top MM^\\top e_l = 0 \\quad \\text{dla } k \\neq l.\n\\] Zatem \\[\n\\operatorname{Cov}(U_k, V_l) = 0.\n\\]\n\n\n6 ortonormalność wynika z niezmienniczości korelacji względem długości wektorówPonadto korelacje kanoniczne są niezmiennicze względem odwracalnych przekształceń liniowych \\(X\\) i \\(Y\\): \\[\nX^*=\\mathcal{U}^TX+u,\\quad Y^*=\\mathcal{V}^TY+v \\implies\n\\rho_i(X^*,Y^*)=\\rho_i(X,Y).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wniosek",
    "href": "cca.html#wniosek",
    "title": "Analiza kanoniczna",
    "section": "Wniosek",
    "text": "Wniosek\nPonieważ macierze kowariancji \\(\\Sigma_{XX}\\) i \\(\\Sigma_{YY}\\) są dodatnio określone, ich odwrotności istnieją. Macierze: \\[\n\\Sigma_{XX}^{-1/2}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}\\Sigma_{XX}^{-1/2},\\quad\n\\Sigma_{YY}^{-1/2}\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1/2}\n\\] są symetryczne i dodatnio określone, więc mają rzeczywiste, dodatnie wartości własne i ortonormalne wektory własne. Z lematu Rayleigha–Ritza otrzymujemy maksymalizację ilorazu Rayleigha oraz gwarancję istnienia rozwiązania. Tym samym wykazano, że pary \\((a_k,b_k)\\) istnieją, a odpowiadające im \\(\\rho_k=\\sqrt{\\lambda_k}\\) są kanonicznymi korelacjami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#hipoteza-zerowa-i-alternatywna",
    "href": "cca.html#hipoteza-zerowa-i-alternatywna",
    "title": "Analiza kanoniczna",
    "section": "Hipoteza zerowa i alternatywna",
    "text": "Hipoteza zerowa i alternatywna\nDla zbiorów zmiennych \\(X \\in \\mathbb{R}^p\\), \\(Y \\in \\mathbb{R}^q\\), testujemy\n\n\n\\(H_0: \\rho_1 = \\rho_2 = \\cdots = \\rho_s = 0\\) – brak istotnych korelacji kanonicznych (pierwiastków),\n\n\\(H_1\\): istnieje co najmniej jedna istotna korelacja kanoniczna, tj. \\(\\exists i \\leq s \\ \\text{takie, że } \\rho_i \\ne 0\\).\n\ngdzie \\(s = \\min(p, q)\\), a \\(\\rho_i\\) to \\(i\\)-ta korelacja kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#statystyka-testowa-test-wilka",
    "href": "cca.html#statystyka-testowa-test-wilka",
    "title": "Analiza kanoniczna",
    "section": "Statystyka testowa – test Wilka",
    "text": "Statystyka testowa – test Wilka\nW celu przetestowania tej hipotezy, wykorzystuje się statystykę Wilka, która bazuje na iloczynie składników postaci (\\(1 - \\lambda_i\\)), gdzie \\(\\lambda_i\\) to wartości własne odpowiadające kwadratom korelacji kanonicznych \\[\n\\lambda_i = \\rho_i^2.\n\\] Statystyka Wilkas jest zdefiniowana jako \\[\n\\Lambda = \\prod_{i=1}^s (1 - \\lambda_i).\n\\]\nInterpretacja - im mniejsze wartości \\(\\Lambda\\), tym większa zależność między zbiorami \\(X\\) i \\(Y\\). Duże wartości \\(\\lambda_i\\) (czyli silne korelacje kanoniczne) powodują, że \\(\\Lambda\\) dąży do zera.\nW praktyce, dla próby \\(n\\)-elementowej, stosujemy wersję testu bazującą na macierzach kowariancji estymowanych z próby \\(S_{XX}, S_{XY}, S_{YX}, S_{YY})\\) – odpowiedniki \\(\\Sigma_{XX}, \\Sigma_{XY}, \\Sigma_{YX}, \\Sigma_{YY}\\).\nWówczas \\[\nT^2/n = \\left|I - S_{YY}^{-1} S_{YX} S_{XX}^{-1} S_{XY} \\right| = \\prod_{i=1}^s (1 - \\hat{\\lambda}_i),\n\\] gdzie \\(\\hat{\\lambda}_i\\) to próbkowe wartości własne (szacunki \\(\\lambda_i\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#rozkład-asymptotyczny-i-transformacja-do-rozkładu-chi2",
    "href": "cca.html#rozkład-asymptotyczny-i-transformacja-do-rozkładu-chi2",
    "title": "Analiza kanoniczna",
    "section": "Rozkład asymptotyczny i transformacja do rozkładu \\(\\chi^2\\)\n",
    "text": "Rozkład asymptotyczny i transformacja do rozkładu \\(\\chi^2\\)\n\nWielu autorów (np. Marriott i Gittins (1986)) sugeruje przekształcenie statystyki Wilksa do postaci asymptotycznie zgodnej z rozkładem \\(\\chi^2\\), np. za pomocą transformacji\n\\[\n-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda) \\sim \\chi^2_{pq}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#procedura-testowania",
    "href": "cca.html#procedura-testowania",
    "title": "Analiza kanoniczna",
    "section": "Procedura testowania",
    "text": "Procedura testowania\n\nOszacuj wszystkie korelacje kanoniczne \\(\\hat{\\rho}_1, \\ldots, \\hat{\\rho}_p\\).\nOd \\(k = 0\\) do \\(p-1\\) oblicz \\(\\Lambda_k = \\prod_{i=k+1}^{p}(1 - \\hat{\\rho}_i^2)\\).\nOblicz transformację \\(\\chi^2_k=-\\left(n - \\frac{1}{2}(p + q + 1) \\right) \\cdot \\ln(\\Lambda_k)\\)\n\nPorównaj z odpowiednim kwantylem rozkładu \\(\\chi^2\\) z \\((q - k)(r - k)\\) stopniami swobody.\nJeśli wartość statystyki przekracza ten kwantyl (\\(p&lt;\\alpha\\)), odrzuć \\(H_0^{(k)}\\) i przejdź do \\(k+1\\). Jeśli nie, zatrzymaj się – kolejne korelacje uznajemy za nieistotne.\n\nOcena dopasowania modelu w analizie kanonicznej (CCA – Canonical Correlation Analysis) obejmuje kilka istotnych wskaźników diagnostycznych, które pozwalają zrozumieć siłę i strukturę relacji między dwoma zbiorami zmiennych. Poniżej omówione zostały trzy kluczowe miary: ładunki czynnikowe, wariancja wyjaśniona oraz redundancja.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#ładunki-czynnikowe-ang.-canonical-loadings",
    "href": "cca.html#ładunki-czynnikowe-ang.-canonical-loadings",
    "title": "Analiza kanoniczna",
    "section": "Ładunki czynnikowe (ang. canonical loadings)",
    "text": "Ładunki czynnikowe (ang. canonical loadings)\n\n\nDefinicja - korelacje pomiędzy zmiennymi kanonicznymi (czyli kombinacjami liniowymi wektorów \\(a_k'X\\) i \\(b_k'Y\\)) a oryginalnymi zmiennymi ze zbiorów \\(X\\) i \\(Y\\).\n\nInterpretacja:\n\nPokazują, które konkretne zmienne pierwotne w największym stopniu „ładują się” (czyli kontrybuują) na daną zmienną kanoniczną.\nWysoka wartość (np. &gt; 0.7) wskazuje na silną zależność między zmienną oryginalną a daną zmienną kanoniczną.\nZnaki dodatnie/ujemne pozwalają wnioskować o kierunku związku.\n\n\n\nWzór:\n\nDla zbioru \\(X\\): \\[\n\\text{loadings}_X = \\mathrm{Corr}(X, U_k) = \\Sigma_{XX} a_k\n\\]\n\nDla zbioru \\(Y\\): \\[\n\\text{loadings}_Y = \\mathrm{Corr}(Y, V_k) = \\Sigma_{YY} b_k\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wariancja-wyjaśniona-ang.-variance-explained",
    "href": "cca.html#wariancja-wyjaśniona-ang.-variance-explained",
    "title": "Analiza kanoniczna",
    "section": "Wariancja wyjaśniona (ang. variance explained)",
    "text": "Wariancja wyjaśniona (ang. variance explained)\n\n\nDefinicja - średnia kwadratów ładunków czynnikowych dla każdej zmiennej kanonicznej i każdego zbioru danych.\n\nInterpretacja:\n\nInformuje, jaką część wariancji oryginalnych zmiennych w danym zbiorze (\\(X\\) lub \\(Y\\)) wyjaśnia dana zmienna kanoniczna.\nMożna traktować ten wskaźnik jako odpowiednik współczynnika determinacji \\(R^2\\) dla pojedynczej zmiennej kanonicznej.\nWysoka wartość oznacza, że dana zmienna kanoniczna dobrze reprezentuje zbiór, z którego została utworzona.\n\n\n\nWzór: \\[\n\\text{Explained variance} = \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k)\n\\] gdzie \\(p\\) to liczba zmiennych w zbiorze \\(X\\), a \\(U_k\\) to \\(k\\)-ta zmienna kanoniczna.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#redundancja-ang.-redundancy-index",
    "href": "cca.html#redundancja-ang.-redundancy-index",
    "title": "Analiza kanoniczna",
    "section": "Redundancja (ang. redundancy index)",
    "text": "Redundancja (ang. redundancy index)\n\n\nDefinicja - iloczyn kwadratu korelacji kanonicznej \\(\\rho_k^2\\) oraz wariancji wyjaśnionej przez daną zmienną kanoniczną we własnym zbiorze.\n\nInterpretacja:\n\nInformuje, jaka część przeciętnej wariancji jednej grupy zmiennych jest wyjaśniana przez zmienną kanoniczną utworzoną na podstawie drugiego zbioru.\nMiara ta pokazuje, czy dany zbiór zmiennych wnosi unikalną informację o drugim zbiorze.\nWysoka redundancja oznacza, że istnieje istotny związek między strukturami dwóch zbiorów zmiennych.\n\n\n\nWzór: \\[\n\\text{Redundancy}_X = \\rho_k^2 \\cdot \\left( \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{Corr}^2(X_j, U_k) \\right)\n\\] Analogicznie definiujemy redundancję względem \\(Y\\).\n\n\n\n\n\n\n\n\nMiara\nCo opisuje\nInterpretacja praktyczna\n\n\n\nŁadunki czynnikowe\nSiłę powiązania zmiennej oryginalnej z kanoniczną\nWysoka wartość ⇒ silna reprezentacja zmiennej\n\n\nWariancja wyjaśniona\nŚrednia siła reprezentacji zbioru przez zm. kanoniczną\nMiara dopasowania struktury do zbioru\n\n\nRedundancja\nIlość informacji o jednym zbiorze zawarta w drugim\nMiara istotności relacji między zbiorami",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#normalność-wielowymiarowa",
    "href": "cca.html#normalność-wielowymiarowa",
    "title": "Analiza kanoniczna",
    "section": "Normalność wielowymiarowa",
    "text": "Normalność wielowymiarowa\nZakłada się, że obydwa zbiory zmiennych losowych – \\(X\\) i \\(Y\\) – są wspólnie rozkładem normalnym wielowymiarowym jak podano w Równanie 4.1. Normalność umożliwia stosowanie testów statystycznych (np. testu Wilksa) do oceny liczby istotnych korelacji kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-wartości-odstających-outliers",
    "href": "cca.html#brak-wartości-odstających-outliers",
    "title": "Analiza kanoniczna",
    "section": "Brak wartości odstających (outliers)",
    "text": "Brak wartości odstających (outliers)\nZarówno obserwacje odstające jednowymiarowe, jak i wielowymiarowe mogą istotnie zaburzać wynik analizy kanonicznej. Odstające wartości mogą wpływać na macierze kowariancji, zmieniając kierunki i siły relacji między zbiorami zmiennych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#wystarczająca-liczba-obserwacji",
    "href": "cca.html#wystarczająca-liczba-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "Wystarczająca liczba obserwacji",
    "text": "Wystarczająca liczba obserwacji\nLiczba obserwacji powinna znacząco przekraczać liczbę zmiennych w każdym zbiorze. Liczba obserwacji \\(n\\) w każdej grupie powinna być większa niż suma liczby zmiennych w \\(X\\) i \\(Y\\) \\[\nn &gt; p + q\n\\] Zapewnia odwracalność macierzy kowariancji oraz stabilność estymatorów.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#liniowość-zależności",
    "href": "cca.html#liniowość-zależności",
    "title": "Analiza kanoniczna",
    "section": "Liniowość zależności",
    "text": "Liniowość zależności\nZakłada się, że związki między wszystkimi parami zmiennych są liniowe. Ponieważ CCA opiera się na maksymalizacji liniowych kombinacji, nieliniowe zależności mogą pozostać niewykryte.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#brak-nadmiernej-współliniowości-multikolinearności",
    "href": "cca.html#brak-nadmiernej-współliniowości-multikolinearności",
    "title": "Analiza kanoniczna",
    "section": "Brak nadmiernej współliniowości (multikolinearności)",
    "text": "Brak nadmiernej współliniowości (multikolinearności)\nZmienne wewnątrz każdego zbioru (w \\(X\\) lub w \\(Y\\)) nie powinny być nadmiernie skorelowane. Wysoka współliniowość może prowadzić do niestabilnych i trudnych do interpretacji wektorów kanonicznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "cca.html#niezależność-obserwacji",
    "href": "cca.html#niezależność-obserwacji",
    "title": "Analiza kanoniczna",
    "section": "Niezależność obserwacji",
    "text": "Niezależność obserwacji\nKażda obserwacja powinna pochodzić od innej jednostki (brak powtórzeń pomiarów). Niezależność warunkuje poprawność estymatorów kowariancji.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analiza kanoniczna</span>"
    ]
  },
  {
    "objectID": "fa.html",
    "href": "fa.html",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "Eksploracyjna analiza czynnikowa\nAnaliza czynnikowa należy do klasy metod wielowymiarowych, których celem jest odkrywanie ukrytych struktur stojących za obserwowanymi zmiennymi. W odróżnieniu od metod takich jak analiza głównych składowych1, które opierają się na czysto algebraicznych przekształceniach danych, analiza czynnikowa ma wyraźne odniesienie do modeli statystycznych i psychometrycznych, w których zakłada się istnienie czynników latentnych – czyli zmiennych ukrytych, niewidocznych bezpośrednio, ale wpływających na wartości zmiennych obserwowalnych. Przykładem może być konstrukt „inteligencja”, który przejawia się w wynikach testów logicznych, pamięciowych czy językowych. Głównym celem analizy czynnikowej jest redukcja wymiarowości poprzez reprezentację wielu zmiennych w postaci mniejszej liczby czynników oraz lepsze zrozumienie powiązań między zmiennymi poprzez ujawnienie wspólnych źródeł ich zmienności.\nMożna wyróżnić dwa podstawowe podejścia do analizy czynnikowej. Eksploracyjna analiza czynnikowa (EFA, Exploratory Factor Analysis) jest stosowana, gdy badacz nie ma wcześniej zdefiniowanych hipotez co do liczby czynników czy struktury powiązań między nimi. Jej celem jest odkrycie potencjalnych układów zależności i zidentyfikowanie liczby czynników najlepiej opisujących dane. Konfirmacyjna analiza czynnikowa (CFA, Confirmatory Factor Analysis) jest natomiast podejściem dedukcyjnym – badacz z góry formułuje model teoretyczny (np. że pewne zmienne mierzą „pamięć roboczą”, a inne „myślenie abstrakcyjne”) i testuje jego zgodność z danymi empirycznymi. CFA jest szczególnie istotna w kontekście walidacji narzędzi badawczych, np. kwestionariuszy psychologicznych, i stanowi fundament bardziej zaawansowanych modeli strukturalnych (SEM).\nHistoria analizy czynnikowej sięga początków XX wieku i jest ściśle związana z psychometrią. Jej pionierem był Charles Spearman, który w 1904 roku zaproponował model jednoczynnikowy, interpretując zmienne poznawcze jako przejawy ogólnego czynnika inteligencji. W kolejnych dekadach metoda była rozwijana przez psychologów, takich jak Thurstone, który wprowadził koncepcję wieloczynnikową oraz przez statystyków, którzy rozwijali formalne podstawy estymacji czynników i rotacji macierzy ładunków. W latach 60. i 70. analiza czynnikowa stała się jedną z najczęściej stosowanych metod w badaniach psychologicznych i społecznych, a wraz z rozwojem informatyki zyskała na popularności także w ekonomii, biologii czy medycynie. Dziś analiza czynnikowa jest narzędziem interdyscyplinarnym, stosowanym zarówno do eksploracji struktur danych, jak i do testowania teorii opartych na zmiennych latentnych.\nFormalna postać modelu eksploracyjnej analizy czynnikowej (EFA) zakłada, że zmienne obserwowalne \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)^\\top\\) można wyrazić jako kombinację liniową czynników latentnych oraz składników specyficznych. Model przyjmuje postać („Introduction to Factor Analysis” 2020):\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\]\ngdzie:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#efa",
    "href": "fa.html#efa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) – wektor zmiennych obserwowalnych,\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) – wektor średnich,\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) – macierz ładunków czynnikowych, której element \\(\\lambda_{ij}\\) opisuje wpływ czynnika \\(j\\) na zmienną \\(i\\),\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) – wektor czynników latentnych (czynników wspólnych),\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) – wektor składników specyficznych (unikalnych, błędów pomiaru).\n\n\nZałożenia klasycznego modelu EFA\n\nRozkład czynników wspólnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne mają średnią zero i macierz kowariancji równą macierzy jednostkowej. To założenie oznacza, że czynniki są nieskorelowane i mają wariancję jednostkową (jest to standaryzacja wprowadzona dla identyfikowalności modelu).\nRozkład składników specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzą diagonalną o elementach dodatnich. Oznacza to, że błędy są nieskorelowane między sobą oraz niezależne od czynników \\(\\mathbf{f}\\).\nNiezależność czynników i błędów \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyższej konstrukcji wynika, że kowariancja zmiennych obserwowalnych jest sumą części wspólnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowód\n\n\n\nNiech losowy wektor obserwacji ma postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynników wspólnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor składników specyficznych. Zakładamy, że \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazać, że \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczególności przy \\(\\Phi=I_m\\), że mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a ponieważ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancję \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyrażamy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] Korzystając z liniowości kowariancji i tożsamości \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorów losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skończonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDzięki założeniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikają i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] Jeśli dodatkowo przyjmiemy standardyzację czynników \\(\\Phi=I_m\\) (co jest konwencją identyfikacyjną modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego należało dowieść.\nWarto odnotować, że dowód nie wymaga niezależności \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym — wystarcza nieskorelowanie, aby zniknęły składniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postać \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to można zastosować tzw. whitening czynników \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co równoważnie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), które prowadzą do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to związane z możliwością przeprowadzania różnych transformacji czynników bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogólnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeżeli dokonamy transformacji ortogonalnej czynników \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzą ortogonalną, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, że \\(\\Lambda\\) i \\(\\Phi\\) nie są jednoznacznie wyznaczone. Różne pary \\((\\Lambda, \\Phi)\\) mogą prowadzić do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczególności wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jedną z takich równoważnych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalną rolę, ponieważ jest miejscem, w którym spotykają się dwa składniki zmienności: wspólna i specyficzna. Rozkład \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, że całkowita wariancja i kowariancja obserwowanych zmiennych może być przedstawiona jako suma efektu wspólnych czynników oraz efektu specyficznego, indywidualnego dla każdej zmiennej.\nCzęść \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspólne źródło zmienności, czyli wariancję wyjaśnianą przez czynniki ukryte. To właśnie ta część umożliwia redukcję wymiaru – wiele zmiennych obserwowanych można sprowadzić do kilku czynników, które reprezentują główną strukturę zależności. Interpretacja czynników jako ukrytych wymiarów (np. inteligencja, poziom lęku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uprościć analizę, ale także nadać jej znaczenie teoretyczne w danej dziedzinie badań.\nZ kolei \\(\\Psi\\) odpowiada za wariancję unikalną, czyli tę część zmienności, która nie jest współdzielona z innymi zmiennymi. Obejmuje ona zarówno wariancję czysto specyficzną dla danej cechy, jak i wariancję błędu pomiarowego. Dzięki temu możliwe jest odróżnienie struktury głębokiej (czynnikowej) od elementów przypadkowych i indywidualnych.\nPodsumowując, znaczenie modelu czynnikowego polega na tym, że pozwala on wydzielić istotne, ukryte mechanizmy stojące za współzależnościami zmiennych i oddzielić je od szumów specyficznych dla pojedynczych obserwacji. W praktyce oznacza to możliwość redukcji liczby analizowanych zmiennych, uproszczenie opisu złożonych danych i pogłębienie interpretacji zjawisk społecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynników w praktyce opiera się przede wszystkim na analizie macierzy ładunków czynnikowych \\(\\Lambda\\). Każdy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile związku pomiędzy zmienną obserwowaną \\(x_i\\) a czynnikiem \\(f_j\\). Im wyższa wartość bezwzględna ładunku, tym większy udział danego czynnika w wyjaśnianiu zmienności konkretnej zmiennej. Na przykład w psychologii wysoki ładunek czynnika na zmiennej opisującej pamięć krótkotrwałą i na zmiennej opisującej zdolność rozwiązywania problemów matematycznych może sugerować, że obie cechy są przejawem wspólnego czynnika – inteligencji ogólnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#metody-estymacji-ładunków-czynnikowych",
    "href": "fa.html#metody-estymacji-ładunków-czynnikowych",
    "title": "Analiza czynnikowa",
    "section": "Metody estymacji ładunków czynnikowych",
    "text": "Metody estymacji ładunków czynnikowych\nMetoda największej wiarogodności (ang. Maximal Likelihood, ML) (Lawley 1940)\n\nZałożenia\nZakładamy, że wektor zmiennych obserwowalnych\n\\[\n\\mathbf{x} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\Sigma),\n\\]\ngdzie kowariancja \\(\\Sigma\\) ma postać modelową \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nDla uproszczenia przyjmuje się często, że czynniki \\(\\mathbf{f}\\) są standaryzowane i nieskorelowane, czyli \\(\\Phi = I_m\\). Wówczas macierz kowariancji ma postać\n\\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nFunkcja wiarygodności\nDla próby \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\) funkcja wiarygodności rozkładu normalnego wynosi\n\\[\nL(\\Lambda,\\Psi) = (2\\pi)^{-\\frac{np}{2}} |\\Sigma|^{-\\frac{n}{2}}\n\\exp\\left(-\\tfrac{1}{2}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{x}_i-\\mu)\\right).\n\\]\nczęściej wyrażana w postaci zlogarytmowanej\n\\[\n\\ell(\\Lambda,\\Psi) = -\\frac{n}{2} \\left[ \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) \\right] + C,\n\\]\ngdzie \\(S = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i-\\mu)(\\mathbf{x}_i-\\mu)^\\top\\) jest macierzą kowariancji z próby.\nEstymacja parametrów\nEstymatory \\(\\hat{\\Lambda}, \\hat{\\Psi}\\) dobiera się tak, aby maksymalizowały \\(\\ell(\\Lambda,\\Psi)\\), co odpowiada minimalizacji funkcji rozbieżności:\n\\[\nF(\\Lambda,\\Psi) = \\log |\\Sigma| + \\operatorname{tr}(\\Sigma^{-1} S) - \\log |S| - p.\n\\]\nPowyższa miara rozbieżności powstaje z odległości Kullbacka-Leiblera między rozkładami normalnymi \\(\\mathcal{N}_p(\\mu, \\Sigma)\\) i \\(\\mathcal{N}_p(\\mu, S)\\) i jest równa dokładnie \\(2D_{KL}(S||\\Sigma)\\).\nProcedura obliczeniowa\nW praktyce:\n\nWybiera się liczbę \\(m\\) czynników2.\nUstala się początkowe wartości \\(\\Lambda, \\Psi\\)3.\nIteracyjnie poprawia się parametry, rozwiązując równania warunków pierwszego rzędu\n\n2 wybór liczby czynników zostanie przedstawiony nieco później3 spsoby ewstępnej estymacji zostaną omówione w dalszej części\\[\n\\frac{\\partial \\ell}{\\partial \\Lambda} = 0, \\quad \\frac{\\partial \\ell}{\\partial \\Psi} = 0.\n\\]\n\nTakie postępowanie iteracyjne prowadzi się aż do zbieżności funkcji wiarygodności.\nWłasności\n\nEstymatory ML są efektywne przy spełnieniu założenia o normalności wielowymiarowej danych pierwotnych.\nUmożliwiaja testy istotności liczby czynników:\n\nHipoteza \\(H_0: \\Sigma = \\Lambda\\Lambda^\\top + \\Psi\\) vs \\(H_1: \\Sigma\\) dowolna.\nStatystyka testowa ma w przybliżeniu rozkład \\(\\chi^2\\).\n\n\nPozwalają też konstruować przedziały ufności dla ładunków czynnikowych.\nOgraniczenia\n\nWymagaja dużej próby i spełnienia założenia normalności wielowymiarowej.\nMoże być numerycznie niestabilne, zwłaszcza gdy liczba czynników jest duża w stosunku do liczby zmiennych.\nPrzy małych próbach lub silnym naruszeniu normalności wyniki mogą być obciążone.\nMetoda osi głównych (ang. Principal Axis Factoring, PAF) (Grieder i Steiner 2020)\n\nIdea metody PAF\nW metodzie PAF znanej również jako metoda czynników głównych, zakładamy klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad \\mathrm{Cov}(\\mathbf{x}) = \\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\nCelem jest znalezienie takiego \\(\\Lambda\\) i \\(\\Psi\\), aby zbliżyć się do macierzy kowariancji próbkowej \\(S\\). W odróżnieniu od ML, PAF nie opiera się na funkcji wiarygodności ani na rozbieżności Kullbacka–Leiblera, lecz maksymalizuje wariancję wspólną zmiennych, traktując część specyficzną \\((\\Psi)\\) jako resztę.\nMacierz zredukowanych korelacji\nW metodzie Principal Axis Factoring (PAF) kluczową rolę odgrywa macierz zredukowanych korelacji. Punktem wyjścia jest macierz korelacji \\(\\mathbf{R}\\) pomiędzy zmiennymi obserwowanymi \\(\\mathbf{x}\\). Na diagonali tej macierzy stoją jedynki, odzwierciedlające fakt, że każda zmienna jest w pełni skorelowana sama ze sobą. Jednak w modelu czynnikowym zakładamy, że całkowita wariancja zmiennej \\(x_j\\) może zostać podzielona na część wspólną (zasoby zmienności wspólnej - ang. communalities) i część swoistą (zasoby zmienności swoistej - ang. uniqness):\n\\[\n1 = h_j^2 + \\psi_j, \\quad j=1,\\dots,p,\n\\]\ngdzie \\(h_j^2\\) oznacza zasób zmienności wspólnej, a \\(\\psi_j\\) wariancję swoistą. W konstrukcji macierzy zredukowanych korelacji zamiast jedynek wstawia się w diagonali właśnie wartości \\(h_j^2\\). Otrzymujemy w ten sposób macierz\n\\[\n\\mathbf{R}^* = [r_{ij}^*], \\quad r_{jj}^* = h_j^2.\n\\]\nMacierz \\(\\mathbf{R}^*\\) ma więc charakter „zredukowany”, ponieważ na jej diagonali pozostaje tylko ta część wariancji zmiennej, którą model czynnikowy ma szansę wyjaśnić. Dzięki temu macierz ta może być przybliżana przez strukturę \\(\\Lambda \\Lambda^\\top\\), co odpowiada wspólnej wariancji wszystkich zmiennych.\n\n\n\n\n\n\nWstępne oszacowania zasobów zmienności wspólnej\n\n\n\nProblem polega na tym, że wartości \\(h_j^2\\) nie są znane a priori. Dlatego w praktyce stosuje się różne metody wstępnego ich wyznaczania, które mogą być następnie udoskonalane iteracyjnie w kolejnych krokach procedury PAF. Do najczęściej stosowanych metod należą:\n\nśrednia arytmetyczna współczynników korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\frac{1}{m}\\sum_{j'=1}^m r_{jj'},\\quad j\\ne j'\n\\]\nmaksymalna wartość bezwzględna współczynników korelacji danej zmiennej z innymi zmiennymi \\[\nh_j^2=\\max_{j'}|r_{jj'}|, \\quad j\\ne j',\n\\]\nwspółczynnik determinacji wielokrotnej danej zmiennej z innymi zmiennymi (najczęściej stosowana i wykorzystywana przez R) \\[\nh_j^2=R^2_{j\\cdot 1,2,\\ldots,m},\n\\]\nformuła triad \\[\nh_j^2=\\frac{r_{jj'}r_{jj''}}{r_{j'j''}}, \\quad j\\ne j' \\ne j''\n\\] gdzie \\(r_{jj'}, r_{jj''}\\) - dwie najwyższe wartości współczynników korelacji \\(j\\)-tej zmiennej z innymi zmiennymi.\n\n\n\nRozkład na wartości własne\nW metodzie PAF zakładamy, że tylko część wariancji każdej zmiennej jest wspólna. Oznacza to, że zamiast pełnej macierzy korelacji \\(\\mathbf{R}\\), rozważamy macierz zredukowanych korelacji: \\[\n\\mathbf{R}^* = \\mathbf{R} - \\Psi,\n\\] gdzie na diagonali znajdują się oszacowane zasoby zmienności wspólnej \\(\\hat{h}_j^2\\), zamiast jedynek.\nNastępnie wykonujemy dekompozycję spektralną tej macierzy: \\[\n\\mathbf{R}^* = \\mathbf{Q}^* \\mathbf{D}^* {\\mathbf{Q}^*}^\\top,\n\\] gdzie \\(\\mathbf{Q}^*\\) i \\(\\mathbf{D}^*\\) są odpowiednio wektorami i wartościami własnymi macierzy \\(\\mathbf{R}^*\\).\nEstymator ładunków czynnikowych w PAF ma więc postać \\[\n\\hat{\\Lambda} = \\mathbf{Q}^*_m (\\mathbf{D}^*_m)^{1/2},\n\\]\nbazującą na zmodyfikowanej macierzy korelacji, w której uwzględniono oszacowane komunalności.\nPonieważ \\(\\hat{h}_j^2\\) same zależą od ładunków (są ich sumą kwadratów), w praktyce stosuje się procedurę iteracyjną: zaczynamy od pewnych wartości początkowych, obliczamy dekompozycję spektralną, aktualizujemy komunalności i powtarzamy procedurę aż do zbieżności.\nIteracyjna poprawa komunalności\nPonieważ początkowe komunalności są przybliżone, PAF stosuje procedurę iteracyjną:\n\nSzacujemy \\(\\Lambda\\) na podstawie bieżącego \\(\\mathbf{R}^*\\).\nObliczamy nowe zasoby zmienności wspólnej \\(h_j^2 = \\sum_{k=1}^m \\lambda_{jk}^2\\).\nWstawiamy je na przekątnej \\(\\mathbf{R}^*\\) zamiast starych wartości.\nPowtarzamy rozkład wartości własnych.\n\nProces powtarza się aż do zbieżności, czyli stabilizacji ładunków czynnikowych i zasobów zmienności wspólnej.\nWłasności\n\n\nDopasowanie do wariancji wspólnej – PAF minimalizuje różnice pomiędzy macierzą zredukowanych korelacji \\(\\mathbf{R}^*\\) a aproksymacją \\(\\Lambda \\Lambda^\\top\\). Sskupia się na wariancji wspólnej.\n\nIteracyjność oszacowań – estymatory w PAF powstają w procesie iteracyjnym, w którym kolejne przybliżenia komunalności są poprawiane na podstawie sumy kwadratów aktualnych ładunków czynnikowych. Dzięki temu metoda zbiega do rozwiązań lepiej oddających strukturę wspólną niż proste metody jednorazowe.\n\nNiestandaryzowana postać estymatorów – rozwiązania PAF mogą zależeć od przyjętych wartości początkowych \\(h_j^2\\). Różne wybory startowe mogą prowadzić do nieco innych estymatorów, choć w praktyce po kilku iteracjach zbieżność do stabilnego rozwiązania jest zazwyczaj dobra.\n\nInterpretowalność – ponieważ oszacowane ładunki czynnikowe odzwierciedlają wyłącznie część wspólną wariancji, interpretacja czynników uzyskanych metodą PAF jest bliższa teoretycznemu modelowi czynnikowemu niż w przypadku metod opartych na PCA.\nOgraniczenia\n\n\nBrak optymalności w sensie funkcji wiarygodności – w przeciwieństwie do metody największej wiarygodności (ML), estymatory PAF nie mają znanych własności asymptotycznych, takich jak efektywność czy zgodność w sensie probabilistycznym. Są bardziej heurystyczne niż ściśle statystyczne.\n\nZależność od wartości początkowych komunalności – oszacowania początkowe wpływają na przebieg iteracji i mogą prowadzić do lokalnych minimów. W praktyce wybór metody startowej (np. \\(R^2\\), średnia korelacja, …) ma znaczenie dla szybkości i stabilności algorytmu.\n\nMożliwość uzyskania ujemnych komunalności – w niektórych przypadkach iteracje mogą prowadzić do oszacowań \\(h_j^2 &lt; 0\\) (tzw. przypadek Haywooda), co jest sprzeczne z definicją wariancji wspólnej. Wówczas konieczne stosowanie innych metod estymacji ładunków.\n\nMniejsza przydatność przy małych próbach – ponieważ metoda nie opiera się na pełnym modelu statystycznym, jej własności są mniej stabilne przy niewielkich licznościach obserwacji. Wyniki mogą być wówczas silnie zależne od przypadkowych fluktuacji w danych.\n\nBrak testów statystycznych dopasowania modelu – w odróżnieniu od metody ML, PAF nie pozwala na formalne testowanie hipotez o liczbie czynników czy jakości dopasowania modelu do danych.\nMetoda sładowych głównych (ang. Principal Component Method) (Wang i in. 2008)\n\nMetoda sładowych głównych należy do klasy metod wspólnotowych, czyli takich, które zakładają klasyczny model czynnikowy\n\\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon},\n\\quad \\Sigma = \\Lambda\\Lambda^\\top + \\Psi.\n\\]\nCelem jest oszacowanie macierzy ładunków \\(\\Lambda\\), tak aby jak najlepiej odtworzyć część wspólną wariancji.\nIdea metody\nW metodzie PCM zakładamy, że cała wariancja zmiennej jest wariancją wspólną, tzn. \\[\nh_j^2 = 1, \\quad j=1,\\ldots,p.\n\\]\nOznacza to, że macierz zredukowanych korelacji jest po prostu zwykłą macierzą korelacji \\(\\mathbf{R}\\): \\[\n\\mathbf{R} = \\Lambda \\Lambda^\\top + \\Psi,\n\\] przy czym w PCM przyjmujemy \\(\\Psi = \\mathbf{0}\\).\nNastępnie wykonujemy dekompozycję spektralną \\[\n\\mathbf{R} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^\\top,\n\\] gdzie:\n\n\n\\(\\mathbf{Q} = (q_1, q_2, \\ldots, q_p)\\) – to macierz ortonormalnych wektorów własnych,\n\n\\(\\mathbf{D} = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p)\\) – to macierz wartości własnych uporządkowanych malejąco.\n\nJeśli chcemy oszacować model z \\(m\\) czynnikami, to bierzemy największe \\(m\\) wartości własne i odpowiadające im wektory własne. Estymator ładunków czynnikowych jest wtedy równy \\[\n\\hat{\\Lambda} = \\mathbf{Q}_m \\mathbf{D}_m^{1/2},\n\\] gdzie \\(\\mathbf{Q}_m = (q_1,\\ldots,q_m)\\), a \\(\\mathbf{D}_m = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_m)\\).\nWidzimy więc, że w PCM ładunki są wprost pierwiastkami z największych wartości własnych pomnożonymi przez odpowiadające im wektory własne.\nProcedura estymacji4\n\n\nKonstruujemy macierz korelacji \\(\\mathbf{R}\\).\nObliczamy rozkład wartości i wektorów własnych macierzy \\(\\mathbf{R}\\).\nWybieramy \\(m\\) największych wartości własnych (odpowiadających liczbie czynników w modelu).\nNa tej podstawie konstruujemy macierz ładunków czynnikowych \\(\\Lambda\\).\n4 tu widać największą różnicę pomięcy PCM a PAF; w metodzie PCM występuję jedna iteracja estymacji ładunkówWłasności\n\n\nZgodność z modelem czynnikowym – metoda dąży do aproksymacji struktury wspólnej wariancji, a nie całkowitej wariancji.\n\nZbieżność do stabilnych oszacowań – iteracyjne poprawki komunalności pozwalają uzyskać estymatory spójne z założeniami modelu.\n\nŁatwość interpretacji – podobnie jak PCA, metoda bazuje na analizie spektralnej wartości własnych, co ułatwia intuicyjne rozumienie struktury danych.\nOgraniczenia\n\n\nBrak optymalności statystycznej – podobnie jak PAF, metoda nie ma własności estymatorów opartych na funkcji wiarygodności (ML).\n\nZależność od początkowych oszacowań komunalności – nieprawidłowy wybór startowy może utrudnić uzyskanie sensownych rozwiązań.\n\nHaywood case – zdarza się, że zasoby zmienności wspólnej mogą przyjmować wartości ujemne.\nMetoda minimalizacji reszt (ang. MINRES) (Harman i Jones 1966)\n\nIdea metody MINRES\nW modelu czynnikowym przyjmujemy, że macierz kowariancji (lub korelacji) ma postać \\[\n\\Sigma = \\Lambda \\Lambda' + \\Psi,\n\\] gdzie \\(\\Lambda\\) to macierz ładunków czynnikowych, a \\(\\Psi = \\mathrm{diag}(\\psi_1,\\ldots,\\psi_p)\\) to macierz wariancji swoistych.\nW metodzie MINRES nie próbujemy dokładnie odtworzyć całej macierzy \\(\\Sigma\\). Zamiast tego minimalizujemy reszty pozadiagonalne, czyli różnice między obserwowaną macierzą korelacji \\(\\mathbf{R}\\) a macierzą odtworzoną z modelu \\(\\Lambda \\Lambda^\\top + \\Psi\\), przy czym skupiamy się wyłącznie na elementach pozadiagonalnych.\nFunkcja kryterialna\nFormalnie minimalizowana jest suma kwadratów reszt poza przekątną \\[\nF(\\Lambda, \\Psi) = \\sum_{i \\neq j} \\Big( r_{ij} - \\hat{r}_{ij} \\Big)^2,\n\\] gdzie:\n\n\n\\(r_{ij}\\) to element macierzy korelacji empirycznej \\(\\mathbf{R}\\),\n\n\\(\\hat{r}_{ij}\\) to element macierzy odtworzonej \\(\\Lambda \\Lambda^\\top + \\Psi\\),\nelementy diagonalne nie są uwzględniane (bo zawsze odtwarzane są przez normalizację zmiennych).\n\nMożna to zapisać równoważnie jako \\[\nF(\\Lambda) = | \\mathbf{R} - (\\Lambda \\Lambda' + \\Psi)|^2_{off},\n\\] gdzie \\(|\\cdot|_{off}\\) oznacza normę Frobeniusa liczona tylko na częściach pozadiagonalnych macierzy.\nProcedura estymacyjna\n\nZaczynamy od przybliżonych wartości komunalności \\(\\hat{h}_j^2\\), tak jak w PAF.\nBudujemy macierz reszt \\[\n\\mathbf{U} = \\mathbf{R} - (\\Lambda \\Lambda^\\top + \\Psi).\n\\]\n\nSzukamy takich ładunków \\(\\Lambda\\), które minimalizują sumę kwadratów elementów \\(\\mathbf{U}\\) poza przekątną.\nW praktyce problem redukuje się do iteracyjnego rozwiązywania układów równań własnych, bardzo podobnie jak w PAF, ale z innym warunkiem minimalizacji (PAF dopasowuje wartości własne macierzy zredukowanych korelacji, MINRES – reszty pozadiagonalne).\nZwiązek z dekompozycją spektralną\nW przeciwieństwie do PCM czy PAF, metoda MINRES nie ma bezpośredniego prostego rozwiązania w postaci pierwiastków z wartości własnych. Wymaga zastosowania iteracyjnych algorytmów numerycznych, które szukają \\(\\Lambda\\) minimalizującej \\(F(\\Lambda)\\). Jednak podobnie jak w PAF, punktem startowym mogą być wektory własne macierzy zredukowanych korelacji. Następnie algorytm minimalizacji dopasowuje ładunki tak, by reszty pozadiagonalne były jak najmniejsze.\nWłaściwości i ograniczenia\n\n\nMINRES skupia się tylko na korelacjach pomiędzy zmiennymi, ignorując elementy diagonalne – co sprawia, że estymacja jest mniej wrażliwa na problem ujemnych komunalności (tzw. Heywood cases).\nMetoda jest relatywnie stabilna numerycznie i dobrze sprawdza się przy dużej liczbie zmiennych.\nOgraniczeniem jest to, że wynik zależy od jakości początkowych oszacowań zasobów zmienności wspólnej. Przy złym wyborze startu możliwa jest wolna zbieżność albo zbieżność do lokalnego minimum.\nMetoda uogólnionych najmniejszych kwadratów (ang. Generalized Least Squares, GLS) (Jöreskog i Goldberger 1972)\n\nIdea metody\nGLS, podobnie jak MINRES czy ML, polega na porównaniu macierzy obserwowanej \\(\\mathbf{S}\\) (kowariancji lub korelacji) z macierzą odtworzoną przez model czynnikowy \\(\\hat{\\Sigma} = \\Lambda \\Lambda^\\top + \\Psi\\). Różnica w stosunku do MINRES polega na tym, że w GLS ważymy reszty, czyli błędy odwzorowania poszczególnych elementów macierzy \\(\\mathbf{S}\\).\nFormalnie kryterium minimalizacji ma postać \\[\nF_{\\text{GLS}}(\\Lambda, \\Psi) = \\mathrm{tr}\\Big[ \\big( S - \\hat{\\Sigma} \\big) W \\big( S - \\hat{\\Sigma} \\big) W \\Big],\n\\]\ngdzie \\(W\\) to macierz wag, zwykle przyjmowana jako odwrotność (lub pseudoodwrotność) wariancji estymatora elementów macierzy \\(\\mathbf{S}\\).\nW przeciwieństwie do MINRES (gdzie wszystkie reszty traktowane są jednakowo), w GLS różne elementy macierzy kowariancji otrzymują różne wagi. Wagi te wynikają z asymptotycznych własności estymatora macierzy kowariancji i uwzględniają fakt, że elementy macierzy nie są niezależne i mają różne wariancje. Dzięki temu GLS jest bardziej efektywny statystycznie niż MINRES, ale jednocześnie mniej wymagający niż ML (który zakłada pełną normalność wielowymiarową).\nWłasności\n\nEstymatory GLS są spójne i asymptotycznie efektywne w klasie metod najmniejszych kwadratów, przy założeniu poprawnej specyfikacji modelu.\nGLS, podobnie jak ML, uwzględnia strukturę wariancji elementów macierzy \\(\\mathbf{S}\\), co czyni go bardziej precyzyjnym niż MINRES.\nZ drugiej strony GLS jest mniej czuły na naruszenie założenia normalności niż ML, dlatego bywa rekomendowany przy większych odchyleniach od normalności.\nOgraniczenia\n\nProcedura GLS jest obliczeniowo trudniejsza niż MINRES, ponieważ wymaga oszacowania (lub przyjęcia) odpowiedniej macierzy wag.\nW praktyce GLS bywa niestabilny przy małych próbach lub przy silnych współliniowościach zmiennych.\nW implementacjach programowych często stosuje się GLS jako kompromis pomiędzy prostym MINRES a wymagającym ML.\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nIstnieją również inne metody estymacji ładunków czynnikowych, jak metody bayesowskie (Lu, Chow, i Loken 2016), czy metody z regularyzacją LASSO ale nie są one częścią tego opracowania (Jacobucci i Grimm 2018).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynników",
    "href": "fa.html#oceny-dopasowania-modelu-i-kryteria-doboru-liczby-czynników",
    "title": "Analiza czynnikowa",
    "section": "Oceny dopasowania modelu i kryteria doboru liczby czynników",
    "text": "Oceny dopasowania modelu i kryteria doboru liczby czynników\nOcena dopasowania modelu EFA opiera się na kilku uzupełniających się perspektywach: globalnym dopasowaniu implikowanej macierzy kowariancji do macierzy empirycznej, analizie reszt korelacyjnych, doborze liczby czynników, stabilności rozwiązania oraz jakości lokalnej (ładunki i zasoby zmienności wspólnej). Poniżej przedstawiam najważniejsze procedury wraz z ich interpretacją oraz typowymi pułapkami.\nProporcja wyjaśnionej wariancji przez czynniki\nProporcja wariancji wyjaśnionej przez model czynnikowy, czyli stosunek sumy wariancji wspólnej do całkowitej wariancji wszystkich zmiennych, stanowi podstawową miarę jakości dopasowania. W przypadku standaryzowanych zmiennych całkowita wariancja wynosi \\(p\\), więc proporcja ta ma postać \\[\n\\text{Proporcja wyjaśnionej wariancji} = \\frac{\\sum_{j=1}^p h_j^2}{p}.\n\\] Wyższe wartości (np. powyżej \\(0,6\\)) wskazują na dobrą reprezentację zmiennych przez czynniki, natomiast niskie wartości (np. poniżej \\(0,4\\)) sugerują, że model nie uchwytuje istotnej części struktury danych. Jednak sama proporcja nie uwzględnia liczby czynników ani złożoności modelu, dlatego powinna być interpretowana w kontekście innych wskaźników dopasowania.\nTest chi-kwadrat\nW metodzie ML został przedstawiony test dopasowania oparty na maximum likelihood. Przy założeniu normalności wielowymiarowej i zidentyfikowanym modelu postaci \\[\n\\Sigma=\\Lambda\\Lambda^\\top + \\Psi\n\\] testujemy hipotezę \\(H_0:\\ \\Sigma(\\Lambda,\\Psi)=S\\) w populacji, gdzie \\(S\\) oznacza macierz kowariancji (lub korelacji) z próby. Statystyka \\(\\chi^2\\) rośnie wraz z pogarszającym się dopasowaniem (niestety duże próby sprzyjają odrzucaniu nawet dobrze dopasowanych modeli, a naruszenia normalności mogą zawyżać lub zaniżać wynik).\nWskaźnik RMSEA\nWskaźnik root mean square error of approximation (RMSEA) mierzy błąd aproksymacji na jednostkę stopnia swobody i można go interpretować jako „błąd w populacji”, nie tylko w próbie. Definiujemy go jako \\[\n\\mathrm{RMSEA}=\\sqrt{\\max\\left\\{\\frac{\\chi^2-df}{df(n-1)},0\\right\\}},\n\\] a ocenę uzupełniamy o przedział ufności oparty na niecentralnym rozkładzie chi-kwadrat. Wartości rzędu \\(0,05-0,08\\) tradycyjnie uznawane są za akceptowalne, traktując progi orientacyjnie: wzrost liczby zmiennych i stopni swobody sprzyja niższym RMSEA, natomiast małe próby destabilizują oszacowanie.\nAnaliza reszt\nAnaliza reszt macierzy korelacji stanowi podstawową kontrolę lokalnego dopasowania, niezależnie od sposobu estymacji. Wyznaczamy reszty \\(r_{ij}-\\hat r_{ij}\\) i przeglądamy rozkład wartości bezwzględnych, a dokładnie odsetek przekraczających praktyczne progi (np. \\(0,05\\)). Wskaźniki zbiorcze, takie jak RMSR (root mean square residual) oraz SRMR (standardized RMSR), agregują wielkość reszt poza diagonalą - mniejsze wartości świadczą o lepszym dopasowaniu. Mapa ciepła reszt ułatwia wykrywanie klastrów niedopasowania sugerujących brakujący czynnik lub zbyt małą liczbę czynników.\nKryteria informacyjne\nKryteria informacyjne, takie jak AIC i BIC, służą do porównywania modeli o różnej liczbie czynników, karząc nadmierną złożoność. Definiujemy je przez logarytm funkcji wiarogodności i liczbę parametrów. BIC silniej faworyzuje prostsze modele przy dużych próbach. Bardzo ważne jest aby używać tych metod do porównywania modeli otrzymanych tą samą metodą.\nInne wskaźniki dopasowania\nWskaźniki „globalne” starszej generacji, takie jak GFI i AGFI (goodness of fit index, adjusted GFI), oceniają proporcję wariancji/kowariancji wyjaśnionej przez model. Są wrażliwe na rozmiar próby i liczbę zmiennych, skłonne do optymizmu w dużych modelach i do pesymizmu przy małej liczbie stopni swobody. Możemy je traktować pomocniczo, kładąc większy nacisk na RMSEA oraz analizę reszt.\nAnaliza wartości własnych macierzy reszt uzupełnia powyższe podejścia. Po wyodrębnieniu \\(m\\) czynników obliczamy resztową macierz korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) i badać jej wartości własne. Duże dodatnie wartości własne sygnalizują pozostawioną wspólną wariancję (niedomiar czynników) lub struktury lokalne.\nJakość lokalną rozwiązania oceniać przez zasoby zmienności wspólnej i swoistej. \\[\nh_j^2=\\sum_{k=1}^{m}\\lambda_{jk}^{2}\n\\] mierzą część wariancji zmiennej \\(x_j\\) wyjaśnioną przez czynniki, bardzo niskie \\(h_j^2\\) wskazują słabą reprezentację zmiennej, natomiast bardzo wysokie — wraz z ryzykiem ujemnych \\(\\Psi_j\\) (przypadki Haywooda) — mogą sygnalizować dopasowanie wymuszone lub niewłaściwą liczebność czynników. Sumy kwadratów ładunków per czynnik odzwierciedlają wyjaśnioną wspólną wariancję i służą do oceny równomierności wkładu czynników.\nW rozwiązaniach dopuszczajacych korelacje pomiedzy czynnikami dodatkowym aspektem dopasowania jest macierz korelacji czynników \\(\\Phi\\). Bardzo wysokie korelacje między czynnikami sugerują nadmiarowość i potencjalne przeparametryzowanie. Wówczas warto rozważyć redukcję liczby czynników lub alternatywne struktury.\nNajbardziej znane kryteria doboru liczby czynników to:\nKryterium wykresu osypiska (Scree plot, Cattell (1966))\nNa osi poziomej odkładamy kolejne wartości własne, a na pionowej ich wielkość. Punktem granicznym jest miejsce, gdzie wykres „załamuje się” i przechodzi w „osypisko” – od tego miejsca czynniki interpretowane są jako szum.\n\nZalety: wizualna intuicja, łatwe zastosowanie.\nWady: często subiektywność w określeniu miejsca „łokcia”, szczególnie gdy krzywa nie ma wyraźnego załamania.\nAnaliza równoległa (Parallel analysis, Horn (1965))\nPolega na porównaniu wartości własnych dla danych empirycznych z wartościami własnymi uzyskanymi dla danych losowych o tej samej strukturze (ta sama liczba zmiennych i obserwacji). Zatrzymuje się te czynniki, których wartości własne przewyższają np. 95. percentyl rozkładu wartości losowych.\n\nZalety: jedna z najbardziej rekomendowanych metod, dobrze sprawdza się w praktyce.\nWady: wymaga procedur symulacyjnych, większej mocy obliczeniowej.\nKryterium MAP (Minimum Average Partial, Velicer (1976))\nOpiera się na analizie korelacji cząstkowych. Stopniowo usuwa się kolejne czynniki, a następnie oblicza średnią wartość kwadratu korelacji cząstkowych. Liczba czynników odpowiadająca minimum tej wartości uznawana jest za optymalną.\n\nZalety: metoda oparta na minimalizacji resztowych zależności, obiektywna.\nWady: wrażliwa na naruszenia założeń modelu, mniej intuicyjna dla początkujących.\nTesty statystyczne dopasowania (dla ML)\nPrzy estymacji metodą największej wiarygodności można zastosować test chi-kwadrat dla porównania modelu z \\(m\\) czynnikami z modelem pełnym. Sprawdza się, czy macierz implikowana przez model różni się istotnie od empirycznej. Liczbę czynników dobiera się tak, aby model był jeszcze akceptowalny, ale nie przeparametryzowany.\n\nZalety: formalne podejście statystyczne.\nWady: silna wrażliwość na liczność próby i założenie normalności wielowymiarowej; w dużych próbach nawet dobre modele mogą być odrzucane.\nKryteria informacyjne (AIC, BIC, CAIC)\nPorównują modele o różnej liczbie czynników, równoważąc dopasowanie (log-wiarygodność) i złożoność (liczbę parametrów). Optymalna liczba czynników to ta, dla której wartość kryterium jest minimalna.\n\nZalety: uwzględniają karę za nadmierną złożoność, dobrze sprawdzają się w porównaniach.\nWady: wartości kryteriów są zależne od metody estymacji, więc porównywać można tylko modele oszacowane tą samą metodą.\nAnaliza reszt i spektrum wartości własnych macierzy reszt\nPo przyjęciu liczby czynników oblicza się macierz reszt korelacji \\(\\mathbf{R}-\\hat{\\mathbf{R}}\\) . Jeśli w resztach (poza przekątną) pozostają duże (co do wartości bezwzględnej) wartości własne, oznacza to, że nie wszystkie wspólne zależności zostały uchwycone i potrzebne są dodatkowe czynniki.\n\nZalety: pozwala ocenić niedopasowanie „lokalne” i strukturalne.\nWady: wymaga bardziej zaawansowanej interpretacji.\nUdział wyjaśnionej wariancji\nW praktyce często wymaga się, aby całkowita wyjaśniona wariancja przekraczała określony próg (np. 50% w naukach społecznych). Dodatkowo analizuje się równomierność wkładu poszczególnych czynników.\n\nZalety: intuicyjne i łatwe do raportowania.\nWady: arbitralne progi, zależne od liczby zmiennych i kontekstu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#eksploracyjna-analiza-czynnikowa",
    "href": "fa.html#eksploracyjna-analiza-czynnikowa",
    "title": "Analiza czynnikowa",
    "section": "",
    "text": "\\(\\mathbf{x} \\in \\mathbb{R}^p\\) – wektor zmiennych obserwowalnych,\n\n\\(\\boldsymbol{\\mu} \\in \\mathbb{R}^p\\) – wektor średnich,\n\n\\(\\Lambda \\in \\mathbb{R}^{p \\times m}\\) – macierz ładunków czynnikowych, której element \\(\\lambda_{ij}\\) opisuje wpływ czynnika \\(j\\) na zmienną \\(i\\),\n\n\\(\\mathbf{f} \\in \\mathbb{R}^m\\) – wektor czynników latentnych (czynników wspólnych),\n\n\\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^p\\) – wektor składników specyficznych (unikalnych, błędów pomiaru).\n\nZałożenia klasycznego modelu EFA\n\nRozkład czynników wspólnych \\[\n\\mathbb{E}[\\mathbf{f}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\mathbf{f}) = \\Phi = I_m,\n\\] czyli czynniki latentne mają średnią zero i macierz kowariancji równą macierzy jednostkowej. To założenie oznacza, że czynniki są nieskorelowane i mają wariancję jednostkową (jest to standaryzacja wprowadzona dla identyfikowalności modelu).\nRozkład składników specyficznych \\[\n\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}, \\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon}) = \\Psi,\n\\] gdzie \\(\\Psi\\) jest macierzą diagonalną o elementach dodatnich. Oznacza to, że błędy są nieskorelowane między sobą oraz niezależne od czynników \\(\\mathbf{f}\\).\nNiezależność czynników i błędów \\[\n\\mathrm{Cov}(\\mathbf{f}, \\boldsymbol{\\epsilon}) = 0.\n\\]\nMacierz kowariancji zmiennych obserwowalnych\n\nZ powyższej konstrukcji wynika, że kowariancja zmiennych obserwowalnych jest sumą części wspólnej i specyficznej: \\[\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\n\n\n\n\n\nDowód\n\n\n\nNiech losowy wektor obserwacji ma postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{f}\\) to wektor czynników wspólnych, a \\(\\boldsymbol{\\epsilon}\\) to wektor składników specyficznych. Zakładamy, że \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\mathbf{f})=\\Phi,\\] \\[\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0},\\quad \\operatorname{Cov}(\\boldsymbol{\\epsilon})=\\Psi\\] oraz \\[\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}.\\] Celem jest wykazać, że \\(\\Sigma:=\\operatorname{Cov}(\\mathbf{x})=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), a w szczególności przy \\(\\Phi=I_m\\), że mamy \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\).\nZaczynamy od wycentrowania wektora \\(\\mathbf{x}\\), a ponieważ \\(\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}\\) i \\(\\mathbb{E}[\\boldsymbol{\\epsilon}]=\\mathbf{0}\\), to \\(\\mathbb{E}[\\mathbf{x}]=\\boldsymbol{\\mu}\\), zatem \\(\\mathbf{x}-\\boldsymbol{\\mu}=\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}\\).\nKowariancję \\(\\Sigma=\\operatorname{Cov}(\\mathbf{x})\\) wyrażamy jako \\[\n\\Sigma=\\operatorname{Cov}(\\mathbf{x}-\\boldsymbol{\\mu})=\\operatorname{Cov}(\\Lambda\\mathbf{f}+\\boldsymbol{\\epsilon}).\n\\] Korzystając z liniowości kowariancji i tożsamości \\(\\operatorname{Cov}(A\\mathbf{u}+B\\mathbf{v})=A\\operatorname{Cov}(\\mathbf{u})A^\\top+B\\operatorname{Cov}(\\mathbf{v})B^\\top+A\\operatorname{Cov}(\\mathbf{u}\\mathbf{v})B^\\top+B\\operatorname{Cov}(\\mathbf{v}\\mathbf{u})A^\\top\\) dla dowolnych macierzy \\(A,B\\) i wektorów losowych \\(\\mathbf{u},\\,\\mathbf{v}\\) o skończonych wariancjach. W naszym przypadku \\(A=\\Lambda\\), \\(\\mathbf{u}=\\mathbf{f}\\), \\(B=I_p\\), \\(\\mathbf{v}=\\boldsymbol{\\epsilon}\\).\nDzięki założeniu nieskorelowania \\(\\operatorname{Cov}(\\mathbf{f},\\boldsymbol{\\epsilon})=\\mathbf{0}\\) wyrazy mieszane znikają i pozostaje \\[\n\\Sigma=\\Lambda\\operatorname{Cov}(\\mathbf{f})\\Lambda^\\top + I_p\\operatorname{Cov}(\\boldsymbol{\\epsilon})I_p^\\top\n=\\Lambda\\Phi\\Lambda^\\top + \\Psi.\n\\] Jeśli dodatkowo przyjmiemy standardyzację czynników \\(\\Phi=I_m\\) (co jest konwencją identyfikacyjną modelu EFA), to otrzymujemy \\[\n\\Sigma=\\Lambda\\Lambda^\\top+\\Psi,\n\\] czego należało dowieść.\nWarto odnotować, że dowód nie wymaga niezależności \\(\\mathbf{f}\\) i \\(\\boldsymbol{\\epsilon}\\) w sensie probabilistycznym — wystarcza nieskorelowanie, aby zniknęły składniki mieszane. Ponadto w wersji niestandardowej, gdy \\(\\Phi\\neq I_m\\), model przyjmuje postać \\(\\Sigma=\\Lambda\\Phi\\Lambda^\\top+\\Psi\\), to można zastosować tzw. whitening czynników \\(\\tilde{\\mathbf{f}}=\\Phi^{1/2}\\mathbf{z}\\) z \\(\\operatorname{Cov}(\\mathbf{z})=I_m\\), co równoważnie prowadzi do \\(\\tilde{\\Lambda}=\\Lambda\\Phi^{1/2}\\) i standardowej formy \\(\\Sigma=\\tilde{\\Lambda}\\tilde{\\Lambda}^\\top+\\Psi\\).\nReprezentacja macierzy kowariancji \\(\\Sigma\\) w postaci \\(\\Lambda\\Phi\\Lambda^\\top+\\Psi\\) nie jest unikatowa. Istnieje wiele par \\(\\Lambda, \\Phi\\), które prowadzą do tej samej macierzy kowariancji \\(\\Sigma\\). Jest to związane z możliwością przeprowadzania różnych transformacji czynników bez zmiany struktury kowariancji zmiennych obserwowalnych.\nFormalnie:\n\nW wersji ogólnej mamy \\[\n\\Sigma = \\Lambda \\Phi \\Lambda^\\top + \\Psi.\n\\]\nJeżeli dokonamy transformacji ortogonalnej czynników \\(\\mathbf{f}^* = Q \\mathbf{f}\\), gdzie \\(Q\\) jest macierzą ortogonalną, to: \\[\n\\Lambda \\mathbf{f} = (\\Lambda Q^\\top) (Q\\mathbf{f}) = \\Lambda^* \\mathbf{f}^*,\n\\] przy czym \\[\n\\Lambda^* = \\Lambda Q^\\top, \\quad \\Phi^* = Q \\Phi Q^\\top.\n\\] Wtedy dalej mamy \\[\n\\Sigma = \\Lambda^* \\Phi^* \\Lambda^{*\\top} + \\Psi.\n\\]\nTo pokazuje, że \\(\\Lambda\\) i \\(\\Phi\\) nie są jednoznacznie wyznaczone. Różne pary \\((\\Lambda, \\Phi)\\) mogą prowadzić do tej samej macierzy kowariancji \\(\\Sigma\\).\nW szczególności wprowadzenie wektora \\(z\\) (o kowariancji jednostkowej) i zapisanie modelu jako \\[\n\\Sigma = \\tilde{\\Lambda}\\tilde{\\Lambda}^\\top + \\Psi\n\\] jest jedną z takich równoważnych reprezentacji.\n\n\n\nMacierz kowariancji \\(\\Sigma\\) w analizie czynnikowej odgrywa fundamentalną rolę, ponieważ jest miejscem, w którym spotykają się dwa składniki zmienności: wspólna i specyficzna. Rozkład \\(\\Sigma = \\Lambda \\Lambda^\\top + \\Psi\\) oznacza, że całkowita wariancja i kowariancja obserwowanych zmiennych może być przedstawiona jako suma efektu wspólnych czynników oraz efektu specyficznego, indywidualnego dla każdej zmiennej.\nCzęść \\(\\Lambda \\Lambda^\\top\\) reprezentuje wspólne źródło zmienności, czyli wariancję wyjaśnianą przez czynniki ukryte. To właśnie ta część umożliwia redukcję wymiaru – wiele zmiennych obserwowanych można sprowadzić do kilku czynników, które reprezentują główną strukturę zależności. Interpretacja czynników jako ukrytych wymiarów (np. inteligencja, poziom lęku, satysfakcja zawodowa, czy cechy rynku finansowego) pozwala nie tylko uprościć analizę, ale także nadać jej znaczenie teoretyczne w danej dziedzinie badań.\nZ kolei \\(\\Psi\\) odpowiada za wariancję unikalną, czyli tę część zmienności, która nie jest współdzielona z innymi zmiennymi. Obejmuje ona zarówno wariancję czysto specyficzną dla danej cechy, jak i wariancję błędu pomiarowego. Dzięki temu możliwe jest odróżnienie struktury głębokiej (czynnikowej) od elementów przypadkowych i indywidualnych.\nPodsumowując, znaczenie modelu czynnikowego polega na tym, że pozwala on wydzielić istotne, ukryte mechanizmy stojące za współzależnościami zmiennych i oddzielić je od szumów specyficznych dla pojedynczych obserwacji. W praktyce oznacza to możliwość redukcji liczby analizowanych zmiennych, uproszczenie opisu złożonych danych i pogłębienie interpretacji zjawisk społecznych, psychologicznych, biologicznych czy ekonomicznych.\nInterpretacja czynników w praktyce opiera się przede wszystkim na analizie macierzy ładunków czynnikowych \\(\\Lambda\\). Każdy element \\(\\lambda_{ij}\\) tej macierzy informuje o sile związku pomiędzy zmienną obserwowaną \\(x_i\\) a czynnikiem \\(f_j\\). Im wyższa wartość bezwzględna ładunku, tym większy udział danego czynnika w wyjaśnianiu zmienności konkretnej zmiennej. Na przykład w psychologii wysoki ładunek czynnika na zmiennej opisującej pamięć krótkotrwałą i na zmiennej opisującej zdolność rozwiązywania problemów matematycznych może sugerować, że obie cechy są przejawem wspólnego czynnika – inteligencji ogólnej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#rotacje-czynników",
    "href": "fa.html#rotacje-czynników",
    "title": "Analiza czynnikowa",
    "section": "Rotacje czynników",
    "text": "Rotacje czynników\nRotacja czynników jest etapem analizy czynnikowej, którego celem jest poprawa interpretowalności rozwiązania poprzez uproszczenie struktury ładunków czynnikowych. Matematycznie polega ona na zastosowaniu transformacji liniowej do macierzy ładunków \\(\\Lambda\\). Jeśli \\(\\Lambda\\) jest macierzą \\(p \\times m\\) ładunków (gdzie \\(p\\) to liczba zmiennych, a \\(m\\) liczba czynników), to po rotacji otrzymujemy nową macierz ładunków \\[\n\\Lambda^* = \\Lambda T,\n\\] gdzie \\(T\\) jest macierzą transformacji rotacyjnej o wymiarach \\(m \\times m\\). W zależności od własności macierzy \\(T\\) wyróżnia się dwa główne typy rotacji: ortogonalne i skośne (oblique).\nRotacje ortogonalne\nW przypadku rotacji ortogonalnych macierz \\(T\\) jest macierzą ortogonalną, czyli spełnia warunek: \\[T^\\top T = TT^\\top = I_m.\\] Oznacza to, że czynniki po rotacji pozostają nieskorelowane (\\(\\Phi = I_m\\)).\nNajważniejsze rodzaje rotacji ortogonalnych:\n\nVarimax (Kaiser 1958) - najczęściej stosowana rotacja ortogonalna. Maksymalizuje wariancję kwadratów ładunków w ramach każdego czynnika. Prowadzi do tego, że każda zmienna ma wysokie ładunki tylko na jednym czynniku, a bliskie zeru na pozostałych. Funkcja celu \\[\nV = \\sum_{j=1}^m \\left[ \\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*4} - \\left(\\frac{1}{p} \\sum_{i=1}^p \\lambda_{ij}^{*2}\\right)^2 \\right].\n\\]\nQuartimax - minimalizuje liczbę czynników potrzebnych do opisania każdej zmiennej, upraszczając wiersze macierzy ładunków. Funkcja celu \\[\nQ = \\sum_{i=1}^p \\sum_{j=1}^m \\lambda_{ij}^{*4}.\n\\]\nEquamax - łączy idee varimax i quartimax. Celem jest równoważenie prostoty struktur wierszy i kolumn macierzy ładunków. Funkcja celu \\[\nE = \\frac12(Q + V).\n\\]\nBiquartimax - celem tej rotacji jest jednoczesne uproszczenie wierszy i kolumn macierzy ładunków. W praktyce łączy zalety varimax i quartimax. Funkcja celu \\[\nBQ = \\alpha \\, Q + (1 - \\alpha) \\, V,\n\\] z modyfikacją wag, które równoważą wpływ prostoty wierszy i kolumn. Zmienne mają tendencję do ładowania się mocno na jednym czynniku (jak w varimax), ale jednocześnie ogranicza się sytuacje, w których jedna zmienna ma średnie ładunki na wielu czynnikach (jak w quartimax).\nRotacje skośne (oblique)\nW przypadku rotacji skośnych macierz \\(T\\) nie musi być ortogonalna, więc \\[\nT^\\top T \\neq I_m.\n\\] W efekcie rotowane czynniki mogą być skorelowane, a macierz korelacji czynników \\(\\Phi\\) przyjmuje ogólną postać dodatnio określoną.\nPodstawowe rodzaje:\n\nOblimin (Jennrich i Sampson 1966) - rodzina rotacji z parametrem \\(\\gamma\\), który reguluje stopień skośności. Dla \\(\\gamma = 0\\) rozwiązanie staje się quartimax, a większe \\(\\gamma\\) prowadzą do większej korelacji czynników. Funkcja celu \\[\nF(\\Lambda^*) = \\sum_{i=1}^p \\sum_{j=1}^m \\left(\\lambda_{ij}^{*2} - \\gamma \\frac{\\sum_{k=1}^m \\lambda_{ik}^{*2}}{m}\\right)^2.\n\\]\nPromax (Hendrickson i White 1964) - rotacja skośna oparta na prostym podejściu dwustopniowym. Najpierw stosuje się rotację ortogonalną (najczęściej varimax), następnie ładunki są podnoszone do potęgi \\(k\\) (zwykle 3 lub 4), aby wymusić prostą strukturę, i ponownie dopasowywane przy użyciu metody najmniejszych kwadratów \\[\n\\tilde{\\lambda}{jk} = \\text{sign}(\\lambda^*_{jk}) \\cdot |\\lambda^*_{jk}|^p.\n\\] Rotacja promax pozwala uzyskać bardziej realistyczne struktury, gdy czynniki są rzeczywiście skorelowane.\nGeomin (Everitt i Yates 1989) - minimalizuje średnią geometryczną kwadratów ładunków, co prowadzi do sytuacji, w której każda zmienna ma niewiele istotnych ładunków. Funkcja celu \\[\nG(\\Lambda^*) = \\sum_{i=1}^p \\left( \\prod_{j=1}^m (\\lambda_{ij}^{*2} + \\epsilon) \\right)^{1/m},\n\\] gdzie \\(\\epsilon\\) to mały parametr stabilizujący.\nSimplimax (Kiers 1994) - uogólnienie kryteriów prostoty, które minimalizuje liczbę dużych i małych ładunków w macierzy, pozwalając użytkownikowi sterować liczbą „prostych” elementów.\nWybór rodzaju rotacji\n\nRotacje ortogonalne są preferowane, gdy zakładamy, że czynniki powinny być niezależne teoretycznie.\nRotacje skośne stosuje się, gdy istnieje uzasadnienie, że czynniki mogą być skorelowane (co jest częste w naukach społecznych, psychologii czy biologii).\n\n\nPrzykład 3.1 Na potrzeby ilustracji budowy modelu EFA wykorzystamy dane z pakietu psych, które zawierają wyniki różnych testów poznawczych (Horn 1969). Dane te są często używane jako przykład w literaturze dotyczącej analizy czynnikowej.\n\nKodlibrary(psych)\n\n# Dane: macierz korelacji testów poznawczych\ndata(\"Harman74.cor\")\n\n\n\n\n\n\n\n\n\nZmienna\nOpis\nKategoria testu\n\n\n\nVisualPerception\nRozpoznawanie i analiza relacji przestrzennych w figurach\nZdolności przestrzenne / percepcyjne\n\n\nCubes\nManipulacja wyobrażeniowa brył, rotacje przestrzenne\nZdolności przestrzenne\n\n\nPaperFormBoard\nSkładanie i dopasowywanie elementów figur\nZdolności przestrzenne\n\n\nFlags\nRozpoznawanie wzorów i relacji symboli\nPercepcja wzrokowa / logiczne\n\n\nGeneralInformation\nOgólna wiedza faktograficzna\nZdolności werbalne\n\n\nPargraphComprehension\nRozumienie tekstów pisanych\nZdolności werbalne\n\n\nSentenceCompletion\nUzupełnianie zdań brakującymi słowami\nZdolności werbalne\n\n\nWordClassification\nGrupowanie słów według znaczenia\nZdolności werbalne / semantyczne\n\n\nWordMeaning\nZnajomość i rozumienie znaczeń słów\nZdolności werbalne\n\n\nAddition\nWykonywanie prostych działań arytmetycznych\nZdolności numeryczne\n\n\nCode\nDopasowywanie symboli do liczb według klucza\nSzybkość przetwarzania / percepcja\n\n\nCountingDots\nLiczenie elementów wzrokowych\nSzybkość percepcji / numeryczne\n\n\nStraightCurvedCapitals\nRozpoznawanie prostych i zakrzywionych liter\nPercepcja wizualna / szybkość\n\n\nWordRecognition\nRozpoznawanie słów z listy\nPamięć i zdolności werbalne\n\n\nNumberRecognition\nRozpoznawanie liczb z listy\nPamięć / percepcja numeryczna\n\n\nFigureRecognition\nRozpoznawanie i identyfikacja figur\nPamięć wizualna / percepcja\n\n\nObjectNumber\nDopasowywanie obiektów do liczb\nZłożone zdolności percepcyjno-num.\n\n\nNumberFigure\nDopasowywanie liczb do figur\nZłożone zdolności percepcyjno-num.\n\n\nFigureWord\nDopasowywanie figur do słów\nŁączenie informacji wizualno-werbalnych\n\n\nDeduction\nRozwiązywanie zadań logicznych, wnioskowanie\nRozumowanie logiczne\n\n\nNumericalPuzzles\nZadania numeryczne o charakterze problemowym\nZdolności numeryczne / logiczne\n\n\nProblemReasoning\nRozwiązywanie złożonych problemów\nRozumowanie ogólne\n\n\nSeriesCompletion\nUzupełnianie szeregów logicznych lub numerycznych\nRozumowanie abstrakcyjne / numeryczne\n\n\nArithmeticProblems\nRozwiązywanie zadań arytmetycznych o większej trudności\nZdolności numeryczne\n\n\n\nWidać, że testy można grupować w pięć głównych obszarów: przestrzenne/percepcyjne (np. Cubes, VisualPerception), werbalne (np. WordMeaning, SentenceCompletion), numeryczne (np. Addition, ArithmeticProblems), pamięciowe (np. WordRecognition, NumberRecognition), oraz rozumowania i logiczne (np. Deduction, SeriesCompletion). To właśnie takie powiązania w macierzy korelacji uzasadniają zastosowanie analizy czynnikowej w celu identyfikacji ukrytych wymiarów inteligencji.\nNajpierw sprawdzimy czy dane nadają się do analizy czynnikowej, obliczając test KMO i test sferyczności Bartletta.\n\nKodlibrary(tidyverse)\nlibrary(easystats)\n\ncheck_factorstructure(Harman74.cor$cov, n = 145) \n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(276) = 1545.86, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.88). The individual KMO scores are: VisualPerception (0.90), Cubes (0.84), PaperFormBoard (0.78), Flags (0.85), GeneralInformation (0.88), PargraphComprehension (0.89), SentenceCompletion (0.89), WordClassification (0.92), WordMeaning (0.88), Addition (0.81), Code (0.85), CountingDots (0.84), StraightCurvedCapitals (0.89), WordRecognition (0.85), NumberRecognition (0.88), FigureRecognition (0.89), ObjectNumber (0.85), NumberFigure (0.88), FigureWord (0.83), Deduction (0.93), NumericalPuzzles (0.91), ProblemReasoning (0.93), SeriesCompletion (0.91), ArithmeticProblems (0.92).\n\n\nTest sferyczności Bartletta dostarcza podstawowego potwierdzenia, że w zbiorze danych występują istotne statystycznie korelacje pomiędzy zmiennymi. Wynik \\(\\chi^2(276) = 1545.86,\\ p &lt; 0.001\\) oznacza, że hipoteza zerowa o macierzy korelacji równej macierzy jednostkowej zostaje odrzucona. Innymi słowy, zmienne nie są niezależne, a ich struktura korelacyjna uzasadnia dalsze poszukiwanie wspólnych czynników. Gdyby test okazał się nieistotny, sugerowałby brak uzasadnienia do stosowania analizy czynnikowej, ponieważ nie byłoby wystarczających zależności między zmiennymi.\nMiara adekwatności próby KMO (Kaiser–Meyer–Olkin) wskazuje, na ile obserwowane korelacje mogą być wyjaśnione przez czynniki wspólne w porównaniu z korelacjami cząstkowymi. Wynik ogólny KMO = 0.88 mieści się w przedziale uznawanym za „bardzo dobry” (powyżej 0.80). Oznacza to, że dane dobrze nadają się do analizy czynnikowej i możemy oczekiwać stabilnych, interpretowalnych rozwiązań. Wartości indywidualne dla poszczególnych zmiennych mieszczą się między 0.78 a 0.93, a więc wszystkie osiągają poziom „dobry” lub „bardzo dobry”. Najwyższe wartości, takie jak Deduction (0.93), ProblemReasoning (0.93) czy ArithmeticProblems (0.92), wskazują na wyjątkowo silną reprezentację tych testów w przestrzeni czynnikowej. Z kolei najniższe, jak PaperFormBoard (0.78), są nadal akceptowalne, ale sugerują nieco słabszą integrację tej zmiennej z pozostałymi. Całościowo zarówno wynik globalny, jak i rozkład wartości cząstkowych KMO jednoznacznie potwierdzają zasadność prowadzenia analizy czynnikowej na tym zbiorze danych.\n\nKod# Parallel analysis\nfa.parallel(Harman74.cor$cov, n.obs = 145, fa = \"fa\")\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  NA \n\n\nSamo kryterium paralelne wskazuje na 4 czynniki, choć gdyby brać pod uwagę samo kryterium osypiska to rozwiązanie z 5 czynnikami też wydaje się być właściwe.\n\nKod# Kryterium MAP\nVSS(Harman74.cor$cov, n.obs = 145, plot = F)\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.8  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.85  with  2  factors\n\nThe Velicer MAP achieves a minimum of 0.02  with  4  factors \nBIC achieves a minimum of  -731.36  with  3  factors\nSample Size adjusted BIC achieves a minimum of  -112  with  5  factors\n\nStatistics by number of factors \n  vss1 vss2   map dof chisq    prob sqresid  fit RMSEA  BIC SABIC complex\n1 0.80 0.00 0.025 252   626 8.0e-34    16.8 0.80 0.101 -628   170     1.0\n2 0.55 0.85 0.022 229   428 3.1e-14    12.7 0.85 0.077 -711    13     1.5\n3 0.46 0.79 0.017 207   299 3.0e-05    10.0 0.88 0.055 -731   -76     1.8\n4 0.42 0.74 0.017 186   228 1.9e-02     8.0 0.90 0.039 -698  -109     1.9\n5 0.40 0.71 0.021 166   189 1.1e-01     7.2 0.91 0.030 -637  -112     2.0\n6 0.40 0.71 0.024 147   162 1.8e-01     6.3 0.92 0.026 -569  -104     2.0\n7 0.40 0.70 0.028 129   138 2.7e-01     5.6 0.93 0.021 -504   -95     2.2\n8 0.41 0.70 0.030 112   111 5.0e-01     5.0 0.94 0.000 -446   -92     2.3\n  eChisq  SRMR eCRMS eBIC\n1    748 0.097 0.101 -506\n2    422 0.073 0.080 -718\n3    240 0.055 0.063 -790\n4    133 0.041 0.050 -792\n5    105 0.036 0.047 -721\n6     81 0.032 0.044 -651\n7     62 0.028 0.041 -580\n8     44 0.023 0.037 -514\n\n\n\n\n\n\n\n\nWskaźnik\nInterpretacja\n\n\n\nvss1\nDopasowanie Very Simple Structure przy założeniu jednego czynnika na zmienną; wyższe = lepsze.\n\n\nvss2\nDopasowanie VSS przy założeniu maksymalnie dwóch czynników na zmienną; wyższe = lepsze.\n\n\nmap\nKryterium Velicera; minimum wskazuje optymalną liczbę czynników (eliminuje korelacje cząstkowe).\n\n\ndof\nStopnie swobody testu dopasowania chi-kwadrat.\n\n\nchisq\nWartość statystyki chi-kwadrat; niska w relacji do df sugeruje dobre dopasowanie.\n\n\nprob\nWartość p testu chi-kwadrat; wysoka oznacza brak podstaw do odrzucenia poprawnego dopasowania.\n\n\nsqresid\nSuma kwadratów reszt (różnice R − R̂); niższe wartości = lepsze odwzorowanie danych.\n\n\nfit\nProporcja wyjaśnionej wariancji w macierzy korelacji; wyższe wartości = lepsze dopasowanie.\n\n\nRMSEA\nBłąd aproksymacji w populacji; &lt; 0.05 bardzo dobre, 0.05–0.08 akceptowalne, &gt; 0.10 słabe.\n\n\nBIC\nKryterium informacyjne; niższe wartości = lepszy kompromis dopasowania i prostoty.\n\n\nSABIC\nWersja BIC korygowana o wielkość próby; lepsza przy mniejszych próbach.\n\n\ncomplex\nŚrednia liczba czynników na które ładują się zmienne; niższe = prostsza struktura.\n\n\neChisq\nEstymowana statystyka chi-kwadrat w alternatywnej estymacji; interpretacja analogiczna jak chisq.\n\n\nSRMR\nStandardized Root Mean Square Residual; niski poziom (&lt; 0.08) wskazuje dobre dopasowanie.\n\n\neCRMS\nEstymowany błąd resztowy analogiczny do RMSEA; mniejsze wartości = lepsze dopasowanie.\n\n\neBIC\nEstymowana wersja kryterium BIC; niższe wartości = lepszy model.\n\n\n\nKryterium MAP Velicera wskazuje, że minimalna wartość statystyki została osiągnięta przy czterech czynnikach (MAP = 0.017). Oznacza to, że w ujęciu tego kryterium, czynniki te najlepiej redukują korelacje cząstkowe między zmiennymi – czyli eliminują największą część wariancji niepowiązanej ze wspólną strukturą czynnikową. Innymi słowy, przy czterech czynnikach model najefektywniej odwzorowuje wspólne zależności bez pozostawiania nadmiernych reszt.\nWarto jednak zauważyć, że różne kryteria sugerują odmienne liczby czynników. Kryterium BIC wskazuje na trzy czynniki jako najbardziej oczekiwane rozwiązanie, natomiast skorygowany BIC (SABIC) preferuje pięć czynników. Z kolei wskaźniki VSS (Very Simple Structure) sugerują jedno– lub dwuczynnikowe rozwiązania, maksymalizujące prostotę struktury. Ostateczna decyzja wymaga zatem kompromisu: MAP sugeruje cztery czynniki jako najpełniej oddające wspólną strukturę zmiennych, BIC preferuje trzy jako prostsze, a SABIC wskazuje na pięć. Interpretacja powinna uwzględniać nie tylko statystyki, lecz także sensowność teoretyczną i interpretowalność uzyskanych czynników w kontekście badanego materiału.\n\nKod# Analiza czynnikowa\nfa_model &lt;- fa(Harman74.cor$cov, nfactors = 4, n.obs = 145, \n               fm = \"ml\", rotate = \"varimax\")\n\nfa_model\n\nFactor Analysis using method =  ml\nCall: fa(r = Harman74.cor$cov, nfactors = 4, n.obs = 145, rotate = \"varimax\", \n    fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                        ML1   ML3   ML2  ML4   h2   u2 com\nVisualPerception       0.16  0.69  0.19 0.16 0.56 0.44 1.4\nCubes                  0.12  0.44  0.08 0.10 0.22 0.78 1.3\nPaperFormBoard         0.14  0.57 -0.02 0.11 0.36 0.64 1.2\nFlags                  0.23  0.53  0.10 0.08 0.35 0.65 1.5\nGeneralInformation     0.74  0.19  0.21 0.15 0.65 0.35 1.4\nPargraphComprehension  0.77  0.20  0.07 0.23 0.69 0.31 1.4\nSentenceCompletion     0.81  0.20  0.15 0.07 0.72 0.28 1.2\nWordClassification     0.57  0.34  0.24 0.13 0.51 0.49 2.2\nWordMeaning            0.81  0.20  0.04 0.23 0.74 0.26 1.3\nAddition               0.17 -0.12  0.83 0.17 0.76 0.24 1.2\nCode                   0.18  0.12  0.51 0.37 0.45 0.55 2.2\nCountingDots           0.02  0.21  0.72 0.09 0.56 0.44 1.2\nStraightCurvedCapitals 0.19  0.44  0.53 0.08 0.51 0.49 2.3\nWordRecognition        0.20  0.05  0.08 0.55 0.35 0.65 1.3\nNumberRecognition      0.12  0.12  0.07 0.52 0.30 0.70 1.3\nFigureRecognition      0.07  0.41  0.06 0.53 0.45 0.55 2.0\nObjectNumber           0.14  0.06  0.22 0.57 0.40 0.60 1.4\nNumberFigure           0.03  0.29  0.34 0.46 0.41 0.59 2.6\nFigureWord             0.15  0.24  0.16 0.37 0.24 0.76 2.6\nDeduction              0.38  0.40  0.12 0.30 0.41 0.59 3.0\nNumericalPuzzles       0.17  0.38  0.44 0.22 0.42 0.58 2.8\nProblemReasoning       0.37  0.40  0.12 0.30 0.40 0.60 3.1\nSeriesCompletion       0.37  0.50  0.24 0.24 0.50 0.50 2.9\nArithmeticProblems     0.37  0.16  0.50 0.30 0.50 0.50 2.8\n\n                       ML1  ML3  ML2  ML4\nSS loadings           3.65 2.87 2.66 2.29\nProportion Var        0.15 0.12 0.11 0.10\nCumulative Var        0.15 0.27 0.38 0.48\nProportion Explained  0.32 0.25 0.23 0.20\nCumulative Proportion 0.32 0.57 0.80 1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  276  with the objective function =  11.44 with Chi Square =  1545.86\ndf of  the model are 186  and the objective function was  1.71 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  145 with the empirical chi square  135.74  with prob &lt;  1 \nThe total n.obs was  145  with Likelihood Chi Square =  226.68  with prob &lt;  0.022 \n\nTucker Lewis Index of factoring reliability =  0.951\nRMSEA index =  0.038  and the 90 % confidence intervals are  0.016 0.056\nBIC =  -698.99\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   ML1  ML3  ML2  ML4\nCorrelation of (regression) scores with factors   0.93 0.87 0.91 0.82\nMultiple R square of scores with factors          0.87 0.76 0.83 0.68\nMinimum correlation of possible factor scores     0.73 0.52 0.66 0.36\n\n\nModel czteroczynnikowy oszacowany metodą największej wiarygodności na macierzy korelacji Harman74.cor$cov dobrze odwzorowuje strukturę danych i dostarcza interpretowalnych wyników.\nPierwszy czynnik (ML1) skupia się na kompetencjach werbalnych i wiedzy ogólnej. Najwyższe ładunki uzyskano dla zmiennych takich jak WordMeaning (0.81), SentenceCompletion (0.81), ParagraphComprehension (0.77) czy GeneralInformation (0.74). Wskazuje to, że ML1 reprezentuje wymiar wiedzy językowej i rozumienia tekstu. Zasoby zmienności wspólej dla tych zmiennych są wysokie (powyżej 0.65), co oznacza, że znaczna część ich wariancji została uchwycona przez model.\nDrugi czynnik (ML2) odzwierciedla zdolności arytmetyczne i numeryczne. Najsilniejsze ładunki dotyczą zmiennych Addition (0.83), CountingDots (0.72) i ArithmeticProblems (0.50). Oznacza to, że ML2 reprezentuje wymiar obliczeniowy, obejmujący zarówno proste działania matematyczne, jak i bardziej złożone zadania wymagające operowania na liczbach. Wysokie wartości \\(h_j^2\\) (np. 0.76 dla Addition) sugerują dobrą reprezentację tych zmiennych.\nTrzeci czynnik (ML3) można interpretować jako zdolności wzrokowo-przestrzenne i percepcyjne. Najsilniejsze ładunki wystąpiły dla VisualPerception (0.69), PaperFormBoard (0.57), Flags (0.53) oraz SeriesCompletion (0.50). Grupa ta obejmuje zadania związane z manipulacją figurami, rozpoznawaniem wzorów i orientacją przestrzenną.\nCzwarty czynnik (ML4) wydaje się związany z rozpoznawaniem wzrokowym i pamięcią wzrokową. Największe ładunki dotyczą zmiennych takich jak WordRecognition (0.55), NumberRecognition (0.52), FigureRecognition (0.53) czy ObjectNumber (0.57). Sugeruje to wymiar rozpoznawania i szybkiego identyfikowania bodźców wzrokowych.\nŁącznie cztery czynniki wyjaśniają 48% wariancji całkowitej, co w psychometrii jest uznawane za wartość akceptowalną przy tego typu danych. Dopasowanie globalne modelu również jest dobre: RMSEA = 0.038 (z przedziałem ufności 0.016–0.056) wskazuje na bardzo dobre dopasowanie, a Tucker-Lewis Index wynosi 0.951, co również świadczy o wysokiej jakości modelu. Niskie wartości RMSR (0.04) oraz wysoka zgodność dopasowania poza przekątną (0.98) potwierdzają, że model trafnie odwzorowuje strukturę korelacji między zmiennymi.\nOstatecznie wyniki wskazują, że struktura czteroczynnikowa jest dobrze uzasadniona empirycznie i teoretycznie. Każdy czynnik odpowiada odmiennym zdolnościom poznawczym – werbalnym, numerycznym, przestrzennym i percepcyjno-pamięciowym – a ich interpretacje są zgodne z psychologicznymi ujęciami inteligencji wielowymiarowej.\nDla większej czytelności przedstawiamy ładunki czynnikowe po rotacji varimax w formie tabelarycznej, z wyciętymi ładunkami o niskich wartościach.\n\nKodmodel_parameters(fa_model, sort = TRUE, threshold = \"max\") %&gt;% \n  print_html()\n\n\n\n\n\n\nRotated loadings from Factor Analysis (varimax-rotation)\n\n\nVariable\nML1\nML3\nML2\nML4\nComplexity\nUniqueness\n\n\n\n\nWordMeaning\n0.81\n\n\n\n1.30\n0.26\n\n\nSentenceCompletion\n0.81\n\n\n\n1.21\n0.28\n\n\nPargraphComprehension\n0.77\n\n\n\n1.35\n0.31\n\n\nGeneralInformation\n0.74\n\n\n\n1.39\n0.35\n\n\nWordClassification\n0.57\n\n\n\n2.17\n0.49\n\n\nVisualPerception\n\n0.69\n\n\n1.38\n0.44\n\n\nPaperFormBoard\n\n0.57\n\n\n1.20\n0.64\n\n\nFlags\n\n0.53\n\n\n1.51\n0.65\n\n\nSeriesCompletion\n\n0.50\n\n\n2.87\n0.50\n\n\nCubes\n\n0.44\n\n\n1.33\n0.78\n\n\nDeduction\n\n0.40\n\n\n3.05\n0.59\n\n\nProblemReasoning\n\n0.40\n\n\n3.08\n0.60\n\n\nAddition\n\n\n0.83\n\n1.21\n0.24\n\n\nCountingDots\n\n\n0.72\n\n1.21\n0.44\n\n\nStraightCurvedCapitals\n\n\n0.53\n\n2.27\n0.49\n\n\nCode\n\n\n0.51\n\n2.25\n0.55\n\n\nArithmeticProblems\n\n\n0.50\n\n2.83\n0.50\n\n\nNumericalPuzzles\n\n\n0.44\n\n2.84\n0.58\n\n\nObjectNumber\n\n\n\n0.57\n1.45\n0.60\n\n\nWordRecognition\n\n\n\n0.55\n1.32\n0.65\n\n\nFigureRecognition\n\n\n\n0.53\n1.96\n0.55\n\n\nNumberRecognition\n\n\n\n0.52\n1.26\n0.70\n\n\nNumberFigure\n\n\n\n0.46\n2.62\n0.59\n\n\nFigureWord\n\n\n\n0.37\n2.56\n0.76\n\n\n\nThe 4 latent factors (varimax rotation) accounted for 47.78% of the total variance of the original data (ML1 = 15.20%, ML3 = 11.97%, ML2 = 11.07%, ML4 = 9.54%).\n\n\n\n\n\nMożemy też przedstawić model graficznie.\n\nKodfa.diagram(fa_model, marg = c(1,5,1,1), rsize = 2)\n\n\n\n\n\n\n\nMożna spróbować estymować model z pięcioma czynnikami, co odpowiadałoby pierwotnemu rozpoznaniu obszarów.\n\nKodfa_model_5 &lt;- fa(Harman74.cor$cov, nfactors = 5, n.obs = 145, \n                 fm = \"ml\", rotate = \"varimax\")\nmodel_parameters(fa_model_5, sort = TRUE, threshold = \"max\") %&gt;% \n  print_html()\n\n\n\n\n\n\nRotated loadings from Factor Analysis (varimax-rotation)\n\n\nVariable\nML1\nML3\nML2\nML4\nML5\nComplexity\nUniqueness\n\n\n\n\nSentenceCompletion\n0.81\n\n\n\n\n1.21\n0.28\n\n\nWordMeaning\n0.80\n\n\n\n\n1.31\n0.26\n\n\nPargraphComprehension\n0.77\n\n\n\n\n1.39\n0.29\n\n\nGeneralInformation\n0.74\n\n\n\n\n1.40\n0.36\n\n\nWordClassification\n0.57\n\n\n\n\n2.17\n0.49\n\n\nVisualPerception\n\n0.66\n\n\n\n1.59\n0.45\n\n\nPaperFormBoard\n\n0.56\n\n\n\n1.30\n0.64\n\n\nSeriesCompletion\n\n0.55\n\n\n\n2.72\n0.44\n\n\nFlags\n\n0.53\n\n\n\n1.47\n0.65\n\n\nDeduction\n\n0.45\n\n\n\n3.33\n0.52\n\n\nCubes\n\n0.44\n\n\n\n1.32\n0.78\n\n\nProblemReasoning\n\n0.42\n\n\n\n3.10\n0.58\n\n\nAddition\n\n\n0.84\n\n\n1.21\n0.21\n\n\nCountingDots\n\n\n0.69\n\n\n1.34\n0.44\n\n\nArithmeticProblems\n\n\n0.50\n\n\n2.94\n0.48\n\n\nNumericalPuzzles\n\n\n0.44\n\n\n2.85\n0.56\n\n\nObjectNumber\n\n\n\n0.56\n\n1.46\n0.61\n\n\nWordRecognition\n\n\n\n0.56\n\n1.34\n0.64\n\n\nFigureRecognition\n\n\n\n0.53\n\n1.95\n0.55\n\n\nNumberRecognition\n\n\n\n0.51\n\n1.29\n0.71\n\n\nNumberFigure\n\n\n\n0.45\n\n2.65\n0.60\n\n\nCode\n\n\n\n0.45\n\n3.36\n0.39\n\n\nFigureWord\n\n\n\n0.36\n\n2.56\n0.76\n\n\nStraightCurvedCapitals\n\n\n\n\n0.56\n3.16\n0.26\n\n\n\nThe 5 latent factors (varimax rotation) accounted for 50.25% of the total variance of the original data (ML1 = 15.13%, ML3 = 12.35%, ML2 = 10.23%, ML4 = 9.77%, ML5 = 2.76%).\n\n\n\n\n\nChoć wzrósł nieco poziom wyjaśnionej wariancji przez czynniki, to jednak rozwiązanie, w którym występują pojedyncze zmienne jako czynnik nie są porządane. Dlatego pozostaniemy przy rozwiązaniu z czterema czynnikami.\n\n\n\n\n\n\n\nWskazówka\n\n\n\nLiczba czynników, może być szacowana na podstawie różnych kryteriów, z których każde eksponuje inny aspekt, ale ostateczna decyzja o wyborze liczby czynników powinna być podyktowana głównie zgodnością otrzymany wyników z teorią oraz interpretowalnością. Oczywiście nie powinno się to dziać kosztem znacznego obniżenia poziomu dopasowania modelu.\n\n\nNa temat konfiramcyjnej analizy czynnikowej (CFA) zostnie poświęcony kolejny rozdział.\n\n\n\n\nBartlett, M. S. 1951. „The Effect of Standardization on a χ 2 Approximation in Factor Analysis”. Biometrika 38 (3/4): 337. https://doi.org/10.2307/2332580.\n\n\nCattell, Raymond B. 1966. „The Scree Test For The Number Of Factors”. Multivariate Behavioral Research 1 (2): 245–76. https://doi.org/10.1207/s15327906mbr0102_10.\n\n\nEveritt, B. S., i A. Yates. 1989. „Multivariate Exploratory Data Analysis: A Perspective on Exploratory Factor Analysis.” Biometrics 45 (1): 342. https://doi.org/10.2307/2532065.\n\n\nGrieder, Silvia, i Markus D. Steiner. 2020. „Algorithmic Jingle Jungle: A Comparison of Implementations of Principal Axis Factoring and Promax Rotation in R and SPSS”. http://dx.doi.org/10.31234/osf.io/7hwrm.\n\n\nHarman, Harry H., i Wayne H. Jones. 1966. „Factor Analysis by Minimizing Residuals (Minres)”. Psychometrika 31 (3): 351–68. https://doi.org/10.1007/bf02289468.\n\n\nHendrickson, Alan E., i Paul Owen White. 1964. „PROMAX: A QUICK METHOD FOR ROTATION TO OBLIQUE SIMPLE STRUCTURE”. British Journal of Statistical Psychology 17 (1): 65–70. https://doi.org/10.1111/j.2044-8317.1964.tb00244.x.\n\n\nHorn, John L. 1965. „A Rationale and Test for the Number of Factors in Factor Analysis”. Psychometrika 30 (2): 179–85. https://doi.org/10.1007/bf02289447.\n\n\n———. 1969. „Harry H. Harman Modern Factor Analysis (Second Edition, Revised). Chicago and London: University of Chicago Press, 1967. Pp. Xx + 474. $12.50”. Psychometrika 34 (1): 134–38. https://doi.org/10.1017/s0033312300004580.\n\n\n„Introduction to Factor Analysis”. 2020. W, 1–12. SAGE Publications, Inc. https://doi.org/10.4135/9781544339900.n4.\n\n\nJacobucci, Ross, i Kevin J. Grimm. 2018. „Comparison of Frequentist and Bayesian Regularization in Structural Equation Modeling”. Structural Equation Modeling: A Multidisciplinary Journal 25 (4): 639–49. https://doi.org/10.1080/10705511.2017.1410822.\n\n\nJennrich, R. I., i P. F. Sampson. 1966. „Rotation for Simple Loadings”. Psychometrika 31 (3): 313–23. https://doi.org/10.1007/bf02289465.\n\n\nJöreskog, Karl G., i Arthur S. Goldberger. 1972. „Factor Analysis by Generalized Least Squares”. Psychometrika 37 (3): 243–60. https://doi.org/10.1007/bf02306782.\n\n\nKaiser, Henry F. 1958. „The Varimax Criterion for Analytic Rotation in Factor Analysis”. Psychometrika 23 (3): 187–200. https://doi.org/10.1007/bf02289233.\n\n\n———. 1970. „A Second Generation Little Jiffy”. Psychometrika 35 (4): 401–15. https://doi.org/10.1007/bf02291817.\n\n\nKiers, Henk A. L. 1994. „Simplimax: Oblique Rotation to an Optimal Target with Simple Structure”. Psychometrika 59 (4): 567–79. https://doi.org/10.1007/bf02294392.\n\n\nLawley, D. N. 1940. „VI.The Estimation of Factor Loadings by the Method of Maximum Likelihood”. Proceedings of the Royal Society of Edinburgh 60 (1): 64–82. https://doi.org/10.1017/s037016460002006x.\n\n\nLu, Zhao-Hua, Sy-Miin Chow, i Eric Loken. 2016. „Bayesian Factor Analysis as a Variable-Selection Problem: Alternative Priors and Consequences”. Multivariate Behavioral Research 51 (4): 519–39. https://doi.org/10.1080/00273171.2016.1168279.\n\n\nVelicer, Wayne F. 1976. „Determining the Number of Components from the Matrix of Partial Correlations”. Psychometrika 41 (3): 321–27. https://doi.org/10.1007/bf02293557.\n\n\nWang, Xiaojing, Candace M. Kammerer, Stewart Anderson, Jiang Lu, i Eleanor Feingold. 2008. „A Comparison of Principal Component Analysis and Factor Analysis Strategies for Uncovering Pleiotropic Factors”. Genetic Epidemiology 33 (4): 325–31. https://doi.org/10.1002/gepi.20384.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "fa.html#założenia-dotyczące-danych",
    "href": "fa.html#założenia-dotyczące-danych",
    "title": "Analiza czynnikowa",
    "section": "Założenia dotyczące danych",
    "text": "Założenia dotyczące danych\nAby estymacja modelu eksploracyjnej analizy czynnikowej (EFA) była uzasadniona, dane powinny spełniać szereg założeń teoretycznych i praktycznych.\n\nPo pierwsze, podstawą jest istnienie istotnej struktury korelacyjnej pomiędzy zmiennymi obserwowalnymi. Jeżeli zmienne są w zasadzie nieskorelowane, nie da się wydzielić wspólnych czynników. Warunek ten weryfikuje się wstępnie testem sferyczności Bartletta (Bartlett 1951) oraz miarą adekwatności próby KMO (Kaiser 1970).\nPo drugie, zakłada się odpowiednią wielkość próby. Choć w literaturze nie istnieje jednoznaczna reguła, rekomenduje się co najmniej 5–10 obserwacji na zmienną oraz łączną liczebność rzędu ≥100–200 jednostek, aby uzyskać stabilne rozwiązania i wiarygodne oszacowania ładunków czynnikowych.\nPo trzecie, dane powinny pochodzić z rozkładu wielowymiarowego normalnego, szczególnie jeżeli korzysta się z estymacji metodą największej wiarygodności. Naruszenia normalności mogą prowadzić do zawyżenia błędów standardowych, problemów z testami istotności oraz błędnych przedziałów ufności.\nPo czwarte, model EFA wymaga, aby zmienne były ciągłe lub co najmniej traktowane jako przybliżenie zmiennych ciągłych. W przypadku zmiennych kategorycznych należy sięgnąć po odpowiednie uogólnienia (np. analizy czynnikowe dla danych porządkowych).\nPo piąte, zakłada się addytywność wariancji. Oznacza to, że wariancja każdej zmiennej obserwowanej rozkłada się na część wspólną, wyjaśnianą przez czynniki latentne, oraz część swoistą (unikalną dla danej zmiennej), zgodnie z postacią: \\[\n\\mathbf{x} = \\boldsymbol{\\mu} + \\Lambda \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad\n\\Sigma = \\Lambda \\Lambda^\\top + \\Psi.\n\\]\n\nI na koniec, należy zadbać o brak nadmiernej współliniowości oraz o to, by liczba czynników nie przekraczała liczby zmiennych – w przeciwnym wypadku model byłby nieidentyfikowalny.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analiza czynnikowa</span>"
    ]
  },
  {
    "objectID": "sam.html",
    "href": "sam.html",
    "title": "Modele ścieżkowe",
    "section": "",
    "text": "Modele ścieżkowe (ang. path modeling)",
    "crumbs": [
      "Modele ścieżkowe"
    ]
  },
  {
    "objectID": "sem.html",
    "href": "sem.html",
    "title": "Modele strukturalne",
    "section": "",
    "text": "Konfirmacyjna analiza czynnikowa\nModele strukturalne (ang. Structural Equation Models) stanowią uogólnienie klasycznych modeli regresyjnych do układów wielu równań z jednoczesnymi zależnościami między zmiennymi. W najprostszym wariancie, zwanym path analysis (PA), wszystkie zmienne są obserwowalne, a celem jest estymacja współczynników ścieżek, dekompozycja efektów na bezpośrednie i pośrednie oraz wyjaśnienie współzmienności. Konfirmacyjna analiza czynnikowa (CFA) rozszerza to ujęcie o niewidoczne wprost czynniki latentne, modelując relację wskaźnik–czynnik i separując wariancję wspólną od swoistej. Modele strukturalne (SEM) integrują oba poziomy: pomiarowy (jak w CFA) i strukturalny (jak w path analysis), tworząc jedną ramę, w której czynniki latentne i zmienne obserwowalne łączą się w sieć równań opisujących zależności przyczynowo-interpretacyjne.\nRys historyczny sięga prac Sewalla Wrighta z lat 1918–1934, który wprowadził path analysis i reguły śledzenia ścieżek, pozwalające dekomponować kowariancje na sumy iloczynów współczynników (Wright 1934). Równolegle rozwijała się analiza czynnikowa: Spearman (1961), który postulował czynnik ogólny, a Thurstone (1931) wprowadził czynniki wielowymiarowe. Przełomem był formalny opis CFA i ujęcie SEM przez Jöreskoga (koniec lat 60.), który połączył model pomiarowy i strukturalny w system LISREL (Tarka 2017). Lata 80. i 90. przyniosły rozwój estymacji, wskaźników dopasowania i oprogramowania (m.in. EQS, AMOS), a podręcznikowa synteza Bollen’a (1989) ugruntowała teorię (Bollen 1989). W kolejnych dekadach pojawiały się metody odporne i dla zmiennych porządkowych oraz uogólnienia dla danych longitudalnych i wielopoziomowych, co uczyniło z SEM uniwersalną ramę modelowania.\nKonfirmacyjna analiza czynnikowa (ang. Confirmatory Factor Analysis, CFA), stanowi ujęcie modelu pomiarowego, w którym a priori narzuca strukturę zależności między zmiennymi obserwowalnymi a czynnikami ukrytymi. W odróżnieniu od eksploracyjnej analizy czynnikowej, gdzie pozwala danym „odkrywać” wzorzec ładunków, w CFA określamy, które zmienne ładują się na których czynnikach, które ładunki są równe zeru, a które mogą się różnić, oraz czy dopuszczamy korelacje błędów pomiaru. Celem jest weryfikacja hipotezy o poprawnej budowie narzędzia pomiarowego i o liczbie oraz treści czynników, a następnie oceniamy dopasowanie modelu do macierzy kowariancji/średnich w populacji.\nFormalnie przyjmujemy ten sam model pomiarowy co w EFA, lecz z nałożonymi ograniczeniami strukturalnymi na macierz ładunków. Niech \\(\\mathbf{x}\\in\\mathbb{R}^p\\) oznacza wektor zmiennych obserwowalnych, \\(\\mathbf{f}\\in\\mathbb{R}^m\\) wektor czynników, \\(\\Lambda\\in\\mathbb{R}^{p\\times m}\\) macierz ładunków, \\(\\boldsymbol{\\epsilon}\\in\\mathbb{R}^p\\) wektor składników swoistych. Model przyjmuje wówczas postać \\[\n\\mathbf{x}=\\boldsymbol{\\mu}+\\Lambda\\,\\mathbf{f}+\\boldsymbol{\\epsilon},\\qquad\n\\mathbb{E}(\\mathbf{f})=\\mathbf{0},\\ \\ \\mathbb{E}(\\boldsymbol{\\epsilon})=\\mathbf{0},\\ \\ \\mathrm{Cov}(\\mathbf{f})=\\Phi,\\ \\ \\mathrm{Cov}(\\boldsymbol{\\epsilon})=\\Psi,\n\\] gdzie \\(\\Phi\\) jest dodatnio określoną macierzą kowariancji czynników (dla rotacji skośnych) lub macierzą jednostkową (dla czynników ortogonalnych), a \\(\\Psi\\) z reguły jest macierzą diagonalną, co odpowiada nieskorelowanym błędom pomiaru. Macierz kowariancji implikowana przez model ma postać \\[\n\\Sigma(\\theta)=\\Lambda\\,\\Phi\\,\\Lambda^\\top + \\Psi,\n\\] gdzie \\(\\theta\\) reprezentuje wszystkie parametry modelu.\nKlucz identyfikacji w CFA polegać na tym, że rotacyjna nieoznaczoność znika dzięki z góry zdefiniowanemu wzorcowi zer w \\(\\Lambda\\) (każdy wskaźnik ładujący się wyłącznie na „własnym” czynniku). Aby skalować czynniki, przyjmować jedną z równoważnych konwencji: ustalamy wariancję czynnika na 1 i estymujemy wszystkie ładunki, albo ustalamy po jednym ładunku na 1 w każdej kolumnie \\(\\Lambda\\) i estymujemy wariancje czynników. Praktycznie zapewniamy co najmniej trzy sensowne wskaźniki na czynnik; dwa wskaźniki bywają wystarczające przy dodatkowych ograniczeniach równości lub znanych błędach pomiaru.\nEstymujemy parametry zwykle metodą największej wiarygodności, co przy normalności wielowymiarowej oznacza minimalizowanie rozbieżność między \\(S\\) a \\(\\Sigma(\\theta)\\) i umożliwia wprowadzenie testu globalnego dopasowania \\(\\chi^2\\). Przy naruszeniach normalności stosujemy wersje odporne lub ważone metody najmniejszych kwadratów dla danych porządkowych (WLSMV) (Li 2015).\nInterpretacja CFA opieramy na ładunkach w \\(\\Lambda\\) jako czułościach wskaźników na czynniki, wariancjach i korelacjach czynników w \\(\\Phi\\) jako sile i współwystępowaniu wymiarów latentnych oraz na resztach i modyfikacjach jako sygnałach lokalnego niedopasowania. Siła CFA polega na tym, że pozwala wprost testować hipotezy o narzędziu pomiarowym, porównywać modele teoretycznie motywowane i zapewniać podstawę do dalszych modeli strukturalnych, SEM, w których czynniki stają się zmiennymi wyjaśniającymi i wyjaśnianymi.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#konfirmacyjna-analiza-czynnikowa",
    "href": "sem.html#konfirmacyjna-analiza-czynnikowa",
    "title": "Modele strukturalne",
    "section": "",
    "text": "Przykład 4.1 Przeprowadzimy CFA na danych pochodzących z badania PISA 2009, dotyczących strategii uczenia się uczniów. Wybierzemy 13 pozycji z kwestionariusza ucznia, które mają odzwierciedlać trzy strategie: zapamiętywania (M), opracowywania (E) i kontroli (C). Sprawdzimy, czy dane z Wielkiej Brytanii potwierdzają tę strukturę trójczynnikową.\n\nKod#devtools::install_github(\"talbano/epmr\")\nlibrary(epmr)\nlibrary(gt)\nlibrary(lavaan)\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(sjPlot)\n\n# wybór itemów z testu (łącznie 13), podzielonych wg założonej struktury, \n# którą będziemy weryfikować\n# strategie zapamiętywania, opracowywania i kontroli\nmitems &lt;- c(\"st27q01\", \"st27q03\", \"st27q05\", \"st27q07\")\neitems &lt;- c(\"st27q04\", \"st27q08\", \"st27q10\", \"st27q12\")\ncitems &lt;- c(\"st27q02\", \"st27q06\", \"st27q09\", \"st27q11\", \n  \"st27q13\")\nalitems &lt;- c(mitems, eitems, citems)\n\n# Zawęzimy badania tylko go Wielkiej Brytanii\npisagbr &lt;- PISA09[PISA09$cnt == \"GBR\", alitems]\npisagbr &lt;- pisagbr[complete.cases(pisagbr[, c(mitems, \n  eitems, citems)]), ]\nplot_likert(pisagbr, groups = c(rep(\"Zapamiętywanie\", \n  length(mitems)), rep(\"Opracowywanie\", length(eitems)), \n  rep(\"Kontrola\", length(citems))))\n\n\n\n\n\n\n\nNa potrzeby budowy modelu konfirmacyjnego użyjemy pakietu lavaan. Definiujemy model z trzema czynnikami, gdzie każdy czynnik jest ładowany przez odpowiednie pozycje kwestionariusza. Następnie estymujemy model metodą największej wiarygodności i sprawdzamy dopasowanie modelu do danych.\n\nKod# Definicja modelu CFA\nmodel_cfa &lt;- '\n  # Definicja czynników\n  Zapamiętywanie =~ st27q01 + st27q03 + st27q05 + st27q07\n  Opracowywanie =~ st27q04 + st27q08 + st27q10 + st27q12\n  Kontrola =~ st27q02 + st27q06 + st27q09 + st27q11 + st27q13\n'\n\n# Estymacja modelu CFA\nfit_cfa &lt;- cfa(model_cfa, data = pisagbr, auto.var = TRUE, auto.cov.lv.x = TRUE, std.lv = TRUE)\n\n\n\n\nauto.var = TRUE - estymuje wariancje czynników i błędów pomiarowych, bez potrzeby ręcznego ich dodawania.\n\nauto.cov.lv.x = TRUE - estymuje kowariancje pomiędzy wszystkimi czynnikami latentnymi.\n\nstd.lv = TRUE - ustawia wariancję czynników latentnych na 1, co daje bezpośrednio interpretowalne ładunki czynnikowe jako korelacje.\n\n\nKod# Podsumowanie dopasowania\nmodel_performance(fit_cfa, metrics = c(\"p_Chi2\", \"GFI\", \"AGFI\", \"NFI\", \"NNFI\", \"CFI\", \"RMSEA\", \"RMR\", \"SRMR\", \"RFI\")) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\np_Chi2\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMR\nSRMR\nRFI\n\n\n0.000\n0.936\n0.907\n0.881\n0.856\n0.885\n0.081\n0.042\n0.057\n0.850\n\n\n\n\n\nNajpierw ocenimy dopasowanie modelu:\n\nPo pierwsze, test chi-kwadrat (p_Chi2 = 0.000) jest istotny, co formalnie sugeruje, że model nie odtwarza idealnie macierzy kowariancji w populacji. Jednakże, przy większych próbach test ten jest nadwrażliwy i często prowadzi do odrzucenia nawet dobrze dopasowanych modeli, dlatego nie należy go traktować jako jedynego kryterium oceny.\nJeśli chodzi o wskaźniki dopasowania absolutnego, wartości GFI = 0.936 oraz AGFI = 0.907 wskazują na przyzwoite dopasowanie – oba mieszczą się powyżej progu 0.90, choć nie osiągają poziomu bardzo dobrego (≥ 0.95). Podobnie RMR = 0.042 i SRMR = 0.057 sugerują, że przeciętne reszty między obserwowaną a implikowaną macierzą są umiarkowanie niskie – SRMR &lt; 0.08 jest zwykle uznawane za akceptowalne.\nW przypadku wskaźników dopasowania przyrostowego (NFI = 0.881, NNFI = 0.856, CFI = 0.885, RFI = 0.850), wszystkie wartości są poniżej konwencjonalnego progu 0.90, co wskazuje na pewne niedopasowanie.\nSzczególnie ważny jest RMSEA = 0.081, który mieści się w strefie dopuszczalnej, ale nie idealnej (0.05–0.08 uznaje się za akceptowalne dopasowanie, a &gt; 0.10 za słabe). Wartość 0.081 wskazuje na model na granicy akceptowalności – można go uznać za umiarkowanie dopasowany, ale istnieją przesłanki do jego ulepszania (np. rozważenie korelacji błędów pomiarowych, dodanie lub modyfikacja pozycji).\n\nTeraz przejdźmy do interpretacji parametrów modelu:\n\nKodmodel_parameters(fit_cfa, component = \"loading\", standardize = T) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nTo\nOperator\nFrom\nCoefficient\nSE\nCI_low\nCI_high\nz\np\nComponent\n\n\n\nZapamiętywanie\n=~\nst27q01\n0.585\n0.014\n0.557\n0.613\n40.467\n0.000\nLoading\n\n\nZapamiętywanie\n=~\nst27q03\n0.644\n0.014\n0.618\n0.671\n47.357\n0.000\nLoading\n\n\nZapamiętywanie\n=~\nst27q05\n0.597\n0.014\n0.569\n0.625\n41.809\n0.000\nLoading\n\n\nZapamiętywanie\n=~\nst27q07\n0.601\n0.014\n0.574\n0.629\n42.299\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q04\n0.534\n0.015\n0.505\n0.562\n36.319\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q08\n0.644\n0.013\n0.618\n0.669\n49.774\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q10\n0.706\n0.012\n0.682\n0.729\n58.794\n0.000\nLoading\n\n\nOpracowywanie\n=~\nst27q12\n0.725\n0.012\n0.702\n0.748\n61.801\n0.000\nLoading\n\n\nKontrola\n=~\nst27q02\n0.550\n0.014\n0.523\n0.578\n39.524\n0.000\nLoading\n\n\nKontrola\n=~\nst27q06\n0.662\n0.012\n0.639\n0.685\n55.488\n0.000\nLoading\n\n\nKontrola\n=~\nst27q09\n0.657\n0.012\n0.633\n0.680\n54.562\n0.000\nLoading\n\n\nKontrola\n=~\nst27q11\n0.672\n0.012\n0.649\n0.695\n57.236\n0.000\nLoading\n\n\nKontrola\n=~\nst27q13\n0.588\n0.013\n0.562\n0.614\n44.259\n0.000\nLoading\n\n\n\n\n\nKodmodel_parameters(fit_cfa, component = \"correlation\") %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nTo\nOperator\nFrom\nCoefficient\nSE\nCI_low\nCI_high\nz\np\nComponent\n\n\n\nZapamiętywanie\n~~\nOpracowywanie\n0.368\n0.021\n0.327\n0.409\n17.565\n0.000\nCorrelation\n\n\nZapamiętywanie\n~~\nKontrola\n0.714\n0.015\n0.684\n0.744\n46.863\n0.000\nCorrelation\n\n\nOpracowywanie\n~~\nKontrola\n0.576\n0.017\n0.543\n0.609\n34.248\n0.000\nCorrelation\n\n\n\n\n\n\n\nW przypadku strategii zapamiętywania, wszystkie pozycje (st27q01, st27q03, st27q05, st27q07) ładują się umiarkowanie silnie na czynniku, z wartościami współczynników standaryzowanych w przedziale 0.59–0.64. Oznacza to, że zmienne te są spójnymi wskaźnikami tego konstruktu i wnoszą podobny wkład w jego pomiar. Wysokie istotności statystyczne (p &lt; .001) potwierdzają, że każda z tych zmiennych odgrywa istotną rolę w budowie czynnika.\nDla strategii opracowywania obserwujemy wyższe wartości ładunków czynnikowych – od 0.53 dla st27q04 do 0.73 dla st27q12. Oznacza to, że ta grupa pytań jest silnie związana z konstruktem emocjonalnych strategii uczenia się, a zwłaszcza pozycje st27q10 i st27q12 okazują się najbardziej reprezentatywne. Interpretować to można jako silną spójność wskaźników i dużą trafność pomiarową tego czynnika.\nJeśli chodzi o strategie kontrolne, wszystkie pozycje mają dość wysokie ładunki, od 0.55 do 0.67, co sugeruje dobrą konsystencję wewnętrzną tego konstruktu. Szczególnie istotne są pozycje st27q06, st27q09 i st27q11, które mają najwyższe wartości współczynników, a więc najlepiej odzwierciedlają mechanizmy związane z kontrolą uczenia się.\nWyniki korelacji między trzema strategiami uczenia się wskazują, że są one ze sobą istotnie powiązane, choć w różnym stopniu. Najsilniejszy związek występuje między Zapamiętywaniem a Kontrolą (r = 0.714, p &lt; 0.001), co sugeruje, że monitorowanie procesu uczenia się jest ściśle powiązane ze stosowaniem technik zapamiętywania. Nieco słabsze, ale nadal istotne powiązania obserwuje się między Opracowywaniem a Kontrolą (r = 0.576, p &lt; 0.001) oraz między Zapamiętywaniem a Opracowywaniem (r = 0.368, p &lt; 0.001), co wskazuje, że przetwarzanie materiału oraz kontrola uczenia się są umiarkowanie powiązane z technikami pamięciowymi. Łącznie wyniki te potwierdzają, że strategie tworzą spójny, ale zróżnicowany zbiór powiązanych ze sobą podejść do uczenia się.\n\nModel możemy również przedstawić graficznie.\n\nKodlibrary(semPlot)\nlibrary(RColorBrewer)\n\n# wybieramy pastelową paletę\npastel_cols &lt;- brewer.pal(3, \"Pastel2\")\n\nsemPaths(fit_cfa,\n         whatLabels = \"std\",\n         what = 'std',\n         layout = \"tree2\",\n         groups = \"latents\",\n         sizeMan = 6,\n         sizeLat = 8,\n         nCharNodes = 0,\n         style = \"lisrel\",\n         # kolory pastelowe dla zmiennych latentnych i obserwowanych\n         color = pastel_cols,\n         colorLat = pastel_cols[1],\n         colorMan = pastel_cols[2],\n         edge.color = \"grey70\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#path-analysis",
    "href": "sem.html#path-analysis",
    "title": "Modele strukturalne",
    "section": "Path analysis",
    "text": "Path analysis\nAnaliza ścieżkowa (ang. path analysis) jest jedną z najwcześniejszych form modelowania strukturalnego i stanowi naturalne rozwinięcie regresji wielokrotnej. Jej głównym celem jest badanie złożonych układów zależności przyczynowo-skutkowych między zmiennymi obserwowalnymi, w tym układów obejmujących zmienne pośredniczące (mediatory). Została zaproponowana przez Sewalla Wrighta w latach 20. XX wieku jako narzędzie do formalizacji równań przyczynowych w biologii, a następnie rozwinęła się jako fundament współczesnych modeli SEM.\nFormalnie model analizy ścieżkowej można zapisać jako system równań liniowych \\[\n\\mathbf{y} = B\\mathbf{y} + \\Gamma \\mathbf{x} + \\zeta,\n\\] gdzie:\n\n\n\\(\\mathbf{y}\\) to wektor zmiennych endogenicznych (wyjaśnianych w modelu),\n\n\\(\\mathbf{x}\\) to wektor zmiennych egzogenicznych (traktowanych jako dane, nieobjaśniane w modelu),\n\n\\(B\\) to macierz współczynników regresji pomiędzy zmiennymi endogenicznymi,\n\n\\(\\Gamma\\) to macierz współczynników regresji łączących zmienne egzogeniczne z endogenicznymi,\n\n\\(\\zeta\\) to wektor zakłóceń (błędów strukturalnych).\n\nZałożenia analizy ścieżkowej są w dużej mierze zbieżne z klasycznymi założeniami regresji liniowej. Obejmują one liniowość zależności, brak silnej współliniowości między predyktorami, nieskorelowanie błędów \\(\\zeta\\) z egzogenicznymi zmiennymi \\(\\mathbf{x}\\) oraz odpowiednio dużą próbę, aby zapewnić stabilność estymacji. Dodatkowo zakłada się poprawność teoretyczną modelu – to badacz definiuje strukturę ścieżek na podstawie teorii lub wcześniejszych wyników, a analiza ma na celu jej statystyczną weryfikację.\nEstymacja parametrów w analizie ścieżkowej opiera się najczęściej na metodzie największej wiarygodności (maximum likelihood, ML), która minimalizuje różnicę między macierzą kowariancji obserwowanej a macierzą kowariancji implikowaną przez model. Alternatywnie stosuje się metody oparte na najmniejszych kwadratach (generalized least squares, GLS, ordinary least squares, OLS) (Schweizer i DiStefano 2016), a w przypadku naruszenia normalności rozkładu dostępne są warianty odporne, takie jak robust ML (Schweizer i DiStefano 2016) czy estymacja asymptotycznie niezależna (asymptotically distribution free, ADF) (Huang i Bentler 2015). W nowszych zastosowaniach wykorzystuje się również metody bayesowskie, które pozwalają wprowadzić rozkłady a priori dla parametrów i prowadzić wnioskowanie probabilistyczne o strukturze zależności.\n\nPrzykład 4.2 Wykonamy prostą analizę ścieżkową na danych mtcars, aby zbadać wpływ masy pojazdu (wt) na jego zużycie paliwa (mpg), za pośrednictwem mocy silnika (hp). Hipoteza zakłada, że masa wpływa na moc, która z kolei wpływa na zużycie paliwa.\n\nKod# Model ścieżkowy (wyłącznie zmienne obserwowalne)\n# hp jest mediatorem między wt a mpg\nmodel_pa &lt;- '\n  # równania regresji (część strukturalna)\n  mpg ~ c*wt + b*hp\n  hp  ~ a*wt\n\n  # efekty pośrednie i całkowite\n  ind := a*b\n  tot := c + (a*b)\n'\n\nfit_pa &lt;- sem(model_pa, data = mtcars,\n              estimator = \"MLR\")                         # estymator odporny (robust ML)\n\n# Podsumowanie wyników (standaryzacja, istotności, efekty zdefiniowane)\nsummary(fit_pa, standardized = TRUE, ci = TRUE, rsquare = TRUE)\n\nlavaan 0.6-20 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                            32\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  mpg ~                                                                 \n    wt         (c)   -3.878    0.620   -6.255    0.000   -5.093   -2.663\n    hp         (b)   -0.032    0.007   -4.781    0.000   -0.045   -0.019\n  hp ~                                                                  \n    wt         (a)   46.160    5.734    8.051    0.000   34.922   57.398\n   Std.lv  Std.all\n                  \n   -3.878   -0.630\n   -0.032   -0.361\n                  \n   46.160    0.659\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .mpg               6.095    1.645    3.705    0.000    2.870    9.320\n   .hp             2577.777  996.624    2.587    0.010  624.430 4531.125\n   Std.lv  Std.all\n    6.095    0.173\n 2577.777    0.566\n\nR-Square:\n                   Estimate\n    mpg               0.827\n    hp                0.434\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ind              -1.467    0.351   -4.179    0.000   -2.155   -0.779\n    tot              -5.344    0.634   -8.434    0.000   -6.587   -4.102\n   Std.lv  Std.all\n   -1.467   -0.238\n   -5.344   -0.868\n\nKod# Wizualizacja diagramu ścieżek\nsemPaths(fit_pa,\n         what = \"std\", \n         whatLabels = \"std\",\n         layout = \"circle\",\n         style = \"lisrel\",\n         residuals = FALSE, intercepts = FALSE,\n         nCharNodes = 0, sizeMan = 7,\n         groups = \"manifests\",\n         color = brewer.pal(3, \"Pastel2\"),\n         edge.color = \"grey60\")\n\n\n\n\n\n\n\nModel zapisujemy jako: \\[\n\\mathbf{y} = B\\mathbf{y} + \\Gamma \\mathbf{x} + \\zeta,\n\\] gdzie:\n\n\n\\(\\mathbf{y} = \\begin{bmatrix} mpg \\\\ hp \\end{bmatrix}\\) to zmienne endogeniczne,\n\n\\(\\mathbf{x} = wt\\) to zmienna egzogeniczna,\n\n\\(B\\) to macierz regresji pomiędzy zmiennymi endogenicznymi,\n\n\\(\\Gamma\\) to macierz efektów zmiennych egzogenicznych na endogeniczne,\n\n\\(\\zeta\\) to wektor błędów strukturalnych.\n\nEstymowane równania \\[\n\\begin{aligned}\nmpg &= c \\cdot wt + b \\cdot hp + \\zeta_{mpg}, \\\\\nhp  &= a \\cdot wt + \\zeta_{hp},\n\\end{aligned}\n\\] gdzie:\n\n\\(a = 46.160\\) (standaryzowane \\(0.659\\)) – wpływ masy (wt) na moc (hp),\n\\(b = -0.032\\) (standaryzowane \\(-0.361\\)) – wpływ mocy (hp) na spalanie (mpg),\n\\(c = -3.878\\) (standaryzowane \\(-0.630\\)) – bezpośredni wpływ masy (wt) na spalanie (mpg).\nMacierz \\(B\\) (zależności między endogenicznymi): \\[\nB =\n\\begin{bmatrix}\n0 & b \\\\\n0 & 0\n\\end{bmatrix},\n\\quad b = -0.032.\n\\]\nMacierz \\(\\Gamma\\) (wpływy egzogenicznej zmiennej \\(wt\\)): \\[\n\\Gamma =\n\\begin{bmatrix}\nc \\\\\na\n\\end{bmatrix},\n\\quad c = -3.878, \\quad a = 46.160.\n\\]\nWariancje resztowe (błędy strukturalne): \\[\n\\mathrm{Var}(\\zeta_{mpg}) = 6.095 \\; (17.3\\%),\n\\quad \\mathrm{Var}(\\zeta_{hp}) = 2577.777 \\; (56.6\\%).\n\\]\n\nOznacza to, że model wyjaśnia 82.7% wariancji spalania i 43.4% wariancji mocy.\n\nEfekt pośredni masy na spalanie przez moc \\[\nind = a \\cdot b = 46.160 \\cdot (-0.032) = -1.467\n\\] istotny statystycznie (\\(p &lt; 0.001\\)).\nEfekt całkowity masy na spalanie: \\[\ntot = c + a \\cdot b = -3.878 + (-1.467) = -5.344.\n\\] Oznacza to, że wzrost masy samochodu (o jednostkę standaryzowaną) zmniejsza spalanie o 0.868 jednostki standardowej – w dużej części bezpośrednio, a w mniejszej poprzez wzrost mocy.\n\nModel ścieżkowy wskazuje, że masa samochodu (wt) ma silny negatywny wpływ na oszczędność paliwa (mpg), zarówno bezpośrednio, jak i pośrednio poprzez zwiększanie mocy silnika (hp). Moc natomiast sama w sobie pogarsza spalanie. Wartości \\(R^2\\) potwierdzają, że model bardzo dobrze wyjaśnia zmienność mpg (83%), ale umiarkowanie słabiej radzi sobie z hp (43%).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#założenia-modeli-sem",
    "href": "sem.html#założenia-modeli-sem",
    "title": "Modele strukturalne",
    "section": "Założenia modeli SEM",
    "text": "Założenia modeli SEM\n\nOparcie na teorii - SEM z definicji służy testowaniu i potwierdzaniu modelu teoretycznego. Dlatego punktem wyjścia musi być koncepcja badawcza oparta na wcześniejszych badaniach i spójnej teorii. Model powinien odzwierciedlać hipotezy dotyczące relacji między konstruktami latentnymi i zmiennymi obserwowalnymi.\n\nWielkość próby - zaleca się próby liczące co najmniej około 200 obserwacji, choć ostateczny wymóg zależy od trzech czynników:\n\nrozkładu zmiennych,\nzłożoności modelu,\nmetody estymacji.\n\nDuże próby zwiększają stabilność wyników i odporność na naruszenia założeń.\n\nNormalność rozkładu - ponieważ SEM opiera się na macierzy kowariancji, standardowo zakłada się wielowymiarową normalność rozkładu zmiennych. W praktyce odchylenia od normalności można kompensować, stosując estymatory odporne, np. robust ML czy WLSMV.\nLiniowość związków - zakłada się, że relacje między konstruktami latentnymi a wskaźnikami obserwowalnymi oraz między zmiennymi latentnymi mają charakter liniowy.\nBrak silnej współliniowości - predyktory w modelu powinny być możliwie niezależne. Choć umiarkowana współliniowość zwykle nie jest problemem, silne korelacje mogą prowadzić do trudności w estymacji i interpretacji ścieżek.\nKompletność danych - modele SEM wymagają pełnych danych. Można to osiągnąć poprzez imputację (np. średnią, regresję) albo stosując metody wykorzystujące pełną informację przy brakach danych, jak FIML (Full Information Maximum Likelihood).\nNiezależność błędów pomiarowych - standardowe założenie głosi, że błędy pomiarowe są nieskorelowane. W praktyce jednak niekiedy dopuszcza się ich korelacje, zwłaszcza gdy sugerują to indeksy modyfikacyjne i uzasadnia teoria.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#ocena-dopasowania-modelu-sem",
    "href": "sem.html#ocena-dopasowania-modelu-sem",
    "title": "Modele strukturalne",
    "section": "Ocena dopasowania modelu SEM",
    "text": "Ocena dopasowania modelu SEM\n\n\n\n\n\n\n\nWskaźnik\nWartość idealna\nWartość akceptowalna\n\n\n\nChi-kwadrat (CMIN) *\np &gt; 0,05 (przy α = 0,05)\np &lt; 0,05 (przy α = 0,05)\n\n\nStandaryzowany chi-kwadrat (CMIN/df) *\n&lt; 3\n&lt; 5\n\n\nGFI (Goodness of Fit Index)\n&gt; 0,95\n&gt; 0,90\n\n\nAGFI (Adjusted GFI)\n&gt; 0,90\n&gt; 0,85\n\n\nCFI (Comparative Fit Index) *\n&gt; 0,95\n&gt; 0,90\n\n\nTLI (Tucker-Lewis Index, NNFI)\n&gt; 0,90\n&gt; 0,85\n\n\nNFI (Normed Fit Index)\n&gt; 0,95\n&gt; 0,90\n\n\nPGFI (Parsimonious GFI)\n&gt; 0,50\nbrak sztywnych progów\n\n\nPNFI (Parsimonious NFI)\n&gt; 0,50\nbrak sztywnych progów\n\n\nPCFI (Parsimonious CFI)\n&gt; 0,50\nbrak sztywnych progów\n\n\nSRMR (Standardized RMR) *\n&lt; 0,05\n&lt; 0,08\n\n\nRMSEA (Root Mean Square Error of Approximation) *\n&lt; 0,05 [90% CI]\n&lt; 0,10 [90% CI]\n\n\n\n\nPrzykład 4.3 Zbiór HolzingerSwineford1939 (pakietu lavaan) zawiera wyniki uczniów w dziewięciu testach poznawczych oraz podstawowe cechy demograficzne i szkolne (Turney 1939). Dziewięć pozycji testowych tworzy trzy klasyczne domeny poznawcze: visual (postrzeganie wzrokowe), textual (kompetencje werbalne) i speed (szybkość przetwarzania), po trzy wskaźniki w każdej domenie. Oryginalne zmienne testowe oznaczone są jako x1–x9 i w literaturze przypisuje się je do czynników: - x1, x2, x3 → czynnik Visual, - x4, x5, x6 → czynnik Textual, - x7, x8, x9 → czynnik Speed.\nW danych można znaleźć też zmienne: ageyr (wiek w latach), agemo (nadwyżka miesięcy), sex (płeć, kod 1 = chłopiec, 2 = dziewczynka), school (szkoła), grade (klasa). Do modelu wprowadzony zostanie wiek w latach ciągłych: age = ageyr + agemo/12, a płeć zostanie przekodowana binarnie (sex01: 0 = dziewczynka, 1 = chłopiec) dla przejrzystości interpretacji.\nHipotezy badawcze (wpływy bezpośrednie i pośrednie)\nPrzyjmiemy klasyczną trójczynnikową strukturę pomiarową (Visual, Textual, Speed), a w części strukturalnej założymy wpływy wieku i płci na latentne zdolności oraz zależność między zdolnościami:\n\n\n\\(H_0^1:\\) Wiek dodatnio wpływa na Visual i Textual oraz – pośrednio – na Speed.\n\n\\(H_0^2:\\) Płeć (kod 1 = chłopiec) różnicuje profile: dodatnio wpływa na Visual, natomiast słabiej lub ujemnie na Textual; wpływ na Speed występuje pośrednio poprzez Visual i Textual.\n\n\\(H_0^3:\\) Czynnik Speed zależy wprost od Visual i Textual; efekty wieku i płci na Speed będą zatem częściowo pośredniczone przez Visual i Textual.\n\n\nKoddata(\"HolzingerSwineford1939\")\n\n# Przygotowanie zmiennych egzogenicznych\nhs &lt;- within(HolzingerSwineford1939, {\n  age &lt;- ageyr + agemo/12\n  sex01 &lt;- as.numeric(sex == 1)  # 1=boy, 0=girl (w razie innego kodowania dostosować)\n})\n\n# Specyfikacja modelu SEM: część pomiarowa (CFA) + część strukturalna\nmodel_sem &lt;- '\n  # Część pomiarowa (CFA)\n  Visual  =~ x1 + x2 + x3\n  Textual =~ x4 + x5 + x6\n  Speed   =~ x7 + x8 + x9\n\n  # Część strukturalna (path analysis na latentach)\n  Speed   ~ b1*Visual + b2*Textual\n  Visual  ~ a1*age + a2*sex01\n  Textual ~ a3*age + a4*sex01\n\n  # Efekty pośrednie wieku i płci na Speed\n  ind_age  := a1*b1 + a3*b2\n  ind_sex  := a2*b1 + a4*b2\n\n  # Efekty całkowite wieku i płci na Speed\n  tot_age  := ind_age\n  tot_sex  := ind_sex\n'\n\nfit_sem &lt;- sem(model_sem, data = hs,\n               estimator = \"MLR\",      # robust ML\n               meanstructure = TRUE)\n\n\nW części pomiarowej zdefiniowano trzy czynniki pierwszego rzędu z klasycznym mapowaniem wskaźników. W części strukturalnej założono, że Visual i Textual determinują Speed, a age i sex01 oddziałują na Visual i Textual. Zdefiniowano także etykiety ścieżek, aby policzyć efekty pośrednie i całkowite (:=). Parametry raportowane są w skalach surowych i standaryzowanych.\n\nKodmodel_performance(fit_sem, c(\"Chi2\",\"Chi2_df\",\"p_Chi2\",\"CFI\",\"TLI\",\"RMSEA\",\"RMSEA_CI_low\",\"RMSEA_CI_high\",\"SRMR\")) %&gt;% \n  print_html()\n\n\n\n\n\nChi2\nChi2_df\np_Chi2\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nSRMR\n\n\n172.77\n39\n0\n0.86\n0.11\n0.09\n0.12\n0.11\n\n\n\n\n\nOcena dopasowania modelu SEM zawsze powinna być przeprowadzona z kilku perspektyw: testu chi-kwadrat, wskaźników dopasowania przyrostowych (incremental fit indices) oraz wskaźników błędu aproksymacji. Wyniki uzyskane w analizie wskazują na istotne sygnały niedopasowania modelu.\nTest chi-kwadrat dla modelu dał wartość \\(\\chi^2 = 172,77\\) przy df = 39, co przy dużej liczności prowadzi do p &lt; 0.001. Oznacza to, że w sensie dosłownym odrzucamy hipotezę o pełnym zgodnym odwzorowaniu macierzy kowariancji w populacji przez model. Jednak test chi-kwadrat jest bardzo wrażliwy zarówno na rozmiar próby, jak i złożoność modelu, dlatego wynik ten traktuje się raczej jako punkt wyjścia niż rozstrzygające kryterium.\nWskaźniki przyrostowe pokazują umiarkowanie słabe dopasowanie. Wartości CFI = 0.858 i TLI = 0.803 są wyraźnie poniżej rekomendowanego poziomu 0.90, a tym bardziej 0.95, które zwykle przyjmuje się jako granicę bardzo dobrego dopasowania. To sugeruje, że model w obecnej postaci nie wyjaśnia wystarczająco dobrze struktury zależności obserwowanych w danych i potencjalnie wymaga modyfikacji – np. dodania powiązań reszt, rewizji struktury ścieżek lub przemyślenia samego modelu pomiarowego.\nWskaźnik błędu aproksymacji RMSEA = 0.107 (90% CI: 0.091–0.123) jest stosunkowo wysoki i wykracza poza granicę akceptowalności (zwykle &lt; 0.08, a najlepiej &lt; 0.05). Taki wynik sugeruje, że model charakteryzuje się zauważalnym błędem przybliżenia w stosunku do danych populacyjnych. Z kolei wskaźnik SRMR = 0.108 jest powyżej standardowego progu akceptowalności 0.08, co dodatkowo wskazuje na problemy z odwzorowaniem korelacji obserwowanych przez model.\nChcąc poprawić dopasowanie modelu można zaproponować następujące modyfikacje:\n\nPo pierwsze, dopuścić kowariancję zaburzeń zmiennych latentnych Visual i Textual. W praktyce te dwa konstrukty współdzielą wariancję nie w pełni wyjaśnioną przez wiek i płeć. Dodanie Visual ~~ Textual nie zmienia hipotez o wpływach na Speed, a często istotnie obniża błąd aproksymacji.\nPo drugie, w części pomiarowej można dopuścić wyłącznie te kowariancje reszt wskaźników, które mają jednoznaczne uzasadnienie treściowe. W HolzingerSwineford1939 typowe pary to x1–x2, x2–x3 (ten sam kanał wizualny), x4–x5 (werbalne), x7–x8 (szybkość). Te powiązania korygują lokalne niedopasowania bez naruszania sensu hipotez strukturalnych.\nPo trzecie, skontrolować jakość wskaźników. Jeżeli którykolwiek ładunek w CFA jest niski (np. &lt; 0.40) i generuje duże reszty, rozważyć jego usunięcie lub zamianę (jeśli masz silne uzasadnienie teoretyczne). Usunięcie pojedynczego, słabego wskaźnika często stabilizuje model.\nPo czwarte, uwzględnić współzmienność zmiennych egzogenicznych (age ~~ sex01). To technicznie poprawne i zapobiega „wpychaniu” ich korelacji w część strukturalną.\nPo piąte, upewniać się, że estymacja odpowiada naturze danych. Jeśli wskaźniki są porządkowe, stosować estymację DWLS i macierz polichoryczną; przy ciągłych pozostawić MLR (w naszym przypadku wyniki nie są ze skali Likerta, oryginalne dane zawierały zmienne z różnego zakresu).\nPo szóste, można rozważyć słabą nieliniowość wieku (age^2) tylko wtedy, gdy wskazują na to reszty i teoria; nie zmienia to sensu głównych hipotez (dalej wiek → Visual/Textual → Speed), ale bywa, że poprawia dopasowanie.\n\nPoniżej wariant modelu z minimalnymi, teoretycznie uzasadnionymi modyfikacjami.\n\nKodmodel_sem_refined &lt;- '\n  # CFA\n  Visual  =~ x1 + x2 + x3\n  Textual =~ x4 + x5 + x6\n  Speed   =~ x7 + x8 + x9\n\n  # Strukturalny (bez zmian hipotez)\n  Speed   ~ b1*Visual + b2*Textual\n  Visual  ~ a1*age + a2*sex01\n  Textual ~ a3*age + a4*sex01\n\n  # Dodatkowe kowariancje zgodne z teorią\n  Visual ~~ Textual          # współdzielona wariancja latentów\n  age ~~ sex01               # współzmienność egzogenicznych\n\n  # Skorelowane unikalności (tylko wewnątrz domen i z uzasadnieniem treściowym)\n  x1 ~~ x2\n  x2 ~~ x3\n  x4 ~~ x5\n  x7 ~~ x8\n\n  # Efekty pośrednie i całkowite (jak dotąd)\n  ind_age  := a1*b1 + a3*b2\n  ind_sex  := a2*b1 + a4*b2\n  tot_age  := ind_age\n  tot_sex  := ind_sex\n'\n\nfit_sem_refined &lt;- sem(model_sem_refined, data = hs,\n                       estimator = \"MLR\", \n                       meanstructure = TRUE)\n\nmodel_performance(fit_sem_refined, c(\"Chi2\",\"Chi2_df\",\"p_Chi2\",\"CFI\",\"TLI\",\"RMSEA\",\"RMSEA_CI_low\",\"RMSEA_CI_high\",\"SRMR\")) %&gt;% \n  print_html()\n\n\n\n\n\nChi2\nChi2_df\np_Chi2\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nSRMR\n\n\n97.03\n34\n5.64e-08\n0.93\n0.08\n0.06\n0.10\n0.06\n\n\n\n\nKod# Pomocniczo: gdzie są największe niedopasowania?\nmodindices(fit_sem, sort.=TRUE, minimum.value = 10)[1:12, c(\"lhs\",\"op\",\"rhs\",\"mi\",\"epc\",\"sepc.all\")] %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = c(\"mi\",\"epc\",\"sepc.all\"), decimals = 3) \n\n\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.all\n\n\n\nTextual\n~\nSpeed\n47.445\n1.782\n1.118\n\n\nx7\n~~\nx8\n36.699\n0.649\n1.063\n\n\nTextual\n=~\nx1\n34.020\n0.374\n0.316\n\n\nVisual\n~~\nTextual\n30.719\n0.282\n0.419\n\n\nTextual\n~\nVisual\n30.719\n0.567\n0.414\n\n\nVisual\n~\nTextual\n30.719\n0.310\n0.423\n\n\nVisual\n=~\nx9\n26.943\n0.558\n0.401\n\n\nage\n~\nSpeed\n25.940\n0.694\n0.421\n\n\nSpeed\n~\nage\n22.920\n0.200\n0.330\n\n\nVisual\n~\nSpeed\n20.687\n1.558\n1.337\n\n\nTextual\n~~\nSpeed\n18.965\n0.696\n1.307\n\n\nVisual\n=~\nx7\n18.063\n−0.483\n−0.321\n\n\n\n\n\n\nObecny model po modyfikacjach prezentuje już znacznie lepsze dopasowanie niż pierwotny, choć nadal nie jest idealny. Wskaźniki globalne wskazują, że dopasowanie można uznać za umiarkowanie dobre. Statystyka \\(\\chi^2\\) (97.03, df = 34, p &lt; 0.001) nadal jest istotna, co przy relatywnie małej liczbie stopni swobody sygnalizuje pewne niedopasowanie modelu do danych. Jednak należy pamiętać, że test \\(\\chi^2\\) jest bardzo czuły i w praktyce często odrzuca modele nawet przy akceptowalnym dopasowaniu. Lepszą informację dają indeksy: CFI = 0.933 mieści się w strefie „akceptowalnej”, ale jeszcze poniżej progu 0.95 sugerującego bardzo dobre dopasowanie. TLI = 0.892 jest blisko progu 0.90 i również wskazuje na umiarkowane dopasowanie. RMSEA = 0.078 (90% CI: 0.060–0.097) mieści się w przedziale akceptowalnym (&lt; 0.10), a dolna granica jest blisko 0.05, co sugeruje, że model jest względnie bliski dobrego dopasowania. SRMR = 0.055 jest niewiele powyżej granicy 0.05 i można go uznać za dość dobry wynik.\nAnaliza indeksów modyfikacyjnych1 pokazuje, że największe niedopasowania koncentrują się w kilku obszarach. Po pierwsze, sugerowane są dodatkowe powiązania strukturalne między czynnikami latentnymi a zmienną Speed (np. Textual ~ Speed, Visual ~ Speed), które jednak wykraczałyby poza pierwotnie założone hipotezy mediacyjne. Po drugie, wskazywane są silne powiązania między wskaźnikami tego samego czynnika (np. x7 ~~ x8), co można interpretować jako efekty metody lub nadmierne podobieństwo treściowe pozycji testowych. Po trzecie, pojawiają się sugestie dotyczące alternatywnych ładunków wskaźników (np. Textual =~ x1, Visual =~ x9), co wskazuje na pewne problemy z czystością czynników, ale ich wprowadzenie mogłoby zmienić interpretację teoretyczną czynników.\n1 Indeksy modyfikacyjne (modification indices, MI) wskazują, o ile zmniejszyłby się chi-kwadrat modelu, gdyby wprowadzono daną modyfikację (np. dodano ścieżkę lub skorelowano błędy). Wysokie wartości MI sugerują potencjalne obszary niedopasowania modelu do danych i mogą być punktem wyjścia do rozważań nad jego ulepszeniem. Należy jednak podchodzić do nich ostrożnie i zawsze w kontekście teorii, aby uniknąć nadmiernego dopasowania modelu do konkretnego zbioru danych.Godząc się na nieidealne dopasowanie, można uznać, że model w obecnej formie jest wystarczająco dobry do testowania głównych hipotez badawczych. Wprowadzenie pewnych zmian sugerowanych przez indeksy modyfikacyjne mogłoby utrudnić weryfikację postawionych hipotez.\n\nKodmodel_parameters(fit_sem_refined, standardized = TRUE, ci = TRUE) \n\n# Loading\n\nLink          | Coefficient |   SE |     100% CI |     z |      p\n-----------------------------------------------------------------\nVisual =~ x1  |        1.00 | 0.00 |             |       | &lt; .001\nVisual =~ x2  |        0.54 | 0.12 | [-Inf, Inf] |  4.52 | &lt; .001\nVisual =~ x3  |        0.69 | 0.12 | [-Inf, Inf] |  5.77 | &lt; .001\nTextual =~ x4 |        1.00 | 0.00 |             |       | &lt; .001\nTextual =~ x5 |        1.11 | 0.07 | [-Inf, Inf] | 16.57 | &lt; .001\nTextual =~ x6 |        0.94 | 0.13 | [-Inf, Inf] |  7.20 | &lt; .001\nSpeed =~ x7   |        1.00 | 0.00 |             |       | &lt; .001\nSpeed =~ x8   |        1.26 | 0.21 | [-Inf, Inf] |  6.03 | &lt; .001\nSpeed =~ x9   |        2.42 | 0.58 | [-Inf, Inf] |  4.19 | &lt; .001\n\n# Regression\n\nLink                 | Coefficient |   SE |     100% CI |     z |      p\n------------------------------------------------------------------------\nSpeed ~ Visual (b1)  |        0.23 | 0.08 | [-Inf, Inf] |  2.78 | 0.005 \nSpeed ~ Textual (b2) |    8.44e-03 | 0.03 | [-Inf, Inf] |  0.27 | 0.786 \nVisual ~ age (a1)    |   -9.65e-03 | 0.07 | [-Inf, Inf] | -0.13 | 0.897 \nVisual ~ sex01 (a2)  |        0.24 | 0.13 | [-Inf, Inf] |  1.78 | 0.074 \nTextual ~ age (a3)   |       -0.23 | 0.06 | [-Inf, Inf] | -3.79 | &lt; .001\nTextual ~ sex01 (a4) |       -0.07 | 0.14 | [-Inf, Inf] | -0.50 | 0.615 \n\n# Correlation\n\nLink              | Coefficient |   SE |     100% CI |     z |      p\n---------------------------------------------------------------------\nVisual ~~ Textual |        0.43 | 0.09 | [-Inf, Inf] |  4.91 | &lt; .001\nage ~~ sex01      |        0.08 | 0.03 | [-Inf, Inf] |  2.79 | 0.005 \nx1 ~~ x2          |       -0.04 | 0.10 | [-Inf, Inf] | -0.40 | 0.690 \nx2 ~~ x3          |        0.14 | 0.09 | [-Inf, Inf] |  1.52 | 0.129 \nx4 ~~ x5          |        0.03 | 0.12 | [-Inf, Inf] |  0.25 | 0.805 \nx7 ~~ x8          |        0.34 | 0.07 | [-Inf, Inf] |  5.22 | &lt; .001\n\n# Defined\n\nTo        | Coefficient |   SE |     100% CI |     z |     p\n------------------------------------------------------------\n(ind_age) |   -4.15e-03 | 0.02 | [-Inf, Inf] | -0.22 | 0.828\n(ind_sex) |        0.05 | 0.03 | [-Inf, Inf] |  1.69 | 0.091\n(tot_age) |   -4.15e-03 | 0.02 | [-Inf, Inf] | -0.22 | 0.828\n(tot_sex) |        0.05 | 0.03 | [-Inf, Inf] |  1.69 | 0.091\n\n\nModel pomiarowy\nWyniki dla czynników latentnych (Visual, Textual, Speed) pokazują, że wszystkie wskaźniki (x1–x9) istotnie ładują się na odpowiednich czynnikach (p &lt; 0.001), co potwierdza poprawność konstrukcji pomiarowej. Ładunki są zróżnicowane: np. dla Visual zmienne x2 i x3 mają umiarkowane wartości (0.54, 0.69), natomiast dla Textual wskaźniki x5 i x6 są mocno związane z czynnikiem (ok. 1.1 i 0.94). Czynnik Speed jest silnie określony przez x7–x9, przy czym x9 ma wyjątkowo wysoki ładunek (2.42), co może sugerować, że ta zmienna dominuje w definiowaniu konstruktu. Generalnie jednak wszystkie zmienne obserwowalne wnoszą istotny wkład, co wspiera trafność pomiarową.\nModel strukturalny\nHipotezy dotyczące wpływów bezpośrednich częściowo się potwierdziły. Czynnik Visual istotnie przewiduje Speed (b1 = 0.23, p = 0.005), co oznacza, że im wyższe zdolności wizualne, tym lepsze wyniki w zadaniach szybkościowych. Z kolei wpływ Textual na Speed okazał się nieistotny (b2 ≈ 0, p = 0.786), co przeczy hipotezie o jego znaczącym wkładzie. Wśród zmiennych egzogenicznych, wiek istotnie i negatywnie wpływa na Textual (a3 = -0.23, p &lt; 0.001), co można interpretować tak, że starsze osoby mają gorsze wyniki w zadaniach tekstowych. Płeć (sex01) nie odgrywa istotnej roli ani w Visual, ani w Textual (p &gt; 0.05), choć dla Visual efekt był bliski istotności (a2 = 0.24, p = 0.074), sugerując potencjalny trend.\nZależności między czynnikami\nIstnieje istotna korelacja między Visual i Textual (0.43, p &lt; .001), co wskazuje, że obie zdolności współwystępują, ale nie są tożsame. Korelacja ta wspiera tezę o współzależności różnych typów zdolności poznawczych. Dodatkowo, zidentyfikowano korelację między zmiennymi resztowymi x7 i x8 (0.34, p &lt; .001), co można interpretować jako częściowo wspólny czynnik specyficzny dla tych zadań szybkościowych.\nEfekty pośrednie i całkowite\nŚcieżki pośrednie przez Visual i Textual nie były istotne w przypadku wieku (ind_age ≈ 0, p = 0.828), natomiast dla płci (ind_sex = 0.05, p = 0.091) pojawił się trend w kierunku efektu pośredniego, choć bez pełnej istotności. Efekty całkowite (tot_age, tot_sex) powtarzają te same wnioski – brak efektów dla wieku i jedynie potencjalny, słaby wpływ płci.\nPodsumowanie\nModel po modyfikacjach w poprawny sposób mierzy czynniki i potwierdza istotną rolę zdolności wizualnych w wyjaśnianiu szybkości, podczas gdy zdolności tekstowe odgrywają mniejszą rolę. Wiek ma wyraźny negatywny wpływ na zdolności tekstowe, natomiast płeć nie wpływa istotnie na żaden z czynników, choć jej rola dla zdolności wizualnych może wymagać dalszej analizy. Wyniki wskazują, że hipotezy dotyczące bezpośredniego wpływu Visual na Speed oraz wieku na Textual znajdują potwierdzenie, natomiast hipotezy dotyczące Textual → Speed i sex01 → czynniki nie znajdują mocnego wsparcia.\n\nKod# Wizualizacja modelu SEM\nsemPaths(fit_sem_refined,\n         what = \"std\", \n         whatLabels = \"std\",\n         layout = \"tree2\",\n         style = \"lisrel\",\n         residuals = T, intercepts = F,\n         nCharNodes = 0, sizeMan = 5,\n         groups = \"latent\",\n         color = brewer.pal(3, \"Pastel2\"),\n         edge.color = \"grey60\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#modele-strukturalne-sem",
    "href": "sem.html#modele-strukturalne-sem",
    "title": "Modele strukturalne",
    "section": "Modele strukturalne (SEM)",
    "text": "Modele strukturalne (SEM)\nModele typu covariance-based structural equation modeling (CB-SEM), określane po prostu jako SEM, stanowią rozwinięcie i uogólnienie dwóch podejść: analizy czynnikowej (CFA) oraz analizy ścieżkowej (path analysis). Istota SEM polega na tym, że pozwala ono jednocześnie badamy trafność pomiaru zmiennych latentnych oraz testujemy hipotezy dotyczące relacji między tymi zmiennymi. Dzięki temu SEM stanowi narzędzie integrujące w sobie modelowanie pomiarowe i strukturalne, umożliwiając analizę złożonych układów zależności obserwowalnych i nieobserwowalnych.\nFormalnie model SEM zapisuje się jako system równań macierzowych. Model pomiarowy dla zmiennych egzogenicznych ma postać \\[\n\\mathbf{x} = \\Lambda_x \\boldsymbol{\\xi} + \\boldsymbol{\\delta},\n\\] gdzie \\(\\mathbf{x}\\) to wektor zmiennych obserwowalnych, \\(\\boldsymbol{\\xi}\\) – wektor latentnych zmiennych egzogenicznych, \\(\\Lambda_x\\) – macierz ładunków czynnikowych, a \\(\\boldsymbol{\\delta}\\) – błędy pomiarowe. Analogicznie model pomiarowy dla zmiennych endogenicznych przyjmuje formę \\[\n\\mathbf{y} = \\Lambda_y \\boldsymbol{\\eta} + \\boldsymbol{\\epsilon},\n\\] gdzie \\(\\mathbf{y}\\) oznacza obserwowalne zmienne endogeniczne, \\(\\boldsymbol{\\eta}\\) – latentne zmienne endogeniczne, \\(\\Lambda_y\\) – macierz ładunków, a \\(\\boldsymbol{\\epsilon}\\) – błędy pomiaru. Trzecim elementem jest model strukturalny \\[\n\\boldsymbol{\\eta} = B \\boldsymbol{\\eta} + \\Gamma \\boldsymbol{\\xi} + \\boldsymbol{\\zeta},\n\\] który opisuje relacje pomiędzy zmiennymi latentnymi endogenicznymi \\((B)\\) oraz wpływ zmiennych egzogenicznych na endogeniczne \\((\\Gamma)\\), z uwzględnieniem zakłóceń strukturalnych \\((\\boldsymbol{\\zeta})\\).\nW modelach SEM kluczowe znaczenie mają zmienne latentne \\((\\xi, \\eta)\\), które reprezentują konstrukty teoretyczne trudne do bezpośredniego pomiaru, np. satysfakcję z życia czy strategie uczenia się. Zmienne obserwowalne \\((x, y)\\) stanowią wskaźniki tych konstruktów. Ładunki czynnikowe \\(\\Lambda\\) wskazują, jak silnie dana zmienna obserwowalna powiązana jest z konstruktem latentnym. Macierze \\(B\\) i \\(\\Gamma\\) opisują odpowiednio zależności między konstruktami oraz ich uwarunkowania przez zmienne egzogeniczne. Błędy pomiarowe \\((\\delta, \\epsilon)\\) i zakłócenia strukturalne (\\(\\zeta\\)) odzwierciedlają niewyjaśnioną wariancję.\nParametry SEM mogą być estymowane różnymi metodami. Najczęściej stosuje się metodę największej wiarygodności (ML), która minimalizuje rozbieżność między macierzą kowariancji modelową a empiryczną. Alternatywą są metody najmniejszych kwadratów: GLS (ang. Generalized Least Squares), ULS (ang. Unweighted Least Squares, mniej wymagająca co do rozkładów, lecz bez klasycznych testów istotności) oraz DWLS (ang. Diagonally Weighted Least Squares), szczególnie polecana przy danych porządkowych. W przypadku naruszeń normalności rozkładu stosuje się wersje odporne, takie jak MLR (ang. Maximum Likelihood Robust) czy MLM (ang. Maximum Likelihood Mean-adjusted), które korygują wariancje i błędy standardowe („Supplemental Material for The Performance of ML, DWLS, and ULS Estimation With Robust Corrections in Structural Equation Models With Ordinal Variables” 2016; KILIÇ, UYSAL, i ATAR 2020; Li 2021; Kyriazos i Poga-Kyriazou 2023).\nZnaczenie SEM polega na tym, że łączy ono analizę czynnikową i analizę ścieżkową w jeden spójny model. W części pomiarowej pozwala sprawdzić, czy narzędzie badawcze dobrze odwzorowuje zamierzone konstrukty, natomiast w części strukturalnej umożliwia testowanie hipotez o związkach między zmiennymi ukrytymi. Dzięki temu SEM jest traktowane jako złoty standard w psychometrii, naukach społecznych i zarządzaniu, oferując zarówno rzetelną ocenę jakości pomiaru, jak i analizę zależności przyczynowych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#cb-sem-vs.-pls-sem",
    "href": "sem.html#cb-sem-vs.-pls-sem",
    "title": "Modele strukturalne",
    "section": "CB-SEM vs. PLS-SEM",
    "text": "CB-SEM vs. PLS-SEM\nAnaliza równań strukturalnych (SEM) rozwija się w dwóch głównych nurtach: CB-SEM (ang. Covariance Based SEM) oraz PLS-SEM (ang. Partial Least Squares SEM). Różnica pomiędzy CB-SEM a PLS-SEM sprowadza się przede wszystkim do podejścia badawczego i celu analizy. CB-SEM koncentruje się na globalnym dopasowaniu całego modelu do danych i jest metodą konfirmacyjną – służy do testowania hipotez wyprowadzonych z teorii. Wymaga dobrze zdefiniowanego modelu, dużych prób i spełnienia klasycznych założeń statystycznych, a w zamian dostarcza bogaty zestaw wskaźników dopasowania i rzetelnych testów statystycznych. Z kolei PLS-SEM opiera się na minimalizacji reszt na poziomie zależności między zmiennymi i traktuje model bardziej jako narzędzie predykcyjne niż potwierdzające (Latan i Noonan 2017). Jest elastyczniejszy, lepiej sprawdza się przy małych próbach i nienormalnych danych, ale oferuje mniej rozwinięte kryteria dopasowania i bywa obciążony w sensie statystycznym. W praktyce CB-SEM wybiera się do badań potwierdzających teorię, a PLS-SEM – do badań eksploracyjnych i predykcyjnych.\nCB-SEM stosuje się głównie w sytuacjach, gdy badacz chce przetestować ugruntowany model teoretyczny, wymagający dużych prób i danych o normalnym rozkładzie. PLS-SEM natomiast okazuje się przydatny w badaniach eksploracyjnych, przy mniejszych próbach i danych odchylających się od normalności. Trzeba jednak pamiętać, że wyniki PLS-SEM mogą być bardziej obciążone, a sama metoda wciąż jest rozwijana. Najnowsze podejścia, takie jak PLSc-SEM, starają się połączyć zalety obu nurtów.\n\n\n\n\n\n\n\nKryterium\nCB-SEM\nPLS-SEM\n\n\n\nCel analizy\nPotwierdzanie całościowego modelu i dobrze zdefiniowanej teorii\nEksploracja i predykcja, rozwój teorii w początkowej fazie\n\n\nMetoda estymacji\nNajczęściej największa wiarygodność (ML), wymagająca normalności\nMetoda najmniejszych kwadratów (Partial Least Squares), odporna na nienormalność\n\n\nZmienne latentne\nModele czynnikowe (factor-based), akcent na konstrukty latentne\nModele kompozytowe (composite-based), akcent na wskaźniki i prognozowanie\n\n\nDopasowanie modelu\nBogaty zestaw wskaźników globalnego dopasowania (χ², RMSEA, CFI, GFI)\nOgraniczony zestaw wskaźników – głównie R², AVE, α Cronbacha\n\n\nElastyczność modelu\nMniej elastyczny, restrykcyjny, wymaga dokładnego określenia teorii\nBardziej elastyczny, radzi sobie z małymi próbami i brakiem normalności\n\n\nWymagania co do próby\nZwykle duża próba (≥200), dane normalne\nMoże być stosowany dla mniejszych prób, brak wymogu normalności\n\n\nTyp badań\nPotwierdzające, weryfikacja teorii\nEksploracyjne, poszukujące nowych zależności\n\n\nRozwój metody\nStabilna, ugruntowana tradycja\nWciąż rozwijana (np. PLSc-SEM), wyniki mogą być obciążone",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  },
  {
    "objectID": "sem.html#tworzenie-i-adaptacja-narzędzi-pomiarowych-w-sem",
    "href": "sem.html#tworzenie-i-adaptacja-narzędzi-pomiarowych-w-sem",
    "title": "Modele strukturalne",
    "section": "Tworzenie i adaptacja narzędzi pomiarowych w SEM",
    "text": "Tworzenie i adaptacja narzędzi pomiarowych w SEM\nPoniżej zostanie przedstawiona pełna, uporządkowana ścieżka tworzenia oraz adaptacji narzędzia pomiarowego (np. psychometrycznego) – od konceptualizacji do finalnego podręcznika – wraz z kluczowymi statystykami, ich wzorami, sposobem liczenia i interpretacją. Całość formułować w duchu klasycznej teorii testów, uzupełniając o elementy analizy czynnikowej oraz SEM.\n\nKonceptualizacja konstruktu i specyfikacja treści\n\nRozpoczynać od precyzyjnego zdefiniowania konstruktu (dziedzina, zakres, wymiary) na podstawie literatury. Tworzymy mapę treści (tzw. blueprint), która łączy wymiary teoretyczne z planowanymi obszarami itemów i formatem odpowiedzi. Już na tym etapie określamy typ modelu pomiarowego (refleksyjny czy formatywny)2, bo determinuje to dalszą metodologię.\n2 Model refleksyjny zakłada, że konstrukt latentny wywołuje pewien poziom odpowiedzi na pozycje: zmienne obserwowalne są efektami wspólnej przyczyny, ich błędy są specyficzne i niepowiązane, a wysoka współzależność pozycji jest oczekiwana. Model formatywny zakłada przeciwny kierunek przyczynowy: wskaźniki „tworzą” konstrukt (kompozyt), więc itemy nie muszą być skorelowane, a miary spójności wewnętrznej nie mają zastosowania.\n\nGenerowanie puli pozycji i weryfikacja treści\n\nBudujemy szeroką pulę itemów o zróżnicowanej trudności/poziomie (w testach osiągnięć) lub „natężeniu” treści (w skalach postaw). Zapewniać jednoznaczność językową, unikać sformułowań double-barreled („Czy jest Pan zadowolony z obsługi i ceny usługi?” - pytanie jest jednocześnie o obsługę i cenę) i niepotrzebnych zaprzeczeń („Nie zgadzam się z tym, że nie powinno się zabraniać palenia w restauracjach”).\nDla trafności treściowej stosuje się ocenę ekspertów. W praktyce używa się współczynnika Aikena \\(V\\) dla ocen skali porządkowej (np. 1–4) \\[\nV \\;=\\; \\frac{\\sum_{i=1}^{N} (s_i - s_{\\min})}{N\\,(s_{\\max}-s_{\\min})},\n\\] gdzie \\(s_i\\) to ocena \\(i\\)-tego eksperta, a \\(N\\) liczba ekspertów. Wysokie \\(V\\) (np. \\(\\ge 0,70\\)) sugeruje dobrą zgodność co do trafności treści.\n\nAdaptacja językowo-kulturowa\n\nPrzy przenoszeniu narzędzia między językami stosować forward translation (2 niezależne tłumaczenia), back-translation, konsensus zespołu i decentering (ew. korekta źródła). Wykonuje się cognitive interviews (parafrazy - powtarzanie pytań własnymi słowami, think-aloud - respondent mówi na głos, jak rozumie pytanie i dlaczego wybiera daną odpowiedź) w grupie docelowej, aby sprawdzić proces odpowiedzi. Wersję pilotażową poprzedza się audytem językowym i kulturowym przykładów/skal.\n\nPilotaż i analiza pozycji\n\nW badaniu pilotażowym szacuje się własności pozycji. W klasycznej teorii testów kluczowe są:\n\nkorelacja pozycja–wynik całkowity (skorygowana o daną pozycję); wartości \\(\\ge 0.30\\) wskazuje satysfakcjonującą dyskryminację;\ntrudność pozycji (w testach osiągnięć) jako średni wynik lub odsetek poprawnych odpowiedzi \\(p\\in[0,1]\\); pożądany rozkład trudności dla zakresu zdolności badanych;\nwpływ usunięcia pozycji na rzetelność (alpha if item deleted).\n\n\nWstępna struktura czynnikowa (EFA)\n\nNa oddzielnej próbie wykonuje się eksploracyjną analizę czynnikową (EFA). Ustala się liczbę czynników stosując* parallel analysis* i kryterium MAP Velicera. Następnie stosuje się estymatjcę modelu za pomocą PAF lub ML, z rotacją ortogonalną (np. varimax) lub ukośną (np. oblimin), zależnie od oczekiwanej korelacji czynników. Wartości ładunków \\(|\\lambda| \\ge 0.40\\) zwykle uznaje się za użyteczne; diagnozuje się ewentualne ładunki krzyżowe i jeśli takie wystąpią starami się je eliminować.\n\nKonfirmacja analiza czynniowa (CFA) i model pomiarowy\n\nUstalamy model \\[\n\\mathbf{x} \\;=\\; \\Lambda \\mathbf{f} \\;+\\; \\boldsymbol{\\epsilon},\n\\qquad \\mathrm{Cov}(\\mathbf{f})=\\Phi,\\quad \\mathrm{Cov}(\\boldsymbol{\\epsilon})=\\Psi,\n\\] co implikuje macierz kowariancji \\[\n\\Sigma(\\theta) \\;=\\; \\Lambda \\,\\Phi\\, \\Lambda^\\top \\;+\\; \\Psi.\n\\] Estymujemy parametry metodą ML lub odporną (np. MLR), dla danych porządkowych – DWLS. Ocena globalna dopasowania opierać na:\n\nstatystyce \\(\\chi^2\\) rozbieżności;\nRMSEA z 90% PU;\nCFI i TLI (przyrostowe w stosunku do modelu niezależnego):\n\n\\(\\mathrm{CFI} = 1 - \\frac{\\max(\\chi^2_{\\text{model}}-df_{\\text{model}},\\,0)}{\\max(\\chi^2_{\\text{baseline}}-df_{\\text{baseline}},\\,0)},\\)\n\\(\\mathrm{TLI} = \\frac{\\chi^2_{\\text{baseline}}/df_{\\text{baseline}} - \\chi^2_{\\text{model}}/df_{\\text{model}}}{\\chi^2_{\\text{baseline}}/df_{\\text{baseline}} - 1};\\)\n\n\nSRMR jako średni moduł reszt standaryzowanych.\n\n\nRzetelność skali\n\nW klasycznym ujęciu rzetelność skali \\(\\rho_{XX’}\\) to udział wariancji prawdziwej w wariancji obserwowanej: \\[\n\\rho_{XX’} \\;=\\; \\frac{\\sigma_{T}^2}{\\sigma_{X}^2}.\n\\] Najczęściej używane miary do oceny rzetelności to:\n\nalfa Cronbacha (spójność wewnętrzna), dla \\(k\\) pozycji \\[\n\\alpha \\;=\\; \\frac{k}{k-1}\\left(1 - \\frac{\\sum_{i=1}^{k}\\sigma_i^2}{\\sigma_X^2}\\right)\\!,\n\\] gdzie \\(\\sigma_i^2\\) to wariancja pozycji, a \\(\\sigma_X^2\\) wariancja sumy skali. \\(\\alpha \\ge 0.70\\) często uznawane jest za akceptowalne (zależnie od celu).\nomega McDonalda \\[\n\\omega \\;=\\; \\frac{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2}{\\left(\\sum_{i=1}^{k} \\lambda_i\\right)^2 + \\sum_{i=1}^{k}\\psi_{ii}},\n\\] gdzie \\(\\lambda_i\\) to ładunki czynnika ogólnego, a \\(\\psi_{ii}\\) wariancje unikalne. Dla rozwiązań hierarchicznych używamy \\(\\omega_h\\) (udział czynnika ogólnego).\nmetoda split-half i korekta Spearmana–Browna dla dwóch równoległych połówek z korelacją r: _{} ;=; .\n\n\nTrafność\n\nW CFA/SEM oceniamy trafność zbieżną i rozbieżną:\n\n\ncomposite reliability (CR) \\[\n\\mathrm{CR} = \\frac{\\left(\\sum \\lambda_i\\right)^2}{\\left(\\sum \\lambda_i\\right)^2 + \\sum \\theta_i},\n\\] gdzie \\(\\theta_i\\) to wariancje błędu; wartości \\(\\ge 0.70\\) pożądane;\n\naverage variance extracted (AVE) \\[\n\\mathrm{AVE} = \\frac{\\sum \\lambda_i^2}{\\sum \\lambda_i^2 + \\sum \\theta_i},\n\\] \\(\\mathrm{AVE} \\ge 0.50\\) sugeruje trafność zbieżną;\nkryterium Fornella–Larckera - \\(\\sqrt{\\mathrm{AVE}}\\) czynnika powinna przekraczać jego korelacje z innymi czynnikami;\nHTMT (heterotrait–monotrait ratio) - \\(\\mathrm{HTMT} \\;=\\; \\frac{\\text{średnia korelacja między pozycjami z różnych konstruktów}}{\\text{średnia korelacja między pozycjami w obrębie konstruktów}},\\) wartości \\(&lt; 0.85{-}0.90\\) wskazują rozróżnialność konstruktów.\n\n\nSkalowanie, punktacja i normy\n\nDecydujemy o sposobie punktowania: suma/średnia pozycji (po ewentualnym odwróceniu kodowania) czy punktacja czynnikowa (regresyjna/ Bartlett’a3). Ustalamy również normy na podstawie np. siatki stenowej (niski, przeciętny i wysoki poziom skali).\n3 Bartlett scores są nieobciążonymi estymatorami czynników latentnych, ale mogą być mniej stabilne w małych próbach i przy słabych ładunkach.\nPrzykład 4.4 Dla ilustracji adaptacji narzędzia pomiarowego, wykorzystamy oszacowaną już strukturę czynnikową z Przykład 4.1. Mieliśmy tam 13 pozycji opisujących trzy strategie: zapamiętywania, opracowywania i kontroli dla osób z Wielkiej Brytanii. Załóżmy, że tą samą strukturę chcemy przenieść na rynek Hiszpański. W tym celu sprawdzimy dopasowanie modelu konfirmacyjnego na danych hiszpańskich.\n\nKodpisaspa &lt;- PISA09[PISA09$cnt == \"ESP\", c(alitems, \"sex\")]\npisaspa &lt;- pisaspa[complete.cases(pisaspa[, c(mitems, \n  eitems, citems)]), ]\n\n# Definicja modelu CFA\nmodel_cfa &lt;- '\n  # Definicja czynników\n  Zapamiętywanie =~ st27q01 + st27q03 + st27q05 + st27q07\n  Opracowywanie =~ st27q04 + st27q08 + st27q10 + st27q12\n  Kontrola =~ st27q02 + st27q06 + st27q09 + st27q11 + st27q13\n'\n\n# Estymacja modelu CFA\nfit_cfa_spa &lt;- cfa(model_cfa, data = pisaspa, auto.var = TRUE, auto.cov.lv.x = TRUE, std.lv = TRUE)\n\n\n\nKodcompare_performance(fit_cfa, fit_cfa_spa, metrics = c(\"p_Chi2\", \"GFI\", \"AGFI\", \"NFI\", \"NNFI\", \"CFI\", \"RMSEA\", \"RMR\", \"SRMR\", \"RFI\")) %&gt;% \n  gt() %&gt;% \n  fmt_number(\n    columns = is.double,\n    decimals = 3)\n\n\n\n\n\nName\nModel\np_Chi2\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMR\nSRMR\nRFI\n\n\n\nfit_cfa\nlavaan\n0.000\n0.936\n0.907\n0.881\n0.856\n0.885\n0.081\n0.042\n0.057\n0.850\n\n\nfit_cfa_spa\nlavaan\n0.000\n0.940\n0.912\n0.882\n0.854\n0.884\n0.079\n0.053\n0.059\n0.851\n\n\n\n\n\n\nOcena jakości dopasowania modelu pokazuje, że przyjęta struktura dla Wielkiej Brytanii sprawdza się również dla Hiszpanii. To dopiero pierwszy (oczywiście pominąwszy wszystkie wcześniejsze kroki jak tłumaczenie, czy analiza eksploracyjna) krok do adaptacji narzędzia do nowych warunków.\nTeraz ocenimy rzetelność skali.\n\nKodtab_itemscale(df = pisaspa, factor.groups = c(rep(\"Zapamiętywanie\", 4), rep(\"Opracowanie\", 4), rep(\"Kontrola\", 5)), factor.groups.titles = c(\"Kontrola\", \"Opracowanie\", \"Zapamiętywanie\")) \n\n\nKontrola\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nst27q02\n0.00 %\n2.93\n0.91\n-0.44\n0.73\n0.46\n0.72\n\n\n\nst27q06\n0.00 %\n3.07\n0.92\n-0.6\n0.77\n0.57\n0.68\n\n\n\nst27q09\n0.00 %\n2.71\n0.89\n-0.14\n0.68\n0.56\n0.68\n\n\n\nst27q11\n0.00 %\n3.17\n0.87\n-0.78\n0.79\n0.54\n0.69\n\n\n\nst27q13\n0.00 %\n2.45\n1.02\n0.12\n0.61\n0.43\n0.73\n\n\n\nMean inter-item-correlation=0.371 · Cronbach's α=0.744\n\n\n\n \n\nOpracowanie\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nst27q04\n0.00 %\n2.49\n1.01\n0.05\n0.62\n0.47\n0.72\n\n\n\nst27q08\n0.00 %\n2.01\n0.94\n0.62\n0.50\n0.53\n0.68\n\n\n\nst27q10\n0.00 %\n2.33\n0.95\n0.21\n0.58\n0.56\n0.66\n\n\n\nst27q12\n0.00 %\n2.17\n0.93\n0.39\n0.54\n0.57\n0.66\n\n\n\nMean inter-item-correlation=0.416 · Cronbach's α=0.738\n\n\n\n \n\nZapamiętywanie\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nst27q01\n0.00 %\n2.6\n0.98\n0.02\n0.65\n0.54\n0.64\n\n\n\nst27q03\n0.00 %\n2.85\n0.95\n-0.33\n0.71\n0.52\n0.65\n\n\n\nst27q05\n0.00 %\n2.35\n1.03\n0.21\n0.59\n0.54\n0.64\n\n\n\nst27q07\n0.00 %\n2.92\n0.96\n-0.43\n0.73\n0.44\n0.70\n\n\n\nMean inter-item-correlation=0.390 · Cronbach's α=0.719\n\n\n\n \n\n\n\n\n\n\n\n\n\n \nComponent 1\nComponent 2\nComponent 3\n\n\nComponent 1\nα=0.744\n \n \n\n\nComponent 2\n0.521(&lt;.001)\n\nα=0.738\n \n\n\nComponent 3\n0.376(&lt;.001)\n\n0.222(&lt;.001)\n\nα=0.719\n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nSpójność wewnętrzna poszczególnych komponentów\nKażda z trzech skal (komponentów) osiąga akceptowalny poziom rzetelności wewnętrznej. Wartości współczynnika Cronbacha \\(\\alpha\\) wynoszą odpowiednio 0.738 dla strategii opracowywania, 0.744 dla kontroli oraz 0.719 dla zapamiętywania. Są to wartości przekraczające próg 0.70, co w badaniach psychometrycznych uznaje się za wystarczające dla narzędzi we wczesnym etapie walidacji. Wskazuje to, że pozycje w każdej ze skal mierzą spójny konstrukt.\nWartości korelacji między pozycjami (mean inter-item correlation) mieszczą się w zakresie 0.37–0.42, co uznaje się za optymalne (wartości zbyt niskie &lt;0.20 sugerują brak spójności, natomiast zbyt wysokie &gt;0.70 nadmiarowość). Oznacza to, że pozycje są ze sobą skorelowane w stopniu umiarkowanym, zachowując jednocześnie różnorodność treściową.\nAnaliza jakości pozycji\nWszystkie pozycje mają trudność (item difficulty) w przedziale 0.50–0.79, co oznacza, że średnie odpowiedzi respondentów oscylują wokół środka skali, bez efektów podłogowych czy sufitowych. Pozycje mają także umiarkowane lub wysokie wartości dyskryminacji (item discrimination w przedziale 0.43–0.57), wskazujące, że dobrze różnicują osoby z wyższymi i niższymi wynikami ogólnymi. Żadna z pozycji nie obniża znacząco rzetelności całej skali (wszystkie wartości „α if deleted” pozostają na poziomie podobnym lub niższym niż pełne α).\nZwiązki między komponentami\nKorelacje pomiędzy skalami są wszystkie istotne statystycznie, ale mają zróżnicowaną siłę. Najsilniejszy związek występuje pomiędzy Kontrolą a Opracowaniem (r = 0.521, p &lt; .001). Można to interpretować tak, że osoby, które dbają o planowanie i monitorowanie swojego uczenia się, częściej stosują także strategie głębszego opracowywania materiału. Skala Zapamiętywanie koreluje umiarkowanie z Opracowaniem (r = 0.376, p &lt; .001), co jest zgodne z intuicją: aby skutecznie zapamiętać, często trzeba wcześniej przetworzyć materiał. Najsłabszy, choć istotny związek obserwujemy między Kontrolą a Zapamiętywaniem (r = 0.222, p &lt; .001), co sugeruje, że te dwie strategie są bardziej odrębne, a ich powiązanie jest ograniczone.\nPodsumowanie\nOtrzymane wyniki sugerują, że narzędzie jest psychometrycznie poprawne: ma akceptowalną spójność wewnętrzną, zrównoważony poziom trudności pozycji, dobre wskaźniki dyskryminacji oraz pozwala rozróżniać trzy powiązane, ale odrębne strategie uczenia się. Komponent 2 wydaje się pełnić rolę centralną, ponieważ jest najwyraźniej powiązany zarówno z komponentem 1, jak i 3. To może sugerować jego bardziej ogólny charakter lub funkcję „pomostu” między dwoma innymi wymiarami.\nPo wykonaniu analizy CFA i ocenie rzetelności kolejnym etapem adaptacji narzędzia jest przeprowadzenie rozszerzonych analiz stabilności, takich jak test–retest czy współczynnik ICC, które pozwalają ocenić powtarzalność wyników (tych nie wykonamy, ponieważ do tego potrzeba przeprowadzenia ankiety w dwóch momentach czasowych). Należy także zweryfikować trafność – konwergencyjną, dyskryminacyjną i kryterialną – aby upewnić się, że narzędzie mierzy to, co zakładano teoretycznie.\n\nKodlibrary(semTools)\nAVE(fit_cfa_spa) \n\nZapamiętywanie  Opracowywanie       Kontrola \n         0.395          0.418          0.373 \n\nKodcompRelSEM(fit_cfa_spa)\n\nZapamiętywanie  Opracowywanie       Kontrola \n         0.722          0.744          0.752 \n\nKodhtmt(model_cfa, data = pisaspa)\n\n               Zpmęty Oprcwy Kontrl\nZapamiętywanie  1.000              \nOpracowywanie   0.288  1.000       \nKontrola        0.482  0.680  1.000\n\n\nWyniki można interpretować na trzech poziomach: rzetelności wewnętrznej (CR), trafności konwergencyjnej (AVE) oraz trafności dyskryminacyjnej (HTMT).\n\nWspółczynniki Composite Reliability (CR) mieszczą się w przedziale od 0.72 do 0.75. Są to wartości powyżej progu 0.70, co sugeruje akceptowalną spójność wewnętrzną każdej ze skal. Oznacza to, że wskaźniki w ramach czynnika zapamiętywania, opracowywania i kontroli dostarczają relatywnie stabilnej informacji o zmiennej latentnej.\nŚrednia wyjaśniona wariancja (AVE) dla wszystkich trzech czynników jest niska: 0.395, 0.418 i 0.373. Kryterium akceptowalne to zwykle AVE ≥ 0.50, co oznacza, że czynnik powinien wyjaśniać przynajmniej połowę wariancji swoich wskaźników. Tutaj wartości poniżej 0.5 wskazują, że wyjaśniona część wariancji jest mniejsza niż ta przypisana błędowi. Może to oznaczać, że wskaźniki są dość zróżnicowane, a ich wspólna treść (latentna) nie jest wystarczająco silnie uchwycona. W praktyce oznacza to ograniczoną trafność konwergencyjną – czyli wskaźniki nie „zbiegają się” wystarczająco na wspólny konstrukt.\nMacierz HTMT wskazuje na poziom rozróżnialności czynników (trafności dyskryminacyjnej). Przyjmuje się, że wartości HTMT &lt; 0.85 (lub bardziej liberalnie &lt; 0.90) oznaczają satysfakcjonującą rozróżnialność. W tym przypadku: • Zapamiętywanie–Opracowywanie = 0.288 – bardzo niski współczynnik, dobra rozróżnialność, • Zapamiętywanie–Kontrolne = 0.482 – umiarkowany, nadal bezpieczny, • Opracowywanie–Kontrolne = 0.680 – wyższy, ale poniżej progu 0.85, więc rozróżnialność jest zachowana.\n\nPodsumowując, model charakteryzuje się akceptowalną rzetelnością i zadowalającą trafnością dyskryminacyjną, ale ograniczoną trafnością konwergencyjną. W praktyce oznacza to, że choć skale mierzą różne konstrukty i są spójne wewnętrznie, to konstrukty te nie są jeszcze w pełni „czysto” uchwycone przez zestaw wskaźników – być może potrzebna byłaby rewizja niektórych pozycji, ich dodanie lub modyfikacja.\nIstotnym krokiem jest również badanie równoważności pomiaru (measurement invariance) przy użyciu CFA wielogrupowej, co umożliwia porównywanie wyników między grupami, np. ze względu na płeć czy wiek. My wykonamy analizę w podziale ze względu na płeć, aby dowiedzieć się czy narzędzie mierzy badane konstrukty podobnie w obu grupach.\n\nKod# równoważność konfiguracyjna \nfit_config &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", std.lv = TRUE)\n\n# równoważność metryczna (równe ładunki czynnikowe)\nfit_metric &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = \"loadings\", std.lv = TRUE)\n\n# równoważność skalowa (równe ładunki i przecięcia)\nfit_scalar &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = c(\"loadings\", \"intercepts\"), std.lv = TRUE)\n\n# równoważność ścisła (ładunki, przecięcia i błędy pomiarowe)\nfit_strict &lt;- cfa(model_cfa, data = pisaspa, group = \"sex\", group.equal = c(\"loadings\", \"intercepts\", \"residuals\"), std.lv = TRUE)\n\n# Porównania dopasowania\nanova(fit_config, fit_metric, fit_scalar, fit_strict)\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nfit_config 124 246657 247240 3035.8                                           \nfit_metric 134 246700 247214 3099.0     63.216 0.037427      10  8.880e-10 ***\nfit_scalar 144 246922 247366 3340.7    241.727 0.078100      10  &lt; 2.2e-16 ***\nfit_strict 157 246967 247321 3411.6     70.901 0.034240      13  5.477e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nKodfitMeasures(fit_config, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.886 0.079 0.054 \n\nKodfitMeasures(fit_metric, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.884 0.076 0.056 \n\nKodfitMeasures(fit_scalar, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.875 0.076 0.058 \n\nKodfitMeasures(fit_strict, c(\"cfi\",\"rmsea\",\"srmr\"))\n\n  cfi rmsea  srmr \n0.873 0.074 0.059 \n\n\nWyniki analizy grupowej CFA wskazują, że proces sprawdzania równoważności pomiaru ze względu na płeć w tym narzędziu napotyka istotne ograniczenia.\nModel konfiguracyjny (czyli sam układ czynników i wskaźników, bez dodatkowych ograniczeń) osiąga umiarkowane dopasowanie: CFI = 0.886, RMSEA = 0.079, SRMR = 0.054. Oznacza to, że struktura czynnikowa jest w obu grupach podobna, choć model nie jest idealnie dopasowany.\nDodanie ograniczenia równości ładunków czynnikowych (równoważność metryczna) nie powoduje znaczącego pogorszenia dopasowania – CFI spada minimalnie (0.886 → 0.884), RMSEA poprawia się nieznacznie (0.079 → 0.076), a SRMR pozostaje na porównywalnym poziomie. Pomimo istotnego testu różnicy chi-kwadrat (Δχ² = 63.2, p &lt; 0.001), kryteria praktyczne (ΔCFI &lt; 0.01, ΔRMSEA &lt; 0.015) sugerują, że równoważność metryczna jest akceptowalna. Można więc uznać, że wskaźniki w równym stopniu odzwierciedlają konstrukty w obu grupach.\nPrzejście do równoważność skalowej (dodanie równości przecięć) wyraźnie pogarsza dopasowanie – spadek CFI do 0.875 oraz brak poprawy w RMSEA i SRMR, przy bardzo dużym i istotnym przyroście χ² (Δχ² = 241.7, p &lt; 0.001). To oznacza, że przecięcia nie są równoważne między płciami, czyli nie można bezpośrednio porównywać średnich czynników latentnych.\nW modelu ścisłym (zrównanie dodatkowo wariancji błędów) dopasowanie pozostaje na podobnym poziomie (CFI = 0.873, RMSEA = 0.074, SRMR = 0.059), a test chi-kwadrat znów wskazuje istotne pogorszenie.\nPodsumowując, udało się uzyskać równoważność konfiguracyjną i metryczną, ale nie skalową. Oznacza to, że konstrukty są podobnie reprezentowane w obu grupach, lecz nie można ich średnich bezpośrednio porównywać, bo przecięcia różnią się w zależności od płci. W praktyce, w takim przypadku można rozważyć testowanie częściowej równoważność (partial invariance), czyli identyfikowanie tych pozycji, które rzeczywiście spełniają warunek równości przecięć i opieranie porównań tylko na nich.\nNa koniec utworzymy skale, które można łatwo intepretować przez osoby, które nie znają narzędzia i jego konstrukcji dobrze.\n\nKod# --- KLUCZE SKAL (dokładnie jak w modelu) ---\nzap_items &lt;- c(\"st27q01\",\"st27q03\",\"st27q05\",\"st27q07\")                 # Zapamiętywanie\nopr_items &lt;- c(\"st27q04\",\"st27q08\",\"st27q10\",\"st27q12\")                 # Opracowywanie\nkon_items &lt;- c(\"st27q02\",\"st27q06\",\"st27q09\",\"st27q11\",\"st27q13\")       # Kontrola\n\n# Uwaga: jeśli jakieś pozycje wymagają odwrócenia, zastosować recoding przed agregacją.\n\nscores_simple &lt;- pisaspa %&gt;%\n  mutate(\n    Zapam_mean = rowMeans(across(all_of(zap_items)), na.rm = TRUE),\n    Oprac_mean = rowMeans(across(all_of(opr_items)), na.rm = TRUE),\n    Kontr_mean = rowMeans(across(all_of(kon_items)), na.rm = TRUE),\n    Zapam_sum  = rowSums(across(all_of(zap_items)), na.rm = TRUE),\n    Oprac_sum  = rowSums(across(all_of(opr_items)), na.rm = TRUE),\n    Kontr_sum  = rowSums(across(all_of(kon_items)), na.rm = TRUE)\n  )\n\n# Wyniki czynnikowe – metoda Bartletta (bardziej \"czyste\" wobec błędów specyficznych)\nfs_bartlett &lt;- lavPredict(fit_cfa_spa, method = \"Bartlett\")\n\n# Złożyć do wspólnej ramki (zachowując ewentualne zmienne grupujące)\nscores_latent &lt;- cbind(\n  pisaspa %&gt;% select(any_of(c(\"sex\",\"age\"))),\n  as.data.frame(fs_bartlett)\n)\n\n\n\nKodto_sten &lt;- function(z){\n  # transformacja przybliżona; wyniki przycięte do 1..10\n  s &lt;- round(2*z + 5.5)\n  pmin(10, pmax(1, s))\n}\n\n# Percentyle z ECDF\nperc_ecdf &lt;- function(x) round(ecdf(x)(x)*100, 1)\n\n# --- NORMY GLOBALNE dla wyników latentnych Bartletta ---\nlatent_names &lt;- c(\"Zapamiętywanie\",\"Opracowywanie\",\"Kontrola\")\n\nnorms_global &lt;- scores_latent %&gt;%\n  mutate(\n    across(all_of(latent_names), scale, .names = \"{.col}_z\") %&gt;% as.data.frame()\n  )\n\n# Dla wygody wylicz stens, percentyle dla każdej skali latentnej\nfor(lat in latent_names){\n  zcol &lt;- paste0(lat, \"_z\")\n  norms_global[[paste0(lat,\"_sten\")]]    &lt;- to_sten(norms_global[[zcol]])\n  norms_global[[paste0(lat,\"_pct\")]]     &lt;- perc_ecdf(scores_latent[[lat]])\n}\n\n# Podgląd wybranych kolumn\nhead(norms_global %&gt;% select(any_of(c(\"sex\",\"age\")),\n                             ends_with(\"_z\"),\n                             ends_with(\"_sten\"),\n                             ends_with(\"_pct\")), n = 20) %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 2) %&gt;% \n  tab_options(\n    table.font.size = px(10), \n  )\n\n\n\n\n\nsex\nZapamiętywanie_z\nOpracowywanie_z\nKontrola_z\nZapamiętywanie_sten\nOpracowywanie_sten\nKontrola_sten\nZapamiętywanie_pct\nOpracowywanie_pct\nKontrola_pct\n\n\n\nf\n0.86\n−0.65\n0.20\n7.00\n4.00\n6.00\n78.60\n27.10\n55.00\n\n\nm\n−0.51\n−0.38\n−0.69\n4.00\n5.00\n4.00\n31.90\n34.80\n22.90\n\n\nm\n0.24\n0.74\n0.26\n6.00\n7.00\n6.00\n59.00\n77.50\n57.70\n\n\nm\n−1.23\n2.44\n1.69\n3.00\n10.00\n9.00\n11.50\n98.30\n96.50\n\n\nm\n−0.18\n0.35\n1.09\n5.00\n6.00\n8.00\n43.80\n65.40\n87.10\n\n\nm\n−0.25\n−0.34\n0.30\n5.00\n5.00\n6.00\n40.80\n39.10\n58.40\n\n\nf\n−0.70\n−1.04\n0.55\n4.00\n3.00\n7.00\n24.90\n17.40\n67.70\n\n\nf\n0.24\n1.07\n−0.93\n6.00\n8.00\n4.00\n58.90\n86.10\n17.30\n\n\nf\n0.31\n0.63\n1.69\n6.00\n7.00\n9.00\n61.60\n72.10\n97.40\n\n\nf\n0.70\n1.74\n0.16\n7.00\n9.00\n6.00\n73.90\n95.10\n54.30\n\n\nf\n−1.06\n−1.73\n−1.36\n3.00\n2.00\n3.00\n14.60\n0.80\n10.30\n\n\nm\n0.12\n2.16\n1.69\n6.00\n10.00\n9.00\n54.10\n98.20\n98.20\n\n\nf\n0.96\n0.99\n1.69\n7.00\n7.00\n9.00\n81.00\n81.80\n96.40\n\n\nm\n0.03\n1.32\n0.95\n6.00\n8.00\n7.00\n51.20\n88.70\n81.90\n\n\nm\n−0.25\n0.04\n0.11\n5.00\n6.00\n6.00\n40.80\n53.30\n50.10\n\n\nm\n−1.32\n−0.03\n−1.37\n3.00\n5.00\n3.00\n9.50\n51.60\n8.40\n\n\nm\n−1.74\n−0.72\n−0.88\n2.00\n4.00\n4.00\n4.40\n25.10\n18.30\n\n\nm\n−0.94\n−0.34\n−1.37\n4.00\n5.00\n3.00\n18.60\n39.10\n9.60\n\n\nf\n−1.79\n−0.76\n−0.24\n2.00\n4.00\n5.00\n4.20\n23.30\n37.30\n\n\nf\n−0.56\n0.46\n−0.09\n4.00\n6.00\n5.00\n29.00\n69.10\n43.70\n\n\n\n\n\nKod# --- NORMY WEDŁUG PŁCI (grupowe) ---\nnorms_by_sex &lt;- scores_latent %&gt;%\n  group_by(sex) %&gt;%\n  mutate(\n    # z-score wewnątrz płci\n    across(all_of(latent_names),\n           ~ as.numeric(scale(.x)), .names = \"{.col}_z_g\"),\n    # stens wewnątrz płci\n    across(ends_with(\"_z_g\"),\n           ~ to_sten(.x), .names = \"{.col}_sten\"),\n  ) %&gt;%\n  # percentyle z ECDF wewnątrz płci\n  mutate(\n    across(all_of(latent_names),\n           ~ perc_ecdf(.x), .names = \"{.col}_pct_g\")\n  ) %&gt;%\n  ungroup()\n\n# Podgląd\nhead(norms_by_sex %&gt;%\n       select(sex, starts_with(\"Zapamiętywanie_\"),\n                    starts_with(\"Opracowywanie_\"),\n                    starts_with(\"Kontrola_\")), n = 20)%&gt;% \n  gt() %&gt;% \n  fmt_number(columns = is.double, decimals = 2) %&gt;% \n  tab_options(\n    table.font.size = px(10), \n  )\n\n\n\n\n\nsex\nZapamiętywanie_z_g\nZapamiętywanie_z_g_sten\nZapamiętywanie_pct_g\nOpracowywanie_z_g\nOpracowywanie_z_g_sten\nOpracowywanie_pct_g\nKontrola_z_g\nKontrola_z_g_sten\nKontrola_pct_g\n\n\n\nf\n0.82\n7.00\n77.40\n−0.61\n4.00\n29.10\n0.06\n6.00\n49.30\n\n\nm\n−0.42\n5.00\n35.10\n−0.42\n5.00\n32.20\n−0.53\n4.00\n27.50\n\n\nm\n0.31\n6.00\n61.70\n0.69\n7.00\n76.60\n0.38\n6.00\n63.20\n\n\nm\n−1.12\n3.00\n13.50\n2.39\n10.00\n97.90\n1.76\n9.00\n97.10\n\n\nm\n−0.10\n5.00\n46.20\n0.31\n6.00\n63.70\n1.19\n8.00\n89.40\n\n\nm\n−0.17\n5.00\n43.50\n−0.38\n5.00\n37.00\n0.43\n6.00\n63.80\n\n\nf\n−0.80\n4.00\n21.50\n−1.00\n3.00\n18.70\n0.43\n6.00\n63.20\n\n\nf\n0.18\n6.00\n56.00\n1.12\n8.00\n86.40\n−1.15\n3.00\n13.40\n\n\nf\n0.25\n6.00\n59.00\n0.67\n7.00\n73.50\n1.64\n9.00\n97.30\n\n\nf\n0.65\n7.00\n72.00\n1.79\n9.00\n95.40\n0.02\n6.00\n48.50\n\n\nf\n−1.18\n3.00\n12.30\n−1.70\n2.00\n1.10\n−1.61\n2.00\n7.00\n\n\nm\n0.19\n6.00\n56.80\n2.11\n10.00\n97.90\n1.76\n9.00\n98.50\n\n\nf\n0.92\n7.00\n79.90\n1.03\n8.00\n82.50\n1.64\n9.00\n95.80\n\n\nm\n0.10\n6.00\n53.60\n1.28\n8.00\n88.40\n1.05\n8.00\n85.30\n\n\nm\n−0.17\n5.00\n43.50\n0.00\n5.00\n51.60\n0.24\n6.00\n55.50\n\n\nm\n−1.21\n3.00\n11.30\n−0.07\n5.00\n49.80\n−1.19\n3.00\n10.70\n\n\nm\n−1.62\n2.00\n6.00\n−0.76\n4.00\n23.30\n−0.71\n4.00\n22.10\n\n\nm\n−0.84\n4.00\n21.30\n−0.38\n5.00\n37.00\n−1.19\n3.00\n12.60\n\n\nf\n−1.94\n2.00\n2.70\n−0.72\n4.00\n25.20\n−0.41\n5.00\n31.80\n\n\nf\n−0.66\n4.00\n25.80\n0.50\n7.00\n70.60\n−0.25\n5.00\n38.10\n\n\n\n\n\n\n\n\n\n\n\nBollen, Kenneth A. 1989. „Structural Equation Models with Observed Variables”. Structural Equations with Latent Variables, kwiecień, 80–150. https://doi.org/10.1002/9781118619179.ch4.\n\n\nHuang, Yafei, i Peter M. Bentler. 2015. „Behavior of Asymptotically Distribution Free Test Statistics in Covariance Versus Correlation Structure Analysis”. Structural Equation Modeling: A Multidisciplinary Journal 22 (4): 489–503. https://doi.org/10.1080/10705511.2014.954078.\n\n\nKILIÇ, Abdullah, İbrahim UYSAL, i Burcu ATAR. 2020. „Comparison of Confirmatory Factor Analysis Estimation Methods on Binary Data”. International Journal of Assessment Tools in Education 7 (3): 451–87. https://doi.org/10.21449/ijate.660353.\n\n\nKyriazos, Theodoros, i Mary Poga-Kyriazou. 2023. „Applied Psychometrics: Estimator Considerations in Commonly Encountered Conditions in CFA, SEM, and EFA Practice”. Psychology 14 (05): 799–828. https://doi.org/10.4236/psych.2023.145043.\n\n\nLatan, Hengky, i Richard Noonan, red. 2017. Partial Least Squares Path Modeling. Springer International Publishing. https://doi.org/10.1007/978-3-319-64069-3.\n\n\nLi, Cheng-Hsien. 2015. „Confirmatory Factor Analysis with Ordinal Data: Comparing Robust Maximum Likelihood and Diagonally Weighted Least Squares”. Behavior Research Methods 48 (3): 936–49. https://doi.org/10.3758/s13428-015-0619-7.\n\n\n———. 2021. „Statistical Estimation of Structural Equation Models with a Mixture of Continuous and Categorical Observed Variables”. Behavior Research Methods 53 (5): 2191–2213. https://doi.org/10.3758/s13428-021-01547-z.\n\n\nSchweizer, Karl, i Christine DiStefano, red. 2016. Principles and Methods of Test Construction. Hogrefe Publishing. https://doi.org/10.1027/00449-000.\n\n\nSpearman, C. 1961. „\"General Intelligence\" Objectively Determined and Measured.” W, 59–73. Appleton-Century-Crofts. https://doi.org/10.1037/11491-006.\n\n\n„Supplemental Material for The Performance of ML, DWLS, and ULS Estimation With Robust Corrections in Structural Equation Models With Ordinal Variables”. 2016. Psychological Methods. https://doi.org/10.1037/met0000093.supp.\n\n\nTarka, Piotr. 2017. „An Overview of Structural Equation Modeling: Its Beginnings, Historical Development, Usefulness and Controversies in the Social Sciences”. Quality & Quantity 52 (1): 313–54. https://doi.org/10.1007/s11135-017-0469-8.\n\n\nThurstone, L. L. 1931. „Multiple Factor Analysis.” Psychological Review 38 (5): 406–27. https://doi.org/10.1037/h0069792.\n\n\nTurney, A. H. 1939. „Factor Analysis Makes ProgressA Study in Factor Analysis: The Stability of a Bi-Factor Solution. Karl J. Holzinger , Frances Swineford”. The School Review 47 (9): 709–11. https://doi.org/10.1086/440440.\n\n\nWright, Sewall. 1934. „The Method of Path Coefficients”. The Annals of Mathematical Statistics 5 (3): 161–215. https://doi.org/10.1214/aoms/1177732676.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele strukturalne</span>"
    ]
  }
]