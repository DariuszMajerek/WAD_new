---
output: html_document
number-sections: false
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

# Analiza korespondencji

## Rys historyczny

Analizę korespondencji (ang. *correspondence analysis*) warto osadzić w
szerszym tle historycznym, ponieważ metoda ta powstała jako odpowiedź na
potrzebę opisywania złożonych zależności między zmiennymi jakościowymi. Jej
korzenie sięgają pierwszej połowy XX wieku, kiedy francuscy statystycy zaczęli
rozwijać techniki umożliwiające graficzne przedstawianie relacji w dużych
tabelach kontyngencji. Jednym z kluczowych momentów było wprowadzenie pojęcia
profili wierszy i kolumn oraz koncepcji przestrzeni czynnikowej, która
pozwalała redukować wielowymiarowe dane do kilku najważniejszych wymiarów
interpretacyjnych.

Istotną rolę odegrała praca @Benzecri1997, uznawanego za twórcę szkoły analizy
danych we Francji. Zaproponował on sformalizowaną wersję analizy korespondencji
jako metody opartej na dekompozycji wartości osobliwych, łączącej elegancję
geometrycznego podejścia z praktyczną interpretowalnością wyników. W kolejnych
dekadach metoda została rozbudowana o warianty takie jak analiza korespondencji
wieloraka czy analiza korespondencji kanonicznej, które umożliwiały
analizowanie bardziej złożonych struktur, w tym zbiorów wielu tabel lub relacji
między zestawami zmiennych.

Zastosowania analizy korespondencji zaczęły pojawiać się w socjologii,
psychologii, biologii, marketingu i politologii, zwłaszcza tam, gdzie
dominowały dane kategoryczne. Dzięki czytelnym mapom percepcyjnym, metoda
pozwalała identyfikować ukryte wzorce, grupy podobnych kategorii oraz kierunki
dominujących zależności. Jej popularność wzrosła wraz z rozwojem komputerów,
które umożliwiły wykonywanie bardziej złożonych obliczeń oraz wizualizacji.

Obecnie analiza korespondencji stanowi ważne narzędzie wielowymiarowej
statystyki opisowej. Umożliwia przedstawianie danych w sposób intuicyjny, a
jednocześnie bazuje na ścisłych podstawach matematycznych. Pozwala to
wykorzystywać ją zarówno jako metodę eksploracji danych, jak i wsparcie dla
interpretacji zależności obserwowanych w tabelach kontyngencji, szczególnie
wtedy, gdy relacje między kategoriami nie są bezpośrednio uchwytne w formie
tabelarycznej.

## Cele analizy korespondencji

Celem stosowania analizy korespondencji jest uchwycenie oraz zobrazowanie struktury zależności między zmiennymi jakościowymi zapisanymi w tabeli kontyngencji. Metoda pozwala przedstawiać profile wierszy i kolumn w zredukowanej przestrzeni wymiarów, tak aby odległości między punktami odzwierciedlały odchylenia od modelu niezależności. Umożliwia to identyfikowanie zbliżonych kategorii, wykrywanie ukrytych wzorców oraz interpretowanie relacji, które trudno dostrzec jedynie na podstawie tabeli liczebności. Dzięki temu analiza korespondencji służy zarówno eksploracji danych, jak i wspieraniu interpretacji statystycznej zależności między cechami nominalnymi.

Istotnym elementem analizy korespondencji jest tworzenie tzw. mapy percepcji, czyli graficznego odwzorowania kategorii wierszy i kolumn w przestrzeni o dwóch lub trzech wymiarach. Taka mapa umożliwia analizowanie relacji między kategoriami w sposób intuicyjny: punkty położone blisko siebie reprezentują profile o podobnej strukturze, a duże odległości wskazują na wyraźne różnice względem modelu niezależności. Mapa percepcji ułatwia więc identyfikowanie grup powiązanych kategorii, obserwowanie kierunków dominujących zależności oraz wyciąganie wniosków, które w układzie tabelarycznym pozostałyby trudne do zauważenia.

## Definicja matematyczna modelu

Zakłada się, że punktem wyjścia jest tabela kontyngencji
$$
N = (n_{ij})_{i=1,\dots,I;\, j=1,\dots,J},
$$
gdzie $n_{ij}$ jest liczebnością obserwacji należących jednocześnie do kategorii wiersza $i$ i kolumny $j$. 
$$
\begin{array}{c|cccc|c}
X/Y& j=1 & j=2 & \cdots & j=J & \text{Razem} \\
\hline
i=1 & n_{11} & n_{12} & \cdots & n_{1J} & n_{1\cdot} \\
i=2 & n_{21} & n_{22} & \cdots & n_{2J} & n_{2\cdot} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
i=I & n_{I1} & n_{I2} & \cdots & n_{IJ} & n_{I\cdot} \\
\hline
\text{Razem} & n_{\cdot 1} & n_{\cdot 2} & \cdots & n_{\cdot J} & n
\end{array}
$$
gdzie $n_{i\cdot} = \sum_{j=1}^J n_{ij}$ oznacza liczebność brzegową wiersza $i$, $n_{\cdot j} = \sum_{i=1}^I n_{ij}$ oznacza liczebność brzegową kolumny $j$, a $n = \sum_{i=1}^I \sum_{j=1}^J n_{ij}$ jest liczebnością całkowitą.

Wprowadza wówczas się macierz częstości względnych
$P = (p_{ij})$ postaci 

$$
\begin{array}{c|cccc|c}
X/Y & j=1 & j=2 & \cdots & j=J & \text{Razem} \\
\hline
i=1 & p_{11} & p_{12} & \cdots & p_{1J} & r_{1} \\
i=2 & p_{21} & p_{22} & \cdots & p_{2J} & r_{2} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
i=I & p_{I1} & p_{I2} & \cdots & p_{IJ} & r_{I} \\
\hline
\text{Razem} & c_{1} & c_{2} & \cdots & c_{J} & 1
\end{array}
$$
gdzie $p_{ij} = n_{ij}/n$ są częstościami względnymi, $r_i = \sum_{j=1}^J p_{ij}$ oznacza masę wiersza (udział kategorii $i$), $c_j = \sum_{i=1}^I p_{ij}$ oznacza masę kolumny, a liczba w prawym dolnym rogu równa 1 jest sumą całej macierzy częstości względnych. Przeciętny profil wierszowy definiujemy jako wektor proporcji kategorii kolumnowych w całej tabeli. Tworzą go masy kolumnowe
$(c_1, c_2, \dots, c_J)$. Oznacza to, że przeciętny wiersz „zachowuje się” tak, jak średnia struktura rozkładu kategorii kolumnowych w całej próbie — stanowi więc naturalny punkt referencyjny przy ocenie, czy dany wiersz nadreprezentuje lub niedoreprezentuje wybrane kolumny względem tego średniego rozkładu. Analogicznie przeciętny profil kolumnowy jest wektorem $(r_1, r_2, \dots, r_I),$
czyli masami wierszy. Profil ten pokazuje przeciętny udział kategorii wierszowych w całej tabeli i służy porównaniu rzeczywistych profili kolumn ze strukturą „typową” wynikającą z rozkładu marginalnego wierszy.

Podstawowym modelem odniesienia jest model niezależności zmiennych jakościowych:
$$
p_{ij}^{(0)} = r_i c_j,\quad i=1,\dots,I,\ j=1,\dots,J.
$$
Oznacza to, że przy braku zależności między kategoriami wierszy i kolumn, częstość wspólna powinna być równa iloczynowi częstości brzegowych. Analiza korespondencji bada odchylenia rzeczywistych częstości $p_{ij}$ od wartości oczekiwanych $r_i\cdot c_j$ oraz przedstawia je w zredukowanej przestrzeni wymiarów.

Odchylenia od modelu niezależności można zapisać jako
$$
\Delta_{ij} = p_{ij} - r_i c_j.
$$
Ważone odchylenia od niezależności leżą u podstaw konstrukcji statystyki chi-kwadrat oraz pojęcia inercji w analizie korespondencji. Wprowadza się macierze diagonalne z masami wierszy i kolumn:
\begin{align*}
D_r =& \mathrm{diag}(r_1,\dots,r_I),\\
D_c =& \mathrm{diag}(c_1,\dots,c_J).
\end{align*}
Znormalizowane reszty definiuje się jako
$$
S = D_r^{-1/2}\,(P - r c^\top)\,D_c^{-1/2}.
$$
Element o indeksach $(i,j)$ ma postać
$$
s_{ij} = \frac{p_{ij} - r_i c_j}{\sqrt{r_i c_j}}.
$$
Całkowita „inercja” analizowanej tabeli jest równa średniej ważonej kwadratu tych reszt i jest powiązana ze statystyką chi-kwadrat:
$$
\chi^2 = n \sum_{i=1}^I \sum_{j=1}^J \frac{(p_{ij} - r_i c_j)^2}{r_i c_j},
\quad
I_{\text{całk.}} = \sum_{i,j} \frac{(p_{ij} - r_i c_j)^2}{r_i c_j}
= \frac{\chi^2}{n}.
$$
Analiza korespondencji dąży do takiego odwzorowania kategorii wierszy i kolumn w przestrzeni euklidesowej, aby odległości w tej przestrzeni odzwierciedlały właśnie te ważone odchylenia od modelu niezależności.

Kluczowym krokiem w wyznaczeniu współrzędnych nowego układu współrzędnych (zwanego mapą percepcji) jest dekompozycja wartości osobliwych macierzy znormalizowanych reszt $S$:
$$
S = U\,\Sigma\,V^\top,
$$
gdzie $U$ jest macierzą $I \times K$ ortonormalnych wektorów własnych (dla wierszy), $V$ jest macierzą $J \times K$ ortonormalnych wektorów własnych (dla kolumn), $\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_K)$ zawiera dodatnie wartości osobliwe, a $K = \min(I-1, J-1)$ jest maksymalną liczbą niezerowych wymiarów (po odjęciu wymiaru związanego z sumą do 1).

Wartości własne analizy korespondencji definiuje się jako
$$
\lambda_k = \sigma_k^2, \quad k = 1,\dots,K,
$$
a suma tych wartości
$$
\sum_{k=1}^K \lambda_k = I_{\text{całk.}}
$$
odpowiada całkowitej inercji tabeli. Poszczególne osie czynnikowe (wymiary) są więc uporządkowane malejąco według wyjaśnianej części inercji.

O doborze $K$ mogą decydować kryteria takie, jak kryterium wyjaśnionej inerncji (co najmniej 60%), kryterium osypiska czy kryterium liczby cech, lecz najczęściej wybiera się 2 wymiary aby móc profile przedstawić w dwuwymiarowej mapie percepcji.

Wynik analizy korespondencji przedstawia się jako współrzędne kategorii w przestrzeni czynnikowej, przyjmując
$$
F = D_r^{-1/2} U \Sigma,
$$
oraz
$$
G = D_c^{-1/2} V \Sigma.
$$
Wiersz $i$ jest więc reprezentowany przez wektor współrzędnych $f_i = (f_{i1},\dots,f_{iK})$ – $i$-ty wiersz macierzy $F$, kolumna $j$ przez wektor $g_j = (g_{j1},\dots,g_{jK})$ – $j$-ty wiersz macierzy $G$. W takim ujęciu odległości euklidesowe między punktami odpowiadają odległościom chi-kwadrat między profilami, przy czym:

– profil wiersza $i$ to wektor warunkowych częstości
$$
(p_{i1}/r_i,\dots,p_{iJ}/r_i),
$$
– profil kolumny $j$ to wektor
$$
(p_{1j}/c_j,\dots,p_{Ij}/c_j).
$$

Analiza korespondencji może być także interpretowana jako rodzaj ważonej analizy głównych składowych zastosowanej do zbioru profilów wierszy (lub kolumn) z metryką chi-kwadrat.

Istotną własnością modelu jest tzw. własność barycentryczna. Współrzędne wiersza są średnią ważoną współrzędnych kolumn, z wagami równymi warunkowym prawdopodobieństwom kategorii kolumnowych w danym wierszu, i odwrotnie. Dokładniej:
$$
f_i = \frac{1}{r_i} \sum_{j=1}^J p_{ij}\, D_c^{-1}\, g_j
$$
oraz analogicznie dla kolumn. Intuicyjnie, punkt reprezentujący kategorię wiersza znajduje się w geometrycznym środku (barycentrum) punktów kolumnowych, ważonym strukturą profilu tego wiersza. To zapewnia spójność geometryczną i ułatwia interpretację wykresów.

## Miary jakości odtworzenia przestrzeni czynnikowej

Miary jakości odtworzenia przestrzeni czynnikowej opisują, jak dobrze poszczególne kategorie oraz poszczególne wymiary (osie) oddają strukturę zależności zakodowaną w tabeli kontyngencji. Obejmują trzy grupy wskaźników: korelację punktu z osią (tzw. $\cos^2$), udział punktu w wymiarze oraz udział wymiaru w inercji. Poniżej znajduje się opis każdej z miar, ich interpretacja oraz wzory.

### Korelacja punktu z osią ($\cos^2$)

Korelacja punktu z daną osią mierzy, jaka część inercji danego punktu (wiersza lub kolumny) jest odwzorowana w przestrzeni czynnikowej wzdłuż tej osi. Stanowi to odpowiednik „jakości reprezentacji” punktu. Dla punktu wiersza $i$ w wymiarze $k$ definiujemy:
$$
\cos^2(i,k) = \frac{f_{ik}^2}{\sum_{l=1}^{K} f_{il}^2},
$$
gdzie $f_{ik}$ jest współrzędną punktu $i$ na osi $k$. Analogicznie dla kolumn:
$$
\cos^2(j,k) = \frac{g_{jk}^2}{\sum_{l=1}^{K} g_{jl}^2}.
$$
Interpretacja polega na ocenie, czy punkt leży „blisko osi” i czy dane wymiary dobrze go opisują. Wartości bliskie 1 oznaczają dobrą jakość odwzorowania, wartości małe — że punkt jest słabo reprezentowany w danym wymiarze i jego położenia nie należy nadmiernie interpretować.

### Udział punktu w tworzeniu wymiaru (ang. *contribution*)

Udział punktu w tworzeniu danego wymiaru mierzy, jak duży wkład dana kategoria wnosi do inercji na danej osi. W przeciwieństwie do $\cos^2$, które mierzą jakość reprezentacji punktu, *contribution* mierzy „ważność” punktu dla danej osi. Dla wierszy:
$$
\mathrm{ctr}(i,k) = \frac{r_i f_{ik}^2}{\lambda_k},
$$
gdzie $r_i$ jest masą wiersza, a $\lambda_k$ wartością własną odpowiadającą osi $k$. Dla kolumn:
$$
\mathrm{ctr}(j,k) = \frac{c_j g_{jk}^2}{\lambda_k}.
$$
Interpretujemy to jako względną część wariancji danego wymiaru „pochodzącą” od danego punktu. Punkty o dużych wartościach *contribution* dominują dany wymiar i wskazują, które kategorie determinują kierunek zależności uchwycony przez oś.

### Udział wymiaru w inercji całkowitej

Udział wymiaru w inercji określa, jaka część całkowitej zmienności reszt z modelu niezależności jest odtwarzana przez daną oś. Wynika bezpośrednio z wartości własnych:
$$
\text{Udział osi } k = \frac{\lambda_k}{\sum_{l=1}^{K} \lambda_l}.
$$
Wartości własne $\lambda_k$ odzwierciedlają siłę zróżnicowania uchwyconą w danym wymiarze. Im większa wartość, tym większy wpływ danej osi na odwzorowanie zależności między kategoriami.


## Interpretacja elementów składowych modelu

Poszczególne elementy modelu mają następujące znaczenie interpretacyjne:

– macierz $P$ - rzeczywista struktura częstości wspólnych badanych kategorii;
– wektory $r$ i $c$ - „wielkość” kategorii (częstości brzegowe), określające masy punktów wierszy i kolumn;
– macierz $P - r c^\top$ - odchylenia od modelu niezależności;
– macierz znormalizowanych reszt $S$ - odchylenia wyrażone w jednostkach odpowiadających testowi chi-kwadrat, co umożliwia porównywanie kategorii o różnych masach;
– wartości własne $\lambda_k$ - wyjaśniana inercja wzdłuż kolejnych osi, interpretowana jako siła zróżnicowania struktury zależności w tym wymiarze;
– wektory własne (kolumny $U$ i $V$) - kierunki głównych inercji odpowiednio w przestrzeni wierszy i kolumn;
– współrzędne $F$ i $G$ - położenie kategorii w przestrzeni czynnikowej, wykorzystywane do wizualizacji; ich rozrzut wokół środka odzwierciedla strukturę zależności między kategoriami.

Na mapie czynnikowej kategorie wierszy i kolumn, które leżą blisko siebie, interpretuje się jako powiązane: wiersze „preferują” te kolumny (mają ponadprzeciętne częstości względem modelu niezależności), a kategorie oddalone od siebie są rzadko łączone. Odległości między punktami wierszy są interpretowane jako odległości chi-kwadrat między profilami wierszy, a analogicznie dla kolumn.

W praktyce interpretacji najczęściej ogranicza się do dwóch lub trzech pierwszych wymiarów (o największych wartościach własnych) i bada położenie kategorii względem osi oraz względem siebie nawzajem. Dodatkowo można analizować wkłady kategorii do inercji poszczególnych wymiarów oraz cosinusy kwadratów kątów (miary jakości reprezentacji), ale są to już bardziej szczegółowe wskaźniki opierające się na opisanej wyżej strukturze własnych wektorów i wartości.

:::{#exm-1}
```{r}
library(FactoMineR)
library(factoextra)
HairEyeColor

# wybieramy tylko kolory oczu i włosów bez podziału na płeć
tab <- margin.table(HairEyeColor, margin = c(1, 2))
tab

# Analiza korespondencji
res.ca <- CA(tab, graph = FALSE)

# Wyniki modelu
summary(res.ca)

# Podstawowe podsumowanie wyników
res.ca$eig          # wartości własne i udział inercji
res.ca$row$coord    # współrzędne wierszy
res.ca$col$coord    # współrzędne kolumn
```

- Na początku można stwierdzić, że statystyka chi-kwadrat jest bardzo wysoka, a wartość p-value praktycznie równa zeru. Oznacza to zdecydowane odrzucenie modelu niezależności, czyli istnienie silnego związku pomiędzy kolorem włosów a kolorem oczu. Związek ten nie jest przypadkową fluktuacją liczebności, lecz strukturalnym odchyleniem od przeciętnych profili.
- Wartości własne pokazują, że pierwszy wymiar wyjaśnia około dziewięćdziesiąt procent całej inercji. Drugi wymiar wnosi niespełna dziesięć procent, a trzeci ma znaczenie znikome. W praktyce interpretacja zależności między kategoriami ograniczać się będzie do dwóch pierwszych wymiarów, ponieważ to one odzwierciedlają niemal całą strukturę odchyleń od modelu niezależności. Silna dominacja wymiaru pierwszego wskazuje, że główna oś różnicowania tabeli jest jednoznaczna i przedstawia podstawowy kierunek współwystępowania kolorów włosów i oczu.
- W kategoriach wierszy największą inercją charakteryzuje się „Blond”, która tworzy wyraźnie silny sygnał odróżniający ją od pozostałych kategorii. Wysoka wartość $\cos^2$ na wymiarze pierwszym oznacza, że jej położenie na mapie jest niemal w pełni opisane przez ten jeden wymiar, a zatem interpretacja tej kategorii powinna opierać się przede wszystkim na pierwszej osi. Podobnie kategorię „Black” również dobrze opisuje wymiar pierwszy, choć jej pozycja częściowo opiera się również na wymiarze drugim. Kategoria „Red” natomiast jest silnie związana z wymiarem drugim: jej $\cos^2$ w osi drugiej dominuje nad pozostałymi współrzędnymi, co sugeruje, że ta kategoria wnosi zróżnicowanie względnie prostopadłe do podstawowej osi.
- W zakresie wkładów (*contributions*) wierszy wymiar pierwszy jest w głównej mierze kształtowany przez kategorię „Blond”, która odpowiada za zdecydowaną większość inercji tej osi. „Black” daje istotny wkład, natomiast kategorie „Red” i „Brown” mają względnie niewielki wpływ. To wskazuje, że wymiar ten opisuje przeciwstawność dwóch skrajnych charakterystyk: jasnych i bardzo ciemnych kolorów włosów. Natomiast wymiar drugi jest definiowany przede wszystkim przez kategorię „Red”, co potwierdza jej szczególne znaczenie w drugorzędnym kierunku zależności.
- W odniesieniu do kolumn widoczna jest podobna struktura. Wymiar pierwszy jest silnie ukształtowany przez „Brown” i „Blue”, które leżą na przeciwnych stronach osi, a ich $\cos^2$ oraz wysoki wkład w ten wymiar wskazują, że to te kategorie określają jego znaczenie. Wymiar drugi zdominowany jest przez „Green” i „Hazel”, co sugeruje, że te dwa kolory oczu odpowiadają za dodatkowe zróżnicowanie, nieuchwycone przez wymiar pierwszy.

```{r}
# Mapa percepcji: wiersze i kolumny na jednym wykresie
fviz_ca_biplot(
  res.ca,
  repel = TRUE,
  title = "Mapa percepcji: kolor włosów vs kolor oczu"
)
```

Mapa percepcji przedstawia zależności między kategoriami koloru włosów (punkty niebieskie) i koloru oczu (punkty czerwone) w przestrzeni dwóch wymiarów analizy korespondencji. Oś pierwsza odpowiada za zdecydowaną większość inercji, dlatego to ona wyznacza główny kierunek różnicowania kategorii. Na osi poziomej widoczne jest wyraźne zagęszczenie między kategoriami „Blond” i „Blue” po stronie ujemnej oraz kategoriami „Black” i „Brown eyes” po stronie dodatniej. Interpretuejmy to jako dominujące współwystępowanie jasnych włosów z niebieskimi oczami oraz ciemnych włosów z brązowymi oczami. Oś ta odwzorowuje więc gradient od jasnych do ciemnych fenotypów. Oś pionowa, wyjaśniająca znacznie mniejszą część inercji, wprowadza drugi — subtelniejszy — wymiar struktury. W górnej części wykresu znajdują się „Red” oraz „Green”, co sugeruje, że te rzadziej występujące cechy współwystępują częściej niż wynikałoby to z modelu niezależności. Z kolei „Hazel” pozycjonuje się w dodatniej części osi pierwszej i jednocześnie nieco dodatniej części osi drugiej, co wskazuje, że jej profil jest nieznacznie zbliżony do ciemniejszych kolorów włosów, choć z pewnym odchyleniem kierunkowym widocznym na drugiej osi. Punkty „Brown hair” oraz „Brown eyes” znajdują się bliżej środka niż pozostałe kategorie. Interpretujemy to jako cechy bardziej „przeciętne”, których profile nie odbiegają silnie od profili przeciętnych. 

Mapa ta potwierdza klasyczną strukturę zależności fenotypowych: jasne włosy częściej współwystępują z jasnymi oczami, ciemne włosy z ciemnymi oczami, a odmienne kombinacje są rzadsze. Drugi wymiar wyodrębnia rzadkie kombinacje związane z czerwonymi włosami oraz zielonymi oczami, tworząc charakterystyczny kierunek dodatkowy. Taka konfiguracja jest typowa dla danych fenotypowych, w których dominują jedna lub dwie silne osie zróżnicowania, a dodatkowe kierunki odzwierciedlają rzadsze, ale strukturalnie spójne wzorce współwystępowania.
:::
